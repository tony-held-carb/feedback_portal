
To determine the SQLAlchemy types and Python types from a model using inspect, you can use the sqlalchemy.inspection.inspect function. This function helps you inspect the model and retrieve the columns, along with their types.

Here's a step-by-step guide:

Example SQLAlchemy Model:
python
Copy code
from sqlalchemy import Column, Integer, String, create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

Base = declarative_base()

class User(Base):
    __tablename__ = 'users'
    id = Column(Integer, primary_key=True)
    name = Column(String)
Inspecting the Model:
You can use inspect to get information about the table's columns and types.

python
Copy code
from sqlalchemy.inspection import inspect

def get_column_info(model):
    # Get the table inspector for the model
    inspector = inspect(model)
    
    columns_info = {}
    for column in inspector.columns:
        col_name = column.name
        sqlalchemy_type = column.type
        python_type = column.type.python_type
        
        columns_info[col_name] = {
            'sqlalchemy_type': sqlalchemy_type,
            'python_type': python_type
        }
    
    return columns_info

# Example usage
column_info = get_column_info(User)
for col_name, col_types in column_info.items():
    print(f"Column: {col_name}, SQLAlchemy Type: {col_types['sqlalchemy_type']}, Python Type: {col_types['python_type']}")


# suppress warnings about data validation - Excel's extensions aren't fully compatible with openpyxl but these modified templates should work
# https://www.reddit.com/r/learnpython/comments/mfy9qa/openpyxl_deleting_data_validation/
# with warnings.catch_warnings():
#     warnings.filterwarnings("ignore", category=UserWarning)

Tables associated with incidences

emission_identified_flag
emission_type
incidence_qa_flag
inspection_flag
mitigation_type
sources

# this is a multiple foreign key association table (do this last)
plume_incidence_mapping



when we close, set the incidence end timestamp


python define a class in a function to avoid circular imports

def create_class():
    class MyClass:
        def __init__(self, value):
            self.value = value

        def do_something(self):
            print(f"Doing something with {self.value}")

    return MyClass

# Use the function to create the class
MyClass = create_class()

# Create an instance of the class
obj = MyClass(10)
obj.do_something()


Naming Conventions
---------------------
* pass an sql model as model_rows and iterate through it with for model_row in model_rows
* Use model_row if you have a single row from a query
* don't use form as a variable in html, it is confusing.  if you want to pass a WTForm, use the variable wtf_form

Datetime string conventions
---------------------------
- When you create an SQLAlchemy model, you can use a dict to represent the json column
- That dict can contain any python types, but if you try to write the json to the database, the type must serialize as a string
- datetimes coming into and out of the postgres database should follow the format "2024-09-03 16:55:00.000000"
      - logger.debug(f"Attempting to cast datetime to string")
      - value = datetime.datetime.strftime(value, "%Y-%m-%d %H:%M:%S.%f")
      - if isinstance(obj, datetime):
          return obj.strftime("%Y-%m-%d %H:%M:%S.%f")
          value = datetime.datetime.strftime(value, "%Y-%m-%d %H:%M:%S.%f")

Date-time issues
---------------------------
- if you set the raw_data for a date field you can get unexpected results!

- turned off editing the raw data
---------------------------------
- notes on an incidence created through the web-interface
    - seems to initialize and show dates properly
    - no validation data are presented (that may be by design, but it could be nice if it validated)
    - when you validate, the dates don't disappear and things sorta seem to work as desired
    - the creation process creates new instances each time you validate
- notes on GETing an incidence that already exists
    - date stamps are not in the right format and are deleted when you post
    - get does not show the validation errors (likely because i turned off raw data)
- notes on POSTing an incidence that already exists

- turned on editing the raw data
---------------------------------
- notes on an incidence created through the web-interface
    - GETing seems like things are populated properly, but there is no validation
    - When you post things seem to work as expected and get saved properly
- when you try and load, you get errors, likely because the json field has a string rather than a datetime object

currently datetimes are converted to at model_to_wtform to ensure they are datetime objects.
This works, but it means that datetimes in models may be strings or actual datetime objects depending on
if they were created as a python object or deserialized from the database or json file

secondary and tertiary causes are not mapping to their selects properly.
I think it is because the drop-down does not have the same name as the element for the emission_cause and dropdown
+2024-11-15 14:29:32.350 | DEBUG    | portal_03.wtf_util | wtf_util.py | 579 | Initializing emission_cause.  field.data=-1
+2024-11-15 14:29:32.350 | DEBUG    | portal_03.wtf_util | wtf_util.py | 579 | Initializing emission_cause_secondary.  field.data='Offline Gas Collection Well(s)'
+2024-11-15 14:29:32.350 | DEBUG    | portal_03.wtf_util | wtf_util.py | 579 | Initializing emission_cause_tertiary.  field.data='Construction - New Well Installation'

* trying to figure out the repeat incidence on validation for new incidences (fixed!)
    - testing creation through dummy data
        - on first get page, the database is not updated
        - if you validate, it creates an incidence
        - if you validate again, it creates another
    - options are to try and fix the existing structure (try first)
      or to get rid of the concept of creation pages (would get confusing on incidence id's being set automatically)


* It looks like validation is done on raw_data not the data, so if you try to run validation with GET
you get all sorts of errors because you are not setting the raw_data (which must be a string)

Monday fixes 11/19/24
  - change database location to dan's new db
    - old: satellite_tracker_sample2
    - new: satellite_tracker_demo1
  - change table with incidence info in column 'misc_json'
    - old: incidences_with_json
    - new: incidences
  - change table references to incidences not incidences_with_json
  - fix source id = none on the index page

Sunday prep priorities

* 11/17/24, deploying portal for dry-run purposes, clone will be on main branch that should be stable/working


*. double check o&g and landfill model validation (second pass)

Going to see if remote development is faster than developing off my laptop ...
initially looks like the remote is quite a lot faster at startup, but i don't think
the debugger is on, so i need to set some ec2 variables to make this a fair fight.
  * ec2 flask run --host=0.0.0.0 -p 2113 --debug --no-reload
    * 0.5 seconds
        start   +2024-11-22 19:27:48.479
        finish  +2024-11-22 19:27:49.030
  * laptop flask run --host=0.0.0.0 -p 2113 --debug --no-reload
    * 13 seconds
       start    +2024-11-22 11:29:51.591
       finish   +2024-11-22 11:30:04.193
  * laptop flask run -p 2113 --debug --no-reload
    * 13 seconds
      * start     +2024-11-22 11:55:10.854
      * finish    +2024-11-22 11:55:23.184

flask_logger.log replaced with portal_logger.log  changed to operator_portal.log that is not tracked by git

* db has entries for landfills (plural) should be changed to landfill for consistency
    (note that the names for sectors have changed, so this may be moot)

finding startup times using remote development
    * laptop
        flask run -p 2113 --debug
            try 1 (25 seconds)
                start  +2024-11-25 09:36:50.960
                finish +2024-11-25 09:37:15.633
            try 2 (21 seconds)
                start  +2024-11-25 09:38:14.310
                finish +2024-11-25 09:38:35.417
            try 3 (23 seconds)
                start  +2024-11-25 10:09:38.432
                finish +2024-11-25 10:10:01.867
            source code change 1 (12 seconds)
                start  +2024-11-25 10:12:30.690 | INFO     | werkzeug | _internal.py | 97 |  * Detected change in 'C:\\Users\\theld\\OneDrive - California Air Resources Board\\code\\pycharm\\plume_portal\\portal_03\\app.py', reloading
                finish +2024-11-25 10:12:42.601 | INFO     | werkzeug | _internal.py | 97 |  * Debugger PIN: 834-679-507
            source code change 2 (12 seconds)
                start  +2024-11-25 10:13:55.408 | INFO     | werkzeug | _internal.py | 97 |  * Detected change in 'C:\\Users\\theld\\OneDrive - California Air Resources Board\\code\\pycharm\\plume_portal\\portal_03\\app.py', reloading
                finish +2024-11-25 10:14:07.832 | INFO     | werkzeug | _internal.py | 97 |  * Debugger PIN: 834-679-507
            source code change 3 (12 seconds)
                start  +2024-11-25 10:15:00.613 | INFO     | werkzeug | _internal.py | 97 |  * Detected change in 'C:\\Users\\theld\\OneDrive - California Air Resources Board\\code\\pycharm\\plume_portal\\portal_03\\app.py', reloading
                finish +2024-11-25 10:15:12.311 | INFO     | werkzeug | _internal.py | 97 |  * Debugger PIN: 834-679-507

        flask run -p 2113 --debug --no-reload
            try 1 (10 seconds)
                start  +2024-11-25 09:39:33.235
                finish +2024-11-25 09:39:43.203
            try 2 (10 seconds)
                start  +2024-11-25 09:40:49.854
                finish +2024-11-25 09:40:59.408

    * ec2
        flask run -p 2113 --debug
            try 1 (3 seconds)
                start  +2024-11-25 17:52:31.838
                finish +2024-11-25 17:52:34.660
            try 2 (3 seconds)
                start  +2024-11-25 17:55:28.976
                finish +2024-11-25 17:55:31.997
            try 3 (2 seconds)
                start  +2024-11-25 17:59:21.290
                finish +2024-11-25 17:59:23.102
            source code change 1 (3 seconds)
                +2024-11-25 18:03:11.720 | INFO     | werkzeug | _internal.py | 97 |  * Detected change in '/home/theld/code/pycharm/plume_portal/portal_03/app.py', reloading
                +2024-11-25 18:03:12.293 | INFO     | werkzeug | _internal.py | 97 |  * Restarting with stat
                +2024-11-25 18:03:14.103 | INFO     | portal_03.config | config.py | 98 | Logger Initialized
                +2024-11-25 18:03:14.735 | INFO     | werkzeug | _internal.py | 97 |  * Debugger PIN: 113-184-331
            source code change 2 (4.5 seconds)
                +2024-11-25 18:04:54.226 | INFO     | werkzeug | _internal.py | 97 |  * Detected change in '/home/theld/code/pycharm/plume_portal/portal_03/app.py', reloading
                +2024-11-25 18:04:59.162 | INFO     | werkzeug | _internal.py | 97 |  * Debugger PIN: 113-184-331
            source code change 3 (3.5 seconds)
                +2024-11-25 18:06:12.552 | INFO     | werkzeug | _internal.py | 97 |  * Detected change in '/home/theld/code/pycharm/plume_portal/portal_03/app.py', reloading
                +2024-11-25 18:06:16.011 | INFO     | werkzeug | _internal.py | 97 |  * Debugger PIN: 113-184-331
        flask run -p 2113 --debug --no-reload
            try 1 (0.5 seconds)
                start  +2024-11-25 17:56:13.931
                finish +2024-11-25 17:56:14.455
            try 2 (0.5 seconds)
                start  +2024-11-25 17:57:01.104
                finish +2024-11-25 17:57:01.636

* To delete, then recreate the database (only if using sqlite):
    * Close flask app
    * disconnect pycharm from database
    * run flask run
    * navigate to  http://127.0.0.1:5000/run_sql_script
    * navigate to  http://127.0.0.1:5000/add_form_dummy_data

* Changing first name, second name, to just name
contact_first_name -> contact_name
contact_last_name -> deleted
First Name -> Contact Name
First Name

wtf_forms.py references
class LandfillFeedback(FlaskForm)
  contact_first_name = StringField(label='Contact First Name:', validators=[InputRequired()])
  contact_last_name = StringField(label='Contact Last Name:', validators=[InputRequired()])

class OGFeedback(FlaskForm):
  contact_first_name = StringField(label='Contact First Name:', validators=[InputRequired()])
  contact_last_name = StringField(label='Contact Last Name:', validators=[InputRequired()])

-------------------------------------------------------------------
Folder structure:
-------------------------------------------------------------------

/root/
  ├── branch_01/
       ├── sub_branch 01/
       └── sub_branch 02/
            └── sub_branch 03/
  ├── branch 02/
  └── branch 03/

-------------------------------------------------------------------
If you're working in a Python project, a good structure might be:
-------------------------------------------------------------------
/tests/                  # test code itself
/test_results/           # raw output from test runs
/reports/                # human-readable reports (e.g., pytest-cov, HTML)
/diagnostics/            # runtime logs, stack traces, system info dumps

-------------------------------------------------------------------
Getting ready to create a new feedback_portal pycharm repo because
it has becoming weird/unstable since i updated pycharm

how to run from command line:
(mini_conda_01) C:\one_drive\code\pycharm\feedback_portal\source\production\arb>flask --app wsgi run

folders marked as untracked:
    archive
    logs
    portal_uploads
    source/gpt_recommendations
    source/production/arb/logs
    source/tutorials

folders i have as source root
-------------------------------------------------------------------
source/production

folders i have marked as template - may need to set jinja as flask env
-------------------------------------------------------------------
source/production/arb/portal/templates

pycharm settings
-------------------------------------------------------------------
templating language: jinja2

getting git to work correctly
-------------------------------------------------------------------
go to settings version control directory mappings
remove any directories or projects, then add
C:\one_drive\code\pycharm\feedback_portal>

the shortcuts for one_drive are likely the source of the problem

git tag -a v1.0.0 -m "Debug version 1.0.0. Stable before GPT refactor."
git tag -a v1.1.0 -m "Debug version 1.1.0. Stable after GPT refactor."
git tag -d v1.1.0


Possible starting point for best practices for archiving/versioning:

1. Include a RELEASE_NOTES.md file at the root of your file structure.  Example content of this file:

```
## v1.0.0 - 2025-04-28
- Feedback portal first stable release using ISD/ED approached spreadsheet feedback forms
- Current versions of feedback forms
  - energy_operator_feedback_v002.xlsx (Schema: energy_v00_01.json)
  - landfill_operator_feedback_v070.xlsx (Schema: landfill_v01_00.json)
  - oil_and_gas_operator_feedback_v070.xlsx (Schema: oil_and_gas_v01_00.json)
```

2. There are two options for including a version file at the root of your code, one is to have an __init__.py file with the line:
__version__ = "1.0.0".
The other option is to create a file named VERSION that is a plain text file with only one line of non-comments that includes the version number of your code.  For example "1.0.0"

3. Version Tagging in Git
Example git tag -a v1.0.0 -m "Stable release v1.0.0 - ready for archive"
- Benefits include:
  - Built-in to Git
  - Easy to find later
  - Common practice (PyPI, GitHub, etc.)

4. Create an archive (.zip or .tar.gz)

- After tagging, you can create a snapshot from the command line.  For example:
  - git archive --format=zip --output=feedback_portal_v1.0.0.zip v1.0.0
- Benefits include:
  - Can store it offline, S3, external drive, etc.

chatgpt request template
----------------------------
I would like ***

1) app.py current has all the routes, please create a file named routes.py that has all routes associated with this project
2) in the routes file, use a blueprinting system where all the routes will use a newly created 'main' route
3) app.py should create a code factory app that is called in a separate wsgi file to make the app more portable.

Please refactor the following python code using the following style preferences:
1) use google formatting docstrings
2) use modern type hinting in the docstrings (not in the function definition).
3) docstring type hinting should be in the format:  variable_name (type_hint): variable description.
4) Include extensive documentation and examples.
5) Review all of the code and provide all suggestions in one response, not incrementally prompting me for additional information.
6) Include the full text for any file you create or propose to change.
7) Retain, all previous documentation, notes, and todos where possible
8) do not import typing, rather use modern type hinting such as the | symbol
9) do not omit any code for brevity, i'm going to copy paste your code into a new file.

Please create a new function named run_diagnostics that includes testing of key features of this file.


### Notes on naming systems:
* todo (consider) is used in python comments to indicate optional todo items
* todo (future) a potential item to address in the future

* naming convention
  * model: an SQLAlchemy model instance
  * wtf_form: a wtform instance

I have a git repo that has a root directory of /home/theld/code/git_repos/feedback_portal
In the directory /home/theld/code/git_repos/feedback_portal/shell_scripts I have two shell scripts named launch_with_screen.sh and stop_with_screen.sh
when i pull the scripts for the first time, i have to chmod +x to make them executable.  When I try to pull in the latest
branch, i get errors saying that i have to commit these files.

what is your recommended strategy to automate the chmod and allow for the pulls?

Attached is my source code for a flask app that stores/updates user feedback.

Please check the code related to how data are stored and retrieved from the database to look for ways that
it may become corrupted.  also check how data is converted from sqlalchemy to wtforms and back to see if there
are any potential errors.  perform any other tests or make any other suggestions on how to ensure that
the data are properly being stored, retrieved, and updated.


Oil and Gas Import Issues that I had to fix to get the drop-downs to import properly:
The following fields did not import properly (both are drop-downs)
Q10 was Unintentional-leak in spreadsheet, but did not seem to import (it is Please Select)
Q13 was Unintentional-leak in spreadsheet, but did not seem to import (it is Please Select)

Related warning on the import
+2025-05-14 17:35:59.694 | WARNING  | app_logger       | user:anonymous | 222   | xl_parse.py          | Warning: <ogi_result> value at <$D$56> is <Unintentional-leak> and is of type <<class 'str'>> whereas it should be of type <<class 'float'>>.  Attempting to convert the value to the correct type
+2025-05-14 17:35:59.694 | WARNING  | app_logger       | user:anonymous | 236   | xl_parse.py          | Type conversion failed, resetting value to None
+2025-05-14 17:35:59.694 | WARNING  | app_logger       | user:anonymous | 222   | xl_parse.py          | Warning: <method21_result> value at <$D$59> is <Unintentional-leak> and is of type <<class 'str'>> whereas it should be of type <<class 'float'>>.  Attempting to convert the value to the correct type
+2025-05-14 17:35:59.694 | WARNING  | app_logger       | user:anonymous | 236   | xl_parse.py          | Type conversion failed, resetting value to None

fixed those, but I still have these: does not seem to be a big problem to go from int to float ... looks like int is decide by excel
+2025-05-14 17:49:52.832 | WARNING  | app_logger       | user:anonymous | 222   | xl_parse.py          | Warning: <lat_carb> value at <$D$35> is <32> and is of type <<class 'int'>> whereas it should be of type <<class 'float'>>.  Attempting to convert the value to the correct type
+2025-05-14 17:49:52.832 | INFO     | app_logger       | user:anonymous | 234   | xl_parse.py          | Type conversion successful.  value is now <32.0> with type: <<class 'float'>>
+2025-05-14 17:49:52.833 | WARNING  | app_logger       | user:anonymous | 222   | xl_parse.py          | Warning: <long_carb> value at <$D$36> is <33> and is of type <<class 'int'>> whereas it should be of type <<class 'float'>>.  Attempting to convert the value to the correct type
+2025-05-14 17:49:52.833 | INFO     | app_logger       | user:anonymous | 234   | xl_parse.py          | Type conversion successful.  value is now <33.0> with type: <<class 'float'>>



figure out why some are being coerced and others are not, come up with a general way to reset to 'Please Select' if
the input data are garbage?  It could be that this is not a reasonable test case as it is unlikely total garbage
would be allowed to be entered into the selector?  Maybe fix the other validations first?

It could turn out to be easy, just check and see if a value is in the choice list and make it None if it is not.
May need logic on how to cascade down from the first to the last question, could already sorta be there and you
just need to validate selectors in the order of how contingent selectors are updated.

I made a fix, just gotta test it and make sure I did not break anything

getting the git history from command line with date information:
TZ=America/Los_Angeles git log --since="21 days ago" --pretty=format:"%ad %an %s" --date=format-local:"%a %Y-%m-%d %H:%M:%S %Z" >> git_status.txt

adding to git from linux
git add git_status.txt && git commit -m "adding status" && git push

-------------------------------------------------------------------------------
Looking for ways that 'Please Select' could be written to the database.
Will likely update the logging of portal changes and portal_update_table changes
-------------------------------------------------------------------------------
Routines that can affect the database have "session." in them
    - apply_json_patch_and_log
        - called by
            - update_model_with_payload
            - wtform_to_model
    - update_model_with_payload
        - called by
            - dict_to_database
    - dict_to_database
        - called by
            - xl_dict_to_database
            - og_incidence_create
            - landfill_incidence_create
    - wtform_to_model
        - called by
            - incidence_prep
    - add_commit_and_log_model
        - called by
            - incidence_prep
        - this adds the whole row (not just the misc_json)
        - likely outdated and just for logging

    Notes
        dict_to_database seems to only update the json column (may create primary key)
        add_commit_and_log_model seems to

        - updates the portal change table, seems like that should always be the mechanism to update the table
      even in deletes

     - ok seems like all changes to the json should be made via dict to database so that uniform
       logging will happen
     - when a row is deleted, consider doing a dict to database with an empty dictionary or some
       other way to indiciate deletion
     - have the search portal deal with missing incidences (if they are deleted)
     - have links to be missing tracker be dealt with?

     1st, figure out where direct injection of dummy data happens
        dummy uses dict_to_database

created:
{"id_plume": 1001, "observation_timestamp": "2025-06-03T00:43:00+00:00", "lat_carb": 100.05, "long_carb": 100.06, "id_message": "id_message response", "facility_name": "facility_name response", "id_arb_eggrt": "1001", "contact_name": "contact_name response", "contact_phone": "(555) 555-5555", "contact_email": "my_email@email.com", "venting_description_1": "venting_description_1 response", "ogi_date": "2025-06-03T00:43:00+00:00", "method21_date": "2025-06-03T00:43:00+00:00", "initial_leak_concentration": 1004, "venting_description_2": "venting_description_2 response", "initial_mitigation_plan": "initial_mitigation_plan response", "equipment_other_description": "equipment_other_description response", "component_other_description": "component_other_description response", "repair_timestamp": "2025-06-03T00:43:00+00:00", "final_repair_concentration": 101.05, "repair_description": "repair_description response", "additional_notes": "additional_notes response", "sector": "Oil & Gas", "sector_type": "Oil & Gas"}

looks like timestamps are loosing their +00:00
{"id_plume": 1001, "observation_timestamp": "2025-06-02T17:43:00"      , "lat_carb": 100.05, "long_carb": 100.06, "id_message": "id_message response", "facility_name": "facility_name response", "id_arb_eggrt": "1001", "contact_name": "contact_name response", "contact_phone": "(555) 555-5555", "contact_email": "my_email@email.com", "venting_description_1": "venting_description_1 response", "ogi_date": "2025-06-02T17:43:00"      , "method21_date": "2025-06-02T17:43:00"      , "initial_leak_concentration": 1004, "venting_description_2": "venting_description_2 response", "initial_mitigation_plan": "initial_mitigation_plan response", "equipment_other_description": "equipment_other_description response", "component_other_description": "component_other_description response", "repair_timestamp": "2025-06-02T17:43:00"      , "final_repair_concentration": 101.05, "repair_description": "repair_description response", "additional_notes": "additional_notes response", "sector": null, "sector_type": null, "carb_notes": "", "equipment_at_source": "Please Select", "ogi_result": "Please Select", "method21_performed": "Please Select", "venting_exclusion": "Please Select", "component_at_source": "Please Select", "method21_result": "Please Select", "ogi_performed": "Please Select"}


looks like there is an error on how timestamps are being saved on the webform.  Here is the log when i add a new oil and gas incidence.  after I save the form, it seems to corrupt the timestamp resulting in the following error

+2025-06-02 18:04:14.597 | INFO     | app_logger       | user:anonymous | 441   | wtf_forms_util.py    | wtform_to_model payload_changes: {'carb_notes': '', 'component_at_source': 'Please Select', 'equipment_at_source': 'Please Select', 'method21_date': '2025-06-02T18:04:00', 'method21_performed': 'Please Select', 'method21_result': 'Please Select', 'observation_timestamp': '2025-06-02T18:04:00', 'ogi_date': '2025-06-02T18:04:00', 'ogi_performed': 'Please Select', 'ogi_result': 'Please Select', 'repair_timestamp': '2025-06-02T18:04:00', 'venting_exclusion': 'Please Select'}


ValueError
ValueError: Failed to cast '2025-06-02T18:04:00' to <class 'datetime.datetime'>: Missing timezone info in ISO 8601 string.

Traceback (most recent call last)
File "C:\Users\theld\OneDrive - California Air Resources Board\code\pycharm\feedback_portal\source\production\arb\utils\json.py", line 307, in cast_model_value
dt = iso8601_to_utc_dt(value)
     ^^^^^^^^^^^^^^^^^^^^^^^^
File "C:\Users\theld\OneDrive - California Air Resources Board\code\pycharm\feedback_portal\source\production\arb\utils\date_and_time.py", line 165, in iso8601_to_utc_dt
  except (ValueError, TypeError) as e:
    raise ValueError(f"Invalid ISO 8601 datetime string: '{iso_string}'") from e

  if dt.tzinfo is None:
    if error_on_missing_tz:
      raise ValueError("Missing timezone info in ISO 8601 string.")
      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    logger.warning(f"Assuming UTC for naive ISO string: {iso_string}")
    dt = dt.replace(tzinfo=UTC_TZ)

  return dt.astimezone(UTC_TZ)

During handling of the above exception, another exception occurred:
File "C:\Users\theld\AppData\Local\miniconda3\envs\mini_conda_01\Lib\site-packages\flask\app.py", line 1498, in __call__
return self.wsgi_app(environ, start_response)
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "C:\Users\theld\AppData\Local\miniconda3\envs\mini_conda_01\Lib\site-packages\flask\app.py", line 1476, in wsgi_app
response = self.handle_exception(e)
           ^^^^^^^^^^^^^^^^^^^^^^^^
File "C:\Users\theld\AppData\Local\miniconda3\envs\mini_conda_01\Lib\site-packages\flask\app.py", line 1473, in wsgi_app
response = self.full_dispatch_request()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "C:\Users\theld\AppData\Local\miniconda3\envs\mini_conda_01\Lib\site-packages\flask\app.py", line 882, in full_dispatch_request
rv = self.handle_user_exception(e)
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "C:\Users\theld\AppData\Local\miniconda3\envs\mini_conda_01\Lib\site-packages\flask\app.py", line 880, in full_dispatch_request
rv = self.dispatch_request()
     ^^^^^^^^^^^^^^^^^^^^^^^
File "C:\Users\theld\AppData\Local\miniconda3\envs\mini_conda_01\Lib\site-packages\flask\app.py", line 865, in dispatch_request
return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "C:\Users\theld\OneDrive - California Air Resources Board\code\pycharm\feedback_portal\source\production\arb\portal\routes.py", line 83, in incidence_update
return incidence_prep(model_row,
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "C:\Users\theld\OneDrive - California Air Resources Board\code\pycharm\feedback_portal\source\production\arb\portal\app_util.py", line 478, in incidence_prep
model_to_wtform(model_row, wtf_form)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "C:\Users\theld\OneDrive - California Air Resources Board\code\pycharm\feedback_portal\source\production\arb\utils\wtf_forms_util.py", line 362, in model_to_wtform
parsed_dict = deserialize_dict(model_json_dict, type_map, convert_time_to_ca=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "C:\Users\theld\OneDrive - California Air Resources Board\code\pycharm\feedback_portal\source\production\arb\utils\json.py", line 432, in deserialize_dict
result[key] = cast_model_value(value, type_map[key], convert_time_to_ca)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File "C:\Users\theld\OneDrive - California Air Resources Board\code\pycharm\feedback_portal\source\production\arb\utils\json.py", line 314, in cast_model_value
raise ValueError(f"Failed to cast {value!r} to {value_type}: {e}")
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Failed to cast '2025-06-02T18:04:00' to <class 'datetime.datetime'>: Missing timezone info in ISO 8601 string.
The debugger caught an exception in your WSGI application. You can now look at the traceback which led to the error.
To switch between the interactive traceback and the plaintext one, you can click on the "Traceback" headline. From the text traceback you can also create a paste of it. For code execution mouse-over the frame you want to debug and click on the console icon on the right side.

You can execute arbitrary Python code in the stack frames and there are some extra helpers available for introspection:

dump() shows all variables in the frame
dump(obj) dumps all that's known about the object
Brought to you by DON'T PANIC, your friendly Werkzeug powered traceback interpreter.


creating oil & gas feedback json on first creation:
{"id_incidence": 2001, "id_plume": 1001, "observation_timestamp": "2025-06-03T18:21:00+00:00", "lat_carb": 100.05, "long_carb": 100.06, "id_message": "id_message response", "facility_name": "facility_name response", "id_arb_eggrt": "1001", "contact_name": "contact_name response", "contact_phone": "(555) 555-5555", "contact_email": "my_email@email.com", "venting_description_1": "venting_description_1 response", "ogi_date": "2025-06-03T18:21:00+00:00", "method21_date": "2025-06-03T18:21:00+00:00", "initial_leak_concentration": 1004, "venting_description_2": "venting_description_2 response", "initial_mitigation_plan": "initial_mitigation_plan response", "equipment_other_description": "equipment_other_description response", "component_other_description": "component_other_description response", "repair_timestamp": "2025-06-03T18:21:00+00:00", "final_repair_concentration": 101.05, "repair_description": "repair_description response", "additional_notes": "additional_notes response", "sector": "Oil & Gas", "sector_type": "Oil & Gas"}
{"id_incidence": 2001, "id_plume": 1001, "observation_timestamp": "2025-06-03T18:21:00+00:00", "lat_carb": 100.05, "long_carb": 100.06, "id_message": "id_message response", "facility_name": "facility_name response", "id_arb_eggrt": "1001", "contact_name": "contact_name response", "contact_phone": "(555) 555-5555", "contact_email": "my_email@email.com", "venting_description_1": "venting_description_1 response", "ogi_date": "2025-06-03T18:21:00+00:00", "method21_date": "2025-06-03T18:21:00+00:00", "initial_leak_concentration": 1004, "venting_description_2": "venting_description_2 response", "initial_mitigation_plan": "initial_mitigation_plan response", "equipment_other_description": "equipment_other_description response", "component_other_description": "component_other_description response", "repair_timestamp": "2025-06-03T18:21:00+00:00", "final_repair_concentration": 101.05, "repair_description": "repair_description response", "additional_notes": "additional_notes response", "sector": "Oil & Gas", "sector_type": "Oil & Gas", "carb_notes": "", "component_at_source": "Please Select", "equipment_at_source": "Please Select", "method21_performed": "Please Select", "method21_result": "Please Select", "ogi_performed": "Please Select", "ogi_result": "Please Select", "venting_exclusion": "Please Select"}
{"id_incidence": 2001, "id_plume": null, "observation_timestamp": "2025-06-03T18:21:00+00:00", "lat_carb": null, "long_carb": null, "id_message": "", "facility_name": "", "id_arb_eggrt": "", "contact_name": "", "contact_phone": "", "contact_email": "", "venting_description_1": "", "ogi_date": null, "method21_date": null, "initial_leak_concentration": null, "venting_description_2": "", "initial_mitigation_plan": "", "equipment_other_description": "", "component_other_description": "", "repair_timestamp": null, "final_repair_concentration": null, "repair_description": "repair_description response", "additional_notes": "", "sector": "Oil & Gas", "sector_type": "Oil & Gas", "carb_notes": "", "component_at_source": "Please Select", "equipment_at_source": "Please Select", "method21_performed": "Please Select", "method21_result": "Please Select", "ogi_performed": "Please Select", "ogi_result": "Please Select", "venting_exclusion": "Please Select"}
{"id_incidence": 2001, "id_plume": null, "observation_timestamp": null, "lat_carb": null, "long_carb": null, "id_message": "", "facility_name": "", "id_arb_eggrt": "", "contact_name": "", "contact_phone": "", "contact_email": "", "venting_description_1": "", "ogi_date": null, "method21_date": null, "initial_leak_concentration": null, "venting_description_2": "", "initial_mitigation_plan": "", "equipment_other_description": "", "component_other_description": "", "repair_timestamp": null, "final_repair_concentration": null, "repair_description": "repair_description response", "additional_notes": "", "sector": "Oil & Gas", "sector_type": "Oil & Gas", "carb_notes": "", "component_at_source": "Please Select", "equipment_at_source": "Please Select", "method21_performed": "Please Select", "method21_result": "Please Select", "ogi_performed": "Please Select", "ogi_result": "Please Select", "venting_exclusion": "Please Select"}

  # db_create is slow, consider using a fast load mechanism:
  # https://chatgpt.com/share/681eec4d-8b74-800b-9d0c-bdb08da62fd2


Attached is my code base before and after a refactor.

production_before_updating_docstrings_and_typehints.zip was my code before I updated type hinting and docstrings.

I'm concerned that I may have adjusted variables or code that will alter how the codebase works.

Can you please do a before/after analysis and determine if the codebase will introduce no errors or change in performance.



Consider correcting the docstring label from "Depreciated" → "Deprecated"



getting docs to work
cd "C:\one_drive\code\pycharm\feedback_portal\source\production"
set PYTHONPATH=%cd%
mkdocs serve

below is my source code for mkdocs.yml and index.md.

I like the documentation site, but I don't want the site name from mkdocs.yml
on the left navigation bar and is it already on the top line and seems redundant with the Index Page.

what options do we have to have the unnecessary first line/second line nested structure at the top of the left navigation pane?


mkdocs.yml contents:

site_name: CalSMP Operator Feedback Portal Documentation & Source Code
site_url: https://tony-held-carb.github.io/feedback_portal

theme:
  name: material
  features:
    - navigation.sections
    - navigation.expand        # Fully expanded left nav
    - navigation.top
    - navigation.instant
    - content.code.annotate    # Show code comments on hover

markdown_extensions:
  - admonition
  - codehilite
  - pymdownx.details
  - pymdownx.superfences
  - pymdownx.tabbed
  - pymdownx.highlight
  - pymdownx.inlinehilite
  - pymdownx.snippets
  - pymdownx.magiclink

plugins:
  - search
  - gen-files:
      scripts:
        - docs/gen_docs.py
  - literate-nav:
      nav_file: SUMMARY.md
  - mkdocstrings:
      handlers:
        python:
          options:
            show_source: true
            docstring_style: google
            show_root_heading: true
            show_signature: true
            separate_signature: true
            heading_level: 2
            merge_init_into_class: true

# No `nav:` section means index.md is the top page,
# and literate-nav + gen-files control everything else


index.md contents:
# CalSMP Operator Feedback Portal

Welcome to the backend documentation site for the CalSMP Operator Feedback Portal.

This documentation is auto-generated from Python source code and includes:

- Flask route handlers and form logic
- Utility functions and helpers
- Excel file schema generation and parsing
- SQLAlchemy integration and metadata support

Use the left sidebar to explore the available modules.

Pushing to github docs
    mkdocs build
    mkdocs gh-deploy --clean
run local server
    mkdocs serve

Setting windows variables:
Open PowerShell (not Command Prompt):
    Press Win + S, type "PowerShell", and open it.
    [System.Environment]::SetEnvironmentVariable("<variable_name>", "<variable_value>", "User")
    [System.Environment]::SetEnvironmentVariable("portal", "C:\one_drive\code\pycharm\feedback_portal", "User")

CMD change the prompt (failed)
    PyCharm → Settings → Tools → Terminal → Shell path:
        old path: cmd.exe /k "set PATH=C:\tony_apps\terraform;C:\tony_apps\AWS_CLI_V2;%PATH%"
        new path: cmd.exe /k "set PATH=C:\tony_apps\terraform;C:\tony_apps\AWS_CLI_V2;%PATH% && prompt [$N]$G"
        cmd.exe /k "set PATH=C:\tony_apps\terraform;C:\tony_apps\AWS_CLI_V2;%PATH% && prompt ($N)$G"
        cmd.exe /k "set PATH=C:\tony_apps\terraform;C:\tony_apps\AWS_CLI_V2;%PATH% && prompt $G"
        cmd.exe /k "set PATH=C:\tony_apps\terraform;C:\tony_apps\AWS_CLI_V2;%PATH% && conda activate mini_conda_01 && prompt $G"

Changing conda hooks (worked):
    echo %CONDA_PREFIX%
        C:\Users\theld\AppData\Local\miniconda3\envs\mini_conda_01
    created file:
        C:\Users\theld\AppData\Local\miniconda3\envs\mini_conda_01\etc\conda\activate.d\set_prompt.bat
    with content:
        @echo off
        prompt [$P]$_$G
    created file:
        C:\Users\theld\AppData\Local\miniconda3\envs\mini_conda_01\etc\conda\deactivate.d\reset_prompt.bat
    with content:
        @echo off
        prompt $P$G



def upload_and_update_db(db: SQLAlchemy,
                         upload_dir: str | Path,
                         request_file: FileStorage,
                         base: AutomapBase
                         ) -> tuple[str, int | None, str | None]:
  """
  Save uploaded file, parse contents, and insert or update DB rows.

  Args:
    db (SQLAlchemy): Database instance.
    upload_dir (str | Path): Directory where file will be saved.
    request_file (FileStorage): Flask `request.files[...]` object.
    base (AutomapBase): Automapped schema metadata.

  Returns:
    tuple[str, int | None, str | None]: Filename, id_incidence, sector.
  """
  logger.debug(f"upload_and_update_db() called with {request_file=}")
  id_ = None
  sector = None

  file_name = upload_single_file(upload_dir, request_file)
  add_file_to_upload_table(db, file_name, status="File Added", description=None)

  # if file is xl and can be converted to json,
  # save a json version of the file and return the filename
  json_file_name = get_json_file_name(file_name)
  if json_file_name:
    id_, sector = json_file_to_db(db, json_file_name, base)

  return file_name, id_, sector

Hi Steve and Dan,

The Portal now has a bunch of new features, perhaps most visible is a documentation/usage site that can be found here:

https://tony-held-carb.github.io/feedback_portal/

A representative screenshot is attached.

I created this using the docstrings from the source code using mkdocs and some python scripts.

The portal is now refactored and up at: http://10.93.112.44:2113/

The github page associated with the code is at: https://github.com/tony-held-carb/feedback_portal

The github page has a read.me file for how to run the portal on a laptop and/or ec2.

More show-and-tell to come :) Tony


pycharm logs appear at:

C:\Users\theld\AppData\Local\JetBrains\PyCharm2025.1\log\idea.log

turn off spell checking for next line:
# noinspection SpellCheckingInspection

To suppress a specific spelling warning in a docstring (such as for a filename or module name like sqla_models), while keeping spellchecking active for the rest of the docstring, use PyCharm’s spelling suppression comment inside the docstring.

Here’s how you can do it:

✅ Option 1: Use <!--suppress SpellCheckingInspection--> inline
In multiline docstrings, you can do this:
  """
  Example:
      <!--suppress SpellCheckingInspection-->
      import arb.portal.sqla_models as models
  """
PyCharm will suppress spellchecking for that line only, without turning it off for the whole docstring.

✅ Option 2: Inline backtick workaround (lower fidelity)
PyCharm tends to ignore spellchecking for words inside single backticks or double backticks when they're assumed to be code:
  Example:
      `import arb.portal.sqla_models as models`
This avoids the warning for sqla_models, but doesn't explicitly suppress the inspection.

✅ Option 3: Save word to dictionary (for permanent allow-list)
If sqla is a project-wide convention (e.g., all your SQLAlchemy files are sqla_x.py), you might prefer:

Right-click on the underlined sqla in the docstring.

Choose "Save to dictionary".

This will whitelist the word across the project and remove the typo warning wherever it appears.

Files for logging
--------------------
from pathlib import Path
from arb.__get_logger import get_logger

logger, pp_log = get_logger()
logger.debug(f'Loading File: "{Path(__file__).name}". Full Path: "{Path(__file__)}"')


C:\tony_apps\Sublime\Data\Packages\User
C:\tony_apps\Sublime\Data\Packages\User\Preferences.sublime-settings
// Settings in here override those in "Default/Preferences.sublime-settings",
// and are overridden in turn by syntax-specific settings.
// {
// 	"draw_white_space": "all"
// }
changed to:
{
	"ignored_packages": []
}

current review_staged.html works great and i'm going to see if i can remove sorting on the first column

6/25/25 created a personal access token for cursor
trying to add credentials
C:\Users\theld\.git-credentials

had to remove the password from this file because github blocked it and fruck out so i have to hard commit to an older state

set PYTHONPATH=D:\local\cursor\feedback_portal\source\production
python arb/utils/excel/xl_create.py

running python in cursor Terminal
cd /d "D:\local\cursor\feedback_portal\source\production"
python -m arb.utils.excel.xl_create

cd /d "D:\local\cursor\feedback_portal"

Usually, you do NOT need to set PYTHONPATH if you run from the correct directory and use -m.
If you run from a different directory, you might need to set PYTHONPATH to include the path to your top-level package (e.g., source/production).


i'm concerned that i will have other circular imports, not just when we are trying to run xl_create

Here are the files previously identified as excellent candidates for unit testing because they are likely to be composed mostly of standalone, pure functions (i.e., minimal dependencies on Flask, global state, or the database):
arb/portal/
constants.py
json_update_util.py

arb/utils/
date_and_time.py  (already tested)
json.py
misc.py
file_io.py
generate_github_file_metadata.py
time_launch.py
constants.py
io_wrappers.py


It is my understanding that we have:

implemented unit testing for:
arb/utils/
date_and_time.py  

Partially implemented unit testing for 
arb/utils/
json.py

updated the docstrings and are ready to implement unit testing for:
arb/utils/
misc.py
file_io.py
constants.py
io_wrappers.py

Here is the list of files in arb/utils with an asterisk () next to those for which we have already refactored the docstrings:
* date_and_time.py
* json.py
* misc.py
* file_io.py
* constants.py
* io_wrappers.py
wtf_forms_util.py
sql_alchemy.py
database.py
web_html.py
diagnostics.py
__init__.py
log_util.py

using a local postgres database
  export DATABASE_URI=postgresql+psycopg2://postgres:<your_password>@localhost:5432/tony_home_tracker

 checking the python path
python -c "import sys; print('\n'.join(sys.path))"

Best Value for Your Project
D:\local\cursor\feedback_portal\source\production

2. How to Set PYTHONPATH (Windows, CMD)
You can set it for your current session with:
set PYTHONPATH=D:\local\cursor\feedback_portal\source\production

Or, to make it permanent for your user:
setx PYTHONPATH "D:\local\cursor\feedback_portal\source\production"

2. How to Append/Prepend
To Prepend (recommended for your project):
set PYTHONPATH=D:\local\cursor\feedback_portal\source\production;%PYTHONPATH%

This puts your project directory first, so Python will look there before any other paths.
set PYTHONPATH=%PYTHONPATH%;D:\local\cursor\feedback_portal\source\production

How Dan creates a backup snapshot of the database
while [ 1 ]
do
  env LD_LIBRARY_PATH=/home/plumes/apps/builds/postgresql-16.1/src/interfaces/libpq /home/plumes/apps/builds/postgresql-16.1/src/bin/pg_dump/pg_dump  --schema=satellite_tracker_new -d postgres://methane:methaneCH4@prj-bus-methane-aurora-postgresql-instance-1.cdae8kkz3fpi.us-west-2.rds.amazonaws.com:5432/plumetracker | gzip | aws s3 cp - s3://carbmethane/backups/plume_tracker_db/current_satellite_tracker`date +%y%m%d%H%M`.sql.gz
  sleep 3600
done

importing the snapshot into my home postgress system
psql -U postgres -c "DROP DATABASE IF EXISTS tony_home_tracker;"
psql -U postgres -c "CREATE DATABASE tony_home_tracker;"
psql -U postgres -c "CREATE ROLE methane LOGIN;"
psql -U postgres -d tony_home_tracker -c "CREATE EXTENSION postgis;"
psql -U postgres -d tony_home_tracker -f current_satellite_tracker2507092101.sql > import_02.log 2>&1

set PYTHONPATH=D:\local\cursor\feedback_portal\source\production && 
pytest tests/arb/utils/test_wtf_forms_util.py -v > arb_utils_test_wtf_forms_util_output.txt 
2>&1

7/11/25 8pm confirmed unit testing status for files with the question:

do you agree with the status for file(s) xx-yy?  Do you think they could benefit from testing now that we have access to the database?

Files that were double checked
1-43

## Progress Table 3 - Authentication related

| # | File (arb/auth/auth_example_app)              | Enhanced Documentation | Unit Testing Status         |
|---|-----------------------------------------------|:---------------------:|:--------------------------:|
|  1| arb/auth/__init__.py                          | No                    | No                         |
|  2| arb/auth/default_settings.py                  | No                    | No                         |
|  3| arb/auth/email_util.py                        | No                    | No                         |
|  4| arb/auth/forms.py                             | No                    | No                         |
|  5| arb/auth/login_manager.py                     | No                    | No                         |
|  6| arb/auth/migrate_roles.py                     | No                    | No                         |
|  7| arb/auth/models.py                            | No                    | No                         |
|  8| arb/auth/okta_settings.py                     | No                    | No                         |
|  9| arb/auth/role_decorators.py                   | No                    | No                         |
| 10| arb/auth/role_examples.py                     | No                    | No                         |
| 11| arb/auth/routes.py                            | No                    | No                         |
| 12| arb/auth/test_auth.py                         | No                    | No                         |
| 13| arb/auth_example_app/__init__.py              | No                    | No                         |
| 14| arb/auth_example_app/app.py                   | No                    | No                         |
| 15| arb/auth_example_app/config.py                | No                    | No                         |
| 16| arb/auth_example_app/extensions.py            | No                    | No                         |
| 17| arb/auth_example_app/wsgi.py                  | No                    | No                         |

---

For this session, please name all output files in D:/local/cursor/feedback_portal/diagnostics/cursor/ using a brief, descriptive filename that summarizes the command (e.g., git_status.txt, git_branch.txt, pip_list.txt).
Do not include any timestamp or date information in the filename.
Always use the full absolute path for output redirection, and ensure the filename clearly reflects the command or output content.

git branch -a > D:/local/cursor/feedback_portal/diagnostics/cursor/git_branch.txt 2>&1
cd /d "D:/local/cursor/feedback_portal" && git status &2>1 > D:/local/cursor/feedback_portal/diagnostics/cursor/git_status.txt

Installed git bash to my home and work machines so i can use linux consistently

$ echo $HOME
/c/Users/tonyh

explorer . -> opens explore in pwd 

Most Important Dotfiles in Git Bash
| Filename              | Purpose                                                                      |
| --------------------- | ---------------------------------------------------------------------------- |
| **`~/.bashrc`**       | Main interactive shell config. Aliases, functions, env vars.                 |
| **`~/.bash_profile`** | Run on **login shells** (loads `.bashrc` manually).                          |
| **`~/.profile`**      | Generic POSIX startup file, often ignored in Bash if `.bash_profile` exists. |
| **`~/.inputrc`**      | Customizes keyboard and tab-completion behavior in Bash.                     |
| **`~/.gitconfig`**    | Git-specific config (e.g. user.name, aliases).                               |
| **`~/.ssh/config`**   | SSH settings for GitHub, remotes, keys.                                      |


/c/Users/tonyh/miniconda3/Scripts/conda.exe init bash

The logic that powers conda shell.bash hook lives inside the Python scripts of your Conda installation — particularly in:
C:\Users\tonyh\miniconda3\lib\site-packages\conda\shell\bash.py

C:\Users\tonyh\AppData\Roaming\Cursor\User\settings.json
{
    "window.commandCenter": true,
    "terminal.integrated.defaultProfile.windows": "Command Prompt",
    "workbench.colorTheme": "Daynight Darcula",
    "git.confirmSync": false,
    "git.enableSmartCommit": true
}

{
  "window.commandCenter": true,
  "terminal.integrated.defaultProfile.windows": "Git Bash",
  "terminal.integrated.profiles.windows": {
    "Git Bash": {
      "path": "C:\\Program Files\\Git\\bin\\bash.exe"
    }
  },
  "workbench.colorTheme": "Daynight Darcula",
  "git.confirmSync": false,
  "git.enableSmartCommit": true
}

my git bash is at: "C:\Program Files\Git\git-bash.exe"

path mangling was fixed by uninstalling and then reinstalling the plugins wtih shift-control-x


/c/Users/tonyh/miniconda3/Scripts/conda.exe init bash


feedback_portal$ env
ProgramFiles(x86)=C:\Program Files (x86)
CommonProgramFiles(x86)=C:\Program Files (x86)\Common Files
SHELL=/usr/bin/bash.exe
NUMBER_OF_PROCESSORS=24
FPS_BROWSER_USER_PROFILE_STRING=Default
COLORTERM=truecolor
PROCESSOR_LEVEL=6
TERM_PROGRAM_VERSION=1.2.4
CONDA_EXE=C:/Users/tonyh/miniconda3/Scripts/conda.exe
MINGW_PREFIX=C:/Program Files/Git/mingw64
DATABASE_URI=postgresql+psycopg2://postgres:methane@localhost:5432/tony_home_tracker
PKG_CONFIG_PATH=C:\Program Files\Git\mingw64\lib\pkgconfig;C:\Program Files\Git\mingw64\share\pkgconfig
USERDOMAIN_ROAMINGPROFILE=TONY_DESKTOP
HOSTNAME=tony_desktop
CUDA_PATH_V11_2=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2
PROGRAMFILES=C:\Program Files
MSYSTEM=MINGW64
PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC
JAVA_HOME=C:\java\jdk_1.8
ORIGINAL_TEMP=C:/Users/tonyh/AppData/Local/Temp
MINGW_CHOST=x86_64-w64-mingw32
OS=Windows_NT
HOMEDRIVE=C:
MSYSTEM_CARCH=x86_64
SPARK_LOCAL_HOSTNAME=localhost
USERDOMAIN=TONY_DESKTOP
PWD=/d/local/cursor/feedback_portal
USERPROFILE=C:\Users\tonyh
CONDA_PREFIX=C:\Users\tonyh\miniconda3
MANPATH=C:\Program Files\Git\mingw64\local\man;C:\Program Files\Git\mingw64\share\man;C:\Program Files\Git\usr\local\man;C:\Program Files\Git\usr\share\man;C:\Program Files\Git\usr\man;C:\Program Files\Git\share\man
VSCODE_GIT_ASKPASS_NODE=C:\Users\tonyh\AppData\Local\Programs\cursor\Cursor.exe
MINGW_PACKAGE_PREFIX=mingw-w64-x86_64
ALLUSERSPROFILE=C:\ProgramData
ORIGINAL_PATH=/mingw64/bin:/usr/bin:/c/Users/tonyh/bin:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/libnvvp:/c/WINDOWS/system32:/c/WINDOWS:/c/WINDOWS/System32/Wbem:/c/WINDOWS/System32/WindowsPowerShell/v1.0:/c/WINDOWS/System32/OpenSSH:/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/c/Program Files/Microsoft SQL Server/150/Tools/Binn:/c/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn:/c/Program Files/dotnet:/c/Program Files/Graphviz/bin:/c/Program Files/Docker/Docker/resources/bin:/c/Program Files/PuTTY:%SPARK_HOME%/bin:%HADOOP_HOME%/bin:%JAVA_HOME%/bin:/c/Program Files/nodejs:/c/Program Files/NVIDIA Corporation/Nsight Compute 2020.3.1:/cmd:/c/Users/tonyh/v_env/py_01/Scripts:/c/Users/tonyh/AppData/Local/Microsoft/WindowsApps:/c/Users/tonyh/.dotnet/tools:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin:/c/Users/tonyh/AppData/Local/Programs/Microsoft VS Code/bin:/c/Users/tonyh/AppData/Roaming/npm:/c/Users/tonyh/AppData/Local/Programs/cursor/resources/app/bin:/c/tony_apps/PostgreSQL/14/bin
CommonProgramW6432=C:\Program Files\Common Files
HOME=/c/Users/tonyh
USERNAME=tonyh
SSH_ASKPASS=C:/Program Files/Git/mingw64/bin/git-askpass.exe
LANG=en_US.UTF-8
VBOX_MSI_INSTALL_PATH=C:\Program Files\Oracle\VirtualBox\
PLINK_PROTOCOL=ssh
OneDrive=C:\Users\tonyh\OneDrive
COMSPEC=C:\WINDOWS\system32\cmd.exe
NVCUDASAMPLES11_2_ROOT=C:\ProgramData\NVIDIA Corporation\CUDA Samples\v11.2
EFC_1892_2775293581=1
CONDA_PROMPT_MODIFIER=(base)
TMPDIR=/tmp
GIT_ASKPASS=c:\Users\tonyh\AppData\Local\Programs\cursor\resources\app\extensions\git\dist\askpass.sh
APPDATA=C:\Users\tonyh\AppData\Roaming
NVCUDASAMPLES_ROOT=C:\ProgramData\NVIDIA Corporation\CUDA Samples\v11.2
SYSTEMROOT=C:\WINDOWS
LOCALAPPDATA=C:\Users\tonyh\AppData\Local
EFC_1892_2283032206=1
COMPUTERNAME=TONY_DESKTOP
INFOPATH=C:\Program Files\Git\mingw64\local\info;C:\Program Files\Git\mingw64\share\info;C:\Program Files\Git\usr\local\info;C:\Program Files\Git\usr\share\info;C:\Program Files\Git\usr\info;C:\Program Files\Git\share\info  
VSCODE_GIT_ASKPASS_EXTRA_ARGS=
PYSPARK_PYTHON=C:\Users\tonyh\v_env\py_01\Scripts\python.exe
PYTHONPATH=D:\local\cursor\feedback_portal\source\production;
TERM=xterm
HADOOP_HOME=C:\hadoop
LOGONSERVER=\\TONY_DESKTOP
PYSPARK_HOME=C:\Users\tonyh\v_env\py_01\Scripts\python.exe
ACLOCAL_PATH=C:\Program Files\Git\mingw64\share\aclocal;C:\Program Files\Git\usr\share\aclocal
CURL_CA_BUNDLE=C:\tony_apps\PostgreSQL\14\ssl\certs\ca-bundle.crt
PSModulePath=C:\Program Files\WindowsPowerShell\Modules;C:\WINDOWS\system32\WindowsPowerShell\v1.0\Modules      
NVTOOLSEXT_PATH=C:\Program Files\NVIDIA Corporation\NvToolsExt\
VSCODE_GIT_IPC_HANDLE=\\.\pipe\vscode-git-2bbd251d92-sock
CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2
CONDA_SHLVL=1
TEMP=/tmp
MSYSTEM_CHOST=x86_64-w64-mingw32
DISPLAY=needs-to-be-defined
SHLVL=3
ORIGINAL_TMP=C:/Users/tonyh/AppData/Local/Temp
PROCESSOR_REVISION=b701
DriverData=C:\Windows\System32\Drivers\DriverData
EFC_1892_1262719628=1
SPARK_HOME=C:\spark\spark-3.3.2-bin-hadoop2
PROJ_LIB=C:\tony_apps\PostgreSQL\14\share\contrib\postgis-3.4\proj
COMMONPROGRAMFILES=C:\Program Files\Common Files
CONDA_PYTHON_EXE=C:/Users/tonyh/miniconda3/python.exe
MOZ_PLUGIN_PATH=C:\Program Files (x86)\Foxit Software\Foxit PDF Reader\plugins\
LC_CTYPE=en_US.UTF-8
POSTGIS_GDAL_ENABLED_DRIVERS=GTiff PNG JPEG GIF XYZ DTED USGSDEM AAIGrid
SSL_CERT_FILE=C:\Users\tonyh\cacert.pem
EXEPATH=C:\Program Files\Git\bin
PROCESSOR_IDENTIFIER=Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
SESSIONNAME=Console
PS1=\[\]$(basename "$PWD")\$ \[\]
CONDA_DEFAULT_ENV=base
EFC_1892_1592913036=1
POSTGIS_ENABLE_OUTDB_RASTERS=1
PKG_CONFIG_SYSTEM_LIBRARY_PATH=C:/Program Files/Git/mingw64/lib
VSCODE_GIT_ASKPASS_MAIN=c:\Users\tonyh\AppData\Local\Programs\cursor\resources\app\extensions\git\dist\askpass-main.js
CHROME_CRASHPAD_PIPE_NAME=\\.\pipe\crashpad_1840_LYUDSCHNLLUQVGGK
HOMEPATH=\Users\tonyh
TMP=/tmp
GDAL_DATA=C:\tony_apps\PostgreSQL\14\gdal-data
CONFIG_SITE=C:/Program Files/Git/etc/config.site
PATH=/mingw64/bin:/usr/bin:/c/Users/tonyh/bin:/c/Users/tonyh/miniconda3:/c/Users/tonyh/miniconda3/Library/mingw-w64/bin:/c/Users/tonyh/miniconda3/Library/usr/bin:/c/Users/tonyh/miniconda3/Library/bin:/c/Users/tonyh/miniconda3/Scripts:/c/Users/tonyh/miniconda3/bin:/c/Users/tonyh/miniconda3/condabin:/c/Users/tonyh/bin:/mingw64/bin:/usr/local/bin:/usr/bin:/usr/bin:/mingw64/bin:/usr/bin:/c/Users/tonyh/bin:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/libnvvp:/c/WINDOWS/system32:/c/WINDOWS:/c/WINDOWS/System32/Wbem:/c/WINDOWS/System32/WindowsPowerShell/v1.0:/c/WINDOWS/System32/OpenSSH:/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/c/Program Files/Microsoft SQL Server/150/Tools/Binn:/c/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn:/c/Program Files/dotnet:/c/Program Files/Graphviz/bin:/c/Program Files/Docker/Docker/resources/bin:/c/Program Files/PuTTY:%SPARK_HOME%/bin:%HADOOP_HOME%/bin:%JAVA_HOME%/bin:/c/Program Files/nodejs:/c/Program Files/NVIDIA Corporation/Nsight Compute 2020.3.1:/cmd:/c/Users/tonyh/v_env/py_01/Scripts:/c/Users/tonyh/AppData/Local/Microsoft/WindowsApps:/c/Users/tonyh/.dotnet/tools:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin:/c/Users/tonyh/AppData/Local/Programs/Microsoft VS Code/bin:/c/Users/tonyh/AppData/Roaming/npm:/c/Users/tonyh/AppData/Local/Programs/cursor/resources/app/bin:/c/tony_apps/PostgreSQL/14/bin:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/Users/tonyh/bin:/c/Users/tonyh/bin
EFC_1892_3789132940=1
ProgramW6432=C:\Program Files
MSYSTEM_PREFIX=C:/Program Files/Git/mingw64
ORIGINAL_XDG_CURRENT_DESKTOP=undefined
WINDIR=C:\WINDOWS
FPS_BROWSER_APP_PROFILE_STRING=Internet Explorer
PROCESSOR_ARCHITECTURE=AMD64
PUBLIC=C:\Users\Public
PKG_CONFIG_SYSTEM_INCLUDE_PATH=C:/Program Files/Git/mingw64/include
SYSTEMDRIVE=C:
TERM_PROGRAM=vscode
ProgramData=C:\ProgramData
CURSOR_TRACE_ID=9f881b42df854b37911b65588116d9bd
_=/usr/bin/env
feedback_portal$ where conda
C:\Users\tonyh\miniconda3\Library\bin\conda.bat
C:\Users\tonyh\miniconda3\Scripts\conda.exe
C:\Users\tonyh\miniconda3\condabin\conda.bat
feedback_portal$ conda activate mini_conda_01

CondaError: Run 'conda init' before 'conda activate'

feedback_portal$ conda init
no change     C:\Users\tonyh\miniconda3\Scripts\conda.exe
no change     C:\Users\tonyh\miniconda3\Scripts\conda-env.exe
no change     C:\Users\tonyh\miniconda3\Scripts\conda-script.py
no change     C:\Users\tonyh\miniconda3\Scripts\conda-env-script.py
no change     C:\Users\tonyh\miniconda3\condabin\conda.bat
no change     C:\Users\tonyh\miniconda3\Library\bin\conda.bat
no change     C:\Users\tonyh\miniconda3\condabin\_conda_activate.bat
no change     C:\Users\tonyh\miniconda3\condabin\rename_tmp.bat
no change     C:\Users\tonyh\miniconda3\condabin\conda_auto_activate.bat
no change     C:\Users\tonyh\miniconda3\condabin\conda_hook.bat
no change     C:\Users\tonyh\miniconda3\Scripts\activate.bat
no change     C:\Users\tonyh\miniconda3\condabin\activate.bat
no change     C:\Users\tonyh\miniconda3\condabin\deactivate.bat
no change     C:\Users\tonyh\miniconda3\Scripts\activate
no change     C:\Users\tonyh\miniconda3\Scripts\deactivate
no change     C:\Users\tonyh\miniconda3\etc\profile.d\conda.sh
no change     C:\Users\tonyh\miniconda3\etc\fish\conf.d\conda.fish
no change     C:\Users\tonyh\miniconda3\shell\condabin\Conda.psm1
no change     C:\Users\tonyh\miniconda3\shell\condabin\conda-hook.ps1
no change     C:\Users\tonyh\miniconda3\Lib\site-packages\xontrib\conda.xsh
no change     C:\Users\tonyh\miniconda3\etc\profile.d\conda.csh
modified      C:\Users\tonyh\OneDrive\Documents\WindowsPowerShell\profile.ps1
modified      HKEY_CURRENT_USER\Software\Microsoft\Command Processor\AutoRun

==> For changes to take effect, close and re-open your current shell. <==

feedback_portal$ env
ProgramFiles(x86)=C:\Program Files (x86)
CommonProgramFiles(x86)=C:\Program Files (x86)\Common Files
SHELL=/usr/bin/bash.exe
NUMBER_OF_PROCESSORS=24
FPS_BROWSER_USER_PROFILE_STRING=Default
COLORTERM=truecolor
PROCESSOR_LEVEL=6
TERM_PROGRAM_VERSION=1.2.4
CONDA_EXE=C:/Users/tonyh/miniconda3/Scripts/conda.exe
MINGW_PREFIX=C:/Program Files/Git/mingw64
DATABASE_URI=postgresql+psycopg2://postgres:methane@localhost:5432/tony_home_tracker
PKG_CONFIG_PATH=C:\Program Files\Git\mingw64\lib\pkgconfig;C:\Program Files\Git\mingw64\share\pkgconfig
USERDOMAIN_ROAMINGPROFILE=TONY_DESKTOP
HOSTNAME=tony_desktop
CUDA_PATH_V11_2=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2
PROGRAMFILES=C:\Program Files
MSYSTEM=MINGW64
PATHEXT=.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC
JAVA_HOME=C:\java\jdk_1.8
ORIGINAL_TEMP=C:/Users/tonyh/AppData/Local/Temp
MINGW_CHOST=x86_64-w64-mingw32
OS=Windows_NT
HOMEDRIVE=C:
MSYSTEM_CARCH=x86_64
SPARK_LOCAL_HOSTNAME=localhost
USERDOMAIN=TONY_DESKTOP
PWD=/d/local/cursor/feedback_portal
USERPROFILE=C:\Users\tonyh
CONDA_PREFIX=C:\Users\tonyh\miniconda3
MANPATH=C:\Program Files\Git\mingw64\local\man;C:\Program Files\Git\mingw64\share\man;C:\Program Files\Git\usr\local\man;C:\Program Files\Git\usr\share\man;C:\Program Files\Git\usr\man;C:\Program Files\Git\share\man
VSCODE_GIT_ASKPASS_NODE=C:\Users\tonyh\AppData\Local\Programs\cursor\Cursor.exe
MINGW_PACKAGE_PREFIX=mingw-w64-x86_64
ALLUSERSPROFILE=C:\ProgramData
ORIGINAL_PATH=/mingw64/bin:/usr/bin:/c/Users/tonyh/bin:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/libnvvp:/c/WINDOWS/system32:/c/WINDOWS:/c/WINDOWS/System32/Wbem:/c/WINDOWS/System32/WindowsPowerShell/v1.0:/c/WINDOWS/System32/OpenSSH:/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/c/Program Files/Microsoft SQL Server/150/Tools/Binn:/c/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn:/c/Program Files/dotnet:/c/Program Files/Graphviz/bin:/c/Program Files/Docker/Docker/resources/bin:/c/Program Files/PuTTY:%SPARK_HOME%/bin:%HADOOP_HOME%/bin:%JAVA_HOME%/bin:/c/Program Files/nodejs:/c/Program Files/NVIDIA Corporation/Nsight Compute 2020.3.1:/cmd:/c/Users/tonyh/v_env/py_01/Scripts:/c/Users/tonyh/AppData/Local/Microsoft/WindowsApps:/c/Users/tonyh/.dotnet/tools:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin:/c/Users/tonyh/AppData/Local/Programs/Microsoft VS Code/bin:/c/Users/tonyh/AppData/Roaming/npm:/c/Users/tonyh/AppData/Local/Programs/cursor/resources/app/bin:/c/tony_apps/PostgreSQL/14/bin
CommonProgramW6432=C:\Program Files\Common Files
HOME=/c/Users/tonyh
USERNAME=tonyh
SSH_ASKPASS=C:/Program Files/Git/mingw64/bin/git-askpass.exe
LANG=en_US.UTF-8
VBOX_MSI_INSTALL_PATH=C:\Program Files\Oracle\VirtualBox\
PLINK_PROTOCOL=ssh
OneDrive=C:\Users\tonyh\OneDrive
COMSPEC=C:\WINDOWS\system32\cmd.exe
NVCUDASAMPLES11_2_ROOT=C:\ProgramData\NVIDIA Corporation\CUDA Samples\v11.2
EFC_1892_2775293581=1
CONDA_PROMPT_MODIFIER=(base)
TMPDIR=/tmp
GIT_ASKPASS=c:\Users\tonyh\AppData\Local\Programs\cursor\resources\app\extensions\git\dist\askpass.sh
APPDATA=C:\Users\tonyh\AppData\Roaming
NVCUDASAMPLES_ROOT=C:\ProgramData\NVIDIA Corporation\CUDA Samples\v11.2
SYSTEMROOT=C:\WINDOWS
LOCALAPPDATA=C:\Users\tonyh\AppData\Local
EFC_1892_2283032206=1
COMPUTERNAME=TONY_DESKTOP
INFOPATH=C:\Program Files\Git\mingw64\local\info;C:\Program Files\Git\mingw64\share\info;C:\Program Files\Git\usr\local\info;C:\Program Files\Git\usr\share\info;C:\Program Files\Git\usr\info;C:\Program Files\Git\share\info  
VSCODE_GIT_ASKPASS_EXTRA_ARGS=
PYSPARK_PYTHON=C:\Users\tonyh\v_env\py_01\Scripts\python.exe
PYTHONPATH=D:\local\cursor\feedback_portal\source\production;
TERM=xterm
HADOOP_HOME=C:\hadoop
LOGONSERVER=\\TONY_DESKTOP
PYSPARK_HOME=C:\Users\tonyh\v_env\py_01\Scripts\python.exe
ACLOCAL_PATH=C:\Program Files\Git\mingw64\share\aclocal;C:\Program Files\Git\usr\share\aclocal
CURL_CA_BUNDLE=C:\tony_apps\PostgreSQL\14\ssl\certs\ca-bundle.crt
PSModulePath=C:\Program Files\WindowsPowerShell\Modules;C:\WINDOWS\system32\WindowsPowerShell\v1.0\Modules      
NVTOOLSEXT_PATH=C:\Program Files\NVIDIA Corporation\NvToolsExt\
VSCODE_GIT_IPC_HANDLE=\\.\pipe\vscode-git-2bbd251d92-sock
CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2
CONDA_SHLVL=1
TEMP=/tmp
MSYSTEM_CHOST=x86_64-w64-mingw32
DISPLAY=needs-to-be-defined
SHLVL=3
ORIGINAL_TMP=C:/Users/tonyh/AppData/Local/Temp
PROCESSOR_REVISION=b701
DriverData=C:\Windows\System32\Drivers\DriverData
EFC_1892_1262719628=1
SPARK_HOME=C:\spark\spark-3.3.2-bin-hadoop2
PROJ_LIB=C:\tony_apps\PostgreSQL\14\share\contrib\postgis-3.4\proj
COMMONPROGRAMFILES=C:\Program Files\Common Files
CONDA_PYTHON_EXE=C:/Users/tonyh/miniconda3/python.exe
MOZ_PLUGIN_PATH=C:\Program Files (x86)\Foxit Software\Foxit PDF Reader\plugins\
LC_CTYPE=en_US.UTF-8
POSTGIS_GDAL_ENABLED_DRIVERS=GTiff PNG JPEG GIF XYZ DTED USGSDEM AAIGrid
SSL_CERT_FILE=C:\Users\tonyh\cacert.pem
EXEPATH=C:\Program Files\Git\bin
PROCESSOR_IDENTIFIER=Intel64 Family 6 Model 183 Stepping 1, GenuineIntel
SESSIONNAME=Console
PS1=\[\]$(basename "$PWD")\$ \[\]
CONDA_DEFAULT_ENV=base
EFC_1892_1592913036=1
POSTGIS_ENABLE_OUTDB_RASTERS=1
PKG_CONFIG_SYSTEM_LIBRARY_PATH=C:/Program Files/Git/mingw64/lib
VSCODE_GIT_ASKPASS_MAIN=c:\Users\tonyh\AppData\Local\Programs\cursor\resources\app\extensions\git\dist\askpass-main.js
CHROME_CRASHPAD_PIPE_NAME=\\.\pipe\crashpad_1840_LYUDSCHNLLUQVGGK
HOMEPATH=\Users\tonyh
TMP=/tmp
GDAL_DATA=C:\tony_apps\PostgreSQL\14\gdal-data
CONFIG_SITE=C:/Program Files/Git/etc/config.site
PATH=/mingw64/bin:/usr/bin:/c/Users/tonyh/bin:/c/Users/tonyh/miniconda3:/c/Users/tonyh/miniconda3/Library/mingw-w64/bin:/c/Users/tonyh/miniconda3/Library/usr/bin:/c/Users/tonyh/miniconda3/Library/bin:/c/Users/tonyh/miniconda3/Scripts:/c/Users/tonyh/miniconda3/bin:/c/Users/tonyh/miniconda3/condabin:/c/Users/tonyh/bin:/mingw64/bin:/usr/local/bin:/usr/bin:/usr/bin:/mingw64/bin:/usr/bin:/c/Users/tonyh/bin:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/libnvvp:/c/WINDOWS/system32:/c/WINDOWS:/c/WINDOWS/System32/Wbem:/c/WINDOWS/System32/WindowsPowerShell/v1.0:/c/WINDOWS/System32/OpenSSH:/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/c/Program Files/Microsoft SQL Server/150/Tools/Binn:/c/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn:/c/Program Files/dotnet:/c/Program Files/Graphviz/bin:/c/Program Files/Docker/Docker/resources/bin:/c/Program Files/PuTTY:%SPARK_HOME%/bin:%HADOOP_HOME%/bin:%JAVA_HOME%/bin:/c/Program Files/nodejs:/c/Program Files/NVIDIA Corporation/Nsight Compute 2020.3.1:/cmd:/c/Users/tonyh/v_env/py_01/Scripts:/c/Users/tonyh/AppData/Local/Microsoft/WindowsApps:/c/Users/tonyh/.dotnet/tools:/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin:/c/Users/tonyh/AppData/Local/Programs/Microsoft VS Code/bin:/c/Users/tonyh/AppData/Roaming/npm:/c/Users/tonyh/AppData/Local/Programs/cursor/resources/app/bin:/c/tony_apps/PostgreSQL/14/bin:/usr/bin/vendor_perl:/usr/bin/core_perl:/c/Users/tonyh/bin:/c/Users/tonyh/bin
EFC_1892_3789132940=1
ProgramW6432=C:\Program Files
MSYSTEM_PREFIX=C:/Program Files/Git/mingw64
ORIGINAL_XDG_CURRENT_DESKTOP=undefined
WINDIR=C:\WINDOWS
FPS_BROWSER_APP_PROFILE_STRING=Internet Explorer
PROCESSOR_ARCHITECTURE=AMD64
PUBLIC=C:\Users\Public
PKG_CONFIG_SYSTEM_INCLUDE_PATH=C:/Program Files/Git/mingw64/include
SYSTEMDRIVE=C:
TERM_PROGRAM=vscode
ProgramData=C:\ProgramData
CURSOR_TRACE_ID=9f881b42df854b37911b65588116d9bd
_=/usr/bin/env
feedback_portal$ 

7/16/25 ran comparison of feedback spreadsheets. see comparison for details.
the only key file that differed was dairy_digester_operator_feedback_v006.xlsx
both dairy_digester_operator_feedback_v006.xlsx and dairy_digester_operator_feedback_v006_for_review.xlsx were copied to C:\tony_local\pycharm\feedback_portal\feedback_forms\current_versions
7/17/25 ran GenerateAllTemplates for
  wb_names = Array( _
    "dairy_digester_operator_feedback_v006.xlsx", _
    "energy_operator_feedback_v003.xlsx", _
    "generic_operator_feedback_v002.xlsx", _
    "landfill_operator_feedback_v070.xlsx", _
    "landfill_operator_feedback_v071.xlsx", _
    "oil_and_gas_operator_feedback_v070.xlsx" _
  )

Creating testing spreadsheets for e2e testing:


EXCEL_TEMPLATES = [
    {
        "sector": "Dairy Digester",
        "schema_version": "dairy_digester_v01_00",
        "prefix": "dairy_digester_operator_feedback",
        "version": "v006",
        "payload_name": "dairy_digester_payload_01",  # reusing oil and gas payload
    },
         testing id range 1002000-1002009

    {
        "sector": "Energy",
        "schema_version": "energy_v01_00",
        "prefix": "energy_operator_feedback",
        "version": "v003",
        "payload_name": "oil_and_gas_payload_01",  # reusing oil and gas payload
    },
         testing id range 1002010-1002019

    {
        "sector": "Generic",
        "schema_version": "generic_v01_00",
        "prefix": "generic_operator_feedback",
        "version": "v002",
        "payload_name": "generic_payload_01",  # reusing oil and gas payload
    },
        testing id range 1002020-1002029

    {
        "sector": "Landfill",
        "schema_version": "landfill_v01_00",
        "prefix": "landfill_operator_feedback",
        "version": "v070",
        "payload_name": "landfill_payload_01",
    },
        testing id range 1002030-1002039
        landfill dummy gets 1002030

    {
        "sector": "Landfill",
        "schema_version": "landfill_v01_01",
        "prefix": "landfill_operator_feedback",
        "version": "v071",
        "payload_name": "landfill_payload_01",
    },
        testing id range 1002040-1002049

    {
        "sector": "Oil and Gas",
        "schema_version": "oil_and_gas_v01_00",
        "prefix": "oil_and_gas_operator_feedback",
        "version": "v070",
        "payload_name": "oil_and_gas_payload_01",
    },
        testing id range 1002050-1002059
        landfill dummy gets 1002050
]



dairy_digester_operator_feedback_v006_test_01_good_data.xlsx
energy_operator_feedback_v003_test_01_good_data.xlsx
generic_operator_feedback_v002_test_01_good_data.xlsx
landfill_operator_feedback_v070_test_01_good_data.xlsx
landfill_operator_feedback_v071_test_01_good_data.xlsx
oil_and_gas_operator_feedback_v070_test_01_good_data.xlsx




Running pytests

1. Ensure you are in the root folder to run tests
      cd /d/local/cursor/feedback_portal
2. General pytest run
      pytest <file or folder> -v -s --maxfail=3 -k "match01 or match02"  | tee output_file_01.txt
      pytest <file or folder> -v -s --maxfail=3 -k "match01 or match02"  > output_file_01.txt 2>&1

3. Flag meanings:
      --maxfail=num         Exit after first num failures or errors
      -v, --verbose         Increase verbosity
      -s                    Don't suppress print statements that are in test files
      -k EXPRESSION         Only run tests which match the given substring expression. An expression is a Python evaluable     
                            expression where all names are substring-matched against test names and their parent classes.      
                            Example: -k 'test_method or test_other' matches all test functions and classes whose name contains 
                            'test_method' or 'test_other', while -k 'not test_method' matches those that don't contain
                            'test_method' in their names. -k 'not test_method and not test_other' will eliminate the matches.  
                            Additionally keywords are matched to classes and functions containing extra names in their
                            'extra_keyword_matches' set, as well as functions which have names assigned directly to them. The  
                            matching is case-insensitive.

4. tee versus redirection
    | tee <filename>        you get terminal feedback of the tests while simultaneously sending to log file
    > <filename> 2>&1       captures standard output and error and may be more reliable in some situations

5. example runs:

    pytest tests/e2e/test_excel_upload_playwright_pytest.py --only-discard-tests -v | tee pytest_output_04.txt
    pytest tests/e2e/test_excel_upload_playwright_pytest.py -v --maxfail=3 | tee pytest_output_04.txt
    pytest tests/e2e/test_excel_upload_playwright_pytest.py -v | tee pytest_output_04.txt
    pytest tests/e2e/test_excel_upload_playwright_pytest.py -v --maxfail=3 -k "discard or malformed" | tee pytest_output_04.txt
    pytest tests/e2e/test_excel_upload_playwright_pytest.py -v -k "test_diagnostics_overlay_on_diagnostic_test_page" | tee pytest_output_04.txt
    pytest tests/e2e/test_diagnostics.py -v  | tee pytest_diag_03.txt
    pytest tests/e2e/test_excel_upload_playwright_pytest.py -v -k "discard or malformed" | tee pytest_output_04.txt
    pytest tests/e2e/test_diagnostics.py -v | tee pytest_diag_28.txt
    pytest tests/e2e/test_diagnostics.py -v -k test_diagnostics_overlay_on_diagnostic_test_page | tee pytest_diag_12.txt
    pytest tests/e2e/test_feedback_updates.py -v > pytest_feedback_updates.txt 2>&1
    pytest tests/e2e -v  | tee pytest_all_02.txt
    pytest tests/arb -v  | tee pytest_all_07.txt
    pytest -s tests/arb/portal/test_file_upload_suite.py | tee pytest_06.txt