{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome & Instructions Welcome to the backend documentation site for the CalSMP Operator Feedback Portal. This documentation is auto-generated from Python source code and includes: Flask route handlers and form logic Utility functions and helpers Excel file schema generation and parsing SQLAlchemy integration and metadata support Use the left sidebar to explore the available modules.","title":"Welcome &amp; Instructions"},{"location":"#welcome-instructions","text":"Welcome to the backend documentation site for the CalSMP Operator Feedback Portal. This documentation is auto-generated from Python source code and includes: Flask route handlers and form logic Utility functions and helpers Excel file schema generation and parsing SQLAlchemy integration and metadata support Use the left sidebar to explore the available modules.","title":"Welcome &amp; Instructions"},{"location":"reference/arb/wsgi/","text":"arb.wsgi WSGI entry point for serving the ARB Feedback Portal Flask app. This file enables the application to be run via a WSGI server (e.g., Gunicorn or uWSGI) or directly via flask run or python wsgi.py . It provides detailed notes for various execution contexts, Flask CLI behavior, debugging strategies, and developer workflows including PyCharm integration. Module_Attributes app (Flask): The Flask application instance created by create_app(). logger (logging.Logger): Logger instance for this module. Examples: Run with Flask CLI: flask run --app wsgi Run directly: python wsgi.py Run with Gunicorn: gunicorn arb.wsgi:app Notes See the detailed notes below for configuration, environment variables, and best practices. Use app.run(debug=True) for development and debugging, but not for production deployment. The project root directory is \"feedback_portal\". For PyCharm debugging, set FLASK_ENV=development and FLASK_DEBUG=0. Note on running Flask Apps: 1) You can run a flask app from the CLI in two ways: * python * run flask app directly from python without the flask CLI * Errors shown in terminal; browser only shows generic 500 unless debug=True in source code * flask run * uses the Flask CLI * makes use of Flask related environment variables and command line arguments 2) Flask configuration precedence: The effective behavior of your Flask app depends on how it's launched and which configuration values are set at various levels. The following precedence applies: Precedence Order (from strongest to weakest): 1. Arguments passed directly to app.run(...) 2. Flask CLI command-line options - e.g., flask run --port=8000 overrides any FLASK_RUN_PORT setting. 3. Environment variables - e.g., FLASK_ENV, FLASK_DEBUG, FLASK_RUN_PORT 3) Environmental variables and running from the Flask CLI * FLASK_APP: sets the default name for the flask app if not specified. \"flask run\" is equivalent to \"flask --app FLASK_APP run\" Likely FLASK_APP=app.py or FLASK_APP=wsgi * FLASK_ENV: can be 'development' or 'production' development enables debug mode, auto-reload, and detailed error pages, production disables them. Likely want FLASK_ENV=development for CARB development * FLASK_DEBUG: 1 enables the interactive browser debugger (Werkzeug); 0 disables it. * PYTHONPATH: Adds directories to Python's module resolution path (sometimes needed for imports) 4) Flask CLI arguments * key options * --app * --debug or --no-debug * determines if the Werkzeug browser debugger is on/off * --no-reload <-- faster load time and does not restart app on source code changes 5) Commonly used arguments for app.run() : * host (str, optional): The IP address to bind to. Defaults to '127.0.0.1' . Use '0.0.0.0' to make the app publicly accessible (e.g., on a local network). * port (int, optional): The port number to listen to. Defaults to 5000 . * debug (bool, optional): Enables debug mode, which activates auto-reload and the interactive browser debugger. Defaults to None . * use_reloader (bool, optional): Enables the auto-reloader to restart the server on code changes. Defaults to True if debug is enabled. * use_debugger (bool, optional): Enables the interactive debugger in the browser when errors occur. Defaults to True if debug is enabled. * threaded (bool, optional): Run the server in multithreaded mode. Useful for handling multiple concurrent requests. Defaults to False . * processes (int, optional): Number of worker processes for handling requests. Mutually exclusive with threaded=True . Defaults to 1 . * load_dotenv (bool, optional): Whether to load environment variables from a .env file. Defaults to True . 6) Best practices: * Use app.run(debug=True) in wsgi.py for development (not for production) * Use 'development' over 'production' until product is final * For browser-based testing: flask run --app wsgi * For PyCharm debugging: Run wsgi.py with debug=True, FLASK_ENV=development, FLASK_DEBUG=0 * Combined workflow: PyCharm + browser, debug=True, FLASK_ENV=development, FLASK_DEBUG=1 7) Root directory notes: - The project root directory is \"feedback_portal\" - If the app is run from wsgi.py at feedback_portal/source/production/arb/wsgi.py Path( file ).resolve().parents[3] \u2192 .../feedback_portal","title":"arb.wsgi"},{"location":"reference/arb/wsgi/#arbwsgi","text":"WSGI entry point for serving the ARB Feedback Portal Flask app. This file enables the application to be run via a WSGI server (e.g., Gunicorn or uWSGI) or directly via flask run or python wsgi.py . It provides detailed notes for various execution contexts, Flask CLI behavior, debugging strategies, and developer workflows including PyCharm integration. Module_Attributes app (Flask): The Flask application instance created by create_app(). logger (logging.Logger): Logger instance for this module. Examples:","title":"arb.wsgi"},{"location":"reference/arb/wsgi/#arb.wsgi--run-with-flask-cli","text":"","title":"Run with Flask CLI:"},{"location":"reference/arb/wsgi/#arb.wsgi--flask-run-app-wsgi","text":"","title":"flask run --app wsgi"},{"location":"reference/arb/wsgi/#arb.wsgi--run-directly","text":"","title":"Run directly:"},{"location":"reference/arb/wsgi/#arb.wsgi--python-wsgipy","text":"","title":"python wsgi.py"},{"location":"reference/arb/wsgi/#arb.wsgi--run-with-gunicorn","text":"","title":"Run with Gunicorn:"},{"location":"reference/arb/wsgi/#arb.wsgi--gunicorn-arbwsgiapp","text":"Notes See the detailed notes below for configuration, environment variables, and best practices. Use app.run(debug=True) for development and debugging, but not for production deployment. The project root directory is \"feedback_portal\". For PyCharm debugging, set FLASK_ENV=development and FLASK_DEBUG=0. Note on running Flask Apps: 1) You can run a flask app from the CLI in two ways: * python * run flask app directly from python without the flask CLI * Errors shown in terminal; browser only shows generic 500 unless debug=True in source code * flask run * uses the Flask CLI * makes use of Flask related environment variables and command line arguments 2) Flask configuration precedence: The effective behavior of your Flask app depends on how it's launched and which configuration values are set at various levels. The following precedence applies: Precedence Order (from strongest to weakest): 1. Arguments passed directly to app.run(...) 2. Flask CLI command-line options - e.g., flask run --port=8000 overrides any FLASK_RUN_PORT setting. 3. Environment variables - e.g., FLASK_ENV, FLASK_DEBUG, FLASK_RUN_PORT 3) Environmental variables and running from the Flask CLI * FLASK_APP: sets the default name for the flask app if not specified. \"flask run\" is equivalent to \"flask --app FLASK_APP run\" Likely FLASK_APP=app.py or FLASK_APP=wsgi * FLASK_ENV: can be 'development' or 'production' development enables debug mode, auto-reload, and detailed error pages, production disables them. Likely want FLASK_ENV=development for CARB development * FLASK_DEBUG: 1 enables the interactive browser debugger (Werkzeug); 0 disables it. * PYTHONPATH: Adds directories to Python's module resolution path (sometimes needed for imports) 4) Flask CLI arguments * key options * --app * --debug or --no-debug * determines if the Werkzeug browser debugger is on/off * --no-reload <-- faster load time and does not restart app on source code changes 5) Commonly used arguments for app.run() : * host (str, optional): The IP address to bind to. Defaults to '127.0.0.1' . Use '0.0.0.0' to make the app publicly accessible (e.g., on a local network). * port (int, optional): The port number to listen to. Defaults to 5000 . * debug (bool, optional): Enables debug mode, which activates auto-reload and the interactive browser debugger. Defaults to None . * use_reloader (bool, optional): Enables the auto-reloader to restart the server on code changes. Defaults to True if debug is enabled. * use_debugger (bool, optional): Enables the interactive debugger in the browser when errors occur. Defaults to True if debug is enabled. * threaded (bool, optional): Run the server in multithreaded mode. Useful for handling multiple concurrent requests. Defaults to False . * processes (int, optional): Number of worker processes for handling requests. Mutually exclusive with threaded=True . Defaults to 1 . * load_dotenv (bool, optional): Whether to load environment variables from a .env file. Defaults to True . 6) Best practices: * Use app.run(debug=True) in wsgi.py for development (not for production) * Use 'development' over 'production' until product is final * For browser-based testing: flask run --app wsgi * For PyCharm debugging: Run wsgi.py with debug=True, FLASK_ENV=development, FLASK_DEBUG=0 * Combined workflow: PyCharm + browser, debug=True, FLASK_ENV=development, FLASK_DEBUG=1 7) Root directory notes: - The project root directory is \"feedback_portal\" - If the app is run from wsgi.py at feedback_portal/source/production/arb/wsgi.py Path( file ).resolve().parents[3] \u2192 .../feedback_portal","title":"gunicorn arb.wsgi:app"},{"location":"reference/arb/auth/default_settings/","text":"arb.auth.default_settings Default authentication and email settings for ARB Feedback Portal. This file provides default values for all authentication and email-related configuration variables. How to use: - These defaults are imported into your main config/settings.py. - To override any value, set it in your settings.py after importing from this file. - All variables are documented below. Only override what you need to change for your deployment. Variables: - AUTH_MAIL_SERVER: SMTP server address for outgoing email. - AUTH_MAIL_PORT: SMTP server port (usually 587 for TLS, 465 for SSL). - AUTH_MAIL_USE_TLS: Enable TLS encryption for SMTP. - AUTH_MAIL_USE_SSL: Enable SSL encryption for SMTP (mutually exclusive with TLS). - AUTH_MAIL_USERNAME: Username for SMTP authentication (usually the sender email address). - AUTH_MAIL_PASSWORD: Password for SMTP authentication. - AUTH_MAIL_DEFAULT_SENDER: Default \"From\" address for all outgoing emails. - AUTH_MAIL_MAX_EMAILS: Maximum emails to send per connection (for batching). - AUTH_PASSWORD_RESET_EXPIRATION: Time (in seconds) before a password reset token expires. - AUTH_MAX_PASSWORD_RESET_ATTEMPTS: Maximum allowed password reset requests before cooldown. - AUTH_PASSWORD_RESET_COOLDOWN: Cooldown period (in seconds) after too many reset attempts. - AUTH_MAX_LOGIN_ATTEMPTS: Maximum failed login attempts before account lockout. - AUTH_ACCOUNT_LOCKOUT_DURATION: Lockout duration (in seconds) after too many failed logins. - AUTH_SESSION_TIMEOUT: Session expiration time (in seconds) for user logins. - AUTH_REMEMBER_ME_DURATION: Duration (in seconds) for \"remember me\" logins. Security notes: - Never commit real SMTP credentials to version control. - Use environment variables or secrets management for production overrides. - These defaults are safe for development but should be reviewed for production.","title":"arb.auth.default_settings"},{"location":"reference/arb/auth/default_settings/#arbauthdefault_settings","text":"Default authentication and email settings for ARB Feedback Portal. This file provides default values for all authentication and email-related configuration variables. How to use: - These defaults are imported into your main config/settings.py. - To override any value, set it in your settings.py after importing from this file. - All variables are documented below. Only override what you need to change for your deployment. Variables: - AUTH_MAIL_SERVER: SMTP server address for outgoing email. - AUTH_MAIL_PORT: SMTP server port (usually 587 for TLS, 465 for SSL). - AUTH_MAIL_USE_TLS: Enable TLS encryption for SMTP. - AUTH_MAIL_USE_SSL: Enable SSL encryption for SMTP (mutually exclusive with TLS). - AUTH_MAIL_USERNAME: Username for SMTP authentication (usually the sender email address). - AUTH_MAIL_PASSWORD: Password for SMTP authentication. - AUTH_MAIL_DEFAULT_SENDER: Default \"From\" address for all outgoing emails. - AUTH_MAIL_MAX_EMAILS: Maximum emails to send per connection (for batching). - AUTH_PASSWORD_RESET_EXPIRATION: Time (in seconds) before a password reset token expires. - AUTH_MAX_PASSWORD_RESET_ATTEMPTS: Maximum allowed password reset requests before cooldown. - AUTH_PASSWORD_RESET_COOLDOWN: Cooldown period (in seconds) after too many reset attempts. - AUTH_MAX_LOGIN_ATTEMPTS: Maximum failed login attempts before account lockout. - AUTH_ACCOUNT_LOCKOUT_DURATION: Lockout duration (in seconds) after too many failed logins. - AUTH_SESSION_TIMEOUT: Session expiration time (in seconds) for user logins. - AUTH_REMEMBER_ME_DURATION: Duration (in seconds) for \"remember me\" logins. Security notes: - Never commit real SMTP credentials to version control. - Use environment variables or secrets management for production overrides. - These defaults are safe for development but should be reviewed for production.","title":"arb.auth.default_settings"},{"location":"reference/arb/auth/email_util/","text":"arb.auth.email_util Email utility functions for ARB Feedback Portal authentication. send_email_confirmation ( user , token ) Send an email confirmation email to the user with a confirmation token. Source code in arb\\auth\\email_util.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def send_email_confirmation ( user , token ): \"\"\" Send an email confirmation email to the user with a confirmation token. \"\"\" subject = \"Confirm Your Email Address - ARB Feedback Portal\" recipient = user . email confirmation_url = url_for ( 'auth.confirm_email' , token = token , _external = True ) html_body = render_template ( 'emails/email_confirmation.html' , user = user , confirmation_url = confirmation_url ) msg = Message ( subject = subject , recipients = [ recipient ], html = html_body ) # Log email content if suppressed if current_app . config . get ( 'MAIL_SUPPRESS_SEND' , False ): logger . info ( \"=\" * 60 ) logger . info ( \"SUPPRESSED EMAIL - Email Confirmation\" ) logger . info ( \"=\" * 60 ) logger . info ( f \"To: { recipient } \" ) logger . info ( f \"Subject: { subject } \" ) logger . info ( f \"Confirmation URL: { confirmation_url } \" ) logger . info ( \"HTML Body:\" ) logger . info ( html_body ) logger . info ( \"=\" * 60 ) get_mail () . send ( msg ) send_password_reset_email ( user , token ) Send a password reset email to the user with a reset token. Source code in arb\\auth\\email_util.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def send_password_reset_email ( user , token ): \"\"\" Send a password reset email to the user with a reset token. \"\"\" subject = \"Password Reset Request\" recipient = user . email reset_url = url_for ( 'auth.reset_password' , token = token , _external = True ) html_body = render_template ( 'emails/password_reset.html' , user = user , reset_url = reset_url ) msg = Message ( subject = subject , recipients = [ recipient ], html = html_body ) # Log email content if suppressed if current_app . config . get ( 'MAIL_SUPPRESS_SEND' , False ): logger . info ( \"=\" * 60 ) logger . info ( \"SUPPRESSED EMAIL - Password Reset\" ) logger . info ( \"=\" * 60 ) logger . info ( f \"To: { recipient } \" ) logger . info ( f \"Subject: { subject } \" ) logger . info ( f \"Reset URL: { reset_url } \" ) logger . info ( \"HTML Body:\" ) logger . info ( html_body ) logger . info ( \"=\" * 60 ) get_mail () . send ( msg ) send_welcome_email ( user ) Send a welcome email to the newly registered user. Source code in arb\\auth\\email_util.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def send_welcome_email ( user ): \"\"\" Send a welcome email to the newly registered user. \"\"\" subject = \"Welcome to ARB Feedback Portal\" recipient = user . email html_body = render_template ( 'emails/welcome.html' , user = user ) msg = Message ( subject = subject , recipients = [ recipient ], html = html_body ) # Log email content if suppressed if current_app . config . get ( 'MAIL_SUPPRESS_SEND' , False ): logger . info ( \"=\" * 60 ) logger . info ( \"SUPPRESSED EMAIL - Welcome Email\" ) logger . info ( \"=\" * 60 ) logger . info ( f \"To: { recipient } \" ) logger . info ( f \"Subject: { subject } \" ) logger . info ( \"HTML Body:\" ) logger . info ( html_body ) logger . info ( \"=\" * 60 ) get_mail () . send ( msg )","title":"arb.auth.email_util"},{"location":"reference/arb/auth/email_util/#arbauthemail_util","text":"Email utility functions for ARB Feedback Portal authentication.","title":"arb.auth.email_util"},{"location":"reference/arb/auth/email_util/#arb.auth.email_util.send_email_confirmation","text":"Send an email confirmation email to the user with a confirmation token. Source code in arb\\auth\\email_util.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def send_email_confirmation ( user , token ): \"\"\" Send an email confirmation email to the user with a confirmation token. \"\"\" subject = \"Confirm Your Email Address - ARB Feedback Portal\" recipient = user . email confirmation_url = url_for ( 'auth.confirm_email' , token = token , _external = True ) html_body = render_template ( 'emails/email_confirmation.html' , user = user , confirmation_url = confirmation_url ) msg = Message ( subject = subject , recipients = [ recipient ], html = html_body ) # Log email content if suppressed if current_app . config . get ( 'MAIL_SUPPRESS_SEND' , False ): logger . info ( \"=\" * 60 ) logger . info ( \"SUPPRESSED EMAIL - Email Confirmation\" ) logger . info ( \"=\" * 60 ) logger . info ( f \"To: { recipient } \" ) logger . info ( f \"Subject: { subject } \" ) logger . info ( f \"Confirmation URL: { confirmation_url } \" ) logger . info ( \"HTML Body:\" ) logger . info ( html_body ) logger . info ( \"=\" * 60 ) get_mail () . send ( msg )","title":"send_email_confirmation"},{"location":"reference/arb/auth/email_util/#arb.auth.email_util.send_password_reset_email","text":"Send a password reset email to the user with a reset token. Source code in arb\\auth\\email_util.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def send_password_reset_email ( user , token ): \"\"\" Send a password reset email to the user with a reset token. \"\"\" subject = \"Password Reset Request\" recipient = user . email reset_url = url_for ( 'auth.reset_password' , token = token , _external = True ) html_body = render_template ( 'emails/password_reset.html' , user = user , reset_url = reset_url ) msg = Message ( subject = subject , recipients = [ recipient ], html = html_body ) # Log email content if suppressed if current_app . config . get ( 'MAIL_SUPPRESS_SEND' , False ): logger . info ( \"=\" * 60 ) logger . info ( \"SUPPRESSED EMAIL - Password Reset\" ) logger . info ( \"=\" * 60 ) logger . info ( f \"To: { recipient } \" ) logger . info ( f \"Subject: { subject } \" ) logger . info ( f \"Reset URL: { reset_url } \" ) logger . info ( \"HTML Body:\" ) logger . info ( html_body ) logger . info ( \"=\" * 60 ) get_mail () . send ( msg )","title":"send_password_reset_email"},{"location":"reference/arb/auth/email_util/#arb.auth.email_util.send_welcome_email","text":"Send a welcome email to the newly registered user. Source code in arb\\auth\\email_util.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def send_welcome_email ( user ): \"\"\" Send a welcome email to the newly registered user. \"\"\" subject = \"Welcome to ARB Feedback Portal\" recipient = user . email html_body = render_template ( 'emails/welcome.html' , user = user ) msg = Message ( subject = subject , recipients = [ recipient ], html = html_body ) # Log email content if suppressed if current_app . config . get ( 'MAIL_SUPPRESS_SEND' , False ): logger . info ( \"=\" * 60 ) logger . info ( \"SUPPRESSED EMAIL - Welcome Email\" ) logger . info ( \"=\" * 60 ) logger . info ( f \"To: { recipient } \" ) logger . info ( f \"Subject: { subject } \" ) logger . info ( \"HTML Body:\" ) logger . info ( html_body ) logger . info ( \"=\" * 60 ) get_mail () . send ( msg )","title":"send_welcome_email"},{"location":"reference/arb/auth/forms/","text":"arb.auth.forms Authentication forms for ARB Feedback Portal. This module defines all WTForms used for user authentication.","title":"arb.auth.forms"},{"location":"reference/arb/auth/forms/#arbauthforms","text":"Authentication forms for ARB Feedback Portal. This module defines all WTForms used for user authentication.","title":"arb.auth.forms"},{"location":"reference/arb/auth/login_manager/","text":"arb.auth.login_manager Login manager and user loader for ARB Feedback Portal authentication. Current Implementation: - Loads users from the local database using the User model. - Registers the user_loader function with Flask-Login's LoginManager. Okta Transition Plan: - When USE_OKTA is True, user loading will be based on Okta session/token info instead of the local database. - This file documents the transition plan and provides a single place to update user loading logic for Okta. How to Identify Okta-Ready Code: - The user_loader function checks USE_OKTA and raises NotImplementedError for Okta until integration is complete. load_user ( user_id ) Load a user for Flask-Login session management. - If USE_OKTA is False, loads user from the local database by primary key. - If USE_OKTA is True, loads user from Okta session/token (not yet implemented). Source code in arb\\auth\\login_manager.py 20 21 22 23 24 25 26 27 28 29 30 31 32 def load_user ( user_id ): \"\"\" Load a user for Flask-Login session management. - If USE_OKTA is False, loads user from the local database by primary key. - If USE_OKTA is True, loads user from Okta session/token (not yet implemented). \"\"\" if USE_OKTA : # TODO: Implement user loading from Okta session/token. # Need: Okta user info endpoint, session/token integration. raise NotImplementedError ( \"Okta user loading not implemented yet. Need Okta user info integration.\" ) else : User = get_user_model () return User . query . get ( int ( user_id )) register_user_loader () Register the user loader with the login manager after initialization. Source code in arb\\auth\\login_manager.py 35 36 37 38 def register_user_loader (): \"\"\"Register the user loader with the login manager after initialization.\"\"\" from arb.auth import get_login_manager get_login_manager () . user_loader ( load_user )","title":"arb.auth.login_manager"},{"location":"reference/arb/auth/login_manager/#arbauthlogin_manager","text":"Login manager and user loader for ARB Feedback Portal authentication. Current Implementation: - Loads users from the local database using the User model. - Registers the user_loader function with Flask-Login's LoginManager. Okta Transition Plan: - When USE_OKTA is True, user loading will be based on Okta session/token info instead of the local database. - This file documents the transition plan and provides a single place to update user loading logic for Okta. How to Identify Okta-Ready Code: - The user_loader function checks USE_OKTA and raises NotImplementedError for Okta until integration is complete.","title":"arb.auth.login_manager"},{"location":"reference/arb/auth/login_manager/#arb.auth.login_manager.load_user","text":"Load a user for Flask-Login session management. - If USE_OKTA is False, loads user from the local database by primary key. - If USE_OKTA is True, loads user from Okta session/token (not yet implemented). Source code in arb\\auth\\login_manager.py 20 21 22 23 24 25 26 27 28 29 30 31 32 def load_user ( user_id ): \"\"\" Load a user for Flask-Login session management. - If USE_OKTA is False, loads user from the local database by primary key. - If USE_OKTA is True, loads user from Okta session/token (not yet implemented). \"\"\" if USE_OKTA : # TODO: Implement user loading from Okta session/token. # Need: Okta user info endpoint, session/token integration. raise NotImplementedError ( \"Okta user loading not implemented yet. Need Okta user info integration.\" ) else : User = get_user_model () return User . query . get ( int ( user_id ))","title":"load_user"},{"location":"reference/arb/auth/login_manager/#arb.auth.login_manager.register_user_loader","text":"Register the user loader with the login manager after initialization. Source code in arb\\auth\\login_manager.py 35 36 37 38 def register_user_loader (): \"\"\"Register the user loader with the login manager after initialization.\"\"\" from arb.auth import get_login_manager get_login_manager () . user_loader ( load_user )","title":"register_user_loader"},{"location":"reference/arb/auth/migrate_roles/","text":"arb.auth.migrate_roles Database migration script for multiple roles support. This script updates the existing 'role' column in the users table to support multiple roles using comma-separated values. Run this script once after deploying the updated User model. create_migration_app () Create a minimal Flask app for migration. Source code in arb\\auth\\migrate_roles.py 24 25 26 27 28 29 30 31 32 33 34 35 def create_migration_app (): \"\"\"Create a minimal Flask app for migration.\"\"\" app = Flask ( __name__ ) app . config . from_object ( get_config ()) # Initialize extensions portal_db . init_app ( app ) # Initialize auth package init_auth ( app = app , db = portal_db ) return app migrate_role_column () Migrate the role column to support multiple roles. Source code in arb\\auth\\migrate_roles.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def migrate_role_column (): \"\"\"Migrate the role column to support multiple roles.\"\"\" print ( \"Starting role column migration...\" ) with app . app_context (): # Get all users users = User . query . all () print ( f \"Found { len ( users ) } users to migrate\" ) migrated_count = 0 skipped_count = 0 for user in users : current_role = user . role # Skip if already in multiple role format if ',' in current_role : print ( f \"Skipping user { user . email } - already has multiple roles: { current_role } \" ) skipped_count += 1 continue # Convert single role to list format if current_role and current_role . strip (): old_role = current_role user . set_roles ([ current_role . strip ()]) print ( f \"Migrated user { user . email } : ' { old_role } ' -> ' { user . role } '\" ) else : old_role = current_role user . set_roles ([ 'user' ]) print ( f \"Migrated user { user . email } : ' { old_role } ' -> 'user'\" ) migrated_count += 1 # Commit all changes get_db () . session . commit () print ( f \" \\n Migration completed!\" ) print ( f \"Users migrated: { migrated_count } \" ) print ( f \"Users skipped: { skipped_count } \" ) print ( f \"Total processed: { migrated_count + skipped_count } \" ) update_database_schema () Update the database schema to increase role column size. Source code in arb\\auth\\migrate_roles.py 102 103 104 105 106 107 108 109 110 111 112 113 114 def update_database_schema (): \"\"\"Update the database schema to increase role column size.\"\"\" print ( \"Updating database schema...\" ) with app . app_context (): # This would typically be done with Alembic or raw SQL # For now, we'll just print the SQL that needs to be run print ( \" \\n IMPORTANT: You need to run the following SQL command:\" ) print ( \"ALTER TABLE users ALTER COLUMN role TYPE VARCHAR(255);\" ) print ( \" \\n This increases the role column size to support multiple roles.\" ) print ( \"Run this command in your database before running the migration.\" ) verify_migration () Verify that the migration was successful. Source code in arb\\auth\\migrate_roles.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def verify_migration (): \"\"\"Verify that the migration was successful.\"\"\" print ( \" \\n Verifying migration...\" ) with app . app_context (): users = User . query . all () for user in users : roles = user . get_roles () print ( f \"User { user . email } : roles = { roles } \" ) # Test role checking methods if roles : test_role = roles [ 0 ] has_role = user . has_role ( test_role ) print ( f \" - has_role(' { test_role } '): { has_role } \" ) print ( \"Verification completed!\" )","title":"arb.auth.migrate_roles"},{"location":"reference/arb/auth/migrate_roles/#arbauthmigrate_roles","text":"Database migration script for multiple roles support. This script updates the existing 'role' column in the users table to support multiple roles using comma-separated values. Run this script once after deploying the updated User model.","title":"arb.auth.migrate_roles"},{"location":"reference/arb/auth/migrate_roles/#arb.auth.migrate_roles.create_migration_app","text":"Create a minimal Flask app for migration. Source code in arb\\auth\\migrate_roles.py 24 25 26 27 28 29 30 31 32 33 34 35 def create_migration_app (): \"\"\"Create a minimal Flask app for migration.\"\"\" app = Flask ( __name__ ) app . config . from_object ( get_config ()) # Initialize extensions portal_db . init_app ( app ) # Initialize auth package init_auth ( app = app , db = portal_db ) return app","title":"create_migration_app"},{"location":"reference/arb/auth/migrate_roles/#arb.auth.migrate_roles.migrate_role_column","text":"Migrate the role column to support multiple roles. Source code in arb\\auth\\migrate_roles.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def migrate_role_column (): \"\"\"Migrate the role column to support multiple roles.\"\"\" print ( \"Starting role column migration...\" ) with app . app_context (): # Get all users users = User . query . all () print ( f \"Found { len ( users ) } users to migrate\" ) migrated_count = 0 skipped_count = 0 for user in users : current_role = user . role # Skip if already in multiple role format if ',' in current_role : print ( f \"Skipping user { user . email } - already has multiple roles: { current_role } \" ) skipped_count += 1 continue # Convert single role to list format if current_role and current_role . strip (): old_role = current_role user . set_roles ([ current_role . strip ()]) print ( f \"Migrated user { user . email } : ' { old_role } ' -> ' { user . role } '\" ) else : old_role = current_role user . set_roles ([ 'user' ]) print ( f \"Migrated user { user . email } : ' { old_role } ' -> 'user'\" ) migrated_count += 1 # Commit all changes get_db () . session . commit () print ( f \" \\n Migration completed!\" ) print ( f \"Users migrated: { migrated_count } \" ) print ( f \"Users skipped: { skipped_count } \" ) print ( f \"Total processed: { migrated_count + skipped_count } \" )","title":"migrate_role_column"},{"location":"reference/arb/auth/migrate_roles/#arb.auth.migrate_roles.update_database_schema","text":"Update the database schema to increase role column size. Source code in arb\\auth\\migrate_roles.py 102 103 104 105 106 107 108 109 110 111 112 113 114 def update_database_schema (): \"\"\"Update the database schema to increase role column size.\"\"\" print ( \"Updating database schema...\" ) with app . app_context (): # This would typically be done with Alembic or raw SQL # For now, we'll just print the SQL that needs to be run print ( \" \\n IMPORTANT: You need to run the following SQL command:\" ) print ( \"ALTER TABLE users ALTER COLUMN role TYPE VARCHAR(255);\" ) print ( \" \\n This increases the role column size to support multiple roles.\" ) print ( \"Run this command in your database before running the migration.\" )","title":"update_database_schema"},{"location":"reference/arb/auth/migrate_roles/#arb.auth.migrate_roles.verify_migration","text":"Verify that the migration was successful. Source code in arb\\auth\\migrate_roles.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def verify_migration (): \"\"\"Verify that the migration was successful.\"\"\" print ( \" \\n Verifying migration...\" ) with app . app_context (): users = User . query . all () for user in users : roles = user . get_roles () print ( f \"User { user . email } : roles = { roles } \" ) # Test role checking methods if roles : test_role = roles [ 0 ] has_role = user . has_role ( test_role ) print ( f \" - has_role(' { test_role } '): { has_role } \" ) print ( \"Verification completed!\" )","title":"verify_migration"},{"location":"reference/arb/auth/models/","text":"arb.auth.models User model and authentication/authorization logic for ARB Feedback Portal. Current Implementation: - Uses a local database for user authentication and management. - Handles password hashing, email confirmation, and role-based access. - Supports multiple roles per user using comma-separated values. Okta Transition Plan: - This system is a temporary solution until Okta (OIDC/SAML) is integrated. - When Okta is adopted, authentication will be delegated to Okta, and user sessions will be managed via Okta tokens. - User roles and permissions will be mapped from Okta claims or groups. - The User model and authorization checks are designed to be compatible with Okta-provided user info. - Minimal changes will be needed outside the login and user provisioning logic. Key Features: - Inherits from UserMixin (Flask-Login) to provide default implementations for authentication state. - Stores user credentials, account status, and role for access control. - Supports password hashing, password reset, email confirmation, and account lockout. - Provides methods for role-based authorization (e.g., is_admin, has_role). - Supports multiple roles per user (comma-separated). get_auth_config ( key , default = None ) Helper to fetch auth-related config from app.config, falling back to default_settings.py if not set. Usage: get_auth_config('PASSWORD_RESET_EXPIRATION', 3600) Source code in arb\\auth\\models.py 38 39 40 41 42 43 44 45 46 def get_auth_config ( key , default = None ): \"\"\" Helper to fetch auth-related config from app.config, falling back to default_settings.py if not set. Usage: get_auth_config('PASSWORD_RESET_EXPIRATION', 3600) \"\"\" try : return current_app . config . get ( f 'AUTH_ { key } ' , default ) except Exception : return default","title":"arb.auth.models"},{"location":"reference/arb/auth/models/#arbauthmodels","text":"User model and authentication/authorization logic for ARB Feedback Portal. Current Implementation: - Uses a local database for user authentication and management. - Handles password hashing, email confirmation, and role-based access. - Supports multiple roles per user using comma-separated values. Okta Transition Plan: - This system is a temporary solution until Okta (OIDC/SAML) is integrated. - When Okta is adopted, authentication will be delegated to Okta, and user sessions will be managed via Okta tokens. - User roles and permissions will be mapped from Okta claims or groups. - The User model and authorization checks are designed to be compatible with Okta-provided user info. - Minimal changes will be needed outside the login and user provisioning logic. Key Features: - Inherits from UserMixin (Flask-Login) to provide default implementations for authentication state. - Stores user credentials, account status, and role for access control. - Supports password hashing, password reset, email confirmation, and account lockout. - Provides methods for role-based authorization (e.g., is_admin, has_role). - Supports multiple roles per user (comma-separated).","title":"arb.auth.models"},{"location":"reference/arb/auth/models/#arb.auth.models.get_auth_config","text":"Helper to fetch auth-related config from app.config, falling back to default_settings.py if not set. Usage: get_auth_config('PASSWORD_RESET_EXPIRATION', 3600) Source code in arb\\auth\\models.py 38 39 40 41 42 43 44 45 46 def get_auth_config ( key , default = None ): \"\"\" Helper to fetch auth-related config from app.config, falling back to default_settings.py if not set. Usage: get_auth_config('PASSWORD_RESET_EXPIRATION', 3600) \"\"\" try : return current_app . config . get ( f 'AUTH_ { key } ' , default ) except Exception : return default","title":"get_auth_config"},{"location":"reference/arb/auth/okta_settings/","text":"arb.auth.okta_settings Okta Integration Settings for ARB Feedback Portal Current State: - The portal uses a local database for authentication and authorization. - The USE_OKTA flag controls whether Okta (OIDC/SAML) is used for authentication. Transition Plan: - When Okta is adopted, set USE_OKTA = True. - All routes and functions that check USE_OKTA will switch to Okta-based logic. - This file documents the transition plan and serves as a reference for Okta-related configuration. How to Identify Okta-Ready Code: - Any route or function that checks USE_OKTA is ready for Okta integration. - TODOs and NotImplementedError messages indicate what Okta information is needed. To enable Okta, set USE_OKTA = True and provide required Okta configuration below.","title":"arb.auth.okta_settings"},{"location":"reference/arb/auth/okta_settings/#arbauthokta_settings","text":"Okta Integration Settings for ARB Feedback Portal Current State: - The portal uses a local database for authentication and authorization. - The USE_OKTA flag controls whether Okta (OIDC/SAML) is used for authentication. Transition Plan: - When Okta is adopted, set USE_OKTA = True. - All routes and functions that check USE_OKTA will switch to Okta-based logic. - This file documents the transition plan and serves as a reference for Okta-related configuration. How to Identify Okta-Ready Code: - Any route or function that checks USE_OKTA is ready for Okta integration. - TODOs and NotImplementedError messages indicate what Okta information is needed. To enable Okta, set USE_OKTA = True and provide required Okta configuration below.","title":"arb.auth.okta_settings"},{"location":"reference/arb/auth/role_decorators/","text":"arb.auth.role_decorators Enhanced role decorators for ARB Feedback Portal. This module provides decorators for flexible role-based access control that supports multiple roles per user. Decorators: - roles_required: Access if user has ANY of the specified roles - all_roles_required: Access if user has ALL of the specified roles - role_required: Access if user has the specified role - admin_required: Access if user has admin role (existing functionality) admin_required ( f ) Decorator to restrict access to admin users only. - If USE_OKTA is True, checks Okta claims/groups for admin access. - If USE_OKTA is False, uses local role field. Source code in arb\\auth\\role_decorators.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def admin_required ( f ): \"\"\" Decorator to restrict access to admin users only. - If USE_OKTA is True, checks Okta claims/groups for admin access. - If USE_OKTA is False, uses local role field. \"\"\" @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for 'admin' group/claim. # Need: Okta group/claim mapping for admin users. raise NotImplementedError ( \"Okta admin check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . is_admin (): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function all_roles_required ( * roles ) Decorator to restrict access to users with ALL of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for all role access. - If USE_OKTA is False, uses local role field. Parameters: *roles ( str , default: () ) \u2013 Variable number of role names to check for. Example @all_roles_required('editor', 'qaqc') def qaqc_edit_page(): # Only users with BOTH 'editor' AND 'qaqc' roles can access pass Source code in arb\\auth\\role_decorators.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def all_roles_required ( * roles : str ): \"\"\" Decorator to restrict access to users with ALL of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for all role access. - If USE_OKTA is False, uses local role field. Args: *roles: Variable number of role names to check for. Example: @all_roles_required('editor', 'qaqc') def qaqc_edit_page(): # Only users with BOTH 'editor' AND 'qaqc' roles can access pass \"\"\" def decorator ( f ): @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for all of the specified groups/claims. # Need: Okta group/claim mapping for role access. raise NotImplementedError ( \"Okta all roles check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . has_all_roles ( * roles ): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function return decorator role_required ( role_name ) Decorator to restrict access to users with the specified role. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Parameters: role_name ( str ) \u2013 The role name to check for. Example @role_required('editor') def edit_page(): # Only users with 'editor' role can access pass Source code in arb\\auth\\role_decorators.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def role_required ( role_name : str ): \"\"\" Decorator to restrict access to users with the specified role. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Args: role_name: The role name to check for. Example: @role_required('editor') def edit_page(): # Only users with 'editor' role can access pass \"\"\" def decorator ( f ): @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for the specified group/claim. # Need: Okta group/claim mapping for role access. raise NotImplementedError ( \"Okta role check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . has_role ( role_name ): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function return decorator roles_required ( * roles ) Decorator to restrict access to users with any of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Parameters: *roles ( str , default: () ) \u2013 Variable number of role names to check for. Example @roles_required('editor', 'reviewer') def edit_page(): # Only users with 'editor' OR 'reviewer' role can access pass Source code in arb\\auth\\role_decorators.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def roles_required ( * roles : str ): \"\"\" Decorator to restrict access to users with any of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Args: *roles: Variable number of role names to check for. Example: @roles_required('editor', 'reviewer') def edit_page(): # Only users with 'editor' OR 'reviewer' role can access pass \"\"\" def decorator ( f ): @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for any of the specified groups/claims. # Need: Okta group/claim mapping for role access. raise NotImplementedError ( \"Okta role check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . has_any_role ( * roles ): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function return decorator","title":"arb.auth.role_decorators"},{"location":"reference/arb/auth/role_decorators/#arbauthrole_decorators","text":"Enhanced role decorators for ARB Feedback Portal. This module provides decorators for flexible role-based access control that supports multiple roles per user. Decorators: - roles_required: Access if user has ANY of the specified roles - all_roles_required: Access if user has ALL of the specified roles - role_required: Access if user has the specified role - admin_required: Access if user has admin role (existing functionality)","title":"arb.auth.role_decorators"},{"location":"reference/arb/auth/role_decorators/#arb.auth.role_decorators.admin_required","text":"Decorator to restrict access to admin users only. - If USE_OKTA is True, checks Okta claims/groups for admin access. - If USE_OKTA is False, uses local role field. Source code in arb\\auth\\role_decorators.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def admin_required ( f ): \"\"\" Decorator to restrict access to admin users only. - If USE_OKTA is True, checks Okta claims/groups for admin access. - If USE_OKTA is False, uses local role field. \"\"\" @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for 'admin' group/claim. # Need: Okta group/claim mapping for admin users. raise NotImplementedError ( \"Okta admin check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . is_admin (): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function","title":"admin_required"},{"location":"reference/arb/auth/role_decorators/#arb.auth.role_decorators.all_roles_required","text":"Decorator to restrict access to users with ALL of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for all role access. - If USE_OKTA is False, uses local role field. Parameters: *roles ( str , default: () ) \u2013 Variable number of role names to check for. Example @all_roles_required('editor', 'qaqc') def qaqc_edit_page(): # Only users with BOTH 'editor' AND 'qaqc' roles can access pass Source code in arb\\auth\\role_decorators.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def all_roles_required ( * roles : str ): \"\"\" Decorator to restrict access to users with ALL of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for all role access. - If USE_OKTA is False, uses local role field. Args: *roles: Variable number of role names to check for. Example: @all_roles_required('editor', 'qaqc') def qaqc_edit_page(): # Only users with BOTH 'editor' AND 'qaqc' roles can access pass \"\"\" def decorator ( f ): @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for all of the specified groups/claims. # Need: Okta group/claim mapping for role access. raise NotImplementedError ( \"Okta all roles check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . has_all_roles ( * roles ): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function return decorator","title":"all_roles_required"},{"location":"reference/arb/auth/role_decorators/#arb.auth.role_decorators.role_required","text":"Decorator to restrict access to users with the specified role. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Parameters: role_name ( str ) \u2013 The role name to check for. Example @role_required('editor') def edit_page(): # Only users with 'editor' role can access pass Source code in arb\\auth\\role_decorators.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def role_required ( role_name : str ): \"\"\" Decorator to restrict access to users with the specified role. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Args: role_name: The role name to check for. Example: @role_required('editor') def edit_page(): # Only users with 'editor' role can access pass \"\"\" def decorator ( f ): @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for the specified group/claim. # Need: Okta group/claim mapping for role access. raise NotImplementedError ( \"Okta role check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . has_role ( role_name ): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function return decorator","title":"role_required"},{"location":"reference/arb/auth/role_decorators/#arb.auth.role_decorators.roles_required","text":"Decorator to restrict access to users with any of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Parameters: *roles ( str , default: () ) \u2013 Variable number of role names to check for. Example @roles_required('editor', 'reviewer') def edit_page(): # Only users with 'editor' OR 'reviewer' role can access pass Source code in arb\\auth\\role_decorators.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def roles_required ( * roles : str ): \"\"\" Decorator to restrict access to users with any of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Args: *roles: Variable number of role names to check for. Example: @roles_required('editor', 'reviewer') def edit_page(): # Only users with 'editor' OR 'reviewer' role can access pass \"\"\" def decorator ( f ): @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for any of the specified groups/claims. # Need: Okta group/claim mapping for role access. raise NotImplementedError ( \"Okta role check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . has_any_role ( * roles ): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function return decorator","title":"roles_required"},{"location":"reference/arb/auth/role_examples/","text":"arb.auth.role_examples Examples of how to use the enhanced multiple roles functionality. This file demonstrates various ways to work with multiple roles in the ARB Feedback Portal authentication system. admin_panel () Only users with 'admin' role can access. Source code in arb\\auth\\role_examples.py 43 44 45 46 47 @example . route ( '/admin-panel' ) @admin_required def admin_panel (): \"\"\"Only users with 'admin' role can access.\"\"\" return render_template ( 'admin_panel.html' ) advanced_edit_page () Only users with BOTH 'editor' AND 'qaqc' roles can access. Source code in arb\\auth\\role_examples.py 35 36 37 38 39 @example . route ( '/advanced-edit' ) @all_roles_required ( 'editor' , 'qaqc' ) def advanced_edit_page (): \"\"\"Only users with BOTH 'editor' AND 'qaqc' roles can access.\"\"\" return render_template ( 'advanced_edit.html' ) dashboard () Dashboard with role-based UI elements. Source code in arb\\auth\\role_examples.py 72 73 74 75 76 @example . route ( '/dashboard' ) @login_required def dashboard (): \"\"\"Dashboard with role-based UI elements.\"\"\" return render_template ( 'dashboard.html' ) editor_only_page () Only users with 'editor' role can access this page. Source code in arb\\auth\\role_examples.py 19 20 21 22 23 @example . route ( '/editor-only' ) @role_required ( 'editor' ) def editor_only_page (): \"\"\"Only users with 'editor' role can access this page.\"\"\" return render_template ( 'editor_only.html' ) flexible_access_page () Manual role checking for complex logic. Source code in arb\\auth\\role_examples.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @example . route ( '/flexible-access' ) @login_required def flexible_access_page (): \"\"\"Manual role checking for complex logic.\"\"\" # Check for specific roles if current_user . has_role ( 'admin' ): # Admin gets full access return render_template ( 'admin_view.html' ) elif current_user . has_any_role ( 'editor' , 'reviewer' ): # Editors and reviewers get edit access return render_template ( 'editor_view.html' ) elif current_user . has_role ( 'viewer' ): # Viewers get read-only access return render_template ( 'viewer_view.html' ) else : # Default user access return render_template ( 'basic_view.html' ) manage_user_roles_example () Example of how to programmatically manage user roles. Source code in arb\\auth\\role_examples.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def manage_user_roles_example (): \"\"\"Example of how to programmatically manage user roles.\"\"\" # Get a user user = User . query . filter_by ( email = 'example@carb.ca.gov' ) . first () if user : # Check current roles print ( f \"Current roles: { user . get_roles () } \" ) # Add a new role user . add_role ( 'editor' ) print ( f \"After adding editor: { user . get_roles () } \" ) # Add another role user . add_role ( 'qaqc' ) print ( f \"After adding qaqc: { user . get_roles () } \" ) # Check specific roles print ( f \"Has editor role: { user . has_role ( 'editor' ) } \" ) print ( f \"Has any of editor/reviewer: { user . has_any_role ( 'editor' , 'reviewer' ) } \" ) print ( f \"Has all of editor/qaqc: { user . has_all_roles ( 'editor' , 'qaqc' ) } \" ) # Remove a role user . remove_role ( 'editor' ) print ( f \"After removing editor: { user . get_roles () } \" ) # Set roles to a specific list user . set_roles ([ 'reviewer' , 'qaqc' ]) print ( f \"After setting roles: { user . get_roles () } \" ) migrate_single_role_to_multiple () Helper function to migrate existing single-role users to multiple roles. Run this once after updating the database schema. Source code in arb\\auth\\role_examples.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def migrate_single_role_to_multiple (): \"\"\" Helper function to migrate existing single-role users to multiple roles. Run this once after updating the database schema. \"\"\" from arb.auth import get_db db = get_db () users = User . query . all () for user in users : # Get current single role current_role = user . role # If it's already comma-separated, skip if ',' in current_role : continue # Convert single role to list format if current_role and current_role . strip (): user . set_roles ([ current_role . strip ()]) else : user . set_roles ([ 'user' ]) db . session . commit () print ( \"Migration completed successfully!\" ) review_access_page () Users with 'editor' OR 'reviewer' OR 'qaqc' roles can access. Source code in arb\\auth\\role_examples.py 27 28 29 30 31 @example . route ( '/review-access' ) @roles_required ( 'editor' , 'reviewer' , 'qaqc' ) def review_access_page (): \"\"\"Users with 'editor' OR 'reviewer' OR 'qaqc' roles can access.\"\"\" return render_template ( 'review_access.html' )","title":"arb.auth.role_examples"},{"location":"reference/arb/auth/role_examples/#arbauthrole_examples","text":"Examples of how to use the enhanced multiple roles functionality. This file demonstrates various ways to work with multiple roles in the ARB Feedback Portal authentication system.","title":"arb.auth.role_examples"},{"location":"reference/arb/auth/role_examples/#arb.auth.role_examples.admin_panel","text":"Only users with 'admin' role can access. Source code in arb\\auth\\role_examples.py 43 44 45 46 47 @example . route ( '/admin-panel' ) @admin_required def admin_panel (): \"\"\"Only users with 'admin' role can access.\"\"\" return render_template ( 'admin_panel.html' )","title":"admin_panel"},{"location":"reference/arb/auth/role_examples/#arb.auth.role_examples.advanced_edit_page","text":"Only users with BOTH 'editor' AND 'qaqc' roles can access. Source code in arb\\auth\\role_examples.py 35 36 37 38 39 @example . route ( '/advanced-edit' ) @all_roles_required ( 'editor' , 'qaqc' ) def advanced_edit_page (): \"\"\"Only users with BOTH 'editor' AND 'qaqc' roles can access.\"\"\" return render_template ( 'advanced_edit.html' )","title":"advanced_edit_page"},{"location":"reference/arb/auth/role_examples/#arb.auth.role_examples.dashboard","text":"Dashboard with role-based UI elements. Source code in arb\\auth\\role_examples.py 72 73 74 75 76 @example . route ( '/dashboard' ) @login_required def dashboard (): \"\"\"Dashboard with role-based UI elements.\"\"\" return render_template ( 'dashboard.html' )","title":"dashboard"},{"location":"reference/arb/auth/role_examples/#arb.auth.role_examples.editor_only_page","text":"Only users with 'editor' role can access this page. Source code in arb\\auth\\role_examples.py 19 20 21 22 23 @example . route ( '/editor-only' ) @role_required ( 'editor' ) def editor_only_page (): \"\"\"Only users with 'editor' role can access this page.\"\"\" return render_template ( 'editor_only.html' )","title":"editor_only_page"},{"location":"reference/arb/auth/role_examples/#arb.auth.role_examples.flexible_access_page","text":"Manual role checking for complex logic. Source code in arb\\auth\\role_examples.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 @example . route ( '/flexible-access' ) @login_required def flexible_access_page (): \"\"\"Manual role checking for complex logic.\"\"\" # Check for specific roles if current_user . has_role ( 'admin' ): # Admin gets full access return render_template ( 'admin_view.html' ) elif current_user . has_any_role ( 'editor' , 'reviewer' ): # Editors and reviewers get edit access return render_template ( 'editor_view.html' ) elif current_user . has_role ( 'viewer' ): # Viewers get read-only access return render_template ( 'viewer_view.html' ) else : # Default user access return render_template ( 'basic_view.html' )","title":"flexible_access_page"},{"location":"reference/arb/auth/role_examples/#arb.auth.role_examples.manage_user_roles_example","text":"Example of how to programmatically manage user roles. Source code in arb\\auth\\role_examples.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def manage_user_roles_example (): \"\"\"Example of how to programmatically manage user roles.\"\"\" # Get a user user = User . query . filter_by ( email = 'example@carb.ca.gov' ) . first () if user : # Check current roles print ( f \"Current roles: { user . get_roles () } \" ) # Add a new role user . add_role ( 'editor' ) print ( f \"After adding editor: { user . get_roles () } \" ) # Add another role user . add_role ( 'qaqc' ) print ( f \"After adding qaqc: { user . get_roles () } \" ) # Check specific roles print ( f \"Has editor role: { user . has_role ( 'editor' ) } \" ) print ( f \"Has any of editor/reviewer: { user . has_any_role ( 'editor' , 'reviewer' ) } \" ) print ( f \"Has all of editor/qaqc: { user . has_all_roles ( 'editor' , 'qaqc' ) } \" ) # Remove a role user . remove_role ( 'editor' ) print ( f \"After removing editor: { user . get_roles () } \" ) # Set roles to a specific list user . set_roles ([ 'reviewer' , 'qaqc' ]) print ( f \"After setting roles: { user . get_roles () } \" )","title":"manage_user_roles_example"},{"location":"reference/arb/auth/role_examples/#arb.auth.role_examples.migrate_single_role_to_multiple","text":"Helper function to migrate existing single-role users to multiple roles. Run this once after updating the database schema. Source code in arb\\auth\\role_examples.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def migrate_single_role_to_multiple (): \"\"\" Helper function to migrate existing single-role users to multiple roles. Run this once after updating the database schema. \"\"\" from arb.auth import get_db db = get_db () users = User . query . all () for user in users : # Get current single role current_role = user . role # If it's already comma-separated, skip if ',' in current_role : continue # Convert single role to list format if current_role and current_role . strip (): user . set_roles ([ current_role . strip ()]) else : user . set_roles ([ 'user' ]) db . session . commit () print ( \"Migration completed successfully!\" )","title":"migrate_single_role_to_multiple"},{"location":"reference/arb/auth/role_examples/#arb.auth.role_examples.review_access_page","text":"Users with 'editor' OR 'reviewer' OR 'qaqc' roles can access. Source code in arb\\auth\\role_examples.py 27 28 29 30 31 @example . route ( '/review-access' ) @roles_required ( 'editor' , 'reviewer' , 'qaqc' ) def review_access_page (): \"\"\"Users with 'editor' OR 'reviewer' OR 'qaqc' roles can access.\"\"\" return render_template ( 'review_access.html' )","title":"review_access_page"},{"location":"reference/arb/auth/routes/","text":"arb.auth.routes Authentication routes for ARB Feedback Portal. The 'auth' Blueprint is defined here and should be registered by the host app (see init .py for details). This supports both standalone and pluggable usage of the auth package. Current Implementation: - Handles user login, logout, registration, and account management using a local database and Flask-Login. - Provides decorators and routes for access control (e.g., admin-only pages). - Supports multiple roles per user with flexible role-based access control. Okta Transition Plan: - This authentication system is a temporary solution until Okta (OIDC/SAML) is integrated. - When Okta is adopted, authentication and session management will be delegated to Okta, and user identity will be managed via Okta tokens. - Role and permission checks (e.g., admin_required) will be mapped from Okta claims or groups. - Most route logic will remain, but login/logout and user provisioning will be replaced or adapted for Okta. Key Features: - Modular design to allow easy replacement of authentication backend. - All access control decorators and logic are compatible with external identity providers. - The USE_OKTA flag below controls which authentication backend is used. - Multiple role support with flexible decorators for different access patterns. activity_log () Render the user activity log page (placeholder). In Okta, activity logs may be available via Okta or remain app-specific. Source code in arb\\auth\\routes.py 266 267 268 269 270 271 272 273 274 275 276 277 278 @auth . route ( '/activity_log' ) @login_required def activity_log (): \"\"\" Render the user activity log page (placeholder). In Okta, activity logs may be available via Okta or remain app-specific. \"\"\" if USE_OKTA : # TODO: Integrate with Okta activity log if available, or display app-specific log. # Need: Okta activity log API or documentation. raise NotImplementedError ( \"Okta activity log integration not implemented yet.\" ) else : return render_template ( 'auth/activity_log.html' ) admin_dashboard () Render the admin-only dashboard page. In Okta, access would be restricted based on Okta admin group/claim. Source code in arb\\auth\\routes.py 165 166 167 168 169 170 171 172 173 174 175 176 177 @auth . route ( '/admin/dashboard' ) @admin_required def admin_dashboard (): \"\"\" Render the admin-only dashboard page. In Okta, access would be restricted based on Okta admin group/claim. \"\"\" if USE_OKTA : # TODO: Render admin dashboard for Okta-authenticated admin users. # Need: Okta session and group/claim context. raise NotImplementedError ( \"Okta admin dashboard not implemented yet.\" ) else : return render_template ( 'auth/admin_dashboard.html' ) admin_required ( f ) Decorator to restrict access to admin users only. - If USE_OKTA is True, checks Okta claims/groups for admin access. - If USE_OKTA is False, uses local role field. Source code in arb\\auth\\routes.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def admin_required ( f ): \"\"\" Decorator to restrict access to admin users only. - If USE_OKTA is True, checks Okta claims/groups for admin access. - If USE_OKTA is False, uses local role field. \"\"\" @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for 'admin' group/claim. # Need: Okta group/claim mapping for admin users. raise NotImplementedError ( \"Okta admin check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . is_admin (): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function advanced_edit () Render the advanced editing page. Only users with BOTH 'editor' AND 'qaqc' roles can access. Source code in arb\\auth\\routes.py 222 223 224 225 226 227 228 229 230 231 232 233 @auth . route ( '/advanced/edit' ) @all_roles_required ( 'editor' , 'qaqc' ) def advanced_edit (): \"\"\" Render the advanced editing page. Only users with BOTH 'editor' AND 'qaqc' roles can access. \"\"\" if USE_OKTA : # TODO: Render advanced edit page for Okta-authenticated users with all required roles. raise NotImplementedError ( \"Okta advanced edit not implemented yet.\" ) else : return render_template ( 'auth/advanced_edit.html' ) all_roles_required ( * roles ) Decorator to restrict access to users with ALL of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for all role access. - If USE_OKTA is False, uses local role field. Parameters: *roles ( str , default: () ) \u2013 Variable number of role names to check for. Example @all_roles_required('editor', 'qaqc') def qaqc_edit_page(): # Only users with BOTH 'editor' AND 'qaqc' roles can access pass Source code in arb\\auth\\routes.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def all_roles_required ( * roles : str ): \"\"\" Decorator to restrict access to users with ALL of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for all role access. - If USE_OKTA is False, uses local role field. Args: *roles: Variable number of role names to check for. Example: @all_roles_required('editor', 'qaqc') def qaqc_edit_page(): # Only users with BOTH 'editor' AND 'qaqc' roles can access pass \"\"\" def decorator ( f ): @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for all of the specified groups/claims. # Need: Okta group/claim mapping for role access. raise NotImplementedError ( \"Okta all roles check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . has_all_roles ( * roles ): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function return decorator confirm_email ( token ) Email confirmation route. Verifies the email confirmation token and confirms the user's account. Source code in arb\\auth\\routes.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 @auth . route ( '/confirm-email/<token>' ) def confirm_email ( token ): \"\"\" Email confirmation route. Verifies the email confirmation token and confirms the user's account. \"\"\" logger . debug ( f \"Email confirmation route accessed with token: { token [: 10 ] } ...\" ) if USE_OKTA : # TODO: Implement Okta email confirmation if needed. raise NotImplementedError ( \"Okta email confirmation not implemented yet.\" ) else : User = get_user_model () # Find user by email confirmation token user = User . query . filter_by ( email_confirmation_token = token ) . first () if not user : logger . debug ( \"Email confirmation failed: invalid token\" ) flash ( 'Invalid or expired confirmation link.' , 'danger' ) return redirect ( url_for ( 'auth.login' )) # Verify the token if user . verify_email_confirmation_token ( token ): logger . debug ( f \"Email confirmed successfully for user: { user . email } \" ) flash ( 'Your email has been confirmed successfully! You can now log in.' , 'success' ) else : logger . debug ( f \"Email confirmation failed: expired token for user: { user . email } \" ) flash ( 'The confirmation link has expired. Please register again or contact support.' , 'danger' ) return redirect ( url_for ( 'auth.login' )) editor_dashboard () Render the editor-only dashboard page. Only users with 'editor' role can access. Source code in arb\\auth\\routes.py 180 181 182 183 184 185 186 187 188 189 190 191 @auth . route ( '/editor/dashboard' ) @role_required ( 'editor' ) def editor_dashboard (): \"\"\" Render the editor-only dashboard page. Only users with 'editor' role can access. \"\"\" if USE_OKTA : # TODO: Render editor dashboard for Okta-authenticated editor users. raise NotImplementedError ( \"Okta editor dashboard not implemented yet.\" ) else : return render_template ( 'auth/editor_dashboard.html' ) email_preferences () Render the user email preferences page (placeholder). In Okta, notification preferences may be managed in Okta or remain app-specific. Source code in arb\\auth\\routes.py 251 252 253 254 255 256 257 258 259 260 261 262 263 @auth . route ( '/email_preferences' ) @login_required def email_preferences (): \"\"\" Render the user email preferences page (placeholder). In Okta, notification preferences may be managed in Okta or remain app-specific. \"\"\" if USE_OKTA : # TODO: Integrate with Okta notification/email preferences if available. # Need: Okta notification preferences API or documentation. raise NotImplementedError ( \"Okta email preferences integration not implemented yet.\" ) else : return render_template ( 'auth/email_preferences.html' ) login () User login route. - If USE_OKTA is False, handles local login form and session creation. - If USE_OKTA is True, delegates to Okta login (not yet implemented). Source code in arb\\auth\\routes.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 @auth . route ( '/login' , methods = [ 'GET' , 'POST' ]) def login (): \"\"\" User login route. - If USE_OKTA is False, handles local login form and session creation. - If USE_OKTA is True, delegates to Okta login (not yet implemented). \"\"\" logger . debug ( f \"Login route accessed - Method: { request . method } \" ) if USE_OKTA : # TODO: Implement Okta login redirect/flow. # Need: Okta OIDC/SAML login integration. raise NotImplementedError ( \"Okta login not implemented yet. Need Okta OIDC/SAML integration.\" ) else : if request . method == 'POST' : logger . debug ( \"Processing login POST request\" ) email = request . form . get ( 'email' ) password = request . form . get ( 'password' ) logger . debug ( f \"Login attempt for email: { email } \" ) User = get_user_model () user = User . query . filter_by ( email = email ) . first () if user : logger . debug ( f \"User found: { user . email } , checking password\" ) if user . check_password ( password ): logger . debug ( f \"Password check successful for user: { user . email } \" ) # Check if email is confirmed if not user . is_confirmed : logger . debug ( f \"Login failed: email not confirmed for user: { user . email } \" ) flash ( 'Please confirm your email address before logging in. Check your email for a confirmation link.' , 'warning' ) return render_template ( 'auth/login.html' ) login_user ( user ) flash ( 'Logged in successfully.' , 'success' ) logger . debug ( f \"User { user . email } logged in successfully\" ) return redirect ( url_for ( 'main.index' )) else : logger . debug ( f \"Password check failed for user: { user . email } \" ) flash ( 'Invalid email or password.' , 'danger' ) else : logger . debug ( f \"No user found for email: { email } \" ) flash ( 'Invalid email or password.' , 'danger' ) else : logger . debug ( \"Rendering login form\" ) return render_template ( 'auth/login.html' ) logout () User logout route. - If USE_OKTA is False, logs out the user locally. - If USE_OKTA is True, delegates to Okta logout (not yet implemented). Source code in arb\\auth\\routes.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 @auth . route ( '/logout' ) def logout (): \"\"\" User logout route. - If USE_OKTA is False, logs out the user locally. - If USE_OKTA is True, delegates to Okta logout (not yet implemented). \"\"\" if USE_OKTA : # TODO: Implement Okta logout redirect/flow. # Need: Okta logout endpoint/integration. raise NotImplementedError ( \"Okta logout not implemented yet. Need Okta logout integration.\" ) else : logout_user () flash ( 'You have been logged out.' , 'info' ) return redirect ( url_for ( 'auth.login' )) qaqc_dashboard () Render the QA/QC-only dashboard page. Only users with 'qaqc' role can access. Source code in arb\\auth\\routes.py 194 195 196 197 198 199 200 201 202 203 204 205 @auth . route ( '/qaqc/dashboard' ) @role_required ( 'qaqc' ) def qaqc_dashboard (): \"\"\" Render the QA/QC-only dashboard page. Only users with 'qaqc' role can access. \"\"\" if USE_OKTA : # TODO: Render QA/QC dashboard for Okta-authenticated qaqc users. raise NotImplementedError ( \"Okta qaqc dashboard not implemented yet.\" ) else : return render_template ( 'auth/qaqc_dashboard.html' ) register () User registration route. - If USE_OKTA is False, handles local registration form and user creation. - If USE_OKTA is True, delegates to Okta registration (not yet implemented). Source code in arb\\auth\\routes.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 @auth . route ( '/register' , methods = [ 'GET' , 'POST' ]) def register (): \"\"\" User registration route. - If USE_OKTA is False, handles local registration form and user creation. - If USE_OKTA is True, delegates to Okta registration (not yet implemented). \"\"\" logger . debug ( f \"Register route accessed - Method: { request . method } \" ) if USE_OKTA : # TODO: Implement Okta registration or link to Okta self-service registration. # Need: Okta registration endpoint or documentation. raise NotImplementedError ( \"Okta registration not implemented yet. Need Okta registration integration.\" ) else : if request . method == 'POST' : logger . debug ( \"Processing registration POST request\" ) User = get_user_model () email = request . form . get ( 'email' ) password = request . form . get ( 'password' ) logger . debug ( f \"Registration attempt for email: { email } \" ) if not email or not password : logger . debug ( \"Registration failed: missing email or password\" ) flash ( 'Email and password are required.' , 'danger' ) elif User . query . filter_by ( email = email ) . first (): logger . debug ( f \"Registration failed: email { email } already exists\" ) flash ( 'Email already registered.' , 'warning' ) else : logger . debug ( f \"Creating new user with email: { email } \" ) user = User () setattr ( user , 'email' , email ) user . set_password ( password ) # Keep user unconfirmed until email is verified user . is_confirmed_col = False get_db () . session . add ( user ) get_db () . session . commit () # Generate email confirmation token and send confirmation email try : from arb.auth.email_util import send_email_confirmation token = user . generate_email_confirmation_token () send_email_confirmation ( user , token ) logger . debug ( f \"Email confirmation sent to user: { email } \" ) flash ( 'Registration successful! Please check your email to confirm your account.' , 'success' ) except Exception as e : logger . error ( f \"Failed to send email confirmation to { email } : { str ( e ) } \" ) logger . error ( f \"Error type: { type ( e ) . __name__ } \" ) import traceback logger . error ( f \"Traceback: { traceback . format_exc () } \" ) # Still create the user, but inform them about the email issue flash ( 'Registration successful! However, there was an issue sending the confirmation email. Please contact support.' , 'warning' ) return redirect ( url_for ( 'auth.login' )) else : logger . debug ( \"Rendering registration form\" ) return render_template ( 'auth/register.html' ) resend_confirmation () Resend email confirmation route. Allows users to request a new confirmation email if they didn't receive the first one. Source code in arb\\auth\\routes.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 @auth . route ( '/resend-confirmation' , methods = [ 'GET' , 'POST' ]) def resend_confirmation (): \"\"\" Resend email confirmation route. Allows users to request a new confirmation email if they didn't receive the first one. \"\"\" logger . debug ( f \"Resend confirmation route accessed - Method: { request . method } \" ) if USE_OKTA : # TODO: Implement Okta resend confirmation if needed. raise NotImplementedError ( \"Okta resend confirmation not implemented yet.\" ) else : if request . method == 'POST' : email = request . form . get ( 'email' ) logger . debug ( f \"Resend confirmation request for email: { email } \" ) if not email : flash ( 'Please enter your email address.' , 'danger' ) else : User = get_user_model () user = User . query . filter_by ( email = email ) . first () if not user : # Don't reveal if email exists or not for security flash ( 'If an account with that email exists, a confirmation email has been sent.' , 'info' ) elif user . is_confirmed : flash ( 'Your email is already confirmed. You can log in.' , 'info' ) else : # Generate new confirmation token and send email try : from arb.auth.email_util import send_email_confirmation token = user . generate_email_confirmation_token () send_email_confirmation ( user , token ) logger . debug ( f \"Confirmation email resent to user: { email } \" ) flash ( 'A new confirmation email has been sent. Please check your inbox.' , 'success' ) except Exception as e : logger . error ( f \"Failed to resend confirmation email to { email } : { e } \" ) flash ( 'There was an issue sending the confirmation email. Please try again later.' , 'danger' ) return redirect ( url_for ( 'auth.login' )) else : return render_template ( 'auth/resend_confirmation.html' ) review_dashboard () Render the review dashboard page. Users with 'editor', 'reviewer', OR 'qaqc' roles can access. Source code in arb\\auth\\routes.py 208 209 210 211 212 213 214 215 216 217 218 219 @auth . route ( '/review/dashboard' ) @roles_required ( 'editor' , 'reviewer' , 'qaqc' ) def review_dashboard (): \"\"\" Render the review dashboard page. Users with 'editor', 'reviewer', OR 'qaqc' roles can access. \"\"\" if USE_OKTA : # TODO: Render review dashboard for Okta-authenticated users with appropriate roles. raise NotImplementedError ( \"Okta review dashboard not implemented yet.\" ) else : return render_template ( 'auth/review_dashboard.html' ) role_required ( role_name ) Decorator to restrict access to users with the specified role. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Parameters: role_name ( str ) \u2013 The role name to check for. Example @role_required('editor') def edit_page(): # Only users with 'editor' role can access pass Source code in arb\\auth\\routes.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def role_required ( role_name : str ): \"\"\" Decorator to restrict access to users with the specified role. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Args: role_name: The role name to check for. Example: @role_required('editor') def edit_page(): # Only users with 'editor' role can access pass \"\"\" def decorator ( f ): @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for the specified group/claim. # Need: Okta group/claim mapping for role access. raise NotImplementedError ( \"Okta role check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . has_role ( role_name ): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function return decorator roles_required ( * roles ) Decorator to restrict access to users with any of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Parameters: *roles ( str , default: () ) \u2013 Variable number of role names to check for. Example @roles_required('editor', 'reviewer') def edit_page(): # Only users with 'editor' OR 'reviewer' role can access pass Source code in arb\\auth\\routes.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def roles_required ( * roles : str ): \"\"\" Decorator to restrict access to users with any of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Args: *roles: Variable number of role names to check for. Example: @roles_required('editor', 'reviewer') def edit_page(): # Only users with 'editor' OR 'reviewer' role can access pass \"\"\" def decorator ( f ): @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for any of the specified groups/claims. # Need: Okta group/claim mapping for role access. raise NotImplementedError ( \"Okta role check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . has_any_role ( * roles ): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function return decorator settings () Render the user account settings page (placeholder). In Okta, this page may display Okta-managed profile info or link to Okta profile management. Source code in arb\\auth\\routes.py 236 237 238 239 240 241 242 243 244 245 246 247 248 @auth . route ( '/settings' ) @login_required def settings (): \"\"\" Render the user account settings page (placeholder). In Okta, this page may display Okta-managed profile info or link to Okta profile management. \"\"\" if USE_OKTA : # TODO: Display Okta profile info or link to Okta profile management. # Need: Okta user info endpoint and integration. raise NotImplementedError ( \"Okta settings/profile integration not implemented yet.\" ) else : return render_template ( 'auth/settings.html' )","title":"arb.auth.routes"},{"location":"reference/arb/auth/routes/#arbauthroutes","text":"Authentication routes for ARB Feedback Portal. The 'auth' Blueprint is defined here and should be registered by the host app (see init .py for details). This supports both standalone and pluggable usage of the auth package. Current Implementation: - Handles user login, logout, registration, and account management using a local database and Flask-Login. - Provides decorators and routes for access control (e.g., admin-only pages). - Supports multiple roles per user with flexible role-based access control. Okta Transition Plan: - This authentication system is a temporary solution until Okta (OIDC/SAML) is integrated. - When Okta is adopted, authentication and session management will be delegated to Okta, and user identity will be managed via Okta tokens. - Role and permission checks (e.g., admin_required) will be mapped from Okta claims or groups. - Most route logic will remain, but login/logout and user provisioning will be replaced or adapted for Okta. Key Features: - Modular design to allow easy replacement of authentication backend. - All access control decorators and logic are compatible with external identity providers. - The USE_OKTA flag below controls which authentication backend is used. - Multiple role support with flexible decorators for different access patterns.","title":"arb.auth.routes"},{"location":"reference/arb/auth/routes/#arb.auth.routes.activity_log","text":"Render the user activity log page (placeholder). In Okta, activity logs may be available via Okta or remain app-specific. Source code in arb\\auth\\routes.py 266 267 268 269 270 271 272 273 274 275 276 277 278 @auth . route ( '/activity_log' ) @login_required def activity_log (): \"\"\" Render the user activity log page (placeholder). In Okta, activity logs may be available via Okta or remain app-specific. \"\"\" if USE_OKTA : # TODO: Integrate with Okta activity log if available, or display app-specific log. # Need: Okta activity log API or documentation. raise NotImplementedError ( \"Okta activity log integration not implemented yet.\" ) else : return render_template ( 'auth/activity_log.html' )","title":"activity_log"},{"location":"reference/arb/auth/routes/#arb.auth.routes.admin_dashboard","text":"Render the admin-only dashboard page. In Okta, access would be restricted based on Okta admin group/claim. Source code in arb\\auth\\routes.py 165 166 167 168 169 170 171 172 173 174 175 176 177 @auth . route ( '/admin/dashboard' ) @admin_required def admin_dashboard (): \"\"\" Render the admin-only dashboard page. In Okta, access would be restricted based on Okta admin group/claim. \"\"\" if USE_OKTA : # TODO: Render admin dashboard for Okta-authenticated admin users. # Need: Okta session and group/claim context. raise NotImplementedError ( \"Okta admin dashboard not implemented yet.\" ) else : return render_template ( 'auth/admin_dashboard.html' )","title":"admin_dashboard"},{"location":"reference/arb/auth/routes/#arb.auth.routes.admin_required","text":"Decorator to restrict access to admin users only. - If USE_OKTA is True, checks Okta claims/groups for admin access. - If USE_OKTA is False, uses local role field. Source code in arb\\auth\\routes.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def admin_required ( f ): \"\"\" Decorator to restrict access to admin users only. - If USE_OKTA is True, checks Okta claims/groups for admin access. - If USE_OKTA is False, uses local role field. \"\"\" @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for 'admin' group/claim. # Need: Okta group/claim mapping for admin users. raise NotImplementedError ( \"Okta admin check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . is_admin (): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function","title":"admin_required"},{"location":"reference/arb/auth/routes/#arb.auth.routes.advanced_edit","text":"Render the advanced editing page. Only users with BOTH 'editor' AND 'qaqc' roles can access. Source code in arb\\auth\\routes.py 222 223 224 225 226 227 228 229 230 231 232 233 @auth . route ( '/advanced/edit' ) @all_roles_required ( 'editor' , 'qaqc' ) def advanced_edit (): \"\"\" Render the advanced editing page. Only users with BOTH 'editor' AND 'qaqc' roles can access. \"\"\" if USE_OKTA : # TODO: Render advanced edit page for Okta-authenticated users with all required roles. raise NotImplementedError ( \"Okta advanced edit not implemented yet.\" ) else : return render_template ( 'auth/advanced_edit.html' )","title":"advanced_edit"},{"location":"reference/arb/auth/routes/#arb.auth.routes.all_roles_required","text":"Decorator to restrict access to users with ALL of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for all role access. - If USE_OKTA is False, uses local role field. Parameters: *roles ( str , default: () ) \u2013 Variable number of role names to check for. Example @all_roles_required('editor', 'qaqc') def qaqc_edit_page(): # Only users with BOTH 'editor' AND 'qaqc' roles can access pass Source code in arb\\auth\\routes.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def all_roles_required ( * roles : str ): \"\"\" Decorator to restrict access to users with ALL of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for all role access. - If USE_OKTA is False, uses local role field. Args: *roles: Variable number of role names to check for. Example: @all_roles_required('editor', 'qaqc') def qaqc_edit_page(): # Only users with BOTH 'editor' AND 'qaqc' roles can access pass \"\"\" def decorator ( f ): @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for all of the specified groups/claims. # Need: Okta group/claim mapping for role access. raise NotImplementedError ( \"Okta all roles check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . has_all_roles ( * roles ): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function return decorator","title":"all_roles_required"},{"location":"reference/arb/auth/routes/#arb.auth.routes.confirm_email","text":"Email confirmation route. Verifies the email confirmation token and confirms the user's account. Source code in arb\\auth\\routes.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 @auth . route ( '/confirm-email/<token>' ) def confirm_email ( token ): \"\"\" Email confirmation route. Verifies the email confirmation token and confirms the user's account. \"\"\" logger . debug ( f \"Email confirmation route accessed with token: { token [: 10 ] } ...\" ) if USE_OKTA : # TODO: Implement Okta email confirmation if needed. raise NotImplementedError ( \"Okta email confirmation not implemented yet.\" ) else : User = get_user_model () # Find user by email confirmation token user = User . query . filter_by ( email_confirmation_token = token ) . first () if not user : logger . debug ( \"Email confirmation failed: invalid token\" ) flash ( 'Invalid or expired confirmation link.' , 'danger' ) return redirect ( url_for ( 'auth.login' )) # Verify the token if user . verify_email_confirmation_token ( token ): logger . debug ( f \"Email confirmed successfully for user: { user . email } \" ) flash ( 'Your email has been confirmed successfully! You can now log in.' , 'success' ) else : logger . debug ( f \"Email confirmation failed: expired token for user: { user . email } \" ) flash ( 'The confirmation link has expired. Please register again or contact support.' , 'danger' ) return redirect ( url_for ( 'auth.login' ))","title":"confirm_email"},{"location":"reference/arb/auth/routes/#arb.auth.routes.editor_dashboard","text":"Render the editor-only dashboard page. Only users with 'editor' role can access. Source code in arb\\auth\\routes.py 180 181 182 183 184 185 186 187 188 189 190 191 @auth . route ( '/editor/dashboard' ) @role_required ( 'editor' ) def editor_dashboard (): \"\"\" Render the editor-only dashboard page. Only users with 'editor' role can access. \"\"\" if USE_OKTA : # TODO: Render editor dashboard for Okta-authenticated editor users. raise NotImplementedError ( \"Okta editor dashboard not implemented yet.\" ) else : return render_template ( 'auth/editor_dashboard.html' )","title":"editor_dashboard"},{"location":"reference/arb/auth/routes/#arb.auth.routes.email_preferences","text":"Render the user email preferences page (placeholder). In Okta, notification preferences may be managed in Okta or remain app-specific. Source code in arb\\auth\\routes.py 251 252 253 254 255 256 257 258 259 260 261 262 263 @auth . route ( '/email_preferences' ) @login_required def email_preferences (): \"\"\" Render the user email preferences page (placeholder). In Okta, notification preferences may be managed in Okta or remain app-specific. \"\"\" if USE_OKTA : # TODO: Integrate with Okta notification/email preferences if available. # Need: Okta notification preferences API or documentation. raise NotImplementedError ( \"Okta email preferences integration not implemented yet.\" ) else : return render_template ( 'auth/email_preferences.html' )","title":"email_preferences"},{"location":"reference/arb/auth/routes/#arb.auth.routes.login","text":"User login route. - If USE_OKTA is False, handles local login form and session creation. - If USE_OKTA is True, delegates to Okta login (not yet implemented). Source code in arb\\auth\\routes.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 @auth . route ( '/login' , methods = [ 'GET' , 'POST' ]) def login (): \"\"\" User login route. - If USE_OKTA is False, handles local login form and session creation. - If USE_OKTA is True, delegates to Okta login (not yet implemented). \"\"\" logger . debug ( f \"Login route accessed - Method: { request . method } \" ) if USE_OKTA : # TODO: Implement Okta login redirect/flow. # Need: Okta OIDC/SAML login integration. raise NotImplementedError ( \"Okta login not implemented yet. Need Okta OIDC/SAML integration.\" ) else : if request . method == 'POST' : logger . debug ( \"Processing login POST request\" ) email = request . form . get ( 'email' ) password = request . form . get ( 'password' ) logger . debug ( f \"Login attempt for email: { email } \" ) User = get_user_model () user = User . query . filter_by ( email = email ) . first () if user : logger . debug ( f \"User found: { user . email } , checking password\" ) if user . check_password ( password ): logger . debug ( f \"Password check successful for user: { user . email } \" ) # Check if email is confirmed if not user . is_confirmed : logger . debug ( f \"Login failed: email not confirmed for user: { user . email } \" ) flash ( 'Please confirm your email address before logging in. Check your email for a confirmation link.' , 'warning' ) return render_template ( 'auth/login.html' ) login_user ( user ) flash ( 'Logged in successfully.' , 'success' ) logger . debug ( f \"User { user . email } logged in successfully\" ) return redirect ( url_for ( 'main.index' )) else : logger . debug ( f \"Password check failed for user: { user . email } \" ) flash ( 'Invalid email or password.' , 'danger' ) else : logger . debug ( f \"No user found for email: { email } \" ) flash ( 'Invalid email or password.' , 'danger' ) else : logger . debug ( \"Rendering login form\" ) return render_template ( 'auth/login.html' )","title":"login"},{"location":"reference/arb/auth/routes/#arb.auth.routes.logout","text":"User logout route. - If USE_OKTA is False, logs out the user locally. - If USE_OKTA is True, delegates to Okta logout (not yet implemented). Source code in arb\\auth\\routes.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 @auth . route ( '/logout' ) def logout (): \"\"\" User logout route. - If USE_OKTA is False, logs out the user locally. - If USE_OKTA is True, delegates to Okta logout (not yet implemented). \"\"\" if USE_OKTA : # TODO: Implement Okta logout redirect/flow. # Need: Okta logout endpoint/integration. raise NotImplementedError ( \"Okta logout not implemented yet. Need Okta logout integration.\" ) else : logout_user () flash ( 'You have been logged out.' , 'info' ) return redirect ( url_for ( 'auth.login' ))","title":"logout"},{"location":"reference/arb/auth/routes/#arb.auth.routes.qaqc_dashboard","text":"Render the QA/QC-only dashboard page. Only users with 'qaqc' role can access. Source code in arb\\auth\\routes.py 194 195 196 197 198 199 200 201 202 203 204 205 @auth . route ( '/qaqc/dashboard' ) @role_required ( 'qaqc' ) def qaqc_dashboard (): \"\"\" Render the QA/QC-only dashboard page. Only users with 'qaqc' role can access. \"\"\" if USE_OKTA : # TODO: Render QA/QC dashboard for Okta-authenticated qaqc users. raise NotImplementedError ( \"Okta qaqc dashboard not implemented yet.\" ) else : return render_template ( 'auth/qaqc_dashboard.html' )","title":"qaqc_dashboard"},{"location":"reference/arb/auth/routes/#arb.auth.routes.register","text":"User registration route. - If USE_OKTA is False, handles local registration form and user creation. - If USE_OKTA is True, delegates to Okta registration (not yet implemented). Source code in arb\\auth\\routes.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 @auth . route ( '/register' , methods = [ 'GET' , 'POST' ]) def register (): \"\"\" User registration route. - If USE_OKTA is False, handles local registration form and user creation. - If USE_OKTA is True, delegates to Okta registration (not yet implemented). \"\"\" logger . debug ( f \"Register route accessed - Method: { request . method } \" ) if USE_OKTA : # TODO: Implement Okta registration or link to Okta self-service registration. # Need: Okta registration endpoint or documentation. raise NotImplementedError ( \"Okta registration not implemented yet. Need Okta registration integration.\" ) else : if request . method == 'POST' : logger . debug ( \"Processing registration POST request\" ) User = get_user_model () email = request . form . get ( 'email' ) password = request . form . get ( 'password' ) logger . debug ( f \"Registration attempt for email: { email } \" ) if not email or not password : logger . debug ( \"Registration failed: missing email or password\" ) flash ( 'Email and password are required.' , 'danger' ) elif User . query . filter_by ( email = email ) . first (): logger . debug ( f \"Registration failed: email { email } already exists\" ) flash ( 'Email already registered.' , 'warning' ) else : logger . debug ( f \"Creating new user with email: { email } \" ) user = User () setattr ( user , 'email' , email ) user . set_password ( password ) # Keep user unconfirmed until email is verified user . is_confirmed_col = False get_db () . session . add ( user ) get_db () . session . commit () # Generate email confirmation token and send confirmation email try : from arb.auth.email_util import send_email_confirmation token = user . generate_email_confirmation_token () send_email_confirmation ( user , token ) logger . debug ( f \"Email confirmation sent to user: { email } \" ) flash ( 'Registration successful! Please check your email to confirm your account.' , 'success' ) except Exception as e : logger . error ( f \"Failed to send email confirmation to { email } : { str ( e ) } \" ) logger . error ( f \"Error type: { type ( e ) . __name__ } \" ) import traceback logger . error ( f \"Traceback: { traceback . format_exc () } \" ) # Still create the user, but inform them about the email issue flash ( 'Registration successful! However, there was an issue sending the confirmation email. Please contact support.' , 'warning' ) return redirect ( url_for ( 'auth.login' )) else : logger . debug ( \"Rendering registration form\" ) return render_template ( 'auth/register.html' )","title":"register"},{"location":"reference/arb/auth/routes/#arb.auth.routes.resend_confirmation","text":"Resend email confirmation route. Allows users to request a new confirmation email if they didn't receive the first one. Source code in arb\\auth\\routes.py 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 @auth . route ( '/resend-confirmation' , methods = [ 'GET' , 'POST' ]) def resend_confirmation (): \"\"\" Resend email confirmation route. Allows users to request a new confirmation email if they didn't receive the first one. \"\"\" logger . debug ( f \"Resend confirmation route accessed - Method: { request . method } \" ) if USE_OKTA : # TODO: Implement Okta resend confirmation if needed. raise NotImplementedError ( \"Okta resend confirmation not implemented yet.\" ) else : if request . method == 'POST' : email = request . form . get ( 'email' ) logger . debug ( f \"Resend confirmation request for email: { email } \" ) if not email : flash ( 'Please enter your email address.' , 'danger' ) else : User = get_user_model () user = User . query . filter_by ( email = email ) . first () if not user : # Don't reveal if email exists or not for security flash ( 'If an account with that email exists, a confirmation email has been sent.' , 'info' ) elif user . is_confirmed : flash ( 'Your email is already confirmed. You can log in.' , 'info' ) else : # Generate new confirmation token and send email try : from arb.auth.email_util import send_email_confirmation token = user . generate_email_confirmation_token () send_email_confirmation ( user , token ) logger . debug ( f \"Confirmation email resent to user: { email } \" ) flash ( 'A new confirmation email has been sent. Please check your inbox.' , 'success' ) except Exception as e : logger . error ( f \"Failed to resend confirmation email to { email } : { e } \" ) flash ( 'There was an issue sending the confirmation email. Please try again later.' , 'danger' ) return redirect ( url_for ( 'auth.login' )) else : return render_template ( 'auth/resend_confirmation.html' )","title":"resend_confirmation"},{"location":"reference/arb/auth/routes/#arb.auth.routes.review_dashboard","text":"Render the review dashboard page. Users with 'editor', 'reviewer', OR 'qaqc' roles can access. Source code in arb\\auth\\routes.py 208 209 210 211 212 213 214 215 216 217 218 219 @auth . route ( '/review/dashboard' ) @roles_required ( 'editor' , 'reviewer' , 'qaqc' ) def review_dashboard (): \"\"\" Render the review dashboard page. Users with 'editor', 'reviewer', OR 'qaqc' roles can access. \"\"\" if USE_OKTA : # TODO: Render review dashboard for Okta-authenticated users with appropriate roles. raise NotImplementedError ( \"Okta review dashboard not implemented yet.\" ) else : return render_template ( 'auth/review_dashboard.html' )","title":"review_dashboard"},{"location":"reference/arb/auth/routes/#arb.auth.routes.role_required","text":"Decorator to restrict access to users with the specified role. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Parameters: role_name ( str ) \u2013 The role name to check for. Example @role_required('editor') def edit_page(): # Only users with 'editor' role can access pass Source code in arb\\auth\\routes.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def role_required ( role_name : str ): \"\"\" Decorator to restrict access to users with the specified role. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Args: role_name: The role name to check for. Example: @role_required('editor') def edit_page(): # Only users with 'editor' role can access pass \"\"\" def decorator ( f ): @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for the specified group/claim. # Need: Okta group/claim mapping for role access. raise NotImplementedError ( \"Okta role check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . has_role ( role_name ): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function return decorator","title":"role_required"},{"location":"reference/arb/auth/routes/#arb.auth.routes.roles_required","text":"Decorator to restrict access to users with any of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Parameters: *roles ( str , default: () ) \u2013 Variable number of role names to check for. Example @roles_required('editor', 'reviewer') def edit_page(): # Only users with 'editor' OR 'reviewer' role can access pass Source code in arb\\auth\\routes.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def roles_required ( * roles : str ): \"\"\" Decorator to restrict access to users with any of the specified roles. - If USE_OKTA is True, checks Okta claims/groups for role access. - If USE_OKTA is False, uses local role field. Args: *roles: Variable number of role names to check for. Example: @roles_required('editor', 'reviewer') def edit_page(): # Only users with 'editor' OR 'reviewer' role can access pass \"\"\" def decorator ( f ): @wraps ( f ) def decorated_function ( * args , ** kwargs ): if USE_OKTA : # TODO: Check Okta token for any of the specified groups/claims. # Need: Okta group/claim mapping for role access. raise NotImplementedError ( \"Okta role check not implemented yet. Need Okta group/claim mapping.\" ) else : if not current_user . is_authenticated or not current_user . has_any_role ( * roles ): abort ( 403 ) return f ( * args , ** kwargs ) return decorated_function return decorator","title":"roles_required"},{"location":"reference/arb/auth/routes/#arb.auth.routes.settings","text":"Render the user account settings page (placeholder). In Okta, this page may display Okta-managed profile info or link to Okta profile management. Source code in arb\\auth\\routes.py 236 237 238 239 240 241 242 243 244 245 246 247 248 @auth . route ( '/settings' ) @login_required def settings (): \"\"\" Render the user account settings page (placeholder). In Okta, this page may display Okta-managed profile info or link to Okta profile management. \"\"\" if USE_OKTA : # TODO: Display Okta profile info or link to Okta profile management. # Need: Okta user info endpoint and integration. raise NotImplementedError ( \"Okta settings/profile integration not implemented yet.\" ) else : return render_template ( 'auth/settings.html' )","title":"settings"},{"location":"reference/arb/auth/test_auth/","text":"arb.auth.test_auth","title":"arb.auth.test_auth"},{"location":"reference/arb/auth/test_auth/#arbauthtest_auth","text":"","title":"arb.auth.test_auth"},{"location":"reference/arb/auth_example_app/app/","text":"arb.auth_example_app.app Main Flask application factory for the Auth Example App. This demonstrates how to properly integrate the arb.auth package into a Flask application with multiple roles support. create_app ( config_name = 'default' ) Application factory for the Auth Example App. Parameters: config_name ( str , default: 'default' ) \u2013 Configuration to use ('development', 'testing', 'production', 'default') Returns: Flask ( Flask ) \u2013 Configured Flask application Source code in arb\\auth_example_app\\app.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def create_app ( config_name : str = 'default' ) -> Flask : \"\"\" Application factory for the Auth Example App. Args: config_name: Configuration to use ('development', 'testing', 'production', 'default') Returns: Flask: Configured Flask application \"\"\" app = Flask ( __name__ ) # Load configuration from arb.auth_example_app.config import config app . config . from_object ( config [ config_name ]) # Initialize Flask extensions db . init_app ( app ) mail . init_app ( app ) login_manager . init_app ( app ) # Initialize the auth package init_auth ( app = app , db = db , mail = mail , login_manager = login_manager ) # Register blueprints app . register_blueprint ( main . bp ) app . register_blueprint ( admin . bp ) app . register_blueprint ( user_management . bp ) # Register auth blueprint (from arb.auth) register_auth_blueprint ( app ) # Register the User model with SQLAlchemy before creating tables from arb.auth.models import get_user_model get_user_model () # Create database tables with app . app_context (): db . create_all () # Create some example users if they don't exist create_example_users () return app create_example_users () Create example users with different role combinations. Source code in arb\\auth_example_app\\app.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def create_example_users (): \"\"\"Create example users with different role combinations.\"\"\" from arb.auth.models import get_user_model User = get_user_model () # Check if users already exist if User . query . count () > 0 : return # Create example users with different role combinations users_data = [ { 'email' : 'admin@example.com' , 'password' : 'admin123' , 'roles' : [ 'admin' ] }, { 'email' : 'editor@example.com' , 'password' : 'editor123' , 'roles' : [ 'editor' ] }, { 'email' : 'qaqc@example.com' , 'password' : 'qaqc123' , 'roles' : [ 'qaqc' ] }, { 'email' : 'editor_qaqc@example.com' , 'password' : 'editor_qaqc123' , 'roles' : [ 'editor' , 'qaqc' ] }, { 'email' : 'reviewer@example.com' , 'password' : 'reviewer123' , 'roles' : [ 'reviewer' ] }, { 'email' : 'manager@example.com' , 'password' : 'manager123' , 'roles' : [ 'manager' , 'reviewer' ] }, { 'email' : 'user@example.com' , 'password' : 'user123' , 'roles' : [ 'user' ] } ] for user_data in users_data : user = User () user . email = user_data [ 'email' ] user . set_password ( user_data [ 'password' ]) user . set_roles ( user_data [ 'roles' ]) user . is_confirmed_col = True # Skip email confirmation for demo get_db () . session . add ( user ) get_db () . session . commit () print ( \"Created example users:\" ) for user_data in users_data : print ( f \" - { user_data [ 'email' ] } (roles: { ', ' . join ( user_data [ 'roles' ]) } )\" )","title":"arb.auth_example_app.app"},{"location":"reference/arb/auth_example_app/app/#arbauth_example_appapp","text":"Main Flask application factory for the Auth Example App. This demonstrates how to properly integrate the arb.auth package into a Flask application with multiple roles support.","title":"arb.auth_example_app.app"},{"location":"reference/arb/auth_example_app/app/#arb.auth_example_app.app.create_app","text":"Application factory for the Auth Example App. Parameters: config_name ( str , default: 'default' ) \u2013 Configuration to use ('development', 'testing', 'production', 'default') Returns: Flask ( Flask ) \u2013 Configured Flask application Source code in arb\\auth_example_app\\app.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def create_app ( config_name : str = 'default' ) -> Flask : \"\"\" Application factory for the Auth Example App. Args: config_name: Configuration to use ('development', 'testing', 'production', 'default') Returns: Flask: Configured Flask application \"\"\" app = Flask ( __name__ ) # Load configuration from arb.auth_example_app.config import config app . config . from_object ( config [ config_name ]) # Initialize Flask extensions db . init_app ( app ) mail . init_app ( app ) login_manager . init_app ( app ) # Initialize the auth package init_auth ( app = app , db = db , mail = mail , login_manager = login_manager ) # Register blueprints app . register_blueprint ( main . bp ) app . register_blueprint ( admin . bp ) app . register_blueprint ( user_management . bp ) # Register auth blueprint (from arb.auth) register_auth_blueprint ( app ) # Register the User model with SQLAlchemy before creating tables from arb.auth.models import get_user_model get_user_model () # Create database tables with app . app_context (): db . create_all () # Create some example users if they don't exist create_example_users () return app","title":"create_app"},{"location":"reference/arb/auth_example_app/app/#arb.auth_example_app.app.create_example_users","text":"Create example users with different role combinations. Source code in arb\\auth_example_app\\app.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def create_example_users (): \"\"\"Create example users with different role combinations.\"\"\" from arb.auth.models import get_user_model User = get_user_model () # Check if users already exist if User . query . count () > 0 : return # Create example users with different role combinations users_data = [ { 'email' : 'admin@example.com' , 'password' : 'admin123' , 'roles' : [ 'admin' ] }, { 'email' : 'editor@example.com' , 'password' : 'editor123' , 'roles' : [ 'editor' ] }, { 'email' : 'qaqc@example.com' , 'password' : 'qaqc123' , 'roles' : [ 'qaqc' ] }, { 'email' : 'editor_qaqc@example.com' , 'password' : 'editor_qaqc123' , 'roles' : [ 'editor' , 'qaqc' ] }, { 'email' : 'reviewer@example.com' , 'password' : 'reviewer123' , 'roles' : [ 'reviewer' ] }, { 'email' : 'manager@example.com' , 'password' : 'manager123' , 'roles' : [ 'manager' , 'reviewer' ] }, { 'email' : 'user@example.com' , 'password' : 'user123' , 'roles' : [ 'user' ] } ] for user_data in users_data : user = User () user . email = user_data [ 'email' ] user . set_password ( user_data [ 'password' ]) user . set_roles ( user_data [ 'roles' ]) user . is_confirmed_col = True # Skip email confirmation for demo get_db () . session . add ( user ) get_db () . session . commit () print ( \"Created example users:\" ) for user_data in users_data : print ( f \" - { user_data [ 'email' ] } (roles: { ', ' . join ( user_data [ 'roles' ]) } )\" )","title":"create_example_users"},{"location":"reference/arb/auth_example_app/config/","text":"arb.auth_example_app.config Configuration for the Auth Example App. This demonstrates how to configure the auth package in a Flask application. SECURITY NOTE: Never commit real passwords or sensitive credentials to version control. Use environment variables for all sensitive settings in production. Config Base configuration for the example app. Source code in arb\\auth_example_app\\config.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class Config : \"\"\"Base configuration for the example app.\"\"\" # Flask configuration SECRET_KEY = os . environ . get ( 'SECRET_KEY' ) or 'dev-secret-key-change-in-production' DEBUG = True # Database configuration (using SQLite for simplicity) SQLALCHEMY_DATABASE_URI = os . environ . get ( 'DATABASE_URI' ) or 'sqlite:///auth_example.db' SQLALCHEMY_TRACK_MODIFICATIONS = False # Auth configuration USE_AUTH = True # Enable authentication # ============================================================================ # EMAIL CONFIGURATION # ============================================================================ # # Flask-Mail Settings (these are the actual settings Flask-Mail uses) # # For Gmail: # - MAIL_SERVER = 'smtp.gmail.com' # - MAIL_PORT = 587 # - MAIL_USE_TLS = True # - You'll need to enable \"Less secure app access\" or use App Passwords # # For Outlook/Office365: # - MAIL_SERVER = 'smtp-mail.outlook.com' # - MAIL_PORT = 587 # - MAIL_USE_TLS = True # # For custom SMTP server: # - MAIL_SERVER = 'your-smtp-server.com' # - MAIL_PORT = 587 (or your server's port) # - MAIL_USE_TLS = True (or False depending on your server) # SMTP Server Configuration MAIL_SERVER = os . environ . get ( 'MAIL_SERVER' , 'smtp.gmail.com' ) MAIL_PORT = int ( os . environ . get ( 'MAIL_PORT' , 587 )) MAIL_USE_TLS = os . environ . get ( 'MAIL_USE_TLS' , 'True' ) . lower () == 'true' MAIL_USE_SSL = os . environ . get ( 'MAIL_USE_SSL' , 'False' ) . lower () == 'true' # Email Authentication (use environment variables for security) MAIL_USERNAME = os . environ . get ( 'MAIL_USERNAME' , 'example@carb.ca.gov' ) MAIL_PASSWORD = os . environ . get ( 'MAIL_PASSWORD' , 'your-email-password' ) MAIL_DEFAULT_SENDER = os . environ . get ( 'MAIL_DEFAULT_SENDER' , 'example@carb.ca.gov' ) # Development Settings # Set MAIL_SUPPRESS_SEND = True to print emails to console instead of sending # This is useful for development and testing MAIL_SUPPRESS_SEND = os . environ . get ( 'MAIL_SUPPRESS_SEND' , 'True' ) . lower () == 'true' # ============================================================================ # AUTH PACKAGE EMAIL SETTINGS (Legacy - kept for compatibility) # ============================================================================ # These settings are used by the auth package itself # They should match the Flask-Mail settings above AUTH_MAIL_SERVER = MAIL_SERVER AUTH_MAIL_PORT = MAIL_PORT AUTH_MAIL_USE_TLS = MAIL_USE_TLS AUTH_MAIL_USERNAME = MAIL_USERNAME AUTH_MAIL_PASSWORD = MAIL_PASSWORD AUTH_MAIL_DEFAULT_SENDER = MAIL_DEFAULT_SENDER # Security settings AUTH_MAX_LOGIN_ATTEMPTS = 5 AUTH_ACCOUNT_LOCKOUT_DURATION = 900 # 15 minutes AUTH_SESSION_TIMEOUT = 3600 # 1 hour AUTH_PASSWORD_RESET_EXPIRATION = 3600 # 1 hour DevelopmentConfig Bases: Config Development configuration. Source code in arb\\auth_example_app\\config.py 84 85 86 87 88 89 90 class DevelopmentConfig ( Config ): \"\"\"Development configuration.\"\"\" DEBUG = True SQLALCHEMY_DATABASE_URI = 'sqlite:///auth_example_dev.db' # In development, suppress email sending and print to console MAIL_SUPPRESS_SEND = True ProductionConfig Bases: Config Production configuration. Source code in arb\\auth_example_app\\config.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 class ProductionConfig ( Config ): \"\"\"Production configuration.\"\"\" DEBUG = False # In production, use environment variables for ALL sensitive settings SECRET_KEY = os . environ . get ( 'SECRET_KEY' ) SQLALCHEMY_DATABASE_URI = os . environ . get ( 'DATABASE_URI' ) # Production email settings - MUST use environment variables MAIL_USERNAME = os . environ . get ( 'MAIL_USERNAME' ) MAIL_PASSWORD = os . environ . get ( 'MAIL_PASSWORD' ) MAIL_DEFAULT_SENDER = os . environ . get ( 'MAIL_DEFAULT_SENDER' ) # In production, actually send emails MAIL_SUPPRESS_SEND = False TestingConfig Bases: Config Testing configuration. Source code in arb\\auth_example_app\\config.py 93 94 95 96 97 98 99 100 class TestingConfig ( Config ): \"\"\"Testing configuration.\"\"\" TESTING = True SQLALCHEMY_DATABASE_URI = 'sqlite:///auth_example_test.db' WTF_CSRF_ENABLED = False # In testing, suppress email sending MAIL_SUPPRESS_SEND = True","title":"arb.auth_example_app.config"},{"location":"reference/arb/auth_example_app/config/#arbauth_example_appconfig","text":"Configuration for the Auth Example App. This demonstrates how to configure the auth package in a Flask application. SECURITY NOTE: Never commit real passwords or sensitive credentials to version control. Use environment variables for all sensitive settings in production.","title":"arb.auth_example_app.config"},{"location":"reference/arb/auth_example_app/config/#arb.auth_example_app.config.Config","text":"Base configuration for the example app. Source code in arb\\auth_example_app\\config.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class Config : \"\"\"Base configuration for the example app.\"\"\" # Flask configuration SECRET_KEY = os . environ . get ( 'SECRET_KEY' ) or 'dev-secret-key-change-in-production' DEBUG = True # Database configuration (using SQLite for simplicity) SQLALCHEMY_DATABASE_URI = os . environ . get ( 'DATABASE_URI' ) or 'sqlite:///auth_example.db' SQLALCHEMY_TRACK_MODIFICATIONS = False # Auth configuration USE_AUTH = True # Enable authentication # ============================================================================ # EMAIL CONFIGURATION # ============================================================================ # # Flask-Mail Settings (these are the actual settings Flask-Mail uses) # # For Gmail: # - MAIL_SERVER = 'smtp.gmail.com' # - MAIL_PORT = 587 # - MAIL_USE_TLS = True # - You'll need to enable \"Less secure app access\" or use App Passwords # # For Outlook/Office365: # - MAIL_SERVER = 'smtp-mail.outlook.com' # - MAIL_PORT = 587 # - MAIL_USE_TLS = True # # For custom SMTP server: # - MAIL_SERVER = 'your-smtp-server.com' # - MAIL_PORT = 587 (or your server's port) # - MAIL_USE_TLS = True (or False depending on your server) # SMTP Server Configuration MAIL_SERVER = os . environ . get ( 'MAIL_SERVER' , 'smtp.gmail.com' ) MAIL_PORT = int ( os . environ . get ( 'MAIL_PORT' , 587 )) MAIL_USE_TLS = os . environ . get ( 'MAIL_USE_TLS' , 'True' ) . lower () == 'true' MAIL_USE_SSL = os . environ . get ( 'MAIL_USE_SSL' , 'False' ) . lower () == 'true' # Email Authentication (use environment variables for security) MAIL_USERNAME = os . environ . get ( 'MAIL_USERNAME' , 'example@carb.ca.gov' ) MAIL_PASSWORD = os . environ . get ( 'MAIL_PASSWORD' , 'your-email-password' ) MAIL_DEFAULT_SENDER = os . environ . get ( 'MAIL_DEFAULT_SENDER' , 'example@carb.ca.gov' ) # Development Settings # Set MAIL_SUPPRESS_SEND = True to print emails to console instead of sending # This is useful for development and testing MAIL_SUPPRESS_SEND = os . environ . get ( 'MAIL_SUPPRESS_SEND' , 'True' ) . lower () == 'true' # ============================================================================ # AUTH PACKAGE EMAIL SETTINGS (Legacy - kept for compatibility) # ============================================================================ # These settings are used by the auth package itself # They should match the Flask-Mail settings above AUTH_MAIL_SERVER = MAIL_SERVER AUTH_MAIL_PORT = MAIL_PORT AUTH_MAIL_USE_TLS = MAIL_USE_TLS AUTH_MAIL_USERNAME = MAIL_USERNAME AUTH_MAIL_PASSWORD = MAIL_PASSWORD AUTH_MAIL_DEFAULT_SENDER = MAIL_DEFAULT_SENDER # Security settings AUTH_MAX_LOGIN_ATTEMPTS = 5 AUTH_ACCOUNT_LOCKOUT_DURATION = 900 # 15 minutes AUTH_SESSION_TIMEOUT = 3600 # 1 hour AUTH_PASSWORD_RESET_EXPIRATION = 3600 # 1 hour","title":"Config"},{"location":"reference/arb/auth_example_app/config/#arb.auth_example_app.config.DevelopmentConfig","text":"Bases: Config Development configuration. Source code in arb\\auth_example_app\\config.py 84 85 86 87 88 89 90 class DevelopmentConfig ( Config ): \"\"\"Development configuration.\"\"\" DEBUG = True SQLALCHEMY_DATABASE_URI = 'sqlite:///auth_example_dev.db' # In development, suppress email sending and print to console MAIL_SUPPRESS_SEND = True","title":"DevelopmentConfig"},{"location":"reference/arb/auth_example_app/config/#arb.auth_example_app.config.ProductionConfig","text":"Bases: Config Production configuration. Source code in arb\\auth_example_app\\config.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 class ProductionConfig ( Config ): \"\"\"Production configuration.\"\"\" DEBUG = False # In production, use environment variables for ALL sensitive settings SECRET_KEY = os . environ . get ( 'SECRET_KEY' ) SQLALCHEMY_DATABASE_URI = os . environ . get ( 'DATABASE_URI' ) # Production email settings - MUST use environment variables MAIL_USERNAME = os . environ . get ( 'MAIL_USERNAME' ) MAIL_PASSWORD = os . environ . get ( 'MAIL_PASSWORD' ) MAIL_DEFAULT_SENDER = os . environ . get ( 'MAIL_DEFAULT_SENDER' ) # In production, actually send emails MAIL_SUPPRESS_SEND = False","title":"ProductionConfig"},{"location":"reference/arb/auth_example_app/config/#arb.auth_example_app.config.TestingConfig","text":"Bases: Config Testing configuration. Source code in arb\\auth_example_app\\config.py 93 94 95 96 97 98 99 100 class TestingConfig ( Config ): \"\"\"Testing configuration.\"\"\" TESTING = True SQLALCHEMY_DATABASE_URI = 'sqlite:///auth_example_test.db' WTF_CSRF_ENABLED = False # In testing, suppress email sending MAIL_SUPPRESS_SEND = True","title":"TestingConfig"},{"location":"reference/arb/auth_example_app/extensions/","text":"arb.auth_example_app.extensions Centralized definition of Flask extension instances used throughout the auth example app. This module avoids circular imports by creating extension objects (e.g., db , mail , login_manager ) at the top level, without initializing them until app.init_app() is called elsewhere. Extensions Defined db (SQLAlchemy): SQLAlchemy instance for database operations mail (Mail): Email sending functionality login_manager (LoginManager): User session management for authentication Notes Use with app.app_context(): when accessing db outside a Flask route. Extensions are initialized in the app factory pattern. db = SQLAlchemy () module-attribute SQLAlchemy: Flask SQLAlchemy instance for managing ORM and schema. login_manager = LoginManager () module-attribute LoginManager: Flask-Login extension for user session management. mail = Mail () module-attribute Mail: Flask-Mail extension for email functionality.","title":"arb.auth_example_app.extensions"},{"location":"reference/arb/auth_example_app/extensions/#arbauth_example_appextensions","text":"Centralized definition of Flask extension instances used throughout the auth example app. This module avoids circular imports by creating extension objects (e.g., db , mail , login_manager ) at the top level, without initializing them until app.init_app() is called elsewhere. Extensions Defined db (SQLAlchemy): SQLAlchemy instance for database operations mail (Mail): Email sending functionality login_manager (LoginManager): User session management for authentication Notes Use with app.app_context(): when accessing db outside a Flask route. Extensions are initialized in the app factory pattern.","title":"arb.auth_example_app.extensions"},{"location":"reference/arb/auth_example_app/extensions/#arb.auth_example_app.extensions.db","text":"SQLAlchemy: Flask SQLAlchemy instance for managing ORM and schema.","title":"db"},{"location":"reference/arb/auth_example_app/extensions/#arb.auth_example_app.extensions.login_manager","text":"LoginManager: Flask-Login extension for user session management.","title":"login_manager"},{"location":"reference/arb/auth_example_app/extensions/#arb.auth_example_app.extensions.mail","text":"Mail: Flask-Mail extension for email functionality.","title":"mail"},{"location":"reference/arb/auth_example_app/wsgi/","text":"arb.auth_example_app.wsgi WSGI entry point for the Auth Example App. This file enables the application to be run via a WSGI server (e.g., Gunicorn or uWSGI) or directly via flask run or python wsgi.py . Usage Direct Python execution python wsgi.py Flask CLI flask --app wsgi run --debug -p 5000 WSGI server (production) gunicorn wsgi:app Environment variables export FLASK_APP=wsgi export FLASK_ENV=development flask run","title":"arb.auth_example_app.wsgi"},{"location":"reference/arb/auth_example_app/wsgi/#arbauth_example_appwsgi","text":"WSGI entry point for the Auth Example App. This file enables the application to be run via a WSGI server (e.g., Gunicorn or uWSGI) or directly via flask run or python wsgi.py . Usage","title":"arb.auth_example_app.wsgi"},{"location":"reference/arb/auth_example_app/wsgi/#arb.auth_example_app.wsgi--direct-python-execution","text":"python wsgi.py","title":"Direct Python execution"},{"location":"reference/arb/auth_example_app/wsgi/#arb.auth_example_app.wsgi--flask-cli","text":"flask --app wsgi run --debug -p 5000","title":"Flask CLI"},{"location":"reference/arb/auth_example_app/wsgi/#arb.auth_example_app.wsgi--wsgi-server-production","text":"gunicorn wsgi:app","title":"WSGI server (production)"},{"location":"reference/arb/auth_example_app/wsgi/#arb.auth_example_app.wsgi--environment-variables","text":"export FLASK_APP=wsgi export FLASK_ENV=development flask run","title":"Environment variables"},{"location":"reference/arb/auth_example_app/routes/admin/","text":"arb.auth_example_app.routes.admin Admin routes for the Auth Example App. Demonstrates admin-only functionality and user management. add_user_role ( user_id ) Add a role to a user - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @bp . route ( '/user/<int:user_id>/add-role' , methods = [ 'POST' ]) @admin_required def add_user_role ( user_id ): \"\"\"Add a role to a user - admin only.\"\"\" user = get_user_model () . query . get_or_404 ( user_id ) role = request . form . get ( 'role' ) if role : user . add_role ( role ) flash ( f 'Added role \" { role } \" to user { user . email } ' , 'success' ) else : flash ( 'No role specified' , 'error' ) return redirect ( url_for ( 'admin.user_detail' , user_id = user_id )) admin_dashboard () Admin dashboard - accessible only to admins. Source code in arb\\auth_example_app\\routes\\admin.py 17 18 19 20 21 @bp . route ( '/' ) @admin_required def admin_dashboard (): \"\"\"Admin dashboard - accessible only to admins.\"\"\" return render_template ( 'admin/dashboard.html' ) delete_user ( user_id ) Delete a user - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 @bp . route ( '/user/<int:user_id>/delete' , methods = [ 'POST' ]) @admin_required def delete_user ( user_id ): \"\"\"Delete a user - admin only.\"\"\" User = get_user_model () user = User . query . get_or_404 ( user_id ) # Don't allow deleting yourself if user . id == current_user . id : flash ( 'Cannot delete your own account' , 'error' ) return redirect ( url_for ( 'admin.user_detail' , user_id = user_id )) # Store email for flash message before deletion user_email = user . email # Delete the user get_db () . session . delete ( user ) get_db () . session . commit () flash ( f 'User { user_email } has been permanently deleted' , 'success' ) return redirect ( url_for ( 'admin.user_list' )) remove_user_role ( user_id ) Remove a role from a user - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @bp . route ( '/user/<int:user_id>/remove-role' , methods = [ 'POST' ]) @admin_required def remove_user_role ( user_id ): \"\"\"Remove a role from a user - admin only.\"\"\" user = get_user_model () . query . get_or_404 ( user_id ) role = request . form . get ( 'role' ) if role : user . remove_role ( role ) flash ( f 'Removed role \" { role } \" from user { user . email } ' , 'success' ) else : flash ( 'No role specified' , 'error' ) return redirect ( url_for ( 'admin.user_detail' , user_id = user_id )) set_user_roles ( user_id ) Set all roles for a user - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 74 75 76 77 78 79 80 81 82 83 84 85 86 @bp . route ( '/user/<int:user_id>/set-roles' , methods = [ 'POST' ]) @admin_required def set_user_roles ( user_id ): \"\"\"Set all roles for a user - admin only.\"\"\" user = get_user_model () . query . get_or_404 ( user_id ) roles_str = request . form . get ( 'roles' , '' ) # Parse comma-separated roles roles = [ role . strip () for role in roles_str . split ( ',' ) if role . strip ()] user . set_roles ( roles ) flash ( f 'Updated roles for user { user . email } : { \", \" . join ( roles ) } ' , 'success' ) return redirect ( url_for ( 'admin.user_detail' , user_id = user_id )) toggle_user_active ( user_id ) Toggle user active status - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @bp . route ( '/user/<int:user_id>/toggle-active' , methods = [ 'POST' ]) @admin_required def toggle_user_active ( user_id ): \"\"\"Toggle user active status - admin only.\"\"\" User = get_user_model () user = User . query . get_or_404 ( user_id ) # Don't allow deactivating yourself if user . id == current_user . id : flash ( 'Cannot deactivate your own account' , 'error' ) return redirect ( url_for ( 'admin.user_detail' , user_id = user_id )) user . is_active_col = not user . is_active_col get_db () . session . commit () status = 'activated' if user . is_active_col else 'deactivated' flash ( f 'User { user . email } { status } ' , 'success' ) return redirect ( url_for ( 'admin.user_detail' , user_id = user_id )) user_detail ( user_id ) View user details - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 33 34 35 36 37 38 39 @bp . route ( '/user/<int:user_id>' ) @admin_required def user_detail ( user_id ): \"\"\"View user details - admin only.\"\"\" User = get_user_model () user = User . query . get_or_404 ( user_id ) return render_template ( 'admin/user_detail.html' , user = user ) user_list () List all users - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 24 25 26 27 28 29 30 @bp . route ( '/users' ) @admin_required def user_list (): \"\"\"List all users - admin only.\"\"\" User = get_user_model () users = User . query . all () return render_template ( 'admin/user_list.html' , users = users )","title":"arb.auth_example_app.routes.admin"},{"location":"reference/arb/auth_example_app/routes/admin/#arbauth_example_approutesadmin","text":"Admin routes for the Auth Example App. Demonstrates admin-only functionality and user management.","title":"arb.auth_example_app.routes.admin"},{"location":"reference/arb/auth_example_app/routes/admin/#arb.auth_example_app.routes.admin.add_user_role","text":"Add a role to a user - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @bp . route ( '/user/<int:user_id>/add-role' , methods = [ 'POST' ]) @admin_required def add_user_role ( user_id ): \"\"\"Add a role to a user - admin only.\"\"\" user = get_user_model () . query . get_or_404 ( user_id ) role = request . form . get ( 'role' ) if role : user . add_role ( role ) flash ( f 'Added role \" { role } \" to user { user . email } ' , 'success' ) else : flash ( 'No role specified' , 'error' ) return redirect ( url_for ( 'admin.user_detail' , user_id = user_id ))","title":"add_user_role"},{"location":"reference/arb/auth_example_app/routes/admin/#arb.auth_example_app.routes.admin.admin_dashboard","text":"Admin dashboard - accessible only to admins. Source code in arb\\auth_example_app\\routes\\admin.py 17 18 19 20 21 @bp . route ( '/' ) @admin_required def admin_dashboard (): \"\"\"Admin dashboard - accessible only to admins.\"\"\" return render_template ( 'admin/dashboard.html' )","title":"admin_dashboard"},{"location":"reference/arb/auth_example_app/routes/admin/#arb.auth_example_app.routes.admin.delete_user","text":"Delete a user - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 @bp . route ( '/user/<int:user_id>/delete' , methods = [ 'POST' ]) @admin_required def delete_user ( user_id ): \"\"\"Delete a user - admin only.\"\"\" User = get_user_model () user = User . query . get_or_404 ( user_id ) # Don't allow deleting yourself if user . id == current_user . id : flash ( 'Cannot delete your own account' , 'error' ) return redirect ( url_for ( 'admin.user_detail' , user_id = user_id )) # Store email for flash message before deletion user_email = user . email # Delete the user get_db () . session . delete ( user ) get_db () . session . commit () flash ( f 'User { user_email } has been permanently deleted' , 'success' ) return redirect ( url_for ( 'admin.user_list' ))","title":"delete_user"},{"location":"reference/arb/auth_example_app/routes/admin/#arb.auth_example_app.routes.admin.remove_user_role","text":"Remove a role from a user - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @bp . route ( '/user/<int:user_id>/remove-role' , methods = [ 'POST' ]) @admin_required def remove_user_role ( user_id ): \"\"\"Remove a role from a user - admin only.\"\"\" user = get_user_model () . query . get_or_404 ( user_id ) role = request . form . get ( 'role' ) if role : user . remove_role ( role ) flash ( f 'Removed role \" { role } \" from user { user . email } ' , 'success' ) else : flash ( 'No role specified' , 'error' ) return redirect ( url_for ( 'admin.user_detail' , user_id = user_id ))","title":"remove_user_role"},{"location":"reference/arb/auth_example_app/routes/admin/#arb.auth_example_app.routes.admin.set_user_roles","text":"Set all roles for a user - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 74 75 76 77 78 79 80 81 82 83 84 85 86 @bp . route ( '/user/<int:user_id>/set-roles' , methods = [ 'POST' ]) @admin_required def set_user_roles ( user_id ): \"\"\"Set all roles for a user - admin only.\"\"\" user = get_user_model () . query . get_or_404 ( user_id ) roles_str = request . form . get ( 'roles' , '' ) # Parse comma-separated roles roles = [ role . strip () for role in roles_str . split ( ',' ) if role . strip ()] user . set_roles ( roles ) flash ( f 'Updated roles for user { user . email } : { \", \" . join ( roles ) } ' , 'success' ) return redirect ( url_for ( 'admin.user_detail' , user_id = user_id ))","title":"set_user_roles"},{"location":"reference/arb/auth_example_app/routes/admin/#arb.auth_example_app.routes.admin.toggle_user_active","text":"Toggle user active status - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 @bp . route ( '/user/<int:user_id>/toggle-active' , methods = [ 'POST' ]) @admin_required def toggle_user_active ( user_id ): \"\"\"Toggle user active status - admin only.\"\"\" User = get_user_model () user = User . query . get_or_404 ( user_id ) # Don't allow deactivating yourself if user . id == current_user . id : flash ( 'Cannot deactivate your own account' , 'error' ) return redirect ( url_for ( 'admin.user_detail' , user_id = user_id )) user . is_active_col = not user . is_active_col get_db () . session . commit () status = 'activated' if user . is_active_col else 'deactivated' flash ( f 'User { user . email } { status } ' , 'success' ) return redirect ( url_for ( 'admin.user_detail' , user_id = user_id ))","title":"toggle_user_active"},{"location":"reference/arb/auth_example_app/routes/admin/#arb.auth_example_app.routes.admin.user_detail","text":"View user details - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 33 34 35 36 37 38 39 @bp . route ( '/user/<int:user_id>' ) @admin_required def user_detail ( user_id ): \"\"\"View user details - admin only.\"\"\" User = get_user_model () user = User . query . get_or_404 ( user_id ) return render_template ( 'admin/user_detail.html' , user = user )","title":"user_detail"},{"location":"reference/arb/auth_example_app/routes/admin/#arb.auth_example_app.routes.admin.user_list","text":"List all users - admin only. Source code in arb\\auth_example_app\\routes\\admin.py 24 25 26 27 28 29 30 @bp . route ( '/users' ) @admin_required def user_list (): \"\"\"List all users - admin only.\"\"\" User = get_user_model () users = User . query . all () return render_template ( 'admin/user_list.html' , users = users )","title":"user_list"},{"location":"reference/arb/auth_example_app/routes/main/","text":"arb.auth_example_app.routes.main Main routes for the Auth Example App. Demonstrates basic functionality and role-based access control. advanced_editing () Advanced editing - requires BOTH editor AND qaqc roles. Source code in arb\\auth_example_app\\routes\\main.py 42 43 44 45 46 @bp . route ( '/advanced-editing' ) @all_roles_required ( 'editor' , 'qaqc' ) def advanced_editing (): \"\"\"Advanced editing - requires BOTH editor AND qaqc roles.\"\"\" return render_template ( 'main/advanced_editing.html' ) dashboard () User dashboard - requires login. Source code in arb\\auth_example_app\\routes\\main.py 21 22 23 24 25 @bp . route ( '/dashboard' ) @login_required def dashboard (): \"\"\"User dashboard - requires login.\"\"\" return render_template ( 'main/dashboard.html' ) editor_tools () Editor tools - accessible to editors or admins. Source code in arb\\auth_example_app\\routes\\main.py 28 29 30 31 32 @bp . route ( '/editor-tools' ) @roles_required ( 'editor' , 'admin' ) def editor_tools (): \"\"\"Editor tools - accessible to editors or admins.\"\"\" return render_template ( 'main/editor_tools.html' ) index () Homepage - accessible to everyone. Source code in arb\\auth_example_app\\routes\\main.py 15 16 17 18 @bp . route ( '/' ) def index (): \"\"\"Homepage - accessible to everyone.\"\"\" return render_template ( 'main/index.html' ) public_info () Public information - accessible to everyone. Source code in arb\\auth_example_app\\routes\\main.py 56 57 58 59 @bp . route ( '/public-info' ) def public_info (): \"\"\"Public information - accessible to everyone.\"\"\" return render_template ( 'main/public_info.html' ) qaqc_tools () QA/QC tools - accessible only to qaqc users. Source code in arb\\auth_example_app\\routes\\main.py 35 36 37 38 39 @bp . route ( '/qaqc-tools' ) @role_required ( 'qaqc' ) def qaqc_tools (): \"\"\"QA/QC tools - accessible only to qaqc users.\"\"\" return render_template ( 'main/qaqc_tools.html' ) review_panel () Review panel - accessible to reviewers, managers, or admins. Source code in arb\\auth_example_app\\routes\\main.py 49 50 51 52 53 @bp . route ( '/review-panel' ) @roles_required ( 'reviewer' , 'manager' , 'admin' ) def review_panel (): \"\"\"Review panel - accessible to reviewers, managers, or admins.\"\"\" return render_template ( 'main/review_panel.html' )","title":"arb.auth_example_app.routes.main"},{"location":"reference/arb/auth_example_app/routes/main/#arbauth_example_approutesmain","text":"Main routes for the Auth Example App. Demonstrates basic functionality and role-based access control.","title":"arb.auth_example_app.routes.main"},{"location":"reference/arb/auth_example_app/routes/main/#arb.auth_example_app.routes.main.advanced_editing","text":"Advanced editing - requires BOTH editor AND qaqc roles. Source code in arb\\auth_example_app\\routes\\main.py 42 43 44 45 46 @bp . route ( '/advanced-editing' ) @all_roles_required ( 'editor' , 'qaqc' ) def advanced_editing (): \"\"\"Advanced editing - requires BOTH editor AND qaqc roles.\"\"\" return render_template ( 'main/advanced_editing.html' )","title":"advanced_editing"},{"location":"reference/arb/auth_example_app/routes/main/#arb.auth_example_app.routes.main.dashboard","text":"User dashboard - requires login. Source code in arb\\auth_example_app\\routes\\main.py 21 22 23 24 25 @bp . route ( '/dashboard' ) @login_required def dashboard (): \"\"\"User dashboard - requires login.\"\"\" return render_template ( 'main/dashboard.html' )","title":"dashboard"},{"location":"reference/arb/auth_example_app/routes/main/#arb.auth_example_app.routes.main.editor_tools","text":"Editor tools - accessible to editors or admins. Source code in arb\\auth_example_app\\routes\\main.py 28 29 30 31 32 @bp . route ( '/editor-tools' ) @roles_required ( 'editor' , 'admin' ) def editor_tools (): \"\"\"Editor tools - accessible to editors or admins.\"\"\" return render_template ( 'main/editor_tools.html' )","title":"editor_tools"},{"location":"reference/arb/auth_example_app/routes/main/#arb.auth_example_app.routes.main.index","text":"Homepage - accessible to everyone. Source code in arb\\auth_example_app\\routes\\main.py 15 16 17 18 @bp . route ( '/' ) def index (): \"\"\"Homepage - accessible to everyone.\"\"\" return render_template ( 'main/index.html' )","title":"index"},{"location":"reference/arb/auth_example_app/routes/main/#arb.auth_example_app.routes.main.public_info","text":"Public information - accessible to everyone. Source code in arb\\auth_example_app\\routes\\main.py 56 57 58 59 @bp . route ( '/public-info' ) def public_info (): \"\"\"Public information - accessible to everyone.\"\"\" return render_template ( 'main/public_info.html' )","title":"public_info"},{"location":"reference/arb/auth_example_app/routes/main/#arb.auth_example_app.routes.main.qaqc_tools","text":"QA/QC tools - accessible only to qaqc users. Source code in arb\\auth_example_app\\routes\\main.py 35 36 37 38 39 @bp . route ( '/qaqc-tools' ) @role_required ( 'qaqc' ) def qaqc_tools (): \"\"\"QA/QC tools - accessible only to qaqc users.\"\"\" return render_template ( 'main/qaqc_tools.html' )","title":"qaqc_tools"},{"location":"reference/arb/auth_example_app/routes/main/#arb.auth_example_app.routes.main.review_panel","text":"Review panel - accessible to reviewers, managers, or admins. Source code in arb\\auth_example_app\\routes\\main.py 49 50 51 52 53 @bp . route ( '/review-panel' ) @roles_required ( 'reviewer' , 'manager' , 'admin' ) def review_panel (): \"\"\"Review panel - accessible to reviewers, managers, or admins.\"\"\" return render_template ( 'main/review_panel.html' )","title":"review_panel"},{"location":"reference/arb/auth_example_app/routes/user_management/","text":"arb.auth_example_app.routes.user_management User management routes for the Auth Example App. Demonstrates user self-service functionality and profile management. activity () User activity log - accessible to logged-in users. Source code in arb\\auth_example_app\\routes\\user_management.py 67 68 69 70 71 @bp . route ( '/activity' ) @login_required def activity (): \"\"\"User activity log - accessible to logged-in users.\"\"\" return render_template ( 'user_management/activity.html' ) change_password () Change password - accessible to logged-in users. Source code in arb\\auth_example_app\\routes\\user_management.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @bp . route ( '/change-password' , methods = [ 'GET' , 'POST' ]) @login_required def change_password (): \"\"\"Change password - accessible to logged-in users.\"\"\" if request . method == 'POST' : current_password = request . form . get ( 'current_password' ) new_password = request . form . get ( 'new_password' ) confirm_password = request . form . get ( 'confirm_password' ) # Validate current password if not current_password or not current_user . check_password ( current_password ): flash ( 'Current password is incorrect' , 'error' ) return render_template ( 'user_management/change_password.html' ) # Validate new password if not new_password or new_password != confirm_password : flash ( 'New passwords do not match' , 'error' ) return render_template ( 'user_management/change_password.html' ) if len ( new_password ) < 6 : flash ( 'New password must be at least 6 characters' , 'error' ) return render_template ( 'user_management/change_password.html' ) # Update password current_user . set_password ( new_password ) get_db () . session . commit () flash ( 'Password changed successfully!' , 'success' ) return redirect ( url_for ( 'user_management.profile' )) return render_template ( 'user_management/change_password.html' ) edit_profile () Edit user profile - accessible to logged-in users. Source code in arb\\auth_example_app\\routes\\user_management.py 22 23 24 25 26 27 28 29 30 31 @bp . route ( '/profile/edit' , methods = [ 'GET' , 'POST' ]) @login_required def edit_profile (): \"\"\"Edit user profile - accessible to logged-in users.\"\"\" if request . method == 'POST' : # In a real app, you'd validate and update user data flash ( 'Profile updated successfully!' , 'success' ) return redirect ( url_for ( 'user_management.profile' )) return render_template ( 'user_management/edit_profile.html' ) profile () User profile page - accessible to logged-in users. Source code in arb\\auth_example_app\\routes\\user_management.py 15 16 17 18 19 @bp . route ( '/profile' ) @login_required def profile (): \"\"\"User profile page - accessible to logged-in users.\"\"\" return render_template ( 'user_management/profile.html' ) view_roles () View user roles - accessible to logged-in users. Source code in arb\\auth_example_app\\routes\\user_management.py 74 75 76 77 78 @bp . route ( '/roles' ) @login_required def view_roles (): \"\"\"View user roles - accessible to logged-in users.\"\"\" return render_template ( 'user_management/roles.html' )","title":"arb.auth_example_app.routes.user_management"},{"location":"reference/arb/auth_example_app/routes/user_management/#arbauth_example_approutesuser_management","text":"User management routes for the Auth Example App. Demonstrates user self-service functionality and profile management.","title":"arb.auth_example_app.routes.user_management"},{"location":"reference/arb/auth_example_app/routes/user_management/#arb.auth_example_app.routes.user_management.activity","text":"User activity log - accessible to logged-in users. Source code in arb\\auth_example_app\\routes\\user_management.py 67 68 69 70 71 @bp . route ( '/activity' ) @login_required def activity (): \"\"\"User activity log - accessible to logged-in users.\"\"\" return render_template ( 'user_management/activity.html' )","title":"activity"},{"location":"reference/arb/auth_example_app/routes/user_management/#arb.auth_example_app.routes.user_management.change_password","text":"Change password - accessible to logged-in users. Source code in arb\\auth_example_app\\routes\\user_management.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @bp . route ( '/change-password' , methods = [ 'GET' , 'POST' ]) @login_required def change_password (): \"\"\"Change password - accessible to logged-in users.\"\"\" if request . method == 'POST' : current_password = request . form . get ( 'current_password' ) new_password = request . form . get ( 'new_password' ) confirm_password = request . form . get ( 'confirm_password' ) # Validate current password if not current_password or not current_user . check_password ( current_password ): flash ( 'Current password is incorrect' , 'error' ) return render_template ( 'user_management/change_password.html' ) # Validate new password if not new_password or new_password != confirm_password : flash ( 'New passwords do not match' , 'error' ) return render_template ( 'user_management/change_password.html' ) if len ( new_password ) < 6 : flash ( 'New password must be at least 6 characters' , 'error' ) return render_template ( 'user_management/change_password.html' ) # Update password current_user . set_password ( new_password ) get_db () . session . commit () flash ( 'Password changed successfully!' , 'success' ) return redirect ( url_for ( 'user_management.profile' )) return render_template ( 'user_management/change_password.html' )","title":"change_password"},{"location":"reference/arb/auth_example_app/routes/user_management/#arb.auth_example_app.routes.user_management.edit_profile","text":"Edit user profile - accessible to logged-in users. Source code in arb\\auth_example_app\\routes\\user_management.py 22 23 24 25 26 27 28 29 30 31 @bp . route ( '/profile/edit' , methods = [ 'GET' , 'POST' ]) @login_required def edit_profile (): \"\"\"Edit user profile - accessible to logged-in users.\"\"\" if request . method == 'POST' : # In a real app, you'd validate and update user data flash ( 'Profile updated successfully!' , 'success' ) return redirect ( url_for ( 'user_management.profile' )) return render_template ( 'user_management/edit_profile.html' )","title":"edit_profile"},{"location":"reference/arb/auth_example_app/routes/user_management/#arb.auth_example_app.routes.user_management.profile","text":"User profile page - accessible to logged-in users. Source code in arb\\auth_example_app\\routes\\user_management.py 15 16 17 18 19 @bp . route ( '/profile' ) @login_required def profile (): \"\"\"User profile page - accessible to logged-in users.\"\"\" return render_template ( 'user_management/profile.html' )","title":"profile"},{"location":"reference/arb/auth_example_app/routes/user_management/#arb.auth_example_app.routes.user_management.view_roles","text":"View user roles - accessible to logged-in users. Source code in arb\\auth_example_app\\routes\\user_management.py 74 75 76 77 78 @bp . route ( '/roles' ) @login_required def view_roles (): \"\"\"View user roles - accessible to logged-in users.\"\"\" return render_template ( 'user_management/roles.html' )","title":"view_roles"},{"location":"reference/arb/logging/arb_logging/","text":"arb.logging.arb_logging Logging utilities for the ARB Feedback Portal. This module provides functions to configure logging for the main application and standalone scripts, and a utility for pretty-printing complex objects in logs. It is designed to centralize and standardize logging setup and formatting across the project. Module_Attributes APP_DIR_STRUCTURE (list[str]): Default directory structure for resolving the project root. DEFAULT_LOG_FORMAT (str): Default log message format string. DEFAULT_LOG_DATEFMT (str): Default log date format string. Examples: Web app entry point (e.g., wsgi.py): import logging from arb_logging import setup_app_logging setup_app_logging(\"arb_portal\") logger = logging.getLogger(__name__) from arb.portal.app import create_app app = create_app() Scripts (e.g., xl_create.py): import logging from arb_logging import setup_standalone_logging if __name__ == \"__main__\": setup_standalone_logging(\"xl_create\") logger = logging.getLogger(__name__) All other files (including init .py): import logging from arb_logging import get_pretty_printer logger = logging.getLogger(__name__) _, pp_log = get_pretty_printer() Pretty-printing complex objects in logs: The get_pretty_printer() function provides a consistent way to format complex data structures in logs: from arb_logging import get_pretty_printer data = {\"foo\": [1, 2, 3], \"bar\": {\"baz\": \"qux\"}} logger.info(pp_log(data)) # Output in log: # { # 'foo': [1, 2, 3], # 'bar': {'baz': 'qux'} # } Notes Only the first call to logging.basicConfig in a process sets up the global logging configuration. All loggers propagate messages to the root logger unless handlers are added explicitly. Use get_pretty_printer() for consistent formatting of structured log data. Additional Discussion on Key Logging Concepts logging.getLogger(__name__) returns a logger object for each module, but does NOT create a new log file or handler. All loggers propagate messages to the root logger unless you add handlers to them explicitly. Only a call to logging.basicConfig(...) or explicit handler setup creates log files or configures output. The first such call in a process sets up the global logging configuration. Log messages emitted before basicConfig (or handler setup) are sent to a default handler (stderr) or are ignored, depending on severity and environment. After basicConfig , all loggers (including those created earlier) use the configured handlers (e.g., file, console) for output. Multiple calls to getLogger(__name__) in different files give you different logger objects (with different names), but unless you add handlers, all messages go to the same global output. This design allows you to filter or format logs by module, but keeps log file management centralized and predictable. get_pretty_printer ( ** kwargs ) Return a PrettyPrinter instance and a formatting function for structured logging. Only import and use this in files where you want to pretty-print complex data structures in your logs. Parameters: **kwargs ( Any , default: {} ) \u2013 Options passed to pprint.PrettyPrinter (indent, sort_dicts, width, etc.) Returns: tuple: (PrettyPrinter instance, .pformat method) Examples: Basic usage: from arb.logging.arb_logging import get_pretty_printer _, pp_log = get_pretty_printer() data = {\"foo\": [1, 2, 3], \"bar\": {\"baz\": \"qux\"}} logger.info(pp_log(data)) Output in log: { 'foo': [1, 2, 3], 'bar': {'baz': 'qux'} } Custom formatting: _, pp_log = get_pretty_printer(indent=2, width=80) logger.info(pp_log(data)) Source code in arb\\logging\\arb_logging.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def get_pretty_printer ( ** kwargs : Any ) -> tuple [ pprint . PrettyPrinter , Callable [[ object ], str ]]: \"\"\" Return a PrettyPrinter instance and a formatting function for structured logging. Only import and use this in files where you want to pretty-print complex data structures in your logs. Args: **kwargs: Options passed to pprint.PrettyPrinter (indent, sort_dicts, width, etc.) Returns: tuple: (PrettyPrinter instance, .pformat method) Examples: # Basic usage: from arb.logging.arb_logging import get_pretty_printer _, pp_log = get_pretty_printer() data = {\"foo\": [1, 2, 3], \"bar\": {\"baz\": \"qux\"}} logger.info(pp_log(data)) # Output in log: # { # 'foo': [1, 2, 3], # 'bar': {'baz': 'qux'} # } # Custom formatting: _, pp_log = get_pretty_printer(indent=2, width=80) logger.info(pp_log(data)) \"\"\" options = { \"indent\" : 4 , \"sort_dicts\" : False , \"width\" : 120 } options . update ( kwargs ) pp = pprint . PrettyPrinter ( ** options ) return pp , pp . pformat setup_app_logging ( log_name , log_dir = 'logs' , level = logging . DEBUG , app_dir_structure = None , log_format = DEFAULT_LOG_FORMAT , log_datefmt = DEFAULT_LOG_DATEFMT ) Configure logging for the main application (e.g., in wsgi.py). Should be called before importing the app. Parameters: log_name ( str ) \u2013 Name of the log file (without extension). log_dir ( str | Path , default: 'logs' ) \u2013 Directory for log files (relative to project root). level ( int , default: DEBUG ) \u2013 Logging level (default: DEBUG). app_dir_structure ( list [ str ] | None , default: None ) \u2013 Directory structure to identify project root. log_format ( str , default: DEFAULT_LOG_FORMAT ) \u2013 Log message format string. Defaults to ARB portal format. log_datefmt ( str , default: DEFAULT_LOG_DATEFMT ) \u2013 Log date format string. Defaults to ARB portal format. Examples: setup_app_logging(\"arb_portal\") Configures logging to 'logs/arb_portal.log' at DEBUG level Source code in arb\\logging\\arb_logging.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def setup_app_logging ( log_name : str , log_dir : str | Path = \"logs\" , level : int = logging . DEBUG , app_dir_structure = None , log_format : str = DEFAULT_LOG_FORMAT , log_datefmt : str = DEFAULT_LOG_DATEFMT ): \"\"\" Configure logging for the main application (e.g., in wsgi.py). Should be called before importing the app. Args: log_name (str): Name of the log file (without extension). log_dir (str | Path): Directory for log files (relative to project root). level (int): Logging level (default: DEBUG). app_dir_structure (list[str] | None): Directory structure to identify project root. log_format (str): Log message format string. Defaults to ARB portal format. log_datefmt (str): Log date format string. Defaults to ARB portal format. Examples: setup_app_logging(\"arb_portal\") # Configures logging to 'logs/arb_portal.log' at DEBUG level \"\"\" resolved_dir = _resolve_log_dir ( log_dir , app_dir_structure ) logging . basicConfig ( filename = str ( resolved_dir / f \" { log_name } .log\" ), level = level , format = log_format , datefmt = log_datefmt ) print ( f \"[Logging] App logging configured: { resolved_dir / f ' { log_name } .log' } (level= { logging . getLevelName ( level ) } )\" ) setup_standalone_logging ( log_name , log_dir = 'logs' , level = logging . DEBUG , app_dir_structure = None , log_format = DEFAULT_LOG_FORMAT , log_datefmt = DEFAULT_LOG_DATEFMT ) Configure logging for a standalone script. Should be called in the if __name__ == \"__main__\" block. Parameters: log_name ( str ) \u2013 Name of the log file (without extension). log_dir ( str | Path , default: 'logs' ) \u2013 Directory for log files (relative to project root). level ( int , default: DEBUG ) \u2013 Logging level (default: DEBUG). app_dir_structure ( list [ str ] | None , default: None ) \u2013 Directory structure to identify project root. log_format ( str , default: DEFAULT_LOG_FORMAT ) \u2013 Log message format string. Defaults to ARB portal format. log_datefmt ( str , default: DEFAULT_LOG_DATEFMT ) \u2013 Log date format string. Defaults to ARB portal format. Examples: setup_standalone_logging(\"my_script\") Configures logging to 'logs/my_script.log' at DEBUG level Source code in arb\\logging\\arb_logging.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def setup_standalone_logging ( log_name : str , log_dir : str | Path = \"logs\" , level : int = logging . DEBUG , app_dir_structure = None , log_format : str = DEFAULT_LOG_FORMAT , log_datefmt : str = DEFAULT_LOG_DATEFMT ): \"\"\" Configure logging for a standalone script. Should be called in the `if __name__ == \"__main__\"` block. Args: log_name (str): Name of the log file (without extension). log_dir (str | Path): Directory for log files (relative to project root). level (int): Logging level (default: DEBUG). app_dir_structure (list[str] | None): Directory structure to identify project root. log_format (str): Log message format string. Defaults to ARB portal format. log_datefmt (str): Log date format string. Defaults to ARB portal format. Examples: setup_standalone_logging(\"my_script\") # Configures logging to 'logs/my_script.log' at DEBUG level \"\"\" resolved_dir = _resolve_log_dir ( log_dir , app_dir_structure ) logging . basicConfig ( filename = str ( resolved_dir / f \" { log_name } .log\" ), level = level , format = log_format , datefmt = log_datefmt ) print ( f \"[Logging] Standalone logging configured: { resolved_dir / f ' { log_name } .log' } (level= { logging . getLevelName ( level ) } )\" )","title":"arb.logging.arb_logging"},{"location":"reference/arb/logging/arb_logging/#arbloggingarb_logging","text":"Logging utilities for the ARB Feedback Portal. This module provides functions to configure logging for the main application and standalone scripts, and a utility for pretty-printing complex objects in logs. It is designed to centralize and standardize logging setup and formatting across the project. Module_Attributes APP_DIR_STRUCTURE (list[str]): Default directory structure for resolving the project root. DEFAULT_LOG_FORMAT (str): Default log message format string. DEFAULT_LOG_DATEFMT (str): Default log date format string. Examples: Web app entry point (e.g., wsgi.py): import logging from arb_logging import setup_app_logging setup_app_logging(\"arb_portal\") logger = logging.getLogger(__name__) from arb.portal.app import create_app app = create_app() Scripts (e.g., xl_create.py): import logging from arb_logging import setup_standalone_logging if __name__ == \"__main__\": setup_standalone_logging(\"xl_create\") logger = logging.getLogger(__name__) All other files (including init .py): import logging from arb_logging import get_pretty_printer logger = logging.getLogger(__name__) _, pp_log = get_pretty_printer() Pretty-printing complex objects in logs: The get_pretty_printer() function provides a consistent way to format complex data structures in logs: from arb_logging import get_pretty_printer data = {\"foo\": [1, 2, 3], \"bar\": {\"baz\": \"qux\"}} logger.info(pp_log(data)) # Output in log: # { # 'foo': [1, 2, 3], # 'bar': {'baz': 'qux'} # } Notes Only the first call to logging.basicConfig in a process sets up the global logging configuration. All loggers propagate messages to the root logger unless handlers are added explicitly. Use get_pretty_printer() for consistent formatting of structured log data. Additional Discussion on Key Logging Concepts logging.getLogger(__name__) returns a logger object for each module, but does NOT create a new log file or handler. All loggers propagate messages to the root logger unless you add handlers to them explicitly. Only a call to logging.basicConfig(...) or explicit handler setup creates log files or configures output. The first such call in a process sets up the global logging configuration. Log messages emitted before basicConfig (or handler setup) are sent to a default handler (stderr) or are ignored, depending on severity and environment. After basicConfig , all loggers (including those created earlier) use the configured handlers (e.g., file, console) for output. Multiple calls to getLogger(__name__) in different files give you different logger objects (with different names), but unless you add handlers, all messages go to the same global output. This design allows you to filter or format logs by module, but keeps log file management centralized and predictable.","title":"arb.logging.arb_logging"},{"location":"reference/arb/logging/arb_logging/#arb.logging.arb_logging.get_pretty_printer","text":"Return a PrettyPrinter instance and a formatting function for structured logging. Only import and use this in files where you want to pretty-print complex data structures in your logs. Parameters: **kwargs ( Any , default: {} ) \u2013 Options passed to pprint.PrettyPrinter (indent, sort_dicts, width, etc.) Returns: tuple: (PrettyPrinter instance, .pformat method) Examples:","title":"get_pretty_printer"},{"location":"reference/arb/logging/arb_logging/#arb.logging.arb_logging.get_pretty_printer--basic-usage","text":"from arb.logging.arb_logging import get_pretty_printer _, pp_log = get_pretty_printer() data = {\"foo\": [1, 2, 3], \"bar\": {\"baz\": \"qux\"}} logger.info(pp_log(data))","title":"Basic usage:"},{"location":"reference/arb/logging/arb_logging/#arb.logging.arb_logging.get_pretty_printer--output-in-log","text":"","title":"Output in log:"},{"location":"reference/arb/logging/arb_logging/#arb.logging.arb_logging.get_pretty_printer--_1","text":"","title":"{"},{"location":"reference/arb/logging/arb_logging/#arb.logging.arb_logging.get_pretty_printer--foo-1-2-3","text":"","title":"'foo': [1, 2, 3],"},{"location":"reference/arb/logging/arb_logging/#arb.logging.arb_logging.get_pretty_printer--bar-baz-qux","text":"","title":"'bar': {'baz': 'qux'}"},{"location":"reference/arb/logging/arb_logging/#arb.logging.arb_logging.get_pretty_printer--_2","text":"","title":"}"},{"location":"reference/arb/logging/arb_logging/#arb.logging.arb_logging.get_pretty_printer--custom-formatting","text":"_, pp_log = get_pretty_printer(indent=2, width=80) logger.info(pp_log(data)) Source code in arb\\logging\\arb_logging.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def get_pretty_printer ( ** kwargs : Any ) -> tuple [ pprint . PrettyPrinter , Callable [[ object ], str ]]: \"\"\" Return a PrettyPrinter instance and a formatting function for structured logging. Only import and use this in files where you want to pretty-print complex data structures in your logs. Args: **kwargs: Options passed to pprint.PrettyPrinter (indent, sort_dicts, width, etc.) Returns: tuple: (PrettyPrinter instance, .pformat method) Examples: # Basic usage: from arb.logging.arb_logging import get_pretty_printer _, pp_log = get_pretty_printer() data = {\"foo\": [1, 2, 3], \"bar\": {\"baz\": \"qux\"}} logger.info(pp_log(data)) # Output in log: # { # 'foo': [1, 2, 3], # 'bar': {'baz': 'qux'} # } # Custom formatting: _, pp_log = get_pretty_printer(indent=2, width=80) logger.info(pp_log(data)) \"\"\" options = { \"indent\" : 4 , \"sort_dicts\" : False , \"width\" : 120 } options . update ( kwargs ) pp = pprint . PrettyPrinter ( ** options ) return pp , pp . pformat","title":"Custom formatting:"},{"location":"reference/arb/logging/arb_logging/#arb.logging.arb_logging.setup_app_logging","text":"Configure logging for the main application (e.g., in wsgi.py). Should be called before importing the app. Parameters: log_name ( str ) \u2013 Name of the log file (without extension). log_dir ( str | Path , default: 'logs' ) \u2013 Directory for log files (relative to project root). level ( int , default: DEBUG ) \u2013 Logging level (default: DEBUG). app_dir_structure ( list [ str ] | None , default: None ) \u2013 Directory structure to identify project root. log_format ( str , default: DEFAULT_LOG_FORMAT ) \u2013 Log message format string. Defaults to ARB portal format. log_datefmt ( str , default: DEFAULT_LOG_DATEFMT ) \u2013 Log date format string. Defaults to ARB portal format. Examples: setup_app_logging(\"arb_portal\")","title":"setup_app_logging"},{"location":"reference/arb/logging/arb_logging/#arb.logging.arb_logging.setup_app_logging--configures-logging-to-logsarb_portallog-at-debug-level","text":"Source code in arb\\logging\\arb_logging.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 def setup_app_logging ( log_name : str , log_dir : str | Path = \"logs\" , level : int = logging . DEBUG , app_dir_structure = None , log_format : str = DEFAULT_LOG_FORMAT , log_datefmt : str = DEFAULT_LOG_DATEFMT ): \"\"\" Configure logging for the main application (e.g., in wsgi.py). Should be called before importing the app. Args: log_name (str): Name of the log file (without extension). log_dir (str | Path): Directory for log files (relative to project root). level (int): Logging level (default: DEBUG). app_dir_structure (list[str] | None): Directory structure to identify project root. log_format (str): Log message format string. Defaults to ARB portal format. log_datefmt (str): Log date format string. Defaults to ARB portal format. Examples: setup_app_logging(\"arb_portal\") # Configures logging to 'logs/arb_portal.log' at DEBUG level \"\"\" resolved_dir = _resolve_log_dir ( log_dir , app_dir_structure ) logging . basicConfig ( filename = str ( resolved_dir / f \" { log_name } .log\" ), level = level , format = log_format , datefmt = log_datefmt ) print ( f \"[Logging] App logging configured: { resolved_dir / f ' { log_name } .log' } (level= { logging . getLevelName ( level ) } )\" )","title":"Configures logging to 'logs/arb_portal.log' at DEBUG level"},{"location":"reference/arb/logging/arb_logging/#arb.logging.arb_logging.setup_standalone_logging","text":"Configure logging for a standalone script. Should be called in the if __name__ == \"__main__\" block. Parameters: log_name ( str ) \u2013 Name of the log file (without extension). log_dir ( str | Path , default: 'logs' ) \u2013 Directory for log files (relative to project root). level ( int , default: DEBUG ) \u2013 Logging level (default: DEBUG). app_dir_structure ( list [ str ] | None , default: None ) \u2013 Directory structure to identify project root. log_format ( str , default: DEFAULT_LOG_FORMAT ) \u2013 Log message format string. Defaults to ARB portal format. log_datefmt ( str , default: DEFAULT_LOG_DATEFMT ) \u2013 Log date format string. Defaults to ARB portal format. Examples: setup_standalone_logging(\"my_script\")","title":"setup_standalone_logging"},{"location":"reference/arb/logging/arb_logging/#arb.logging.arb_logging.setup_standalone_logging--configures-logging-to-logsmy_scriptlog-at-debug-level","text":"Source code in arb\\logging\\arb_logging.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def setup_standalone_logging ( log_name : str , log_dir : str | Path = \"logs\" , level : int = logging . DEBUG , app_dir_structure = None , log_format : str = DEFAULT_LOG_FORMAT , log_datefmt : str = DEFAULT_LOG_DATEFMT ): \"\"\" Configure logging for a standalone script. Should be called in the `if __name__ == \"__main__\"` block. Args: log_name (str): Name of the log file (without extension). log_dir (str | Path): Directory for log files (relative to project root). level (int): Logging level (default: DEBUG). app_dir_structure (list[str] | None): Directory structure to identify project root. log_format (str): Log message format string. Defaults to ARB portal format. log_datefmt (str): Log date format string. Defaults to ARB portal format. Examples: setup_standalone_logging(\"my_script\") # Configures logging to 'logs/my_script.log' at DEBUG level \"\"\" resolved_dir = _resolve_log_dir ( log_dir , app_dir_structure ) logging . basicConfig ( filename = str ( resolved_dir / f \" { log_name } .log\" ), level = level , format = log_format , datefmt = log_datefmt ) print ( f \"[Logging] Standalone logging configured: { resolved_dir / f ' { log_name } .log' } (level= { logging . getLevelName ( level ) } )\" )","title":"Configures logging to 'logs/my_script.log' at DEBUG level"},{"location":"reference/arb/portal/app/","text":"arb.portal.app Application factory for the ARB Feedback Portal (Flask app). This module defines the create_app() function, which initializes and configures the Flask application with required extensions, startup behavior, routing, and globals. Examples: from arb.portal.app import create_app app = create_app() Notes Used by WSGI, CLI tools, or testing utilities to create the Flask app instance. The logger emits a debug message when this file is loaded. create_app () Create and configure the ARB Feedback Portal Flask application. Follows the Flask application factory pattern. This function loads configuration, initializes extensions, binds SQLAlchemy to the app, and registers the route blueprints and global utilities. Returns: Flask ( Flask ) \u2013 A fully initialized Flask application instance with: - App context globals (dropdowns, types) - SQLAlchemy base metadata ( app.base ) - Registered routes via blueprints Examples: from arb.portal.app import create_app app = create_app() Source code in arb\\portal\\app.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def create_app () -> Flask : \"\"\" Create and configure the ARB Feedback Portal Flask application. Follows the Flask application factory pattern. This function loads configuration, initializes extensions, binds SQLAlchemy to the app, and registers the route blueprints and global utilities. Returns: Flask: A fully initialized Flask application instance with: - App context globals (dropdowns, types) - SQLAlchemy base metadata (`app.base`) - Registered routes via blueprints Examples: from arb.portal.app import create_app app = create_app() \"\"\" app : Flask = Flask ( __name__ ) # Load configuration from config/settings.py app . config . from_object ( get_config ()) logger . info ( f \"App config SQLALCHEMY_DATABASE_URI: { app . config . get ( 'SQLALCHEMY_DATABASE_URI' ) } \" ) # Setup Jinja2, logging, and app-wide config configure_flask_app ( app ) # Initialize Flask extensions db . init_app ( app ) # GPT recommends this, but I'm commenting it out for now # csrf.init_app(app) # Database initialization and reflection (within app context) with app . app_context (): try : db_initialize_and_create () reflect_database () # Load dropdowns, mappings, and other global data base : AutomapBase = get_reflected_base ( db ) # reuse db.metadata without hitting DB again app . base = base # type: ignore[attr-defined] # \u2705 Attach automap base to app object logger . info ( f \"Automap base classes after reflection: { list ( base . classes . keys ()) } \" ) Globals . load_type_mapping ( app , db , base ) Globals . load_drop_downs ( app , db ) except Exception as e : logger . error ( f \"Error during app database setup: { e } \" ) # Register route blueprints app . register_blueprint ( main ) return app","title":"arb.portal.app"},{"location":"reference/arb/portal/app/#arbportalapp","text":"Application factory for the ARB Feedback Portal (Flask app). This module defines the create_app() function, which initializes and configures the Flask application with required extensions, startup behavior, routing, and globals. Examples: from arb.portal.app import create_app app = create_app() Notes Used by WSGI, CLI tools, or testing utilities to create the Flask app instance. The logger emits a debug message when this file is loaded.","title":"arb.portal.app"},{"location":"reference/arb/portal/app/#arb.portal.app.create_app","text":"Create and configure the ARB Feedback Portal Flask application. Follows the Flask application factory pattern. This function loads configuration, initializes extensions, binds SQLAlchemy to the app, and registers the route blueprints and global utilities. Returns: Flask ( Flask ) \u2013 A fully initialized Flask application instance with: - App context globals (dropdowns, types) - SQLAlchemy base metadata ( app.base ) - Registered routes via blueprints Examples: from arb.portal.app import create_app app = create_app() Source code in arb\\portal\\app.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def create_app () -> Flask : \"\"\" Create and configure the ARB Feedback Portal Flask application. Follows the Flask application factory pattern. This function loads configuration, initializes extensions, binds SQLAlchemy to the app, and registers the route blueprints and global utilities. Returns: Flask: A fully initialized Flask application instance with: - App context globals (dropdowns, types) - SQLAlchemy base metadata (`app.base`) - Registered routes via blueprints Examples: from arb.portal.app import create_app app = create_app() \"\"\" app : Flask = Flask ( __name__ ) # Load configuration from config/settings.py app . config . from_object ( get_config ()) logger . info ( f \"App config SQLALCHEMY_DATABASE_URI: { app . config . get ( 'SQLALCHEMY_DATABASE_URI' ) } \" ) # Setup Jinja2, logging, and app-wide config configure_flask_app ( app ) # Initialize Flask extensions db . init_app ( app ) # GPT recommends this, but I'm commenting it out for now # csrf.init_app(app) # Database initialization and reflection (within app context) with app . app_context (): try : db_initialize_and_create () reflect_database () # Load dropdowns, mappings, and other global data base : AutomapBase = get_reflected_base ( db ) # reuse db.metadata without hitting DB again app . base = base # type: ignore[attr-defined] # \u2705 Attach automap base to app object logger . info ( f \"Automap base classes after reflection: { list ( base . classes . keys ()) } \" ) Globals . load_type_mapping ( app , db , base ) Globals . load_drop_downs ( app , db ) except Exception as e : logger . error ( f \"Error during app database setup: { e } \" ) # Register route blueprints app . register_blueprint ( main ) return app","title":"create_app"},{"location":"reference/arb/portal/constants/","text":"arb.portal.constants Shared application-wide constants for the ARB Methane Feedback Portal. These constants are designed to be immutable and centrally maintained. They support consistent behavior and validation across: - Web form defaults and placeholders - Geospatial input validation - Spreadsheet cell parsing - Time zone-aware datetime formatting - Filename-safe timestamp generation Module_Attributes PLEASE_SELECT (str): Placeholder value for dropdowns where no selection is made. MIN_LATITUDE (float): Minimum possible CA latitude. MAX_LATITUDE (float): Maximum possible CA latitude. MIN_LONGITUDE (float): Minimum possible CA longitude. MAX_LONGITUDE (float): Maximum possible CA longitude. GPS_RESOLUTION (int): Desired decimal precision for GPS values. LATITUDE_VALIDATION (dict): Latitude validation schema for WTForms or other validators. LONGITUDE_VALIDATION (dict): Longitude validation schema for WTForms or other validators. UTC_TIME_ZONE (ZoneInfo): UTC (Zulu) timezone. CA_TIME_ZONE (ZoneInfo): California timezone for datetime conversion and formatting. HTML_LOCAL_TIME_FORMAT (str): HTML5-compatible format string for . DATETIME_WITH_SECONDS (str): Filename-safe datetime string format (includes seconds). logger (logging.Logger): Logger instance for this module. Examples: from arb.portal.constants import PLEASE_SELECT, MIN_LATITUDE print(PLEASE_SELECT) Output: 'Please Select' Notes Constants should always be imported from this module instead of redefined. Time zone constants use zoneinfo.ZoneInfo and are safe for use with timezone-aware datetime objects. The logger emits a debug message when this file is loaded. Run this file directly to execute diagnostics on the constants. CA_TIME_ZONE = ZoneInfo ( 'America/Los_Angeles' ) module-attribute ZoneInfo: California timezone for datetime conversion and formatting. Examples: Input : Used to convert UTC datetime to local time. Output: Datetime is converted to America/Los_Angeles timezone. Notes Use with datetime objects for local time handling. Do not modify at runtime. DATETIME_WITH_SECONDS = '%Y_%m_ %d _%H_%M_%S' module-attribute str: Filename-safe datetime string format (includes seconds). Examples: Input : Used to format a datetime for a filename. Output: String formatted as 'YYYY_MM_DD_HH_MM_SS'. Notes Use for filenames and logs. Do not modify at runtime. GPS_RESOLUTION = 5 module-attribute int: Desired decimal precision for GPS values. Examples: Input : GPS value is rounded for display or storage. Output: Value is rounded to 5 decimal places. Notes Used for formatting and validation. Do not modify at runtime. HTML_LOCAL_TIME_FORMAT = '%Y-%m- %d T%H:%M' module-attribute str: HTML5-compatible format string for . Examples: Input : Used to format a datetime for an HTML input field. Output: String formatted as 'YYYY-MM-DDTHH:MM'. Notes Use for HTML5 datetime-local input fields. Do not modify at runtime. LATITUDE_VALIDATION = { 'min' : MIN_LATITUDE , 'max' : MAX_LATITUDE , 'message' : f 'Latitudes must be blank or valid California number between { MIN_LATITUDE } and { MAX_LATITUDE } .' } module-attribute dict: Latitude validation schema for WTForms or other validators. Examples: Input : Used in a WTForms validator for latitude fields. Output: Ensures latitude is within valid CA range. Notes Used for form and API validation. Do not modify at runtime. LONGITUDE_VALIDATION = { 'min' : MIN_LONGITUDE , 'max' : MAX_LONGITUDE , 'message' : f 'Longitudes must be blank or valid California number between { MIN_LONGITUDE } and { MAX_LONGITUDE } .' } module-attribute dict: Longitude validation schema for WTForms or other validators. Examples: Input : Used in a WTForms validator for longitude fields. Output: Ensures longitude is within valid CA range. Notes Used for form and API validation. Do not modify at runtime. MAX_LATITUDE = 42.0 module-attribute float: Maximum possible CA latitude. Examples: Input : User enters a latitude value. Output: Value must be <= 42.0 to be valid for California. Notes Used for geospatial validation. Do not modify at runtime. MAX_LONGITUDE = - 114.0 module-attribute float: Maximum possible CA longitude. Examples: Input : User enters a longitude value. Output: Value must be <= -114.0 to be valid for California. Notes Used for geospatial validation. Do not modify at runtime. MIN_LATITUDE = 32.0 module-attribute float: Minimum possible CA latitude. Examples: Input : User enters a latitude value. Output: Value must be >= 32.0 to be valid for California. Notes Used for geospatial validation. Do not modify at runtime. MIN_LONGITUDE = - 125.0 module-attribute float: Minimum possible CA longitude. Examples: Input : User enters a longitude value. Output: Value must be >= -125.0 to be valid for California. Notes Used for geospatial validation. Do not modify at runtime. PLEASE_SELECT = 'Please Select' module-attribute str: Placeholder value used for dropdowns where no selection is made. Examples: Input : Used as the default value in a dropdown menu. Output: Dropdown displays 'Please Select' as the initial option. Notes Should be compared using equality (==), not identity (is). Do not modify this value at runtime. UTC_TIME_ZONE = ZoneInfo ( 'UTC' ) module-attribute ZoneInfo: UTC (Zulu) timezone. Examples: Input : Used to create timezone-aware datetime objects. Output: Datetime is set to UTC. Notes Use with datetime objects for consistent time handling. Do not modify at runtime.","title":"arb.portal.constants"},{"location":"reference/arb/portal/constants/#arbportalconstants","text":"Shared application-wide constants for the ARB Methane Feedback Portal. These constants are designed to be immutable and centrally maintained. They support consistent behavior and validation across: - Web form defaults and placeholders - Geospatial input validation - Spreadsheet cell parsing - Time zone-aware datetime formatting - Filename-safe timestamp generation Module_Attributes PLEASE_SELECT (str): Placeholder value for dropdowns where no selection is made. MIN_LATITUDE (float): Minimum possible CA latitude. MAX_LATITUDE (float): Maximum possible CA latitude. MIN_LONGITUDE (float): Minimum possible CA longitude. MAX_LONGITUDE (float): Maximum possible CA longitude. GPS_RESOLUTION (int): Desired decimal precision for GPS values. LATITUDE_VALIDATION (dict): Latitude validation schema for WTForms or other validators. LONGITUDE_VALIDATION (dict): Longitude validation schema for WTForms or other validators. UTC_TIME_ZONE (ZoneInfo): UTC (Zulu) timezone. CA_TIME_ZONE (ZoneInfo): California timezone for datetime conversion and formatting. HTML_LOCAL_TIME_FORMAT (str): HTML5-compatible format string for . DATETIME_WITH_SECONDS (str): Filename-safe datetime string format (includes seconds). logger (logging.Logger): Logger instance for this module. Examples: from arb.portal.constants import PLEASE_SELECT, MIN_LATITUDE print(PLEASE_SELECT)","title":"arb.portal.constants"},{"location":"reference/arb/portal/constants/#arb.portal.constants--output-please-select","text":"Notes Constants should always be imported from this module instead of redefined. Time zone constants use zoneinfo.ZoneInfo and are safe for use with timezone-aware datetime objects. The logger emits a debug message when this file is loaded. Run this file directly to execute diagnostics on the constants.","title":"Output: 'Please Select'"},{"location":"reference/arb/portal/constants/#arb.portal.constants.CA_TIME_ZONE","text":"ZoneInfo: California timezone for datetime conversion and formatting. Examples: Input : Used to convert UTC datetime to local time. Output: Datetime is converted to America/Los_Angeles timezone. Notes Use with datetime objects for local time handling. Do not modify at runtime.","title":"CA_TIME_ZONE"},{"location":"reference/arb/portal/constants/#arb.portal.constants.DATETIME_WITH_SECONDS","text":"str: Filename-safe datetime string format (includes seconds). Examples: Input : Used to format a datetime for a filename. Output: String formatted as 'YYYY_MM_DD_HH_MM_SS'. Notes Use for filenames and logs. Do not modify at runtime.","title":"DATETIME_WITH_SECONDS"},{"location":"reference/arb/portal/constants/#arb.portal.constants.GPS_RESOLUTION","text":"int: Desired decimal precision for GPS values. Examples: Input : GPS value is rounded for display or storage. Output: Value is rounded to 5 decimal places. Notes Used for formatting and validation. Do not modify at runtime.","title":"GPS_RESOLUTION"},{"location":"reference/arb/portal/constants/#arb.portal.constants.HTML_LOCAL_TIME_FORMAT","text":"str: HTML5-compatible format string for . Examples: Input : Used to format a datetime for an HTML input field. Output: String formatted as 'YYYY-MM-DDTHH:MM'. Notes Use for HTML5 datetime-local input fields. Do not modify at runtime.","title":"HTML_LOCAL_TIME_FORMAT"},{"location":"reference/arb/portal/constants/#arb.portal.constants.LATITUDE_VALIDATION","text":"dict: Latitude validation schema for WTForms or other validators. Examples: Input : Used in a WTForms validator for latitude fields. Output: Ensures latitude is within valid CA range. Notes Used for form and API validation. Do not modify at runtime.","title":"LATITUDE_VALIDATION"},{"location":"reference/arb/portal/constants/#arb.portal.constants.LONGITUDE_VALIDATION","text":"dict: Longitude validation schema for WTForms or other validators. Examples: Input : Used in a WTForms validator for longitude fields. Output: Ensures longitude is within valid CA range. Notes Used for form and API validation. Do not modify at runtime.","title":"LONGITUDE_VALIDATION"},{"location":"reference/arb/portal/constants/#arb.portal.constants.MAX_LATITUDE","text":"float: Maximum possible CA latitude. Examples: Input : User enters a latitude value. Output: Value must be <= 42.0 to be valid for California. Notes Used for geospatial validation. Do not modify at runtime.","title":"MAX_LATITUDE"},{"location":"reference/arb/portal/constants/#arb.portal.constants.MAX_LONGITUDE","text":"float: Maximum possible CA longitude. Examples: Input : User enters a longitude value. Output: Value must be <= -114.0 to be valid for California. Notes Used for geospatial validation. Do not modify at runtime.","title":"MAX_LONGITUDE"},{"location":"reference/arb/portal/constants/#arb.portal.constants.MIN_LATITUDE","text":"float: Minimum possible CA latitude. Examples: Input : User enters a latitude value. Output: Value must be >= 32.0 to be valid for California. Notes Used for geospatial validation. Do not modify at runtime.","title":"MIN_LATITUDE"},{"location":"reference/arb/portal/constants/#arb.portal.constants.MIN_LONGITUDE","text":"float: Minimum possible CA longitude. Examples: Input : User enters a longitude value. Output: Value must be >= -125.0 to be valid for California. Notes Used for geospatial validation. Do not modify at runtime.","title":"MIN_LONGITUDE"},{"location":"reference/arb/portal/constants/#arb.portal.constants.PLEASE_SELECT","text":"str: Placeholder value used for dropdowns where no selection is made. Examples: Input : Used as the default value in a dropdown menu. Output: Dropdown displays 'Please Select' as the initial option. Notes Should be compared using equality (==), not identity (is). Do not modify this value at runtime.","title":"PLEASE_SELECT"},{"location":"reference/arb/portal/constants/#arb.portal.constants.UTC_TIME_ZONE","text":"ZoneInfo: UTC (Zulu) timezone. Examples: Input : Used to create timezone-aware datetime objects. Output: Datetime is set to UTC. Notes Use with datetime objects for consistent time handling. Do not modify at runtime.","title":"UTC_TIME_ZONE"},{"location":"reference/arb/portal/db_hardcoded/","text":"arb.portal.db_hardcoded Hardcoded testing data and dropdown lookup values for the ARB Methane Feedback Portal. This module provides Dummy incidence records for Oil & Gas and Landfill sectors Lookup values for HTML dropdowns (independent and contingent) Shared test values for local debugging or spreadsheet seeding Module_Attributes logger (logging.Logger): Logger instance for this module. OIL_AND_GAS_SECTORS (list[str]): Oil & Gas sector names for dropdowns. LANDFILL_SECTORS (list[str]): Landfill sector names for dropdowns. _drop_downs (dict): Canonical dropdown data for forms. Do not mutate at runtime. _drop_downs_contingent (dict): Canonical contingent dropdown data for forms. Do not mutate at runtime. Examples: from arb.portal.db_hardcoded import get_og_dummy_form_data data = get_og_dummy_form_data() print(data[\"sector\"]) Output: 'Oil & Gas' Notes Intended for use during development and offline diagnostics. Not suitable for production database seeding. The logger emits a debug message when this file is loaded. The dropdown transformation logic in get_excel_dropdown_data is imported from arb.utils.web_html.update_selector_dict to ensure consistency with the canonical implementation. get_excel_dropdown_data () Return transformed dropdown lookup values for use in HTML form rendering. Returns: tuple ( tuple [ dict , dict ] ) \u2013 dict[str, list[tuple[str, str] | tuple[str, str, dict[str, Any]]]]: Independent dropdowns keyed by HTML field name, with each value as a tuple suitable for form rendering. dict[str, dict[str, list[str | Any]]]: Contingent dropdowns dependent on parent field values, as deep copies of the module-level canonical data. Examples: drop_downs, contingent = get_excel_dropdown_data() print(list(drop_downs.keys())) Output: [ ... field names ... ] Notes The module-level variables _drop_downs and _drop_downs_contingent contain the canonical, human-readable dropdown data as found in Excel templates and business logic. This function returns a deep copy of _drop_downs, transformed using arb.utils.web_html.update_selector_dict, which applies the canonical (value, label) tuple structure and prepends a disabled 'Please Select' option. The contingent dropdowns (_drop_downs_contingent) are returned as a deep copy, unmodified. The transformation logic is always imported from the canonical implementation to ensure consistency and maintainability. NOT COVERED BY UNIT TESTS: This function is not directly covered by unit tests because the transformation logic is delegated to arb.utils.web_html.update_selector_dict, which is tested elsewhere and imported inside the function. Change with caution. See documentation/docstring_update_for_testing.md for details. Source code in arb\\portal\\db_hardcoded.py 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def get_excel_dropdown_data () -> tuple [ dict , dict ]: \"\"\" Return transformed dropdown lookup values for use in HTML form rendering. Returns: tuple: - dict[str, list[tuple[str, str] | tuple[str, str, dict[str, Any]]]]: Independent dropdowns keyed by HTML field name, with each value as a tuple suitable for form rendering. - dict[str, dict[str, list[str | Any]]]: Contingent dropdowns dependent on parent field values, as deep copies of the module-level canonical data. Examples: drop_downs, contingent = get_excel_dropdown_data() print(list(drop_downs.keys())) # Output: [ ... field names ... ] Notes: - The module-level variables _drop_downs and _drop_downs_contingent contain the canonical, human-readable dropdown data as found in Excel templates and business logic. - This function returns a deep copy of _drop_downs, transformed using arb.utils.web_html.update_selector_dict, which applies the canonical (value, label) tuple structure and prepends a disabled 'Please Select' option. - The contingent dropdowns (_drop_downs_contingent) are returned as a deep copy, unmodified. - The transformation logic is always imported from the canonical implementation to ensure consistency and maintainability. - NOT COVERED BY UNIT TESTS: This function is not directly covered by unit tests because the transformation logic is delegated to arb.utils.web_html.update_selector_dict, which is tested elsewhere and imported inside the function. Change with caution. See documentation/docstring_update_for_testing.md for details. \"\"\" drop_downs = copy . deepcopy ( _drop_downs ) drop_downs_contingent = copy . deepcopy ( _drop_downs_contingent ) from arb.utils.web_html import update_selector_dict drop_downs = update_selector_dict ( drop_downs ) return drop_downs , drop_downs_contingent get_landfill_dummy_form_data () Generate dummy Landfill form data as a dictionary. Returns: dict ( dict ) \u2013 Pre-filled key/value pairs simulating user input from the HTML form. Examples: data = get_landfill_dummy_form_data() print(data[\"sector\"]) Output: 'Landfill' Notes Datetime fields are naive (California local), matching what a user would submit. This data is NOT contract-compliant for direct DB storage; it must be converted to UTC-aware ISO 8601 strings before being saved to the database. Source code in arb\\portal\\db_hardcoded.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 def get_landfill_dummy_form_data () -> dict : \"\"\" Generate dummy Landfill form data as a dictionary. Returns: dict: Pre-filled key/value pairs simulating user input from the HTML form. Examples: data = get_landfill_dummy_form_data() print(data[\"sector\"]) # Output: 'Landfill' Notes: - Datetime fields are naive (California local), matching what a user would submit. - This data is NOT contract-compliant for direct DB storage; it must be converted to UTC-aware ISO 8601 strings before being saved to the database. \"\"\" logger . debug ( f \"in landfill_dummy_data()\" ) json_data = { \"additional_activities\" : \"additional_activities\" , \"additional_notes\" : \"additional_notes\" , \"contact_email\" : \"my_email@email.com\" , \"contact_name\" : \"contact_name\" , \"contact_phone\" : f \"(555) 555-5555\" , # \"emission_cause\": PLEASE_SELECT, \"emission_cause_notes\" : \"emission_cause_notes\" , # \"emission_cause_secondary\": PLEASE_SELECT, # \"emission_cause_tertiary\": PLEASE_SELECT, # \"emission_identified_flag_fk\": PLEASE_SELECT, # \"emission_location\": PLEASE_SELECT, \"emission_location_notes\" : \"emission_location_notes\" , # \"emission_type_fk\": PLEASE_SELECT, \"facility_name\" : \"facility_name\" , \"id_arb_swis\" : \"id_arb_swis\" , \"id_incidence\" : 1002030 , \"id_message\" : \"id_message\" , \"id_plume\" : 1002 , # \"included_in_last_lmr\": PLEASE_SELECT, \"included_in_last_lmr_description\" : \"included_in_last_lmr_description\" , \"initial_leak_concentration\" : 1002.5 , \"inspection_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"instrument\" : \"instrument\" , \"last_component_leak_monitoring_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"last_surface_monitoring_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"lat_carb\" : 102.5 , \"lat_revised\" : 103.5 , \"long_carb\" : 104.5 , \"long_revised\" : 105.5 , \"mitigation_actions\" : \"mitigation_actions\" , \"mitigation_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"observation_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), # \"planned_for_next_lmr\": PLEASE_SELECT, \"planned_for_next_lmr_description\" : \"planned_for_next_lmr_description\" , \"re_monitored_concentration\" : 1002.5 , \"re_monitored_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"sector\" : \"Landfill\" , \"sector_type\" : \"Landfill\" , } return json_data get_og_dummy_form_data () Generate dummy Oil & Gas form data as a dictionary. Returns: dict ( dict ) \u2013 Pre-filled key/value pairs simulating user input from the HTML form. Examples: data = get_og_dummy_form_data() print(data[\"sector\"]) Output: 'Oil & Gas' Notes Datetime fields are naive (California local), matching what a user would submit. This data is NOT contract-compliant for direct DB storage; it must be converted to UTC-aware ISO 8601 strings before being saved to the database. Source code in arb\\portal\\db_hardcoded.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def get_og_dummy_form_data () -> dict : \"\"\" Generate dummy Oil & Gas form data as a dictionary. Returns: dict: Pre-filled key/value pairs simulating user input from the HTML form. Examples: data = get_og_dummy_form_data() print(data[\"sector\"]) # Output: 'Oil & Gas' Notes: - Datetime fields are naive (California local), matching what a user would submit. - This data is NOT contract-compliant for direct DB storage; it must be converted to UTC-aware ISO 8601 strings before being saved to the database. \"\"\" json_data = { \"id_incidence\" : 1002050 , \"id_plume\" : 1001 , \"observation_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"lat_carb\" : 100.05 , \"long_carb\" : 100.06 , \"id_message\" : \"id_message response\" , \"facility_name\" : \"facility_name response\" , \"id_arb_eggrt\" : \"1001\" , \"contact_name\" : \"contact_name response\" , \"contact_phone\" : f \"(555) 555-5555\" , \"contact_email\" : \"my_email@email.com\" , # \"venting_exclusion\": PLEASE_SELECT, \"venting_description_1\" : \"venting_description_1 response\" , # \"ogi_performed\": PLEASE_SELECT, \"ogi_date\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), # \"ogi_result\": PLEASE_SELECT, # \"method21_performed\": PLEASE_SELECT, \"method21_date\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), # \"method21_result\": PLEASE_SELECT, \"initial_leak_concentration\" : 1004 , \"venting_description_2\" : \"venting_description_2 response\" , \"initial_mitigation_plan\" : \"initial_mitigation_plan response\" , # \"equipment_at_source\": PLEASE_SELECT, \"equipment_other_description\" : \"equipment_other_description response\" , # \"component_at_source\": PLEASE_SELECT, \"component_other_description\" : \"component_other_description response\" , \"repair_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"final_repair_concentration\" : 101.05 , \"repair_description\" : \"repair_description response\" , \"additional_notes\" : \"additional_notes response\" , \"sector\" : \"Oil & Gas\" , \"sector_type\" : \"Oil & Gas\" , } return json_data","title":"arb.portal.db_hardcoded"},{"location":"reference/arb/portal/db_hardcoded/#arbportaldb_hardcoded","text":"Hardcoded testing data and dropdown lookup values for the ARB Methane Feedback Portal. This module provides Dummy incidence records for Oil & Gas and Landfill sectors Lookup values for HTML dropdowns (independent and contingent) Shared test values for local debugging or spreadsheet seeding Module_Attributes logger (logging.Logger): Logger instance for this module. OIL_AND_GAS_SECTORS (list[str]): Oil & Gas sector names for dropdowns. LANDFILL_SECTORS (list[str]): Landfill sector names for dropdowns. _drop_downs (dict): Canonical dropdown data for forms. Do not mutate at runtime. _drop_downs_contingent (dict): Canonical contingent dropdown data for forms. Do not mutate at runtime. Examples: from arb.portal.db_hardcoded import get_og_dummy_form_data data = get_og_dummy_form_data() print(data[\"sector\"])","title":"arb.portal.db_hardcoded"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded--output-oil-gas","text":"Notes Intended for use during development and offline diagnostics. Not suitable for production database seeding. The logger emits a debug message when this file is loaded. The dropdown transformation logic in get_excel_dropdown_data is imported from arb.utils.web_html.update_selector_dict to ensure consistency with the canonical implementation.","title":"Output: 'Oil &amp; Gas'"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_excel_dropdown_data","text":"Return transformed dropdown lookup values for use in HTML form rendering. Returns: tuple ( tuple [ dict , dict ] ) \u2013 dict[str, list[tuple[str, str] | tuple[str, str, dict[str, Any]]]]: Independent dropdowns keyed by HTML field name, with each value as a tuple suitable for form rendering. dict[str, dict[str, list[str | Any]]]: Contingent dropdowns dependent on parent field values, as deep copies of the module-level canonical data. Examples: drop_downs, contingent = get_excel_dropdown_data() print(list(drop_downs.keys()))","title":"get_excel_dropdown_data"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_excel_dropdown_data--output-field-names","text":"Notes The module-level variables _drop_downs and _drop_downs_contingent contain the canonical, human-readable dropdown data as found in Excel templates and business logic. This function returns a deep copy of _drop_downs, transformed using arb.utils.web_html.update_selector_dict, which applies the canonical (value, label) tuple structure and prepends a disabled 'Please Select' option. The contingent dropdowns (_drop_downs_contingent) are returned as a deep copy, unmodified. The transformation logic is always imported from the canonical implementation to ensure consistency and maintainability. NOT COVERED BY UNIT TESTS: This function is not directly covered by unit tests because the transformation logic is delegated to arb.utils.web_html.update_selector_dict, which is tested elsewhere and imported inside the function. Change with caution. See documentation/docstring_update_for_testing.md for details. Source code in arb\\portal\\db_hardcoded.py 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def get_excel_dropdown_data () -> tuple [ dict , dict ]: \"\"\" Return transformed dropdown lookup values for use in HTML form rendering. Returns: tuple: - dict[str, list[tuple[str, str] | tuple[str, str, dict[str, Any]]]]: Independent dropdowns keyed by HTML field name, with each value as a tuple suitable for form rendering. - dict[str, dict[str, list[str | Any]]]: Contingent dropdowns dependent on parent field values, as deep copies of the module-level canonical data. Examples: drop_downs, contingent = get_excel_dropdown_data() print(list(drop_downs.keys())) # Output: [ ... field names ... ] Notes: - The module-level variables _drop_downs and _drop_downs_contingent contain the canonical, human-readable dropdown data as found in Excel templates and business logic. - This function returns a deep copy of _drop_downs, transformed using arb.utils.web_html.update_selector_dict, which applies the canonical (value, label) tuple structure and prepends a disabled 'Please Select' option. - The contingent dropdowns (_drop_downs_contingent) are returned as a deep copy, unmodified. - The transformation logic is always imported from the canonical implementation to ensure consistency and maintainability. - NOT COVERED BY UNIT TESTS: This function is not directly covered by unit tests because the transformation logic is delegated to arb.utils.web_html.update_selector_dict, which is tested elsewhere and imported inside the function. Change with caution. See documentation/docstring_update_for_testing.md for details. \"\"\" drop_downs = copy . deepcopy ( _drop_downs ) drop_downs_contingent = copy . deepcopy ( _drop_downs_contingent ) from arb.utils.web_html import update_selector_dict drop_downs = update_selector_dict ( drop_downs ) return drop_downs , drop_downs_contingent","title":"Output: [ ... field names ... ]"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_landfill_dummy_form_data","text":"Generate dummy Landfill form data as a dictionary. Returns: dict ( dict ) \u2013 Pre-filled key/value pairs simulating user input from the HTML form. Examples: data = get_landfill_dummy_form_data() print(data[\"sector\"])","title":"get_landfill_dummy_form_data"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_landfill_dummy_form_data--output-landfill","text":"Notes Datetime fields are naive (California local), matching what a user would submit. This data is NOT contract-compliant for direct DB storage; it must be converted to UTC-aware ISO 8601 strings before being saved to the database. Source code in arb\\portal\\db_hardcoded.py 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 def get_landfill_dummy_form_data () -> dict : \"\"\" Generate dummy Landfill form data as a dictionary. Returns: dict: Pre-filled key/value pairs simulating user input from the HTML form. Examples: data = get_landfill_dummy_form_data() print(data[\"sector\"]) # Output: 'Landfill' Notes: - Datetime fields are naive (California local), matching what a user would submit. - This data is NOT contract-compliant for direct DB storage; it must be converted to UTC-aware ISO 8601 strings before being saved to the database. \"\"\" logger . debug ( f \"in landfill_dummy_data()\" ) json_data = { \"additional_activities\" : \"additional_activities\" , \"additional_notes\" : \"additional_notes\" , \"contact_email\" : \"my_email@email.com\" , \"contact_name\" : \"contact_name\" , \"contact_phone\" : f \"(555) 555-5555\" , # \"emission_cause\": PLEASE_SELECT, \"emission_cause_notes\" : \"emission_cause_notes\" , # \"emission_cause_secondary\": PLEASE_SELECT, # \"emission_cause_tertiary\": PLEASE_SELECT, # \"emission_identified_flag_fk\": PLEASE_SELECT, # \"emission_location\": PLEASE_SELECT, \"emission_location_notes\" : \"emission_location_notes\" , # \"emission_type_fk\": PLEASE_SELECT, \"facility_name\" : \"facility_name\" , \"id_arb_swis\" : \"id_arb_swis\" , \"id_incidence\" : 1002030 , \"id_message\" : \"id_message\" , \"id_plume\" : 1002 , # \"included_in_last_lmr\": PLEASE_SELECT, \"included_in_last_lmr_description\" : \"included_in_last_lmr_description\" , \"initial_leak_concentration\" : 1002.5 , \"inspection_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"instrument\" : \"instrument\" , \"last_component_leak_monitoring_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"last_surface_monitoring_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"lat_carb\" : 102.5 , \"lat_revised\" : 103.5 , \"long_carb\" : 104.5 , \"long_revised\" : 105.5 , \"mitigation_actions\" : \"mitigation_actions\" , \"mitigation_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"observation_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), # \"planned_for_next_lmr\": PLEASE_SELECT, \"planned_for_next_lmr_description\" : \"planned_for_next_lmr_description\" , \"re_monitored_concentration\" : 1002.5 , \"re_monitored_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"sector\" : \"Landfill\" , \"sector_type\" : \"Landfill\" , } return json_data","title":"Output: 'Landfill'"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_og_dummy_form_data","text":"Generate dummy Oil & Gas form data as a dictionary. Returns: dict ( dict ) \u2013 Pre-filled key/value pairs simulating user input from the HTML form. Examples: data = get_og_dummy_form_data() print(data[\"sector\"])","title":"get_og_dummy_form_data"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_og_dummy_form_data--output-oil-gas","text":"Notes Datetime fields are naive (California local), matching what a user would submit. This data is NOT contract-compliant for direct DB storage; it must be converted to UTC-aware ISO 8601 strings before being saved to the database. Source code in arb\\portal\\db_hardcoded.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 def get_og_dummy_form_data () -> dict : \"\"\" Generate dummy Oil & Gas form data as a dictionary. Returns: dict: Pre-filled key/value pairs simulating user input from the HTML form. Examples: data = get_og_dummy_form_data() print(data[\"sector\"]) # Output: 'Oil & Gas' Notes: - Datetime fields are naive (California local), matching what a user would submit. - This data is NOT contract-compliant for direct DB storage; it must be converted to UTC-aware ISO 8601 strings before being saved to the database. \"\"\" json_data = { \"id_incidence\" : 1002050 , \"id_plume\" : 1001 , \"observation_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"lat_carb\" : 100.05 , \"long_carb\" : 100.06 , \"id_message\" : \"id_message response\" , \"facility_name\" : \"facility_name response\" , \"id_arb_eggrt\" : \"1001\" , \"contact_name\" : \"contact_name response\" , \"contact_phone\" : f \"(555) 555-5555\" , \"contact_email\" : \"my_email@email.com\" , # \"venting_exclusion\": PLEASE_SELECT, \"venting_description_1\" : \"venting_description_1 response\" , # \"ogi_performed\": PLEASE_SELECT, \"ogi_date\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), # \"ogi_result\": PLEASE_SELECT, # \"method21_performed\": PLEASE_SELECT, \"method21_date\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), # \"method21_result\": PLEASE_SELECT, \"initial_leak_concentration\" : 1004 , \"venting_description_2\" : \"venting_description_2 response\" , \"initial_mitigation_plan\" : \"initial_mitigation_plan response\" , # \"equipment_at_source\": PLEASE_SELECT, \"equipment_other_description\" : \"equipment_other_description response\" , # \"component_at_source\": PLEASE_SELECT, \"component_other_description\" : \"component_other_description response\" , \"repair_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"final_repair_concentration\" : 101.05 , \"repair_description\" : \"repair_description response\" , \"additional_notes\" : \"additional_notes response\" , \"sector\" : \"Oil & Gas\" , \"sector_type\" : \"Oil & Gas\" , } return json_data","title":"Output: 'Oil &amp; Gas'"},{"location":"reference/arb/portal/extensions/","text":"arb.portal.extensions Centralized definition of Flask extension instances used throughout the portal. This module avoids circular imports by creating extension objects (e.g., db , csrf ) at the top level, without initializing them until app.init_app() is called elsewhere. Module_Attributes db (SQLAlchemy): SQLAlchemy instance shared across all models and routes. csrf (CSRFProtect): CSRF protection used for form validation. logger (logging.Logger): Logger instance for this module. Examples: from arb.portal.extensions import db, csrf db.create_all() Use csrf in Flask forms Notes geoalchemy2.Geometry must be imported for spatial field introspection, even if not directly referenced in code. Use with app.app_context(): when accessing db outside a Flask route. The logger emits a debug message when this file is loaded. csrf = CSRFProtect () module-attribute CSRFProtect: Flask-WTF extension for CSRF form protection. Examples: from arb.portal.extensions import csrf csrf.init_app(app) Notes Do not initialize until app.init_app() is called. Use within Flask app context. db = SQLAlchemy () module-attribute SQLAlchemy: Flask SQLAlchemy instance for managing ORM and schema. Examples: from arb.portal.extensions import db db.create_all() Notes Do not initialize until app.init_app() is called. Use within Flask app context.","title":"arb.portal.extensions"},{"location":"reference/arb/portal/extensions/#arbportalextensions","text":"Centralized definition of Flask extension instances used throughout the portal. This module avoids circular imports by creating extension objects (e.g., db , csrf ) at the top level, without initializing them until app.init_app() is called elsewhere. Module_Attributes db (SQLAlchemy): SQLAlchemy instance shared across all models and routes. csrf (CSRFProtect): CSRF protection used for form validation. logger (logging.Logger): Logger instance for this module. Examples: from arb.portal.extensions import db, csrf db.create_all()","title":"arb.portal.extensions"},{"location":"reference/arb/portal/extensions/#arb.portal.extensions--use-csrf-in-flask-forms","text":"Notes geoalchemy2.Geometry must be imported for spatial field introspection, even if not directly referenced in code. Use with app.app_context(): when accessing db outside a Flask route. The logger emits a debug message when this file is loaded.","title":"Use csrf in Flask forms"},{"location":"reference/arb/portal/extensions/#arb.portal.extensions.csrf","text":"CSRFProtect: Flask-WTF extension for CSRF form protection. Examples: from arb.portal.extensions import csrf csrf.init_app(app) Notes Do not initialize until app.init_app() is called. Use within Flask app context.","title":"csrf"},{"location":"reference/arb/portal/extensions/#arb.portal.extensions.db","text":"SQLAlchemy: Flask SQLAlchemy instance for managing ORM and schema. Examples: from arb.portal.extensions import db db.create_all() Notes Do not initialize until app.init_app() is called. Use within Flask app context.","title":"db"},{"location":"reference/arb/portal/globals/","text":"arb.portal.globals Global variables and dropdown selector loading for Flask/SQLAlchemy applications. This module provides the Globals class for holding runtime-initialized data structures such as dropdown selectors and database column type mappings. Module_Attributes logger (logging.Logger): Logger instance for this module. Globals (type): Class for holding runtime-global mappings. Examples: from arb.portal.globals import Globals Globals.load_drop_downs(app, db) print(Globals.drop_downs) Notes Globals are not intended to be mutable after initialization. Centralizes dropdown and type mapping logic for app-wide reuse. Static values that do not require runtime context should live in constants.py . The logger emits a debug message when this file is loaded. Globals Central class for holding runtime-global mappings used in the Flask app. Attributes: db_column_types ( dict ) \u2013 Mapping of table.column to SQLAlchemy type metadata (includes db_type , sa_type , py_type ). drop_downs ( dict ) \u2013 Field name to independent dropdown options. drop_downs_contingent ( dict ) \u2013 Parent-dependent options for contingent dropdowns (e.g., county \u2192 list of subcounties). Examples: Globals.load_drop_downs(app, db) print(Globals.drop_downs) Source code in arb\\portal\\globals.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class Globals : \"\"\" Central class for holding runtime-global mappings used in the Flask app. Attributes: db_column_types (dict): Mapping of table.column to SQLAlchemy type metadata (includes `db_type`, `sa_type`, `py_type`). drop_downs (dict): Field name to independent dropdown options. drop_downs_contingent (dict): Parent-dependent options for contingent dropdowns (e.g., county \u2192 list of subcounties). Examples: Globals.load_drop_downs(app, db) print(Globals.drop_downs) \"\"\" db_column_types = {} drop_downs = {} drop_downs_contingent = {} @classmethod def load_drop_downs ( cls , flask_app : Flask , db : SQLAlchemy ) -> None : \"\"\" Load dropdown data from hardcoded configuration and cache it globally. Args: flask_app (Flask): The active Flask app instance (not used in this function but passed for consistency). db (SQLAlchemy): SQLAlchemy instance (not used in this function but passed for consistency). Returns: None Examples: Globals.load_drop_downs(app, db) print(Globals.drop_downs) Notes: - Uses `get_excel_dropdown_data()` from `db_hardcoded` to populate form options. - Populates both `Globals.drop_downs` and `Globals.drop_downs_contingent`. - Should be called once after app startup or reflection. - NOT COVERED BY UNIT TESTS: This function is not covered by unit tests because the dependency (get_excel_dropdown_data) is imported inside the method body, making it impossible to robustly patch/mock for testing. Change with caution. See documentation/docstring_update_for_testing.md for details. \"\"\" from arb.portal.db_hardcoded import get_excel_dropdown_data logger . debug ( f \"In load_drop_downs()\" ) Globals . drop_downs , Globals . drop_downs_contingent = get_excel_dropdown_data () logger . debug ( f \"Globals.drop_downs= { Globals . drop_downs } \" ) logger . debug ( f \"Globals.drop_downs_contingent= { Globals . drop_downs_contingent } \" ) @classmethod def load_type_mapping ( cls , flask_app : Flask , db : SQLAlchemy , base ) -> None : \"\"\" Populate column type metadata for all reflected tables in the SQLAlchemy base. Args: flask_app (Flask): The current Flask application (used for context scoping). db (SQLAlchemy): SQLAlchemy instance, already bound to a live database engine. base (AutomapBase): Reflected SQLAlchemy metadata containing all mapped models. Returns: None Examples: Globals.load_type_mapping(app, db, base) print(Globals.db_column_types) Notes: - Uses `arb.utils.sql_alchemy.get_sa_automap_types()` for reflection. - The resulting `Globals.db_column_types` is used in form pre-population and validation. - NOT COVERED BY UNIT TESTS: This function is not covered by unit tests because the dependency (get_sa_automap_types) is imported inside the method body, making it impossible to robustly patch/mock for testing. Change with caution. See documentation/docstring_update_for_testing.md for details. \"\"\" from arb.utils.sql_alchemy import get_sa_automap_types with flask_app . app_context (): engine = db . engine Globals . db_column_types = get_sa_automap_types ( engine , base ) logger . debug ( f \"Database type mapping: Globals.db_column_types= { Globals . db_column_types } \" ) load_drop_downs ( flask_app , db ) classmethod Load dropdown data from hardcoded configuration and cache it globally. Parameters: flask_app ( Flask ) \u2013 The active Flask app instance (not used in this function but passed for consistency). db ( SQLAlchemy ) \u2013 SQLAlchemy instance (not used in this function but passed for consistency). Returns: None \u2013 None Examples: Globals.load_drop_downs(app, db) print(Globals.drop_downs) Notes Uses get_excel_dropdown_data() from db_hardcoded to populate form options. Populates both Globals.drop_downs and Globals.drop_downs_contingent . Should be called once after app startup or reflection. NOT COVERED BY UNIT TESTS: This function is not covered by unit tests because the dependency (get_excel_dropdown_data) is imported inside the method body, making it impossible to robustly patch/mock for testing. Change with caution. See documentation/docstring_update_for_testing.md for details. Source code in arb\\portal\\globals.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @classmethod def load_drop_downs ( cls , flask_app : Flask , db : SQLAlchemy ) -> None : \"\"\" Load dropdown data from hardcoded configuration and cache it globally. Args: flask_app (Flask): The active Flask app instance (not used in this function but passed for consistency). db (SQLAlchemy): SQLAlchemy instance (not used in this function but passed for consistency). Returns: None Examples: Globals.load_drop_downs(app, db) print(Globals.drop_downs) Notes: - Uses `get_excel_dropdown_data()` from `db_hardcoded` to populate form options. - Populates both `Globals.drop_downs` and `Globals.drop_downs_contingent`. - Should be called once after app startup or reflection. - NOT COVERED BY UNIT TESTS: This function is not covered by unit tests because the dependency (get_excel_dropdown_data) is imported inside the method body, making it impossible to robustly patch/mock for testing. Change with caution. See documentation/docstring_update_for_testing.md for details. \"\"\" from arb.portal.db_hardcoded import get_excel_dropdown_data logger . debug ( f \"In load_drop_downs()\" ) Globals . drop_downs , Globals . drop_downs_contingent = get_excel_dropdown_data () logger . debug ( f \"Globals.drop_downs= { Globals . drop_downs } \" ) logger . debug ( f \"Globals.drop_downs_contingent= { Globals . drop_downs_contingent } \" ) load_type_mapping ( flask_app , db , base ) classmethod Populate column type metadata for all reflected tables in the SQLAlchemy base. Parameters: flask_app ( Flask ) \u2013 The current Flask application (used for context scoping). db ( SQLAlchemy ) \u2013 SQLAlchemy instance, already bound to a live database engine. base ( AutomapBase ) \u2013 Reflected SQLAlchemy metadata containing all mapped models. Returns: None \u2013 None Examples: Globals.load_type_mapping(app, db, base) print(Globals.db_column_types) Notes Uses arb.utils.sql_alchemy.get_sa_automap_types() for reflection. The resulting Globals.db_column_types is used in form pre-population and validation. NOT COVERED BY UNIT TESTS: This function is not covered by unit tests because the dependency (get_sa_automap_types) is imported inside the method body, making it impossible to robustly patch/mock for testing. Change with caution. See documentation/docstring_update_for_testing.md for details. Source code in arb\\portal\\globals.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 @classmethod def load_type_mapping ( cls , flask_app : Flask , db : SQLAlchemy , base ) -> None : \"\"\" Populate column type metadata for all reflected tables in the SQLAlchemy base. Args: flask_app (Flask): The current Flask application (used for context scoping). db (SQLAlchemy): SQLAlchemy instance, already bound to a live database engine. base (AutomapBase): Reflected SQLAlchemy metadata containing all mapped models. Returns: None Examples: Globals.load_type_mapping(app, db, base) print(Globals.db_column_types) Notes: - Uses `arb.utils.sql_alchemy.get_sa_automap_types()` for reflection. - The resulting `Globals.db_column_types` is used in form pre-population and validation. - NOT COVERED BY UNIT TESTS: This function is not covered by unit tests because the dependency (get_sa_automap_types) is imported inside the method body, making it impossible to robustly patch/mock for testing. Change with caution. See documentation/docstring_update_for_testing.md for details. \"\"\" from arb.utils.sql_alchemy import get_sa_automap_types with flask_app . app_context (): engine = db . engine Globals . db_column_types = get_sa_automap_types ( engine , base ) logger . debug ( f \"Database type mapping: Globals.db_column_types= { Globals . db_column_types } \" )","title":"arb.portal.globals"},{"location":"reference/arb/portal/globals/#arbportalglobals","text":"Global variables and dropdown selector loading for Flask/SQLAlchemy applications. This module provides the Globals class for holding runtime-initialized data structures such as dropdown selectors and database column type mappings. Module_Attributes logger (logging.Logger): Logger instance for this module. Globals (type): Class for holding runtime-global mappings. Examples: from arb.portal.globals import Globals Globals.load_drop_downs(app, db) print(Globals.drop_downs) Notes Globals are not intended to be mutable after initialization. Centralizes dropdown and type mapping logic for app-wide reuse. Static values that do not require runtime context should live in constants.py . The logger emits a debug message when this file is loaded.","title":"arb.portal.globals"},{"location":"reference/arb/portal/globals/#arb.portal.globals.Globals","text":"Central class for holding runtime-global mappings used in the Flask app. Attributes: db_column_types ( dict ) \u2013 Mapping of table.column to SQLAlchemy type metadata (includes db_type , sa_type , py_type ). drop_downs ( dict ) \u2013 Field name to independent dropdown options. drop_downs_contingent ( dict ) \u2013 Parent-dependent options for contingent dropdowns (e.g., county \u2192 list of subcounties). Examples: Globals.load_drop_downs(app, db) print(Globals.drop_downs) Source code in arb\\portal\\globals.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class Globals : \"\"\" Central class for holding runtime-global mappings used in the Flask app. Attributes: db_column_types (dict): Mapping of table.column to SQLAlchemy type metadata (includes `db_type`, `sa_type`, `py_type`). drop_downs (dict): Field name to independent dropdown options. drop_downs_contingent (dict): Parent-dependent options for contingent dropdowns (e.g., county \u2192 list of subcounties). Examples: Globals.load_drop_downs(app, db) print(Globals.drop_downs) \"\"\" db_column_types = {} drop_downs = {} drop_downs_contingent = {} @classmethod def load_drop_downs ( cls , flask_app : Flask , db : SQLAlchemy ) -> None : \"\"\" Load dropdown data from hardcoded configuration and cache it globally. Args: flask_app (Flask): The active Flask app instance (not used in this function but passed for consistency). db (SQLAlchemy): SQLAlchemy instance (not used in this function but passed for consistency). Returns: None Examples: Globals.load_drop_downs(app, db) print(Globals.drop_downs) Notes: - Uses `get_excel_dropdown_data()` from `db_hardcoded` to populate form options. - Populates both `Globals.drop_downs` and `Globals.drop_downs_contingent`. - Should be called once after app startup or reflection. - NOT COVERED BY UNIT TESTS: This function is not covered by unit tests because the dependency (get_excel_dropdown_data) is imported inside the method body, making it impossible to robustly patch/mock for testing. Change with caution. See documentation/docstring_update_for_testing.md for details. \"\"\" from arb.portal.db_hardcoded import get_excel_dropdown_data logger . debug ( f \"In load_drop_downs()\" ) Globals . drop_downs , Globals . drop_downs_contingent = get_excel_dropdown_data () logger . debug ( f \"Globals.drop_downs= { Globals . drop_downs } \" ) logger . debug ( f \"Globals.drop_downs_contingent= { Globals . drop_downs_contingent } \" ) @classmethod def load_type_mapping ( cls , flask_app : Flask , db : SQLAlchemy , base ) -> None : \"\"\" Populate column type metadata for all reflected tables in the SQLAlchemy base. Args: flask_app (Flask): The current Flask application (used for context scoping). db (SQLAlchemy): SQLAlchemy instance, already bound to a live database engine. base (AutomapBase): Reflected SQLAlchemy metadata containing all mapped models. Returns: None Examples: Globals.load_type_mapping(app, db, base) print(Globals.db_column_types) Notes: - Uses `arb.utils.sql_alchemy.get_sa_automap_types()` for reflection. - The resulting `Globals.db_column_types` is used in form pre-population and validation. - NOT COVERED BY UNIT TESTS: This function is not covered by unit tests because the dependency (get_sa_automap_types) is imported inside the method body, making it impossible to robustly patch/mock for testing. Change with caution. See documentation/docstring_update_for_testing.md for details. \"\"\" from arb.utils.sql_alchemy import get_sa_automap_types with flask_app . app_context (): engine = db . engine Globals . db_column_types = get_sa_automap_types ( engine , base ) logger . debug ( f \"Database type mapping: Globals.db_column_types= { Globals . db_column_types } \" )","title":"Globals"},{"location":"reference/arb/portal/globals/#arb.portal.globals.Globals.load_drop_downs","text":"Load dropdown data from hardcoded configuration and cache it globally. Parameters: flask_app ( Flask ) \u2013 The active Flask app instance (not used in this function but passed for consistency). db ( SQLAlchemy ) \u2013 SQLAlchemy instance (not used in this function but passed for consistency). Returns: None \u2013 None Examples: Globals.load_drop_downs(app, db) print(Globals.drop_downs) Notes Uses get_excel_dropdown_data() from db_hardcoded to populate form options. Populates both Globals.drop_downs and Globals.drop_downs_contingent . Should be called once after app startup or reflection. NOT COVERED BY UNIT TESTS: This function is not covered by unit tests because the dependency (get_excel_dropdown_data) is imported inside the method body, making it impossible to robustly patch/mock for testing. Change with caution. See documentation/docstring_update_for_testing.md for details. Source code in arb\\portal\\globals.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @classmethod def load_drop_downs ( cls , flask_app : Flask , db : SQLAlchemy ) -> None : \"\"\" Load dropdown data from hardcoded configuration and cache it globally. Args: flask_app (Flask): The active Flask app instance (not used in this function but passed for consistency). db (SQLAlchemy): SQLAlchemy instance (not used in this function but passed for consistency). Returns: None Examples: Globals.load_drop_downs(app, db) print(Globals.drop_downs) Notes: - Uses `get_excel_dropdown_data()` from `db_hardcoded` to populate form options. - Populates both `Globals.drop_downs` and `Globals.drop_downs_contingent`. - Should be called once after app startup or reflection. - NOT COVERED BY UNIT TESTS: This function is not covered by unit tests because the dependency (get_excel_dropdown_data) is imported inside the method body, making it impossible to robustly patch/mock for testing. Change with caution. See documentation/docstring_update_for_testing.md for details. \"\"\" from arb.portal.db_hardcoded import get_excel_dropdown_data logger . debug ( f \"In load_drop_downs()\" ) Globals . drop_downs , Globals . drop_downs_contingent = get_excel_dropdown_data () logger . debug ( f \"Globals.drop_downs= { Globals . drop_downs } \" ) logger . debug ( f \"Globals.drop_downs_contingent= { Globals . drop_downs_contingent } \" )","title":"load_drop_downs"},{"location":"reference/arb/portal/globals/#arb.portal.globals.Globals.load_type_mapping","text":"Populate column type metadata for all reflected tables in the SQLAlchemy base. Parameters: flask_app ( Flask ) \u2013 The current Flask application (used for context scoping). db ( SQLAlchemy ) \u2013 SQLAlchemy instance, already bound to a live database engine. base ( AutomapBase ) \u2013 Reflected SQLAlchemy metadata containing all mapped models. Returns: None \u2013 None Examples: Globals.load_type_mapping(app, db, base) print(Globals.db_column_types) Notes Uses arb.utils.sql_alchemy.get_sa_automap_types() for reflection. The resulting Globals.db_column_types is used in form pre-population and validation. NOT COVERED BY UNIT TESTS: This function is not covered by unit tests because the dependency (get_sa_automap_types) is imported inside the method body, making it impossible to robustly patch/mock for testing. Change with caution. See documentation/docstring_update_for_testing.md for details. Source code in arb\\portal\\globals.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 @classmethod def load_type_mapping ( cls , flask_app : Flask , db : SQLAlchemy , base ) -> None : \"\"\" Populate column type metadata for all reflected tables in the SQLAlchemy base. Args: flask_app (Flask): The current Flask application (used for context scoping). db (SQLAlchemy): SQLAlchemy instance, already bound to a live database engine. base (AutomapBase): Reflected SQLAlchemy metadata containing all mapped models. Returns: None Examples: Globals.load_type_mapping(app, db, base) print(Globals.db_column_types) Notes: - Uses `arb.utils.sql_alchemy.get_sa_automap_types()` for reflection. - The resulting `Globals.db_column_types` is used in form pre-population and validation. - NOT COVERED BY UNIT TESTS: This function is not covered by unit tests because the dependency (get_sa_automap_types) is imported inside the method body, making it impossible to robustly patch/mock for testing. Change with caution. See documentation/docstring_update_for_testing.md for details. \"\"\" from arb.utils.sql_alchemy import get_sa_automap_types with flask_app . app_context (): engine = db . engine Globals . db_column_types = get_sa_automap_types ( engine , base ) logger . debug ( f \"Database type mapping: Globals.db_column_types= { Globals . db_column_types } \" )","title":"load_type_mapping"},{"location":"reference/arb/portal/json_update_util/","text":"arb.portal.json_update_util Utility functions to apply updates to an SQLAlchemy model's JSON field and log each change to the portal_updates table for auditing purposes. Features Compares current vs. new values in a model's JSON field Logs only meaningful changes to a structured audit table Excludes no-op or default placeholders (e.g., None, \"\") Module_Attributes logger (logging.Logger): Logger instance for this module. Examples: from arb.portal.json_update_util import apply_json_patch_and_log apply_json_patch_and_log(model, updates) Notes Called when a form submission modifies a feedback record, with changes applied to the model and written to the database via SQLAlchemy. The logger emits diagnostic messages for auditing and debugging. apply_json_patch_and_log ( model , updates , json_field = 'misc_json' , user = 'anonymous' , comments = '' ) Apply updates to a model's JSON field and log each change in portal_updates. This function performs a key-by-key comparison between the current JSON field ( model.misc_json by default) and the proposed updates . For each key where the value has changed: - The field is updated - The change is logged to portal_updates with a timestamp and user info - Redundant or placeholder updates are skipped (e.g., None \u2192 None) Parameters: model ( SQLAlchemy model ) \u2013 A SQLAlchemy ORM instance with a JSON column. updates ( dict ) \u2013 Dictionary of key-value updates to apply. json_field ( str , default: 'misc_json' ) \u2013 Name of the JSON field (default: 'misc_json'). user ( str , default: 'anonymous' ) \u2013 Identifier of the user performing the change (default: 'anonymous'). comments ( str , default: '' ) \u2013 Optional comment for the log entry. Returns: None \u2013 None Raises: AttributeError \u2013 If the specified JSON field does not exist on the model. Examples: apply_json_patch_and_log(model, {\"field1\": \"new_value\"}, user=\"alice\") Notes Filters out non-useful updates (e.g., None \u2192 None, None \u2192 \"\", None \u2192 PLEASE_SELECT). Logs all changes to the portal_updates table for auditing. Commits the session after applying changes and logging. Raises and logs exceptions on commit failure. Source code in arb\\portal\\json_update_util.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def apply_json_patch_and_log ( model , updates : dict , json_field : str = \"misc_json\" , user : str = \"anonymous\" , comments : str = \"\" ) -> None : \"\"\" Apply updates to a model's JSON field and log each change in portal_updates. This function performs a key-by-key comparison between the current JSON field (`model.misc_json` by default) and the proposed `updates`. For each key where the value has changed: - The field is updated - The change is logged to `portal_updates` with a timestamp and user info - Redundant or placeholder updates are skipped (e.g., None \u2192 None) Args: model (SQLAlchemy model): A SQLAlchemy ORM instance with a JSON column. updates (dict): Dictionary of key-value updates to apply. json_field (str): Name of the JSON field (default: 'misc_json'). user (str): Identifier of the user performing the change (default: 'anonymous'). comments (str): Optional comment for the log entry. Returns: None Raises: AttributeError: If the specified JSON field does not exist on the model. Examples: apply_json_patch_and_log(model, {\"field1\": \"new_value\"}, user=\"alice\") Notes: - Filters out non-useful updates (e.g., None \u2192 None, None \u2192 \"\", None \u2192 PLEASE_SELECT). - Logs all changes to the portal_updates table for auditing. - Commits the session after applying changes and logging. - Raises and logs exceptions on commit failure. \"\"\" # \ud83c\udd95 DIAGNOSTIC: Log function entry and model state logger . info ( f \"[apply_json_patch_and_log] ENTRY: model= { type ( model ) . __name__ } , \" f \"model.id_incidence= { getattr ( model , 'id_incidence' , 'N/A' ) } , \" f \"updates= { len ( updates ) } fields, json_field= { json_field } \" ) # Check if model is in session session = object_session ( model ) logger . info ( f \"[apply_json_patch_and_log] Model session: { session is not None } , \" f \"Model in session: { model in session if session else False } \" ) # In the future, may want to handle new rows differently json_data = getattr ( model , json_field ) if json_data is None : json_data = {} is_new_row = True else : is_new_row = False logger . info ( f \"[apply_json_patch_and_log] Initial json_data: { json_data } , is_new_row: { is_new_row } \" ) # Consistency check if \"id_incidence\" in json_data and json_data [ \"id_incidence\" ] != model . id_incidence : logger . warning ( f \"[apply_json_patch_and_log] MISMATCH: model.id_incidence= { model . id_incidence } \" f \"!= misc_json['id_incidence']= { json_data [ 'id_incidence' ] } \" ) # Remove id_incidence from updates to avoid contaminating misc_json if \"id_incidence\" in updates : if updates [ \"id_incidence\" ] != model . id_incidence : logger . warning ( f \"[json_update] Removing conflicting id_incidence from updates: \" f \" { updates [ 'id_incidence' ] } \" ) del updates [ \"id_incidence\" ] changes_made = 0 for key , new_value in updates . items (): old_value = json_data . get ( key ) json_data [ key ] = new_value # Filter out non-useful updates if old_value is None and new_value is None : continue if old_value is None and new_value == \"\" : continue # Note, on the rare situation that \"Please Select\" is a valid entry in a string field - it will be filtered out if old_value is None and new_value == PLEASE_SELECT : continue if old_value != new_value : changes_made += 1 log_entry = PortalUpdate ( timestamp = datetime . datetime . now ( datetime . UTC ), key = key , old_value = str ( old_value ), new_value = str ( new_value ), user = user , comments = comments or \"\" , id_incidence = model . id_incidence , ) db . session . add ( log_entry ) logger . debug ( f \"[apply_json_patch_and_log] Added log entry for { key } : { old_value } -> { new_value } \" ) logger . info ( f \"[apply_json_patch_and_log] Applied { changes_made } changes to json_data\" ) setattr ( model , json_field , json_data ) flag_modified ( model , json_field ) # \ud83c\udd95 DIAGNOSTIC: Log before commit logger . info ( f \"[apply_json_patch_and_log] Before commit: model. { json_field } = { getattr ( model , json_field ) } \" ) logger . info ( f \"[apply_json_patch_and_log] About to commit { changes_made } changes to database\" ) try : db . session . commit () logger . info ( f \"[apply_json_patch_and_log] \u2705 COMMIT SUCCESSFUL: { changes_made } changes committed\" ) # \ud83c\udd95 DIAGNOSTIC: Verify model state after commit logger . info ( f \"[apply_json_patch_and_log] After commit: model. { json_field } = { getattr ( model , json_field ) } \" ) # Check if model is still in session after commit session_after = object_session ( model ) logger . info ( f \"[apply_json_patch_and_log] Model session after commit: { session_after is not None } \" ) except Exception as e : logger . error ( f \"[apply_json_patch_and_log] \u274c COMMIT FAILED: { e } \" ) logger . exception ( f \"[apply_json_patch_and_log] Full exception details:\" ) raise","title":"arb.portal.json_update_util"},{"location":"reference/arb/portal/json_update_util/#arbportaljson_update_util","text":"Utility functions to apply updates to an SQLAlchemy model's JSON field and log each change to the portal_updates table for auditing purposes. Features Compares current vs. new values in a model's JSON field Logs only meaningful changes to a structured audit table Excludes no-op or default placeholders (e.g., None, \"\") Module_Attributes logger (logging.Logger): Logger instance for this module. Examples: from arb.portal.json_update_util import apply_json_patch_and_log apply_json_patch_and_log(model, updates) Notes Called when a form submission modifies a feedback record, with changes applied to the model and written to the database via SQLAlchemy. The logger emits diagnostic messages for auditing and debugging.","title":"arb.portal.json_update_util"},{"location":"reference/arb/portal/json_update_util/#arb.portal.json_update_util.apply_json_patch_and_log","text":"Apply updates to a model's JSON field and log each change in portal_updates. This function performs a key-by-key comparison between the current JSON field ( model.misc_json by default) and the proposed updates . For each key where the value has changed: - The field is updated - The change is logged to portal_updates with a timestamp and user info - Redundant or placeholder updates are skipped (e.g., None \u2192 None) Parameters: model ( SQLAlchemy model ) \u2013 A SQLAlchemy ORM instance with a JSON column. updates ( dict ) \u2013 Dictionary of key-value updates to apply. json_field ( str , default: 'misc_json' ) \u2013 Name of the JSON field (default: 'misc_json'). user ( str , default: 'anonymous' ) \u2013 Identifier of the user performing the change (default: 'anonymous'). comments ( str , default: '' ) \u2013 Optional comment for the log entry. Returns: None \u2013 None Raises: AttributeError \u2013 If the specified JSON field does not exist on the model. Examples: apply_json_patch_and_log(model, {\"field1\": \"new_value\"}, user=\"alice\") Notes Filters out non-useful updates (e.g., None \u2192 None, None \u2192 \"\", None \u2192 PLEASE_SELECT). Logs all changes to the portal_updates table for auditing. Commits the session after applying changes and logging. Raises and logs exceptions on commit failure. Source code in arb\\portal\\json_update_util.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def apply_json_patch_and_log ( model , updates : dict , json_field : str = \"misc_json\" , user : str = \"anonymous\" , comments : str = \"\" ) -> None : \"\"\" Apply updates to a model's JSON field and log each change in portal_updates. This function performs a key-by-key comparison between the current JSON field (`model.misc_json` by default) and the proposed `updates`. For each key where the value has changed: - The field is updated - The change is logged to `portal_updates` with a timestamp and user info - Redundant or placeholder updates are skipped (e.g., None \u2192 None) Args: model (SQLAlchemy model): A SQLAlchemy ORM instance with a JSON column. updates (dict): Dictionary of key-value updates to apply. json_field (str): Name of the JSON field (default: 'misc_json'). user (str): Identifier of the user performing the change (default: 'anonymous'). comments (str): Optional comment for the log entry. Returns: None Raises: AttributeError: If the specified JSON field does not exist on the model. Examples: apply_json_patch_and_log(model, {\"field1\": \"new_value\"}, user=\"alice\") Notes: - Filters out non-useful updates (e.g., None \u2192 None, None \u2192 \"\", None \u2192 PLEASE_SELECT). - Logs all changes to the portal_updates table for auditing. - Commits the session after applying changes and logging. - Raises and logs exceptions on commit failure. \"\"\" # \ud83c\udd95 DIAGNOSTIC: Log function entry and model state logger . info ( f \"[apply_json_patch_and_log] ENTRY: model= { type ( model ) . __name__ } , \" f \"model.id_incidence= { getattr ( model , 'id_incidence' , 'N/A' ) } , \" f \"updates= { len ( updates ) } fields, json_field= { json_field } \" ) # Check if model is in session session = object_session ( model ) logger . info ( f \"[apply_json_patch_and_log] Model session: { session is not None } , \" f \"Model in session: { model in session if session else False } \" ) # In the future, may want to handle new rows differently json_data = getattr ( model , json_field ) if json_data is None : json_data = {} is_new_row = True else : is_new_row = False logger . info ( f \"[apply_json_patch_and_log] Initial json_data: { json_data } , is_new_row: { is_new_row } \" ) # Consistency check if \"id_incidence\" in json_data and json_data [ \"id_incidence\" ] != model . id_incidence : logger . warning ( f \"[apply_json_patch_and_log] MISMATCH: model.id_incidence= { model . id_incidence } \" f \"!= misc_json['id_incidence']= { json_data [ 'id_incidence' ] } \" ) # Remove id_incidence from updates to avoid contaminating misc_json if \"id_incidence\" in updates : if updates [ \"id_incidence\" ] != model . id_incidence : logger . warning ( f \"[json_update] Removing conflicting id_incidence from updates: \" f \" { updates [ 'id_incidence' ] } \" ) del updates [ \"id_incidence\" ] changes_made = 0 for key , new_value in updates . items (): old_value = json_data . get ( key ) json_data [ key ] = new_value # Filter out non-useful updates if old_value is None and new_value is None : continue if old_value is None and new_value == \"\" : continue # Note, on the rare situation that \"Please Select\" is a valid entry in a string field - it will be filtered out if old_value is None and new_value == PLEASE_SELECT : continue if old_value != new_value : changes_made += 1 log_entry = PortalUpdate ( timestamp = datetime . datetime . now ( datetime . UTC ), key = key , old_value = str ( old_value ), new_value = str ( new_value ), user = user , comments = comments or \"\" , id_incidence = model . id_incidence , ) db . session . add ( log_entry ) logger . debug ( f \"[apply_json_patch_and_log] Added log entry for { key } : { old_value } -> { new_value } \" ) logger . info ( f \"[apply_json_patch_and_log] Applied { changes_made } changes to json_data\" ) setattr ( model , json_field , json_data ) flag_modified ( model , json_field ) # \ud83c\udd95 DIAGNOSTIC: Log before commit logger . info ( f \"[apply_json_patch_and_log] Before commit: model. { json_field } = { getattr ( model , json_field ) } \" ) logger . info ( f \"[apply_json_patch_and_log] About to commit { changes_made } changes to database\" ) try : db . session . commit () logger . info ( f \"[apply_json_patch_and_log] \u2705 COMMIT SUCCESSFUL: { changes_made } changes committed\" ) # \ud83c\udd95 DIAGNOSTIC: Verify model state after commit logger . info ( f \"[apply_json_patch_and_log] After commit: model. { json_field } = { getattr ( model , json_field ) } \" ) # Check if model is still in session after commit session_after = object_session ( model ) logger . info ( f \"[apply_json_patch_and_log] Model session after commit: { session_after is not None } \" ) except Exception as e : logger . error ( f \"[apply_json_patch_and_log] \u274c COMMIT FAILED: { e } \" ) logger . exception ( f \"[apply_json_patch_and_log] Full exception details:\" ) raise","title":"apply_json_patch_and_log"},{"location":"reference/arb/portal/routes/","text":"arb.portal.routes Blueprint-based route definitions for the ARB Feedback Portal. This module defines all Flask routes originally found in app.py , now organized under the main Blueprint for modularity. Module_Attributes main (Blueprint): Flask Blueprint for all portal routes. logger (logging.Logger): Logger instance for this module. Examples: from arb.portal.routes import main app.register_blueprint(main) Notes All routes assume that create_app() registers the main Blueprint. Developer diagnostics are inlined near the end of the module. apply_staged_update ( id_ ) Apply a staged update to the database for a specific incidence ID. Parameters: id_ ( int ) \u2013 Incidence ID to apply the staged update to. Returns: Response ( Response ) \u2013 Redirect to the incidence update page after applying the update. Examples: In browser: POST /apply_staged_update/123 Redirects to: /incidence_update/123/ Source code in arb\\portal\\routes.py 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 @main . route ( '/apply_staged_update/<int:id_>' , methods = [ 'POST' ]) def apply_staged_update ( id_ : int ) -> Response : \"\"\" Apply a staged update to the database for a specific incidence ID. Args: id_ (int): Incidence ID to apply the staged update to. Returns: Response: Redirect to the incidence update page after applying the update. Examples: # In browser: POST /apply_staged_update/123 # Redirects to: /incidence_update/123/ \"\"\" logger . info ( f \"route called: apply_staged_update with id_: { id_ } \" ) try : # staging_dir = Path(current_app.config[\"UPLOAD_STAGING_FOLDER\"]) staging_dir = Path ( get_upload_folder ()) / \"staging\" staged_file = staging_dir / f \" { id_ } .json\" if not staged_file . exists (): logger . error ( f \"Staged file does not exist: { staged_file } \" ) flash ( \"Staged file not found.\" , \"danger\" ) return redirect ( url_for ( \"main.upload_file_staged\" )) xl_dict , _ = json_load_with_meta ( staged_file ) base = current_app . base # type: ignore[attr-defined] final_id , sector = xl_dict_to_database ( db , base , xl_dict ) logger . info ( f \"Applied staged update for id= { final_id } , sector= { sector } \" ) try : staged_file . unlink () logger . debug ( f \"Deleted staged file: { staged_file } \" ) except Exception as delete_error : logger . warning ( f \"Could not delete staged file: { delete_error } \" ) return redirect ( url_for ( \"main.incidence_update\" , id_ = final_id )) except Exception as e : logger . exception ( \"Failed to apply staged update.\" ) flash ( \"Error applying update. Please try again.\" , \"danger\" ) return redirect ( url_for ( \"main.upload_file_staged\" )) confirm_staged ( id_ , filename ) Confirm and apply a staged update for a specific incidence ID and file. Parameters: id_ ( int ) \u2013 Incidence ID to update. filename ( str ) \u2013 Name of the staged file to confirm. Returns: Response ( ResponseReturnValue ) \u2013 Redirect to the incidence update page after applying the update. Examples: In browser: POST /confirm_staged/123/myfile.xlsx Redirects to: /incidence_update/123/ Source code in arb\\portal\\routes.py 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 @main . route ( \"/confirm_staged/<int:id_>/<filename>\" , methods = [ \"POST\" ]) def confirm_staged ( id_ : int , filename : str ) -> ResponseReturnValue : \"\"\" Confirm and apply a staged update for a specific incidence ID and file. Args: id_ (int): Incidence ID to update. filename (str): Name of the staged file to confirm. Returns: Response: Redirect to the incidence update page after applying the update. Examples: # In browser: POST /confirm_staged/123/myfile.xlsx # Redirects to: /incidence_update/123/ \"\"\" import shutil logger . info ( f \"route called: confirm_staged with id_: { id_ } and filename: { filename } \" ) # Resolve paths root = get_upload_folder () staged_path = os . path . join ( root , \"staging\" , filename ) processed_dir = os . path . join ( root , \"processed\" ) os . makedirs ( processed_dir , exist_ok = True ) processed_path = os . path . join ( processed_dir , filename ) # Load staged payload and metadata try : staged_data , staged_meta = json_load_with_meta ( Path ( staged_path )) base_misc_json = staged_meta . get ( \"base_misc_json\" , {}) except Exception as e : flash ( f \"Failed to load staged file for ID { id_ } : { e } \" , \"danger\" ) return redirect ( url_for ( \"main.upload_file_staged\" )) # Extract form data from the staged JSON structure # staged_data contains: {'metadata': {...}, 'schemas': {...}, 'tab_contents': {'Feedback Form': {...}}} # We need to extract just the form data from tab_contents and include sector from metadata form_data = extract_tab_and_sector ( staged_data , tab_name = \"Feedback Form\" ) if not form_data : flash ( f \"Failed to extract form data from staged file for ID { id_ } \" , \"danger\" ) return redirect ( url_for ( \"main.upload_file_staged\" )) # Get the database model base : AutomapBase = current_app . base # type: ignore[attr-defined] table_name = 'incidences' table = get_class_from_table_name ( base , table_name ) # Get or create the model row logger . info ( f \"[confirm_staged] Getting/creating model row for id_incidence= { id_ } \" ) model_row , _ , is_new_row = get_ensured_row ( db = db , base = base , table_name = table_name , primary_key_name = \"id_incidence\" , id_ = id_ , add_to_session = True ) logger . info ( f \"[confirm_staged] Model row result: type= { type ( model_row ) } , \" f \"id_incidence= { getattr ( model_row , 'id_incidence' , 'N/A' ) } , \" f \"is_new_row= { is_new_row } \" ) # Check for concurrent DB changes current_misc_json = getattr ( model_row , \"misc_json\" , {}) or {} logger . info ( f \"[confirm_staged] Current misc_json: { current_misc_json } \" ) logger . info ( f \"[confirm_staged] Base misc_json from staging: { base_misc_json } \" ) if current_misc_json != base_misc_json : logger . warning ( f \"[confirm_staged] Concurrent DB changes detected! \" f \"current_misc_json != base_misc_json\" ) flash ( \"\u26a0\ufe0f The database was changed by another user before your updates were confirmed. Please review the new database state and reconfirm which fields you wish to update.\" , \"warning\" ) return redirect ( url_for ( \"main.review_staged\" , id_ = id_ , filename = filename )) # Build update patch only for fields user confirmed patch : dict = {} logger . info ( f \"[confirm_staged] Building patch from { len ( form_data ) } form fields\" ) for key in form_data : checkbox_name = f \"confirm_overwrite_ { key } \" confirmed = checkbox_name in request . form new_val = form_data [ key ] # Ensure we have a dictionary to work with, even if misc_json is None misc_json = getattr ( model_row , \"misc_json\" , {}) or {} old_val = misc_json . get ( key ) if confirmed : patch [ key ] = new_val logger . debug ( f \"[confirm_staged] Added to patch: { key } = { new_val } (confirmed= { confirmed } )\" ) logger . info ( f \"[confirm_staged] Final patch contains { len ( patch ) } fields: { list ( patch . keys ()) } \" ) if not patch : logger . warning ( f \"[confirm_staged] No fields in patch - no changes to save\" ) flash ( \"No fields were confirmed for update. No changes saved.\" , \"warning\" ) return redirect ( url_for ( \"main.upload_file_staged\" )) # \ud83c\udd95 Prepare patch for JSON serialization (type coercion, datetime conversion, etc.) patch = prep_payload_for_json ( patch ) logger . info ( f \"[confirm_staged] Prepared patch for JSON: { patch } \" ) # Apply patch to the database model try : logger . info ( f \"[confirm_staged] About to call apply_json_patch_and_log with { len ( patch ) } fields\" ) apply_json_patch_and_log ( model = model_row , updates = patch , json_field = \"misc_json\" , user = \"anonymous\" , comments = f \"Staged update confirmed for ID { id_ } \" ) logger . info ( f \"[confirm_staged] \u2705 apply_json_patch_and_log completed successfully\" ) # \ud83c\udd95 Commit the database transaction to persist changes logger . info ( f \"[confirm_staged] About to commit database session\" ) db . session . commit () logger . info ( f \"[confirm_staged] \u2705 Database session committed successfully\" ) # Move the staged JSON file to the processed directory shutil . move ( staged_path , processed_path ) logger . info ( f \"[confirm_staged] \u2705 Moved staged file to processed: { processed_path } \" ) flash ( f \"\u2705 Successfully updated record { id_ } . { len ( patch ) } fields changed. Staged file moved to processed directory.\" , \"success\" ) except Exception as e : # Rollback on error to prevent partial commits logger . error ( f \"[confirm_staged] \u274c Error during database update: { e } \" ) logger . exception ( f \"[confirm_staged] Full exception details:\" ) db . session . rollback () flash ( f \"\u274c Error applying updates for ID { id_ } : { e } \" , \"danger\" ) return redirect ( url_for ( \"main.upload_file_staged\" )) return redirect ( url_for ( \"main.upload_file_staged\" )) delete_testing_range () Developer utility: Delete testing rows in a specified id_incidence range. NOTE: This is a developer/destructive route, not covered by E2E tests by design. GET: Show form to specify min_id, max_id, and dry_run. POST: Run delete_testing_rows and show summary/results, including id_incidences if dry run. Source code in arb\\portal\\routes.py 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 @main . route ( '/delete_testing_range' , methods = [ 'GET' , 'POST' ]) def delete_testing_range () -> str : \"\"\" Developer utility: Delete testing rows in a specified id_incidence range. NOTE: This is a developer/destructive route, not covered by E2E tests by design. GET: Show form to specify min_id, max_id, and dry_run. POST: Run delete_testing_rows and show summary/results, including id_incidences if dry run. \"\"\" from flask import current_app logger . info ( f \"route called: delete_testing_range\" ) base = current_app . base # type: ignore[attr-defined] error = None result = None min_id = 1000000 max_id = 2000000 dry_run = True submitted = False portal_updates_ids = [] incidences_ids = [] if request . method == 'POST' : try : min_id = int ( request . form . get ( 'min_id' , 1000000 )) max_id = int ( request . form . get ( 'max_id' , 2000000 )) dry_run = bool ( request . form . get ( 'dry_run' )) submitted = True if min_id < 1000000 or max_id < 1000000 : error = \"Both min and max id_incidence must be at least 1000000.\" elif min_id > max_id : error = \"min_id cannot be greater than max_id.\" else : if dry_run : # Get the IDs that would be deleted preview = list_testing_rows ( db , base , min_id , max_id ) portal_updates_ids = sorted ( set ( row [ 'id_incidence' ] for row in preview [ 'portal_updates' ])) incidences_ids = sorted ( row [ 'id_incidence' ] for row in preview [ 'incidences' ]) result = delete_testing_rows ( db , base , min_id , max_id , dry_run = dry_run ) except Exception as e : error = f \"Error: { e } \" instructions = ( \"<ul>\" \"<li><b>Use this tool to delete test rows from the portal_updates and incidences tables.</b></li>\" \"<li>Specify a min and max id_incidence (both must be at least 1000000).</li>\" \"<li>Check 'Dry Run' to preview what would be deleted without making changes.</li>\" \"<li><b>Warning:</b> This cannot delete real data (id_incidence < 1000000 is not allowed).</li>\" \"<li>For safety, always do a dry run first!</li>\" \"</ul>\" ) # Remove summarize_ids and always pass full lists return render_template ( 'delete_testing_range.html' , min_id = min_id , max_id = max_id , dry_run = dry_run , error = error , result = result , submitted = submitted , instructions = instructions , portal_updates_ids = portal_updates_ids , incidences_ids = incidences_ids ) diagnostics () Display developer diagnostics and runtime information. NOTE: This is a developer-only route, not covered by E2E tests by design. Returns: str ( str ) \u2013 Rendered HTML diagnostics page. Examples: In browser: GET /diagnostics Returns: HTML diagnostics info Source code in arb\\portal\\routes.py 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 @main . route ( '/diagnostics' ) def diagnostics () -> str : \"\"\" Display developer diagnostics and runtime information. NOTE: This is a developer-only route, not covered by E2E tests by design. Returns: str: Rendered HTML diagnostics page. Examples: # In browser: GET /diagnostics # Returns: HTML diagnostics info \"\"\" logger . info ( f \"route called: diagnostics\" ) result = find_auto_increment_value ( db , \"incidences\" , \"id_incidence\" ) html_content = f \"<p><strong>Diagnostic Results=</strong></p> <p> { result } </p>\" return render_template ( 'diagnostics.html' , header = \"Auto-Increment Check\" , subheader = \"Next available ID value in the 'incidences' table.\" , html_content = html_content , modal_title = \"Success\" , modal_message = \"Diagnostics completed successfully.\" , ) discard_staged_update ( id_ , filename ) Discard a staged update for a specific incidence ID and filename. CSRF is disabled for this route. Source code in arb\\portal\\routes.py 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 @csrf . exempt @main . route ( \"/discard_staged_update/<int:id_>/<filename>\" , methods = [ \"POST\" ]) def discard_staged_update ( id_ : int , filename : str ) -> ResponseReturnValue : \"\"\" Discard a staged update for a specific incidence ID and filename. CSRF is disabled for this route. \"\"\" logger . info ( f \"route called: discard_staged_update with id_: { id_ } and filename: { filename } \" ) import unicodedata import time staging_dir = Path ( get_upload_folder ()) / \"staging\" # Normalize filename for cross-platform compatibility safe_filename = unicodedata . normalize ( 'NFC' , filename . strip ()) staged_file = staging_dir / safe_filename logger . info ( f \"[DISCARD] Route called: id_= { id_ } , filename=' { filename } ', safe_filename=' { safe_filename } '\" ) logger . info ( f \"[DISCARD] Starting deletion for: { staged_file } \" ) logger . info ( f \"[DISCARD] File exists before deletion: { staged_file . exists () } \" ) try : if staged_file . exists (): staged_file . unlink () logger . info ( f \"[DISCARD] File deleted: { staged_file } \" ) else : logger . info ( f \"[DISCARD] File not found for deletion: { staged_file } \" ) except Exception as e : logger . error ( f \"[DISCARD] Exception during file deletion: { e } \" ) logger . info ( f \"[DISCARD] File exists after deletion: { staged_file . exists () } \" ) logger . info ( f \"[DISCARD] Completed discard_staged_update for: { staged_file } \" ) # Sleep briefly to ensure filesystem updates propagate (for test timing) time . sleep ( 0.2 ) logger . info ( f \"[DISCARD] Returning redirect to /list_staged\" ) return redirect ( url_for ( \"main.list_staged\" )) export_portal_updates () Export all portal update log entries as a CSV file. Returns: Response ( Response ) \u2013 CSV file download of portal update logs. Notes Respects filters set in the /portal_updates page. Uses standard CSV headers and UTF-8 encoding. Source code in arb\\portal\\routes.py 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 @main . route ( \"/portal_updates/export\" ) def export_portal_updates () -> Response : \"\"\" Export all portal update log entries as a CSV file. Returns: Response: CSV file download of portal update logs. Notes: - Respects filters set in the `/portal_updates` page. - Uses standard CSV headers and UTF-8 encoding. \"\"\" logger . info ( f \"route called: export_portal_updates\" ) query = db . session . query ( PortalUpdate ) query = apply_portal_update_filters ( query , PortalUpdate , request . args ) updates = query . order_by ( PortalUpdate . timestamp . desc ()) . all () si = StringIO () writer = csv . writer ( si ) writer . writerow ([ \"timestamp\" , \"key\" , \"old_value\" , \"new_value\" , \"user\" , \"comments\" , \"id_incidence\" ]) for u in updates : writer . writerow ([ u . timestamp , u . key , u . old_value , u . new_value , u . user , u . comments , u . id_incidence or \"\" ]) return Response ( si . getvalue (), mimetype = \"text/csv\" , headers = { \"Content-Disposition\" : \"attachment; filename=portal_updates_export.csv\" } ) incidence_delete ( id_ ) Delete a specified incidence from the database. Parameters: id_ ( int ) \u2013 Primary key of the incidence to delete. Returns: Response ( ResponseReturnValue ) \u2013 Redirect to the homepage after deletion. Examples: In browser: POST /incidence_delete/123/ Redirects to: / Notes Future: consider adding authorization (e.g., CARB password) to restrict access. Source code in arb\\portal\\routes.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 @main . post ( '/incidence_delete/<int:id_>/' ) def incidence_delete ( id_ : int ) -> ResponseReturnValue : \"\"\" Delete a specified incidence from the database. Args: id_ (int): Primary key of the incidence to delete. Returns: Response: Redirect to the homepage after deletion. Examples: # In browser: POST /incidence_delete/123/ # Redirects to: / Notes: - Future: consider adding authorization (e.g., CARB password) to restrict access. \"\"\" logger . info ( f \"route called: incidence_delete with id= { id_ } \" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] table_name = 'incidences' table = get_class_from_table_name ( base , table_name ) if table is None : abort ( 500 , description = \"Could not get table class for incidences\" ) # Type cast to help with SQLAlchemy typing table_class = table # type: Any model_row = db . session . query ( table_class ) . get_or_404 ( id_ ) # todo - ensure portal changes are properly updated arb . utils . sql_alchemy . delete_commit_and_log_model ( db , model_row , comment = f 'Deleting incidence row { id_ } ' ) return redirect ( url_for ( 'main.index' )) incidence_update ( id_ ) Display and edit a specific incidence record by ID. Parameters: id_ ( int ) \u2013 Primary key of the incidence to edit. Returns: Union [ str , Response ] \u2013 str|Response: Rendered HTML of the feedback form for the selected incidence, or a redirect to the upload page if the ID is missing. Raises: 500 Internal Server Error \u2013 If multiple records are found for the same ID. Examples: In browser: GET /incidence_update/123/ Returns: HTML form for editing incidence 123 Notes Redirects if the ID is not found in the database. Assumes each incidence ID is unique. Source code in arb\\portal\\routes.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 @main . route ( '/incidence_update/<int:id_>/' , methods = ( 'GET' , 'POST' )) def incidence_update ( id_ : int ) -> Union [ str , Response ]: \"\"\" Display and edit a specific incidence record by ID. Args: id_ (int): Primary key of the incidence to edit. Returns: str|Response: Rendered HTML of the feedback form for the selected incidence, or a redirect to the upload page if the ID is missing. Raises: 500 Internal Server Error: If multiple records are found for the same ID. Examples: # In browser: GET /incidence_update/123/ # Returns: HTML form for editing incidence 123 Notes: - Redirects if the ID is not found in the database. - Assumes each incidence ID is unique. \"\"\" logger . info ( f \"route called: incidence_update with id= { id_ } .\" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] table_name = 'incidences' table = get_class_from_table_name ( base , table_name ) # get_or_404 uses the tables primary key # model_row = db.session.query(table).get_or_404(id_) # todo turn this into a get and if it is null, then redirect? to the spreadsheet upload # todo consider turning into one_or_none and have error handling if table is None : abort ( 500 , description = \"Could not get table class for incidences\" ) # Type cast to help with SQLAlchemy typing table_class = table # type: Any rows = db . session . query ( table_class ) . filter_by ( id_incidence = id_ ) . all () if not rows : message = f \"A request was made to edit a non-existent id_incidence ( { id_ } ). Consider uploading the incidence by importing a spreadsheet.\" return redirect ( url_for ( 'main.upload_file' , message = message )) if len ( rows ) > 1 : abort ( 500 , description = f \"Multiple rows found for id= { id_ } \" ) model_row = rows [ 0 ] sector , sector_type = get_sector_info ( db , base , id_ ) logger . debug ( f \"calling incidence_prep()\" ) return incidence_prep ( model_row , crud_type = 'update' , sector_type = sector_type , default_dropdown = PLEASE_SELECT ) index () Display the homepage with a list of all existing incidence records. Returns: str ( str ) \u2013 Rendered HTML for the homepage with incidence records. Examples: In browser: GET / Returns: HTML page with table of incidences Source code in arb\\portal\\routes.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 @main . route ( '/' ) def index () -> str : \"\"\" Display the homepage with a list of all existing incidence records. Returns: str: Rendered HTML for the homepage with incidence records. Examples: # In browser: GET / # Returns: HTML page with table of incidences \"\"\" logger . info ( f \"route called: index.\" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] table_name = 'incidences' colum_name_pk = 'id_incidence' rows = get_rows_by_table_name ( db , base , table_name , colum_name_pk , ascending = False ) return render_template ( 'index.html' , model_rows = rows ) java_script_diagnostic_test () Render a simple page for testing JavaScript diagnostics logging (frontend and backend). NOTE: This is a developer-only route, not covered by E2E tests by design. Source code in arb\\portal\\routes.py 1281 1282 1283 1284 1285 1286 1287 1288 1289 @main . route ( '/java_script_diagnostic_test' ) def java_script_diagnostic_test (): \"\"\" Render a simple page for testing JavaScript diagnostics logging (frontend and backend). NOTE: This is a developer-only route, not covered by E2E tests by design. \"\"\" logger . info ( f \"route called: java_script_diagnostic_test\" ) return render_template ( 'java_script_diagnostic_test.html' ) js_diagnostic_log () JavaScript Diagnostics Logging Endpoint NOTE: This is a developer-only route, not covered by E2E tests by design. Purpose This route allows frontend JavaScript code to send diagnostic or debug messages directly to the backend server. These messages are then written to the backend log file, making it possible to correlate frontend/browser events with backend activity, errors, or user actions. This is especially useful for debugging issues that are hard to capture with browser console logs alone (e.g., page reloads, E2E tests, or user-reported problems). How to Use (Frontend JavaScript): Use the provided JS helper function (see below) to send a POST request to this endpoint: Example JS function: function sendJsDiagnostic(msg, extra) { fetch('/js_diagnostic_log', { method: 'POST', headers: {'Content-Type': 'application/json'}, body: JSON.stringify({msg, ts: new Date().toISOString(), ...extra}) }); } Example usage: sendJsDiagnostic('User clicked discard', {action: '/discard_staged_update/0/file.json', userId: 123}); Expected Payload (JSON): { \"msg\": \"A human-readable message (required)\", \"ts\": \"ISO timestamp (optional, will be included in log if present)\", ... any other key-value pairs (optional, will be logged as 'extra') } What Happens The backend receives the POST request and parses the JSON payload. It extracts the 'msg' (message), 'ts' (timestamp), and any other fields (as 'extra'). It writes a log entry to the backend log file in the format: [JS_DIAG][timestamp] message | extra: {...} Returns a simple JSON response: {\"status\": \"ok\"} Why Use This Console logs in the browser are lost on navigation/reload and are not visible to backend developers. This route allows you to persistently log important frontend events, errors, or user actions for later analysis. Especially useful for E2E testing, debugging user issues, or tracking hard-to-reproduce bugs. Security Note This endpoint is for diagnostics only. Do not send sensitive user data. Rate limiting or authentication can be added if needed for production. Source code in arb\\portal\\routes.py 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 @main . route ( '/js_diagnostic_log' , methods = [ 'POST' ]) def js_diagnostic_log (): \"\"\" JavaScript Diagnostics Logging Endpoint ======================================= NOTE: This is a developer-only route, not covered by E2E tests by design. Purpose: This route allows frontend JavaScript code to send diagnostic or debug messages directly to the backend server. These messages are then written to the backend log file, making it possible to correlate frontend/browser events with backend activity, errors, or user actions. This is especially useful for debugging issues that are hard to capture with browser console logs alone (e.g., page reloads, E2E tests, or user-reported problems). How to Use (Frontend JavaScript): Use the provided JS helper function (see below) to send a POST request to this endpoint: Example JS function: function sendJsDiagnostic(msg, extra) { fetch('/js_diagnostic_log', { method: 'POST', headers: {'Content-Type': 'application/json'}, body: JSON.stringify({msg, ts: new Date().toISOString(), ...extra}) }); } Example usage: sendJsDiagnostic('User clicked discard', {action: '/discard_staged_update/0/file.json', userId: 123}); Expected Payload (JSON): { \"msg\": \"A human-readable message (required)\", \"ts\": \"ISO timestamp (optional, will be included in log if present)\", ... any other key-value pairs (optional, will be logged as 'extra') } What Happens: - The backend receives the POST request and parses the JSON payload. - It extracts the 'msg' (message), 'ts' (timestamp), and any other fields (as 'extra'). - It writes a log entry to the backend log file in the format: [JS_DIAG][timestamp] message | extra: {...} - Returns a simple JSON response: {\"status\": \"ok\"} Why Use This: - Console logs in the browser are lost on navigation/reload and are not visible to backend developers. - This route allows you to persistently log important frontend events, errors, or user actions for later analysis. - Especially useful for E2E testing, debugging user issues, or tracking hard-to-reproduce bugs. Security Note: - This endpoint is for diagnostics only. Do not send sensitive user data. - Rate limiting or authentication can be added if needed for production. \"\"\" logger . info ( f \"route called: js_diagnostic_log\" ) data = request . get_json ( force = True , silent = True ) or {} msg = data . get ( 'msg' , '[NO MSG]' ) ts = data . get ( 'ts' ) extra = { k : v for k , v in data . items () if k not in ( 'msg' , 'ts' )} log_line = f \"[JS_DIAG] { '[' + ts + ']' if ts else '' } { msg } \" if extra : log_line += f \" | extra: { extra } \" logger . info ( log_line ) return jsonify ({ 'status' : 'ok' }) landfill_incidence_create () Create a new dummy Landfill incidence and redirect to its edit form. Returns: Response ( Response ) \u2013 Redirect to the incidence_update page for the newly created ID. Examples: In browser: POST /landfill_incidence_create/ Redirects to: /incidence_update/ / Notes Dummy data is loaded from db_hardcoded.get_landfill_dummy_form_data() . Source code in arb\\portal\\routes.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 @main . route ( '/landfill_incidence_create/' , methods = ( 'GET' , 'POST' )) def landfill_incidence_create () -> Response : \"\"\" Create a new dummy Landfill incidence and redirect to its edit form. Returns: Response: Redirect to the `incidence_update` page for the newly created ID. Examples: # In browser: POST /landfill_incidence_create/ # Redirects to: /incidence_update/<new_id>/ Notes: - Dummy data is loaded from `db_hardcoded.get_landfill_dummy_form_data()`. \"\"\" logger . info ( f \"route called: landfill_incidence_create.\" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] table_name = 'incidences' col_name = 'misc_json' data_dict = arb . portal . db_hardcoded . get_landfill_dummy_form_data () id_ = dict_to_database ( db , base , data_dict , table_name = table_name , json_field = col_name , ) logger . debug ( f \"landfill_incidence_create() - leaving.\" ) return redirect ( url_for ( 'main.incidence_update' , id_ = id_ )) list_staged () List all staged files available for review or processing. Returns: str ( ResponseReturnValue ) \u2013 Rendered HTML showing all staged files. Source code in arb\\portal\\routes.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 @main . route ( '/list_staged' ) def list_staged () -> ResponseReturnValue : \"\"\" List all staged files available for review or processing. Returns: str: Rendered HTML showing all staged files. \"\"\" logger . info ( f \"route called: list_staged\" ) logger . info ( \"[LIST_STAGED] Route called\" ) logger . debug ( \"list_staged route called\" ) logger . warning ( '[DEBUG] /list_staged route called' ) staging_dir = Path ( get_upload_folder ()) / \"staging\" staged_files = [] malformed_files = [] if staging_dir . exists (): for file_path in staging_dir . glob ( \"*.json\" ): filename = file_path . name id_incidence = None sector = \"Unknown\" try : # Try to extract ID from filename (format: id_XXXX_ts_YYYYMMDD_HHMMSS.json) if filename . startswith ( \"id_\" ) and \"_ts_\" in filename : id_part = filename . split ( \"_ts_\" )[ 0 ] id_incidence = int ( id_part . replace ( \"id_\" , \"\" )) # Try to load metadata to get sector and check for required fields try : json_data , metadata = json_load_with_meta ( file_path ) base_misc_json = metadata . get ( \"base_misc_json\" , {}) sector = base_misc_json . get ( \"sector\" , \"Unknown\" ) # Check for required field: id_incidence must be a positive integer id_candidate = None # Try to get id_incidence from JSON if not from filename if id_incidence is None : id_candidate = json_data . get ( \"id_incidence\" ) if isinstance ( id_candidate , int ) and id_candidate > 0 : id_incidence = id_candidate # If still not valid, treat as malformed if not ( isinstance ( id_incidence , int ) and id_incidence > 0 ): raise ValueError ( \"Missing or invalid id_incidence\" ) staged_files . append ({ 'filename' : filename , 'id_incidence' : id_incidence , 'sector' : sector , 'file_size' : file_path . stat () . st_size , 'modified_time' : datetime . datetime . fromtimestamp ( file_path . stat () . st_mtime ), 'malformed' : False }) except Exception as meta_exc : # If JSON loads but required fields are missing, treat as malformed logger . warning ( f \"Malformed staged file (missing fields) { file_path } : { meta_exc } \" ) malformed_files . append ({ 'filename' : filename , 'file_size' : file_path . stat () . st_size , 'modified_time' : datetime . datetime . fromtimestamp ( file_path . stat () . st_mtime ), 'error' : f \"Missing required fields: { meta_exc } \" }) except Exception as e : logger . warning ( f \"Could not process staged file { file_path } : { e } \" ) malformed_files . append ({ 'filename' : filename , 'file_size' : file_path . stat () . st_size , 'modified_time' : datetime . datetime . fromtimestamp ( file_path . stat () . st_mtime ), 'error' : str ( e ) }) # Sort by modification time (newest first) staged_files . sort ( key = lambda x : x [ 'modified_time' ], reverse = True ) malformed_files . sort ( key = lambda x : x [ 'modified_time' ], reverse = True ) logger . warning ( f '[DEBUG] /list_staged rendering { len ( staged_files ) } staged, { len ( malformed_files ) } malformed files' ) # Always pass both variables, even if empty return render_template ( 'staged_list.html' , staged_files = staged_files , malformed_files = malformed_files ) list_uploads () List all files in the upload directory. Returns: str ( str ) \u2013 Rendered HTML showing all uploaded Excel files available on disk. Examples: In browser: GET /list_uploads Returns: HTML page listing uploaded files Source code in arb\\portal\\routes.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 @main . route ( '/list_uploads' ) def list_uploads () -> str : \"\"\" List all files in the upload directory. Returns: str: Rendered HTML showing all uploaded Excel files available on disk. Examples: # In browser: GET /list_uploads # Returns: HTML page listing uploaded files \"\"\" logger . info ( f \"route called: list_uploads\" ) upload_folder = get_upload_folder () # up_dir = Path(\"portal/static/uploads\") # print(f\"{type(up_dir)=}: {up_dir=}\") files = [ x . name for x in upload_folder . iterdir () if x . is_file ()] logger . debug ( f \" { files =} \" ) return render_template ( 'uploads_list.html' , files = files ) og_incidence_create () Create a new dummy Oil & Gas incidence and redirect to its edit form. Returns: Response ( Response ) \u2013 Redirect to the incidence_update page for the newly created ID. Examples: In browser: POST /og_incidence_create/ Redirects to: /incidence_update/ / Notes Dummy data is loaded from db_hardcoded.get_og_dummy_form_data() . Source code in arb\\portal\\routes.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 @main . route ( '/og_incidence_create/' , methods = ( 'GET' , 'POST' )) def og_incidence_create () -> Response : \"\"\" Create a new dummy Oil & Gas incidence and redirect to its edit form. Returns: Response: Redirect to the `incidence_update` page for the newly created ID. Examples: # In browser: POST /og_incidence_create/ # Redirects to: /incidence_update/<new_id>/ Notes: - Dummy data is loaded from `db_hardcoded.get_og_dummy_form_data()`. \"\"\" logger . info ( f \"route called: og_incidence_create.\" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] table_name = 'incidences' col_name = 'misc_json' data_dict = arb . portal . db_hardcoded . get_og_dummy_form_data () id_ = dict_to_database ( db , base , data_dict , table_name = table_name , json_field = col_name , ) logger . debug ( f \"og_incidence_create() - leaving.\" ) return redirect ( url_for ( 'main.incidence_update' , id_ = id_ )) review_staged ( id_ , filename ) Review the contents of a staged file for a specific incidence ID. Parameters: id_ ( int ) \u2013 Incidence ID associated with the staged file. filename ( str ) \u2013 Name of the staged file to review. Returns: str | Response \u2013 str|Response: Rendered HTML for file review, or redirect if not found. Examples: In browser: GET /review_staged/123/myfile.xlsx Returns: HTML review page for the file Source code in arb\\portal\\routes.py 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 @main . route ( \"/review_staged/<int:id_>/<filename>\" , methods = [ \"GET\" ]) def review_staged ( id_ : int , filename : str ) -> str | Response : \"\"\" Review the contents of a staged file for a specific incidence ID. Args: id_ (int): Incidence ID associated with the staged file. filename (str): Name of the staged file to review. Returns: str|Response: Rendered HTML for file review, or redirect if not found. Examples: # In browser: GET /review_staged/123/myfile.xlsx # Returns: HTML review page for the file \"\"\" logger . info ( f \"route called: review_staged with id_: { id_ } and filename: { filename } \" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] staging_dir = Path ( get_upload_folder ()) / \"staging\" staged_json_path = staging_dir / filename if not staged_json_path . exists (): logger . warning ( f \"Staged JSON file not found: { staged_json_path } \" ) return render_template ( \"review_staged.html\" , error = f \"No staged data found for ID { id_ } .\" , is_new_row = False , id_incidence = id_ , staged_fields = [], metadata = {}, filename = filename ) try : staged_data , metadata = json_load_with_meta ( staged_json_path ) staged_payload = extract_tab_and_sector ( staged_data , tab_name = \"Feedback Form\" ) except Exception : logger . exception ( \"Error loading staged JSON\" ) return render_template ( \"review_staged.html\" , error = \"Could not load staged data.\" , is_new_row = False , id_incidence = id_ , staged_fields = [], metadata = {}, filename = filename ) model , _ , is_new_row = get_ensured_row ( db = db , base = base , table_name = \"incidences\" , primary_key_name = \"id_incidence\" , id_ = id_ ) db_json = getattr ( model , \"misc_json\" , {}) or {} staged_fields = compute_field_differences ( new_data = staged_payload , existing_data = db_json ) if is_new_row : logger . info ( f \"\u26a0\ufe0f Staged ID { id_ } did not exist in DB. A blank row was created for review.\" ) logger . debug ( f \"Computed { sum ( f [ 'changed' ] for f in staged_fields ) } changes across { len ( staged_fields ) } fields\" ) return render_template ( \"review_staged.html\" , id_incidence = id_ , staged_fields = staged_fields , is_new_row = is_new_row , metadata = metadata , error = None , filename = filename , ) search () Search for incidences or updates in the portal database. Returns: str ( str ) \u2013 Rendered HTML search results page. Notes Currently echoes the user-submitted query string. Source code in arb\\portal\\routes.py 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 @main . route ( '/search/' , methods = ( 'GET' , 'POST' )) def search () -> str : \"\"\" Search for incidences or updates in the portal database. Returns: str: Rendered HTML search results page. Notes: - Currently echoes the user-submitted query string. \"\"\" logger . info ( f \"route called: search\" ) logger . debug ( f \" { request . form =} \" ) search_string = request . form . get ( 'navbar_search' ) logger . debug ( f \" { search_string =} \" ) return render_template ( 'search.html' , search_string = search_string , ) serve_file ( filename ) Serve a file from the uploads directory. Parameters: filename ( str ) \u2013 Name of the file to serve. Returns: Response ( Response ) \u2013 File response for download or viewing in browser. Examples: In browser: GET /serve_file/myfile.xlsx Returns: File download or inline view Source code in arb\\portal\\routes.py 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 @main . route ( \"/serve_file/<path:filename>\" ) def serve_file ( filename ) -> Response : \"\"\" Serve a file from the uploads directory. Args: filename (str): Name of the file to serve. Returns: Response: File response for download or viewing in browser. Examples: # In browser: GET /serve_file/myfile.xlsx # Returns: File download or inline view \"\"\" logger . info ( f \"route called: serve_file with filename: { filename } \" ) upload_folder = get_upload_folder () file_path = os . path . join ( upload_folder , filename ) if not os . path . exists ( file_path ): abort ( 404 ) return send_from_directory ( upload_folder , filename ) show_database_structure () Show the structure of the portal database (tables, columns, types). Returns: str ( str ) \u2013 Rendered HTML of database structure. Examples: In browser: GET /show_database_structure Returns: HTML with database structure Source code in arb\\portal\\routes.py 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 @main . route ( '/show_database_structure' ) def show_database_structure () -> str : \"\"\" Show the structure of the portal database (tables, columns, types). Returns: str: Rendered HTML of database structure. Examples: # In browser: GET /show_database_structure # Returns: HTML with database structure \"\"\" logger . info ( f \"route called: show_database_structure\" ) result = obj_to_html ( Globals . db_column_types ) result = f \"<p><strong>Postgres Database Structure=</strong></p> <p> { result } </p>\" return render_template ( 'diagnostics.html' , header = \"Database Structure Overview\" , subheader = \"Reflecting SQLAlchemy model metadata.\" , html_content = result , ) show_dropdown_dict () Show the current dropdown dictionary used in forms. Returns: str ( str ) \u2013 Rendered HTML of dropdown dictionary. Notes Useful for verifying dropdown contents used in WTForms. Source code in arb\\portal\\routes.py 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 @main . route ( '/show_dropdown_dict' ) def show_dropdown_dict () -> str : \"\"\" Show the current dropdown dictionary used in forms. Returns: str: Rendered HTML of dropdown dictionary. Notes: - Useful for verifying dropdown contents used in WTForms. \"\"\" logger . info ( f \"route called: show_dropdown_dict\" ) # update drop-down tables Globals . load_drop_downs ( current_app , db ) result1 = obj_to_html ( Globals . drop_downs ) result2 = obj_to_html ( Globals . drop_downs_contingent ) result = ( f \"<p><strong>Globals.drop_downs=</strong></p> <p> { result1 } </p>\" f \"<p><strong>Globals.drop_downs_contingent=</strong></p> <p> { result2 } </p>\" ) return render_template ( 'diagnostics.html' , header = \"Dropdown Dictionaries\" , subheader = \"Loaded dropdown values and contingent mappings.\" , html_content = result , ) show_feedback_form_structure () Show the structure of the feedback form (fields, types, validators). Returns: str ( str ) \u2013 Rendered HTML of feedback form structure. Examples: In browser: GET /show_feedback_form_structure Returns: HTML with feedback form structure Source code in arb\\portal\\routes.py 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 @main . route ( '/show_feedback_form_structure' ) def show_feedback_form_structure () -> str : \"\"\" Show the structure of the feedback form (fields, types, validators). Returns: str: Rendered HTML of feedback form structure. Examples: # In browser: GET /show_feedback_form_structure # Returns: HTML with feedback form structure \"\"\" logger . info ( f \"route called: show_feedback_form_structure\" ) form1 = OGFeedback () fields1 = get_wtforms_fields ( form1 ) result1 = obj_to_html ( fields1 ) form2 = LandfillFeedback () fields2 = get_wtforms_fields ( form2 ) result2 = obj_to_html ( fields2 ) result = ( f \"<p><strong>WTF OGFeedback Form Structure=</strong></p> <p> { result1 } </p>\" f \"<p><strong>WTF LandfillFeedback Form Structure=</strong></p> <p> { result2 } </p>\" ) return render_template ( 'diagnostics.html' , header = \"WTForms Feedback Form Structure\" , subheader = \"Inspecting field mappings in Oil & Gas and Landfill feedback forms.\" , html_content = result , ) show_log_file () Display the last N lines of the portal log file. NOTE: This is a developer-only route, not covered by E2E tests by design. Query Parameters lines (int, optional): Number of lines to show from the end of the log file. Defaults to 1000 if not provided or invalid. Args: None Returns: str ( str ) \u2013 Rendered HTML with the log file content shown inside a block. Example Usage /show_log_file?lines=500 Notes Useful for debugging in development or staging. Efficient for large files using read_file_reverse(). Source code in arb\\portal\\routes.py 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 @main . route ( '/show_log_file' ) def show_log_file () -> str : \"\"\" Display the last N lines of the portal log file. NOTE: This is a developer-only route, not covered by E2E tests by design. Query Parameters: lines (int, optional): Number of lines to show from the end of the log file. Defaults to 1000 if not provided or invalid. Args: None Returns: str: Rendered HTML with the log file content shown inside a <pre> block. Example Usage: /show_log_file?lines=500 Notes: - Useful for debugging in development or staging. - Efficient for large files using read_file_reverse(). \"\"\" logger . info ( f \"route called: show_log_file\" ) default_lines = 1000 try : num_lines = int ( request . args . get ( 'lines' , default_lines )) if num_lines < 1 : raise ValueError except ValueError : num_lines = default_lines logger . info ( f \"Displaying the last { num_lines } lines of the log file as a diagnostic\" ) lines = read_file_reverse ( LOG_FILE , n = num_lines ) file_content = ' \\n ' . join ( lines ) result = f \"<p><strong>Last { num_lines } lines of logger file:</strong></p><p><pre> { file_content } </pre></p>\" return render_template ( 'diagnostics.html' , header = \"Log File Contents\" , html_content = result , ) upload_file ( message = None ) Handle file upload form and process uploaded Excel files. Parameters: message ( str | None , default: None ) \u2013 Optional message to display on the upload page. Returns: Union [ str , Response ] \u2013 str|Response: Rendered HTML for the upload form, or redirect after upload. Examples: In browser: GET /upload Returns: HTML upload form In browser: POST /upload Redirects to: /list_uploads or error page Source code in arb\\portal\\routes.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 @main . route ( '/upload' , methods = [ 'GET' , 'POST' ]) @main . route ( '/upload/<message>' , methods = [ 'GET' , 'POST' ]) def upload_file ( message : str | None = None ) -> Union [ str , Response ]: \"\"\" Handle file upload form and process uploaded Excel files. Args: message (str | None): Optional message to display on the upload page. Returns: str|Response: Rendered HTML for the upload form, or redirect after upload. Examples: # In browser: GET /upload # Returns: HTML upload form # In browser: POST /upload # Redirects to: /list_uploads or error page \"\"\" logger . info ( f \"route called: upload_file with message: { message } \" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] form = UploadForm () # Decode redirect message, if present if message : message = unquote ( message ) logger . debug ( f \"Received redirect message: { message } \" ) upload_folder = get_upload_folder () logger . debug ( f \"Files received: { list ( request . files . keys ()) } , upload_folder= { upload_folder } \" ) if request . method == 'POST' : try : request_file = request . files . get ( 'file' ) if not request_file or not request_file . filename : logger . warning ( \"POST received with no file selected.\" ) return render_template ( 'upload.html' , form = form , upload_message = \"No file selected. Please choose a file.\" ) logger . debug ( f \"Received uploaded file: { request_file . filename } \" ) # Step 1: Save file and attempt DB ingest file_path , id_ , sector = upload_and_update_db ( db , upload_folder , request_file , base ) if id_ : logger . debug ( f \"Upload successful: id= { id_ } , sector= { sector } . Redirecting to update page.\" ) return redirect ( url_for ( 'main.incidence_update' , id_ = id_ )) # If id_ is None, check if likely blocked due to missing/invalid id_incidence if file_path and ( file_path . exists () if hasattr ( file_path , 'exists' ) else True ): # Check log message or just show the message if id_ is None logger . warning ( f \"Upload blocked: missing or invalid id_incidence in { file_path . name } \" ) return render_template ( 'upload.html' , form = form , upload_message = ( \"This file is missing a valid 'Incidence/Emission ID' (id_incidence). \" \"Please add a positive integer id_incidence to your spreadsheet before uploading.\" ) ) # Step 2: Handle schema recognition failure with enhanced diagnostics logger . warning ( f \"Upload failed schema recognition: { file_path =} \" ) error_details = generate_upload_diagnostics ( request_file , file_path ) detailed_message = format_diagnostic_message ( error_details , \"Uploaded file format not recognized.\" ) return render_template ( 'upload.html' , form = form , upload_message = detailed_message ) except Exception as e : logger . exception ( \"Exception occurred during upload or parsing.\" ) # Enhanced error handling with diagnostic information error_details = generate_upload_diagnostics ( request_file , file_path if 'file_path' in locals () else None ) detailed_message = format_diagnostic_message ( error_details ) return render_template ( 'upload.html' , form = form , upload_message = detailed_message ) # GET request: display form return render_template ( 'upload.html' , form = form , upload_message = message ) upload_file_staged ( message = None ) Handle staged file upload form and process staged Excel files. Parameters: message ( str | None , default: None ) \u2013 Optional message to display on the staged upload page. Returns: Union [ str , Response ] \u2013 str|Response: Rendered HTML for the staged upload form, or redirect after upload. Examples: In browser: GET /upload_staged Returns: HTML staged upload form In browser: POST /upload_staged Redirects to: /list_staged or error page Source code in arb\\portal\\routes.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 @main . route ( '/upload_staged' , methods = [ 'GET' , 'POST' ]) @main . route ( '/upload_staged/<message>' , methods = [ 'GET' , 'POST' ]) def upload_file_staged ( message : str | None = None ) -> Union [ str , Response ]: \"\"\" Handle staged file upload form and process staged Excel files. Args: message (str | None): Optional message to display on the staged upload page. Returns: str|Response: Rendered HTML for the staged upload form, or redirect after upload. Examples: # In browser: GET /upload_staged # Returns: HTML staged upload form # In browser: POST /upload_staged # Redirects to: /list_staged or error page \"\"\" logger . info ( f \"route called: upload_file_staged with message: { message } \" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] form = UploadForm () # Decode optional redirect message if message : message = unquote ( message ) logger . debug ( f \"Received redirect message: { message } \" ) upload_folder = get_upload_folder () logger . debug ( f \"Request received with files: { list ( request . files . keys ()) } , upload_folder= { upload_folder } \" ) if request . method == 'POST' : try : request_file = request . files . get ( 'file' ) if not request_file or not request_file . filename : logger . warning ( \"POST received with no file selected.\" ) return render_template ( 'upload_staged.html' , form = form , upload_message = \"No file selected. Please choose a file.\" ) logger . debug ( f \"Received uploaded file: { request_file . filename } \" ) # Save and stage (no DB commit) file_path , id_ , sector , json_data , staged_filename = upload_and_stage_only ( db , upload_folder , request_file , base ) if id_ and staged_filename : logger . debug ( f \"Staged upload successful: id= { id_ } , sector= { sector } , filename= { staged_filename } . Redirecting to review page.\" ) # Enhanced success feedback with staging details success_message = ( f \"\u2705 File ' { request_file . filename } ' staged successfully! \\n \" f \"\ud83d\udccb ID: { id_ } \\n \" f \"\ud83c\udfed Sector: { sector } \\n \" f \"\ud83d\udcc1 Staged as: { staged_filename } \\n \" f \"\ud83d\udd0d Ready for review and confirmation.\" ) flash ( success_message , \"success\" ) return redirect ( url_for ( 'main.review_staged' , id_ = id_ , filename = staged_filename )) # If id_ is None or not staged, check if likely blocked due to missing/invalid id_incidence if file_path and ( file_path . exists () if hasattr ( file_path , 'exists' ) else True ): logger . warning ( f \"Staging blocked: missing or invalid id_incidence in { file_path . name } \" ) return render_template ( 'upload_staged.html' , form = form , upload_message = ( \"This file is missing a valid 'Incidence/Emission ID' (id_incidence). \" \"Please add a positive integer id_incidence to your spreadsheet before uploading.\" ) ) # Fallback: schema recognition failure or other error logger . warning ( f \"Staging failed: missing or invalid id_incidence in { file_path . name } \" ) return render_template ( 'upload_staged.html' , form = form , upload_message = \"This file is missing a valid 'Incidence/Emission ID' (id_incidence). \" \"Please verify the spreadsheet includes that field and try again.\" ) except Exception as e : logger . exception ( \"Exception occurred during staged upload.\" ) # Enhanced error handling with staging-specific diagnostic information error_details = generate_staging_diagnostics ( request_file , file_path if 'file_path' in locals () else None , staged_filename if 'staged_filename' in locals () else None , id_ if 'id_' in locals () else None , sector if 'sector' in locals () else None ) detailed_message = format_diagnostic_message ( error_details , \"Staged upload processing failed.\" ) return render_template ( 'upload_staged.html' , form = form , upload_message = detailed_message ) # GET request: display form return render_template ( 'upload_staged.html' , form = form , upload_message = message ) view_portal_updates () Display a table of all portal update log entries. Returns: str ( str ) \u2013 Rendered HTML table of portal update logs. Notes Supports pagination, filtering, and sorting via query parameters. Default sort is descending by timestamp. Source code in arb\\portal\\routes.py 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 @main . route ( \"/portal_updates\" ) def view_portal_updates () -> str : \"\"\" Display a table of all portal update log entries. Returns: str: Rendered HTML table of portal update logs. Notes: - Supports pagination, filtering, and sorting via query parameters. - Default sort is descending by timestamp. \"\"\" sort_by = request . args . get ( \"sort_by\" , \"timestamp\" ) direction = request . args . get ( \"direction\" , \"desc\" ) page = int ( request . args . get ( \"page\" , 1 )) per_page = int ( request . args . get ( \"per_page\" , 100 )) query = db . session . query ( PortalUpdate ) query = apply_portal_update_filters ( query , PortalUpdate , request . args ) updates = query . order_by ( PortalUpdate . timestamp . desc ()) . all () return render_template ( \"portal_updates.html\" , updates = updates , sort_by = sort_by , direction = direction , page = page , per_page = per_page , total_pages = 1 , filter_key = request . args . get ( \"filter_key\" , \"\" ) . strip (), filter_user = request . args . get ( \"filter_user\" , \"\" ) . strip (), filter_comments = request . args . get ( \"filter_comments\" , \"\" ) . strip (), filter_id_incidence = request . args . get ( \"filter_id_incidence\" , \"\" ) . strip (), start_date = request . args . get ( \"start_date\" , \"\" ) . strip (), end_date = request . args . get ( \"end_date\" , \"\" ) . strip (), )","title":"arb.portal.routes"},{"location":"reference/arb/portal/routes/#arbportalroutes","text":"Blueprint-based route definitions for the ARB Feedback Portal. This module defines all Flask routes originally found in app.py , now organized under the main Blueprint for modularity. Module_Attributes main (Blueprint): Flask Blueprint for all portal routes. logger (logging.Logger): Logger instance for this module. Examples: from arb.portal.routes import main app.register_blueprint(main) Notes All routes assume that create_app() registers the main Blueprint. Developer diagnostics are inlined near the end of the module.","title":"arb.portal.routes"},{"location":"reference/arb/portal/routes/#arb.portal.routes.apply_staged_update","text":"Apply a staged update to the database for a specific incidence ID. Parameters: id_ ( int ) \u2013 Incidence ID to apply the staged update to. Returns: Response ( Response ) \u2013 Redirect to the incidence update page after applying the update. Examples:","title":"apply_staged_update"},{"location":"reference/arb/portal/routes/#arb.portal.routes.apply_staged_update--in-browser-post-apply_staged_update123","text":"","title":"In browser: POST /apply_staged_update/123"},{"location":"reference/arb/portal/routes/#arb.portal.routes.apply_staged_update--redirects-to-incidence_update123","text":"Source code in arb\\portal\\routes.py 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 @main . route ( '/apply_staged_update/<int:id_>' , methods = [ 'POST' ]) def apply_staged_update ( id_ : int ) -> Response : \"\"\" Apply a staged update to the database for a specific incidence ID. Args: id_ (int): Incidence ID to apply the staged update to. Returns: Response: Redirect to the incidence update page after applying the update. Examples: # In browser: POST /apply_staged_update/123 # Redirects to: /incidence_update/123/ \"\"\" logger . info ( f \"route called: apply_staged_update with id_: { id_ } \" ) try : # staging_dir = Path(current_app.config[\"UPLOAD_STAGING_FOLDER\"]) staging_dir = Path ( get_upload_folder ()) / \"staging\" staged_file = staging_dir / f \" { id_ } .json\" if not staged_file . exists (): logger . error ( f \"Staged file does not exist: { staged_file } \" ) flash ( \"Staged file not found.\" , \"danger\" ) return redirect ( url_for ( \"main.upload_file_staged\" )) xl_dict , _ = json_load_with_meta ( staged_file ) base = current_app . base # type: ignore[attr-defined] final_id , sector = xl_dict_to_database ( db , base , xl_dict ) logger . info ( f \"Applied staged update for id= { final_id } , sector= { sector } \" ) try : staged_file . unlink () logger . debug ( f \"Deleted staged file: { staged_file } \" ) except Exception as delete_error : logger . warning ( f \"Could not delete staged file: { delete_error } \" ) return redirect ( url_for ( \"main.incidence_update\" , id_ = final_id )) except Exception as e : logger . exception ( \"Failed to apply staged update.\" ) flash ( \"Error applying update. Please try again.\" , \"danger\" ) return redirect ( url_for ( \"main.upload_file_staged\" ))","title":"Redirects to: /incidence_update/123/"},{"location":"reference/arb/portal/routes/#arb.portal.routes.confirm_staged","text":"Confirm and apply a staged update for a specific incidence ID and file. Parameters: id_ ( int ) \u2013 Incidence ID to update. filename ( str ) \u2013 Name of the staged file to confirm. Returns: Response ( ResponseReturnValue ) \u2013 Redirect to the incidence update page after applying the update. Examples:","title":"confirm_staged"},{"location":"reference/arb/portal/routes/#arb.portal.routes.confirm_staged--in-browser-post-confirm_staged123myfilexlsx","text":"","title":"In browser: POST /confirm_staged/123/myfile.xlsx"},{"location":"reference/arb/portal/routes/#arb.portal.routes.confirm_staged--redirects-to-incidence_update123","text":"Source code in arb\\portal\\routes.py 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 @main . route ( \"/confirm_staged/<int:id_>/<filename>\" , methods = [ \"POST\" ]) def confirm_staged ( id_ : int , filename : str ) -> ResponseReturnValue : \"\"\" Confirm and apply a staged update for a specific incidence ID and file. Args: id_ (int): Incidence ID to update. filename (str): Name of the staged file to confirm. Returns: Response: Redirect to the incidence update page after applying the update. Examples: # In browser: POST /confirm_staged/123/myfile.xlsx # Redirects to: /incidence_update/123/ \"\"\" import shutil logger . info ( f \"route called: confirm_staged with id_: { id_ } and filename: { filename } \" ) # Resolve paths root = get_upload_folder () staged_path = os . path . join ( root , \"staging\" , filename ) processed_dir = os . path . join ( root , \"processed\" ) os . makedirs ( processed_dir , exist_ok = True ) processed_path = os . path . join ( processed_dir , filename ) # Load staged payload and metadata try : staged_data , staged_meta = json_load_with_meta ( Path ( staged_path )) base_misc_json = staged_meta . get ( \"base_misc_json\" , {}) except Exception as e : flash ( f \"Failed to load staged file for ID { id_ } : { e } \" , \"danger\" ) return redirect ( url_for ( \"main.upload_file_staged\" )) # Extract form data from the staged JSON structure # staged_data contains: {'metadata': {...}, 'schemas': {...}, 'tab_contents': {'Feedback Form': {...}}} # We need to extract just the form data from tab_contents and include sector from metadata form_data = extract_tab_and_sector ( staged_data , tab_name = \"Feedback Form\" ) if not form_data : flash ( f \"Failed to extract form data from staged file for ID { id_ } \" , \"danger\" ) return redirect ( url_for ( \"main.upload_file_staged\" )) # Get the database model base : AutomapBase = current_app . base # type: ignore[attr-defined] table_name = 'incidences' table = get_class_from_table_name ( base , table_name ) # Get or create the model row logger . info ( f \"[confirm_staged] Getting/creating model row for id_incidence= { id_ } \" ) model_row , _ , is_new_row = get_ensured_row ( db = db , base = base , table_name = table_name , primary_key_name = \"id_incidence\" , id_ = id_ , add_to_session = True ) logger . info ( f \"[confirm_staged] Model row result: type= { type ( model_row ) } , \" f \"id_incidence= { getattr ( model_row , 'id_incidence' , 'N/A' ) } , \" f \"is_new_row= { is_new_row } \" ) # Check for concurrent DB changes current_misc_json = getattr ( model_row , \"misc_json\" , {}) or {} logger . info ( f \"[confirm_staged] Current misc_json: { current_misc_json } \" ) logger . info ( f \"[confirm_staged] Base misc_json from staging: { base_misc_json } \" ) if current_misc_json != base_misc_json : logger . warning ( f \"[confirm_staged] Concurrent DB changes detected! \" f \"current_misc_json != base_misc_json\" ) flash ( \"\u26a0\ufe0f The database was changed by another user before your updates were confirmed. Please review the new database state and reconfirm which fields you wish to update.\" , \"warning\" ) return redirect ( url_for ( \"main.review_staged\" , id_ = id_ , filename = filename )) # Build update patch only for fields user confirmed patch : dict = {} logger . info ( f \"[confirm_staged] Building patch from { len ( form_data ) } form fields\" ) for key in form_data : checkbox_name = f \"confirm_overwrite_ { key } \" confirmed = checkbox_name in request . form new_val = form_data [ key ] # Ensure we have a dictionary to work with, even if misc_json is None misc_json = getattr ( model_row , \"misc_json\" , {}) or {} old_val = misc_json . get ( key ) if confirmed : patch [ key ] = new_val logger . debug ( f \"[confirm_staged] Added to patch: { key } = { new_val } (confirmed= { confirmed } )\" ) logger . info ( f \"[confirm_staged] Final patch contains { len ( patch ) } fields: { list ( patch . keys ()) } \" ) if not patch : logger . warning ( f \"[confirm_staged] No fields in patch - no changes to save\" ) flash ( \"No fields were confirmed for update. No changes saved.\" , \"warning\" ) return redirect ( url_for ( \"main.upload_file_staged\" )) # \ud83c\udd95 Prepare patch for JSON serialization (type coercion, datetime conversion, etc.) patch = prep_payload_for_json ( patch ) logger . info ( f \"[confirm_staged] Prepared patch for JSON: { patch } \" ) # Apply patch to the database model try : logger . info ( f \"[confirm_staged] About to call apply_json_patch_and_log with { len ( patch ) } fields\" ) apply_json_patch_and_log ( model = model_row , updates = patch , json_field = \"misc_json\" , user = \"anonymous\" , comments = f \"Staged update confirmed for ID { id_ } \" ) logger . info ( f \"[confirm_staged] \u2705 apply_json_patch_and_log completed successfully\" ) # \ud83c\udd95 Commit the database transaction to persist changes logger . info ( f \"[confirm_staged] About to commit database session\" ) db . session . commit () logger . info ( f \"[confirm_staged] \u2705 Database session committed successfully\" ) # Move the staged JSON file to the processed directory shutil . move ( staged_path , processed_path ) logger . info ( f \"[confirm_staged] \u2705 Moved staged file to processed: { processed_path } \" ) flash ( f \"\u2705 Successfully updated record { id_ } . { len ( patch ) } fields changed. Staged file moved to processed directory.\" , \"success\" ) except Exception as e : # Rollback on error to prevent partial commits logger . error ( f \"[confirm_staged] \u274c Error during database update: { e } \" ) logger . exception ( f \"[confirm_staged] Full exception details:\" ) db . session . rollback () flash ( f \"\u274c Error applying updates for ID { id_ } : { e } \" , \"danger\" ) return redirect ( url_for ( \"main.upload_file_staged\" )) return redirect ( url_for ( \"main.upload_file_staged\" ))","title":"Redirects to: /incidence_update/123/"},{"location":"reference/arb/portal/routes/#arb.portal.routes.delete_testing_range","text":"Developer utility: Delete testing rows in a specified id_incidence range. NOTE: This is a developer/destructive route, not covered by E2E tests by design. GET: Show form to specify min_id, max_id, and dry_run. POST: Run delete_testing_rows and show summary/results, including id_incidences if dry run. Source code in arb\\portal\\routes.py 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 @main . route ( '/delete_testing_range' , methods = [ 'GET' , 'POST' ]) def delete_testing_range () -> str : \"\"\" Developer utility: Delete testing rows in a specified id_incidence range. NOTE: This is a developer/destructive route, not covered by E2E tests by design. GET: Show form to specify min_id, max_id, and dry_run. POST: Run delete_testing_rows and show summary/results, including id_incidences if dry run. \"\"\" from flask import current_app logger . info ( f \"route called: delete_testing_range\" ) base = current_app . base # type: ignore[attr-defined] error = None result = None min_id = 1000000 max_id = 2000000 dry_run = True submitted = False portal_updates_ids = [] incidences_ids = [] if request . method == 'POST' : try : min_id = int ( request . form . get ( 'min_id' , 1000000 )) max_id = int ( request . form . get ( 'max_id' , 2000000 )) dry_run = bool ( request . form . get ( 'dry_run' )) submitted = True if min_id < 1000000 or max_id < 1000000 : error = \"Both min and max id_incidence must be at least 1000000.\" elif min_id > max_id : error = \"min_id cannot be greater than max_id.\" else : if dry_run : # Get the IDs that would be deleted preview = list_testing_rows ( db , base , min_id , max_id ) portal_updates_ids = sorted ( set ( row [ 'id_incidence' ] for row in preview [ 'portal_updates' ])) incidences_ids = sorted ( row [ 'id_incidence' ] for row in preview [ 'incidences' ]) result = delete_testing_rows ( db , base , min_id , max_id , dry_run = dry_run ) except Exception as e : error = f \"Error: { e } \" instructions = ( \"<ul>\" \"<li><b>Use this tool to delete test rows from the portal_updates and incidences tables.</b></li>\" \"<li>Specify a min and max id_incidence (both must be at least 1000000).</li>\" \"<li>Check 'Dry Run' to preview what would be deleted without making changes.</li>\" \"<li><b>Warning:</b> This cannot delete real data (id_incidence < 1000000 is not allowed).</li>\" \"<li>For safety, always do a dry run first!</li>\" \"</ul>\" ) # Remove summarize_ids and always pass full lists return render_template ( 'delete_testing_range.html' , min_id = min_id , max_id = max_id , dry_run = dry_run , error = error , result = result , submitted = submitted , instructions = instructions , portal_updates_ids = portal_updates_ids , incidences_ids = incidences_ids )","title":"delete_testing_range"},{"location":"reference/arb/portal/routes/#arb.portal.routes.diagnostics","text":"Display developer diagnostics and runtime information. NOTE: This is a developer-only route, not covered by E2E tests by design. Returns: str ( str ) \u2013 Rendered HTML diagnostics page. Examples:","title":"diagnostics"},{"location":"reference/arb/portal/routes/#arb.portal.routes.diagnostics--in-browser-get-diagnostics","text":"","title":"In browser: GET /diagnostics"},{"location":"reference/arb/portal/routes/#arb.portal.routes.diagnostics--returns-html-diagnostics-info","text":"Source code in arb\\portal\\routes.py 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 @main . route ( '/diagnostics' ) def diagnostics () -> str : \"\"\" Display developer diagnostics and runtime information. NOTE: This is a developer-only route, not covered by E2E tests by design. Returns: str: Rendered HTML diagnostics page. Examples: # In browser: GET /diagnostics # Returns: HTML diagnostics info \"\"\" logger . info ( f \"route called: diagnostics\" ) result = find_auto_increment_value ( db , \"incidences\" , \"id_incidence\" ) html_content = f \"<p><strong>Diagnostic Results=</strong></p> <p> { result } </p>\" return render_template ( 'diagnostics.html' , header = \"Auto-Increment Check\" , subheader = \"Next available ID value in the 'incidences' table.\" , html_content = html_content , modal_title = \"Success\" , modal_message = \"Diagnostics completed successfully.\" , )","title":"Returns: HTML diagnostics info"},{"location":"reference/arb/portal/routes/#arb.portal.routes.discard_staged_update","text":"Discard a staged update for a specific incidence ID and filename. CSRF is disabled for this route. Source code in arb\\portal\\routes.py 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 @csrf . exempt @main . route ( \"/discard_staged_update/<int:id_>/<filename>\" , methods = [ \"POST\" ]) def discard_staged_update ( id_ : int , filename : str ) -> ResponseReturnValue : \"\"\" Discard a staged update for a specific incidence ID and filename. CSRF is disabled for this route. \"\"\" logger . info ( f \"route called: discard_staged_update with id_: { id_ } and filename: { filename } \" ) import unicodedata import time staging_dir = Path ( get_upload_folder ()) / \"staging\" # Normalize filename for cross-platform compatibility safe_filename = unicodedata . normalize ( 'NFC' , filename . strip ()) staged_file = staging_dir / safe_filename logger . info ( f \"[DISCARD] Route called: id_= { id_ } , filename=' { filename } ', safe_filename=' { safe_filename } '\" ) logger . info ( f \"[DISCARD] Starting deletion for: { staged_file } \" ) logger . info ( f \"[DISCARD] File exists before deletion: { staged_file . exists () } \" ) try : if staged_file . exists (): staged_file . unlink () logger . info ( f \"[DISCARD] File deleted: { staged_file } \" ) else : logger . info ( f \"[DISCARD] File not found for deletion: { staged_file } \" ) except Exception as e : logger . error ( f \"[DISCARD] Exception during file deletion: { e } \" ) logger . info ( f \"[DISCARD] File exists after deletion: { staged_file . exists () } \" ) logger . info ( f \"[DISCARD] Completed discard_staged_update for: { staged_file } \" ) # Sleep briefly to ensure filesystem updates propagate (for test timing) time . sleep ( 0.2 ) logger . info ( f \"[DISCARD] Returning redirect to /list_staged\" ) return redirect ( url_for ( \"main.list_staged\" ))","title":"discard_staged_update"},{"location":"reference/arb/portal/routes/#arb.portal.routes.export_portal_updates","text":"Export all portal update log entries as a CSV file. Returns: Response ( Response ) \u2013 CSV file download of portal update logs. Notes Respects filters set in the /portal_updates page. Uses standard CSV headers and UTF-8 encoding. Source code in arb\\portal\\routes.py 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 @main . route ( \"/portal_updates/export\" ) def export_portal_updates () -> Response : \"\"\" Export all portal update log entries as a CSV file. Returns: Response: CSV file download of portal update logs. Notes: - Respects filters set in the `/portal_updates` page. - Uses standard CSV headers and UTF-8 encoding. \"\"\" logger . info ( f \"route called: export_portal_updates\" ) query = db . session . query ( PortalUpdate ) query = apply_portal_update_filters ( query , PortalUpdate , request . args ) updates = query . order_by ( PortalUpdate . timestamp . desc ()) . all () si = StringIO () writer = csv . writer ( si ) writer . writerow ([ \"timestamp\" , \"key\" , \"old_value\" , \"new_value\" , \"user\" , \"comments\" , \"id_incidence\" ]) for u in updates : writer . writerow ([ u . timestamp , u . key , u . old_value , u . new_value , u . user , u . comments , u . id_incidence or \"\" ]) return Response ( si . getvalue (), mimetype = \"text/csv\" , headers = { \"Content-Disposition\" : \"attachment; filename=portal_updates_export.csv\" } )","title":"export_portal_updates"},{"location":"reference/arb/portal/routes/#arb.portal.routes.incidence_delete","text":"Delete a specified incidence from the database. Parameters: id_ ( int ) \u2013 Primary key of the incidence to delete. Returns: Response ( ResponseReturnValue ) \u2013 Redirect to the homepage after deletion. Examples:","title":"incidence_delete"},{"location":"reference/arb/portal/routes/#arb.portal.routes.incidence_delete--in-browser-post-incidence_delete123","text":"","title":"In browser: POST /incidence_delete/123/"},{"location":"reference/arb/portal/routes/#arb.portal.routes.incidence_delete--redirects-to","text":"Notes Future: consider adding authorization (e.g., CARB password) to restrict access. Source code in arb\\portal\\routes.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 @main . post ( '/incidence_delete/<int:id_>/' ) def incidence_delete ( id_ : int ) -> ResponseReturnValue : \"\"\" Delete a specified incidence from the database. Args: id_ (int): Primary key of the incidence to delete. Returns: Response: Redirect to the homepage after deletion. Examples: # In browser: POST /incidence_delete/123/ # Redirects to: / Notes: - Future: consider adding authorization (e.g., CARB password) to restrict access. \"\"\" logger . info ( f \"route called: incidence_delete with id= { id_ } \" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] table_name = 'incidences' table = get_class_from_table_name ( base , table_name ) if table is None : abort ( 500 , description = \"Could not get table class for incidences\" ) # Type cast to help with SQLAlchemy typing table_class = table # type: Any model_row = db . session . query ( table_class ) . get_or_404 ( id_ ) # todo - ensure portal changes are properly updated arb . utils . sql_alchemy . delete_commit_and_log_model ( db , model_row , comment = f 'Deleting incidence row { id_ } ' ) return redirect ( url_for ( 'main.index' ))","title":"Redirects to: /"},{"location":"reference/arb/portal/routes/#arb.portal.routes.incidence_update","text":"Display and edit a specific incidence record by ID. Parameters: id_ ( int ) \u2013 Primary key of the incidence to edit. Returns: Union [ str , Response ] \u2013 str|Response: Rendered HTML of the feedback form for the selected incidence, or a redirect to the upload page if the ID is missing. Raises: 500 Internal Server Error \u2013 If multiple records are found for the same ID. Examples:","title":"incidence_update"},{"location":"reference/arb/portal/routes/#arb.portal.routes.incidence_update--in-browser-get-incidence_update123","text":"","title":"In browser: GET /incidence_update/123/"},{"location":"reference/arb/portal/routes/#arb.portal.routes.incidence_update--returns-html-form-for-editing-incidence-123","text":"Notes Redirects if the ID is not found in the database. Assumes each incidence ID is unique. Source code in arb\\portal\\routes.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 @main . route ( '/incidence_update/<int:id_>/' , methods = ( 'GET' , 'POST' )) def incidence_update ( id_ : int ) -> Union [ str , Response ]: \"\"\" Display and edit a specific incidence record by ID. Args: id_ (int): Primary key of the incidence to edit. Returns: str|Response: Rendered HTML of the feedback form for the selected incidence, or a redirect to the upload page if the ID is missing. Raises: 500 Internal Server Error: If multiple records are found for the same ID. Examples: # In browser: GET /incidence_update/123/ # Returns: HTML form for editing incidence 123 Notes: - Redirects if the ID is not found in the database. - Assumes each incidence ID is unique. \"\"\" logger . info ( f \"route called: incidence_update with id= { id_ } .\" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] table_name = 'incidences' table = get_class_from_table_name ( base , table_name ) # get_or_404 uses the tables primary key # model_row = db.session.query(table).get_or_404(id_) # todo turn this into a get and if it is null, then redirect? to the spreadsheet upload # todo consider turning into one_or_none and have error handling if table is None : abort ( 500 , description = \"Could not get table class for incidences\" ) # Type cast to help with SQLAlchemy typing table_class = table # type: Any rows = db . session . query ( table_class ) . filter_by ( id_incidence = id_ ) . all () if not rows : message = f \"A request was made to edit a non-existent id_incidence ( { id_ } ). Consider uploading the incidence by importing a spreadsheet.\" return redirect ( url_for ( 'main.upload_file' , message = message )) if len ( rows ) > 1 : abort ( 500 , description = f \"Multiple rows found for id= { id_ } \" ) model_row = rows [ 0 ] sector , sector_type = get_sector_info ( db , base , id_ ) logger . debug ( f \"calling incidence_prep()\" ) return incidence_prep ( model_row , crud_type = 'update' , sector_type = sector_type , default_dropdown = PLEASE_SELECT )","title":"Returns: HTML form for editing incidence 123"},{"location":"reference/arb/portal/routes/#arb.portal.routes.index","text":"Display the homepage with a list of all existing incidence records. Returns: str ( str ) \u2013 Rendered HTML for the homepage with incidence records. Examples:","title":"index"},{"location":"reference/arb/portal/routes/#arb.portal.routes.index--in-browser-get","text":"","title":"In browser: GET /"},{"location":"reference/arb/portal/routes/#arb.portal.routes.index--returns-html-page-with-table-of-incidences","text":"Source code in arb\\portal\\routes.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 @main . route ( '/' ) def index () -> str : \"\"\" Display the homepage with a list of all existing incidence records. Returns: str: Rendered HTML for the homepage with incidence records. Examples: # In browser: GET / # Returns: HTML page with table of incidences \"\"\" logger . info ( f \"route called: index.\" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] table_name = 'incidences' colum_name_pk = 'id_incidence' rows = get_rows_by_table_name ( db , base , table_name , colum_name_pk , ascending = False ) return render_template ( 'index.html' , model_rows = rows )","title":"Returns: HTML page with table of incidences"},{"location":"reference/arb/portal/routes/#arb.portal.routes.java_script_diagnostic_test","text":"Render a simple page for testing JavaScript diagnostics logging (frontend and backend). NOTE: This is a developer-only route, not covered by E2E tests by design. Source code in arb\\portal\\routes.py 1281 1282 1283 1284 1285 1286 1287 1288 1289 @main . route ( '/java_script_diagnostic_test' ) def java_script_diagnostic_test (): \"\"\" Render a simple page for testing JavaScript diagnostics logging (frontend and backend). NOTE: This is a developer-only route, not covered by E2E tests by design. \"\"\" logger . info ( f \"route called: java_script_diagnostic_test\" ) return render_template ( 'java_script_diagnostic_test.html' )","title":"java_script_diagnostic_test"},{"location":"reference/arb/portal/routes/#arb.portal.routes.js_diagnostic_log","text":"","title":"js_diagnostic_log"},{"location":"reference/arb/portal/routes/#arb.portal.routes.js_diagnostic_log--javascript-diagnostics-logging-endpoint","text":"NOTE: This is a developer-only route, not covered by E2E tests by design. Purpose This route allows frontend JavaScript code to send diagnostic or debug messages directly to the backend server. These messages are then written to the backend log file, making it possible to correlate frontend/browser events with backend activity, errors, or user actions. This is especially useful for debugging issues that are hard to capture with browser console logs alone (e.g., page reloads, E2E tests, or user-reported problems). How to Use (Frontend JavaScript): Use the provided JS helper function (see below) to send a POST request to this endpoint: Example JS function: function sendJsDiagnostic(msg, extra) { fetch('/js_diagnostic_log', { method: 'POST', headers: {'Content-Type': 'application/json'}, body: JSON.stringify({msg, ts: new Date().toISOString(), ...extra}) }); } Example usage: sendJsDiagnostic('User clicked discard', {action: '/discard_staged_update/0/file.json', userId: 123}); Expected Payload (JSON): { \"msg\": \"A human-readable message (required)\", \"ts\": \"ISO timestamp (optional, will be included in log if present)\", ... any other key-value pairs (optional, will be logged as 'extra') } What Happens The backend receives the POST request and parses the JSON payload. It extracts the 'msg' (message), 'ts' (timestamp), and any other fields (as 'extra'). It writes a log entry to the backend log file in the format: [JS_DIAG][timestamp] message | extra: {...} Returns a simple JSON response: {\"status\": \"ok\"} Why Use This Console logs in the browser are lost on navigation/reload and are not visible to backend developers. This route allows you to persistently log important frontend events, errors, or user actions for later analysis. Especially useful for E2E testing, debugging user issues, or tracking hard-to-reproduce bugs. Security Note This endpoint is for diagnostics only. Do not send sensitive user data. Rate limiting or authentication can be added if needed for production. Source code in arb\\portal\\routes.py 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 @main . route ( '/js_diagnostic_log' , methods = [ 'POST' ]) def js_diagnostic_log (): \"\"\" JavaScript Diagnostics Logging Endpoint ======================================= NOTE: This is a developer-only route, not covered by E2E tests by design. Purpose: This route allows frontend JavaScript code to send diagnostic or debug messages directly to the backend server. These messages are then written to the backend log file, making it possible to correlate frontend/browser events with backend activity, errors, or user actions. This is especially useful for debugging issues that are hard to capture with browser console logs alone (e.g., page reloads, E2E tests, or user-reported problems). How to Use (Frontend JavaScript): Use the provided JS helper function (see below) to send a POST request to this endpoint: Example JS function: function sendJsDiagnostic(msg, extra) { fetch('/js_diagnostic_log', { method: 'POST', headers: {'Content-Type': 'application/json'}, body: JSON.stringify({msg, ts: new Date().toISOString(), ...extra}) }); } Example usage: sendJsDiagnostic('User clicked discard', {action: '/discard_staged_update/0/file.json', userId: 123}); Expected Payload (JSON): { \"msg\": \"A human-readable message (required)\", \"ts\": \"ISO timestamp (optional, will be included in log if present)\", ... any other key-value pairs (optional, will be logged as 'extra') } What Happens: - The backend receives the POST request and parses the JSON payload. - It extracts the 'msg' (message), 'ts' (timestamp), and any other fields (as 'extra'). - It writes a log entry to the backend log file in the format: [JS_DIAG][timestamp] message | extra: {...} - Returns a simple JSON response: {\"status\": \"ok\"} Why Use This: - Console logs in the browser are lost on navigation/reload and are not visible to backend developers. - This route allows you to persistently log important frontend events, errors, or user actions for later analysis. - Especially useful for E2E testing, debugging user issues, or tracking hard-to-reproduce bugs. Security Note: - This endpoint is for diagnostics only. Do not send sensitive user data. - Rate limiting or authentication can be added if needed for production. \"\"\" logger . info ( f \"route called: js_diagnostic_log\" ) data = request . get_json ( force = True , silent = True ) or {} msg = data . get ( 'msg' , '[NO MSG]' ) ts = data . get ( 'ts' ) extra = { k : v for k , v in data . items () if k not in ( 'msg' , 'ts' )} log_line = f \"[JS_DIAG] { '[' + ts + ']' if ts else '' } { msg } \" if extra : log_line += f \" | extra: { extra } \" logger . info ( log_line ) return jsonify ({ 'status' : 'ok' })","title":"JavaScript Diagnostics Logging Endpoint"},{"location":"reference/arb/portal/routes/#arb.portal.routes.landfill_incidence_create","text":"Create a new dummy Landfill incidence and redirect to its edit form. Returns: Response ( Response ) \u2013 Redirect to the incidence_update page for the newly created ID. Examples:","title":"landfill_incidence_create"},{"location":"reference/arb/portal/routes/#arb.portal.routes.landfill_incidence_create--in-browser-post-landfill_incidence_create","text":"","title":"In browser: POST /landfill_incidence_create/"},{"location":"reference/arb/portal/routes/#arb.portal.routes.landfill_incidence_create--redirects-to-incidence_update","text":"Notes Dummy data is loaded from db_hardcoded.get_landfill_dummy_form_data() . Source code in arb\\portal\\routes.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 @main . route ( '/landfill_incidence_create/' , methods = ( 'GET' , 'POST' )) def landfill_incidence_create () -> Response : \"\"\" Create a new dummy Landfill incidence and redirect to its edit form. Returns: Response: Redirect to the `incidence_update` page for the newly created ID. Examples: # In browser: POST /landfill_incidence_create/ # Redirects to: /incidence_update/<new_id>/ Notes: - Dummy data is loaded from `db_hardcoded.get_landfill_dummy_form_data()`. \"\"\" logger . info ( f \"route called: landfill_incidence_create.\" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] table_name = 'incidences' col_name = 'misc_json' data_dict = arb . portal . db_hardcoded . get_landfill_dummy_form_data () id_ = dict_to_database ( db , base , data_dict , table_name = table_name , json_field = col_name , ) logger . debug ( f \"landfill_incidence_create() - leaving.\" ) return redirect ( url_for ( 'main.incidence_update' , id_ = id_ ))","title":"Redirects to: /incidence_update/ Blueprint-based route definitions for the ARB Feedback Portal. This module defines all Flask routes originally found in app.py, now organized under the main Blueprint for modularity. Module_Attributes main (Blueprint): Flask Blueprint for all portal routes. logger (logging.Logger): Logger instance for this module. Examples: from arb.portal.routes import main app.register_blueprint(main) Notes All routes assume that create_app() registers the main Blueprint. Developer diagnostics are inlined near the end of the module. apply_staged_update(id_) Apply a staged update to the database for a specific incidence ID. Parameters: id_ (int) \u2013 Incidence ID to apply the staged update to. Returns: Response( Response ) \u2013 Redirect to the incidence update page after applying the update. Examples: In browser: POST /apply_staged_update/123 Redirects to: /incidence_update/123/ Source code in arb\\portal\\routes.py 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833@main.route(&#39;/apply_staged_update/&lt;int:id_&gt;&#39;, methods=[&#39;POST&#39;]) def apply_staged_update(id_: int) -&gt; Response: &quot;&quot;&quot; Apply a staged update to the database for a specific incidence ID. Args: id_ (int): Incidence ID to apply the staged update to. Returns: Response: Redirect to the incidence update page after applying the update. Examples: # In browser: POST /apply_staged_update/123 # Redirects to: /incidence_update/123/ &quot;&quot;&quot; logger.info(f&quot;route called: apply_staged_update with id_: {id_}&quot;) try: # staging_dir = Path(current_app.config[&quot;UPLOAD_STAGING_FOLDER&quot;]) staging_dir = Path(get_upload_folder()) / &quot;staging&quot; staged_file = staging_dir / f&quot;{id_}.json&quot; if not staged_file.exists(): logger.error(f&quot;Staged file does not exist: {staged_file}&quot;) flash(&quot;Staged file not found.&quot;, &quot;danger&quot;) return redirect(url_for(&quot;main.upload_file_staged&quot;)) xl_dict, _ = json_load_with_meta(staged_file) base = current_app.base # type: ignore[attr-defined] final_id, sector = xl_dict_to_database(db, base, xl_dict) logger.info(f&quot;Applied staged update for id={final_id}, sector={sector}&quot;) try: staged_file.unlink() logger.debug(f&quot;Deleted staged file: {staged_file}&quot;) except Exception as delete_error: logger.warning(f&quot;Could not delete staged file: {delete_error}&quot;) return redirect(url_for(&quot;main.incidence_update&quot;, id_=final_id)) except Exception as e: logger.exception(&quot;Failed to apply staged update.&quot;) flash(&quot;Error applying update. Please try again.&quot;, &quot;danger&quot;) return redirect(url_for(&quot;main.upload_file_staged&quot;)) confirm_staged(id_, filename) Confirm and apply a staged update for a specific incidence ID and file. Parameters: id_ (int) \u2013 Incidence ID to update. filename (str) \u2013 Name of the staged file to confirm. Returns: Response( ResponseReturnValue ) \u2013 Redirect to the incidence update page after applying the update. Examples: In browser: POST /confirm_staged/123/myfile.xlsx Redirects to: /incidence_update/123/ Source code in arb\\portal\\routes.py 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752@main.route(&quot;/confirm_staged/&lt;int:id_&gt;/&lt;filename&gt;&quot;, methods=[&quot;POST&quot;]) def confirm_staged(id_: int, filename: str) -&gt; ResponseReturnValue: &quot;&quot;&quot; Confirm and apply a staged update for a specific incidence ID and file. Args: id_ (int): Incidence ID to update. filename (str): Name of the staged file to confirm. Returns: Response: Redirect to the incidence update page after applying the update. Examples: # In browser: POST /confirm_staged/123/myfile.xlsx # Redirects to: /incidence_update/123/ &quot;&quot;&quot; import shutil logger.info(f&quot;route called: confirm_staged with id_: {id_} and filename: {filename}&quot;) # Resolve paths root = get_upload_folder() staged_path = os.path.join(root, &quot;staging&quot;, filename) processed_dir = os.path.join(root, &quot;processed&quot;) os.makedirs(processed_dir, exist_ok=True) processed_path = os.path.join(processed_dir, filename) # Load staged payload and metadata try: staged_data, staged_meta = json_load_with_meta(Path(staged_path)) base_misc_json = staged_meta.get(&quot;base_misc_json&quot;, {}) except Exception as e: flash(f&quot;Failed to load staged file for ID {id_}: {e}&quot;, &quot;danger&quot;) return redirect(url_for(&quot;main.upload_file_staged&quot;)) # Extract form data from the staged JSON structure # staged_data contains: {&#39;metadata&#39;: {...}, &#39;schemas&#39;: {...}, &#39;tab_contents&#39;: {&#39;Feedback Form&#39;: {...}}} # We need to extract just the form data from tab_contents and include sector from metadata form_data = extract_tab_and_sector(staged_data, tab_name=&quot;Feedback Form&quot;) if not form_data: flash(f&quot;Failed to extract form data from staged file for ID {id_}&quot;, &quot;danger&quot;) return redirect(url_for(&quot;main.upload_file_staged&quot;)) # Get the database model base: AutomapBase = current_app.base # type: ignore[attr-defined] table_name = &#39;incidences&#39; table = get_class_from_table_name(base, table_name) # Get or create the model row logger.info(f&quot;[confirm_staged] Getting/creating model row for id_incidence={id_}&quot;) model_row, _, is_new_row = get_ensured_row( db=db, base=base, table_name=table_name, primary_key_name=&quot;id_incidence&quot;, id_=id_, add_to_session=True ) logger.info(f&quot;[confirm_staged] Model row result: type={type(model_row)}, &quot; f&quot;id_incidence={getattr(model_row, &#39;id_incidence&#39;, &#39;N/A&#39;)}, &quot; f&quot;is_new_row={is_new_row}&quot;) # Check for concurrent DB changes current_misc_json = getattr(model_row, &quot;misc_json&quot;, {}) or {} logger.info(f&quot;[confirm_staged] Current misc_json: {current_misc_json}&quot;) logger.info(f&quot;[confirm_staged] Base misc_json from staging: {base_misc_json}&quot;) if current_misc_json != base_misc_json: logger.warning(f&quot;[confirm_staged] Concurrent DB changes detected! &quot; f&quot;current_misc_json != base_misc_json&quot;) flash( &quot;\u26a0\ufe0f The database was changed by another user before your updates were confirmed. Please review the new database state and reconfirm which fields you wish to update.&quot;, &quot;warning&quot;) return redirect(url_for(&quot;main.review_staged&quot;, id_=id_, filename=filename)) # Build update patch only for fields user confirmed patch: dict = {} logger.info(f&quot;[confirm_staged] Building patch from {len(form_data)} form fields&quot;) for key in form_data: checkbox_name = f&quot;confirm_overwrite_{key}&quot; confirmed = checkbox_name in request.form new_val = form_data[key] # Ensure we have a dictionary to work with, even if misc_json is None misc_json = getattr(model_row, &quot;misc_json&quot;, {}) or {} old_val = misc_json.get(key) if confirmed: patch[key] = new_val logger.debug(f&quot;[confirm_staged] Added to patch: {key}={new_val} (confirmed={confirmed})&quot;) logger.info(f&quot;[confirm_staged] Final patch contains {len(patch)} fields: {list(patch.keys())}&quot;) if not patch: logger.warning(f&quot;[confirm_staged] No fields in patch - no changes to save&quot;) flash(&quot;No fields were confirmed for update. No changes saved.&quot;, &quot;warning&quot;) return redirect(url_for(&quot;main.upload_file_staged&quot;)) # \ud83c\udd95 Prepare patch for JSON serialization (type coercion, datetime conversion, etc.) patch = prep_payload_for_json(patch) logger.info(f&quot;[confirm_staged] Prepared patch for JSON: {patch}&quot;) # Apply patch to the database model try: logger.info(f&quot;[confirm_staged] About to call apply_json_patch_and_log with {len(patch)} fields&quot;) apply_json_patch_and_log( model=model_row, updates=patch, json_field=&quot;misc_json&quot;, user=&quot;anonymous&quot;, comments=f&quot;Staged update confirmed for ID {id_}&quot; ) logger.info(f&quot;[confirm_staged] \u2705 apply_json_patch_and_log completed successfully&quot;) # \ud83c\udd95 Commit the database transaction to persist changes logger.info(f&quot;[confirm_staged] About to commit database session&quot;) db.session.commit() logger.info(f&quot;[confirm_staged] \u2705 Database session committed successfully&quot;) # Move the staged JSON file to the processed directory shutil.move(staged_path, processed_path) logger.info(f&quot;[confirm_staged] \u2705 Moved staged file to processed: {processed_path}&quot;) flash(f&quot;\u2705 Successfully updated record {id_}. {len(patch)} fields changed. Staged file moved to processed directory.&quot;, &quot;success&quot;) except Exception as e: # Rollback on error to prevent partial commits logger.error(f&quot;[confirm_staged] \u274c Error during database update: {e}&quot;) logger.exception(f&quot;[confirm_staged] Full exception details:&quot;) db.session.rollback() flash(f&quot;\u274c Error applying updates for ID {id_}: {e}&quot;, &quot;danger&quot;) return redirect(url_for(&quot;main.upload_file_staged&quot;)) return redirect(url_for(&quot;main.upload_file_staged&quot;)) delete_testing_range() Developer utility: Delete testing rows in a specified id_incidence range. NOTE: This is a developer/destructive route, not covered by E2E tests by design. GET: Show form to specify min_id, max_id, and dry_run. POST: Run delete_testing_rows and show summary/results, including id_incidences if dry run. Source code in arb\\portal\\routes.py 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213@main.route(&#39;/delete_testing_range&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) def delete_testing_range() -&gt; str: &quot;&quot;&quot; Developer utility: Delete testing rows in a specified id_incidence range. NOTE: This is a developer/destructive route, not covered by E2E tests by design. GET: Show form to specify min_id, max_id, and dry_run. POST: Run delete_testing_rows and show summary/results, including id_incidences if dry run. &quot;&quot;&quot; from flask import current_app logger.info(f&quot;route called: delete_testing_range&quot;) base = current_app.base # type: ignore[attr-defined] error = None result = None min_id = 1000000 max_id = 2000000 dry_run = True submitted = False portal_updates_ids = [] incidences_ids = [] if request.method == &#39;POST&#39;: try: min_id = int(request.form.get(&#39;min_id&#39;, 1000000)) max_id = int(request.form.get(&#39;max_id&#39;, 2000000)) dry_run = bool(request.form.get(&#39;dry_run&#39;)) submitted = True if min_id &lt; 1000000 or max_id &lt; 1000000: error = &quot;Both min and max id_incidence must be at least 1000000.&quot; elif min_id &gt; max_id: error = &quot;min_id cannot be greater than max_id.&quot; else: if dry_run: # Get the IDs that would be deleted preview = list_testing_rows(db, base, min_id, max_id) portal_updates_ids = sorted(set(row[&#39;id_incidence&#39;] for row in preview[&#39;portal_updates&#39;])) incidences_ids = sorted(row[&#39;id_incidence&#39;] for row in preview[&#39;incidences&#39;]) result = delete_testing_rows(db, base, min_id, max_id, dry_run=dry_run) except Exception as e: error = f&quot;Error: {e}&quot; instructions = ( &quot;&lt;ul&gt;&quot; &quot;&lt;li&gt;&lt;b&gt;Use this tool to delete test rows from the portal_updates and incidences tables.&lt;/b&gt;&lt;/li&gt;&quot; &quot;&lt;li&gt;Specify a min and max id_incidence (both must be at least 1000000).&lt;/li&gt;&quot; &quot;&lt;li&gt;Check &#39;Dry Run&#39; to preview what would be deleted without making changes.&lt;/li&gt;&quot; &quot;&lt;li&gt;&lt;b&gt;Warning:&lt;/b&gt; This cannot delete real data (id_incidence &lt; 1000000 is not allowed).&lt;/li&gt;&quot; &quot;&lt;li&gt;For safety, always do a dry run first!&lt;/li&gt;&quot; &quot;&lt;/ul&gt;&quot; ) # Remove summarize_ids and always pass full lists return render_template( &#39;delete_testing_range.html&#39;, min_id=min_id, max_id=max_id, dry_run=dry_run, error=error, result=result, submitted=submitted, instructions=instructions, portal_updates_ids=portal_updates_ids, incidences_ids=incidences_ids ) diagnostics() Display developer diagnostics and runtime information. NOTE: This is a developer-only route, not covered by E2E tests by design. Returns: str( str ) \u2013 Rendered HTML diagnostics page. Examples: In browser: GET /diagnostics Returns: HTML diagnostics info Source code in arb\\portal\\routes.py 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992@main.route(&#39;/diagnostics&#39;) def diagnostics() -&gt; str: &quot;&quot;&quot; Display developer diagnostics and runtime information. NOTE: This is a developer-only route, not covered by E2E tests by design. Returns: str: Rendered HTML diagnostics page. Examples: # In browser: GET /diagnostics # Returns: HTML diagnostics info &quot;&quot;&quot; logger.info(f&quot;route called: diagnostics&quot;) result = find_auto_increment_value(db, &quot;incidences&quot;, &quot;id_incidence&quot;) html_content = f&quot;&lt;p&gt;&lt;strong&gt;Diagnostic Results=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result}&lt;/p&gt;&quot; return render_template(&#39;diagnostics.html&#39;, header=&quot;Auto-Increment Check&quot;, subheader=&quot;Next available ID value in the &#39;incidences&#39; table.&quot;, html_content=html_content, modal_title=&quot;Success&quot;, modal_message=&quot;Diagnostics completed successfully.&quot;, ) discard_staged_update(id_, filename) Discard a staged update for a specific incidence ID and filename. CSRF is disabled for this route. Source code in arb\\portal\\routes.py 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785@csrf.exempt @main.route(&quot;/discard_staged_update/&lt;int:id_&gt;/&lt;filename&gt;&quot;, methods=[&quot;POST&quot;]) def discard_staged_update(id_: int, filename: str) -&gt; ResponseReturnValue: &quot;&quot;&quot; Discard a staged update for a specific incidence ID and filename. CSRF is disabled for this route. &quot;&quot;&quot; logger.info(f&quot;route called: discard_staged_update with id_: {id_} and filename: {filename}&quot;) import unicodedata import time staging_dir = Path(get_upload_folder()) / &quot;staging&quot; # Normalize filename for cross-platform compatibility safe_filename = unicodedata.normalize(&#39;NFC&#39;, filename.strip()) staged_file = staging_dir / safe_filename logger.info(f&quot;[DISCARD] Route called: id_={id_}, filename=&#39;{filename}&#39;, safe_filename=&#39;{safe_filename}&#39;&quot;) logger.info(f&quot;[DISCARD] Starting deletion for: {staged_file}&quot;) logger.info(f&quot;[DISCARD] File exists before deletion: {staged_file.exists()}&quot;) try: if staged_file.exists(): staged_file.unlink() logger.info(f&quot;[DISCARD] File deleted: {staged_file}&quot;) else: logger.info(f&quot;[DISCARD] File not found for deletion: {staged_file}&quot;) except Exception as e: logger.error(f&quot;[DISCARD] Exception during file deletion: {e}&quot;) logger.info(f&quot;[DISCARD] File exists after deletion: {staged_file.exists()}&quot;) logger.info(f&quot;[DISCARD] Completed discard_staged_update for: {staged_file}&quot;) # Sleep briefly to ensure filesystem updates propagate (for test timing) time.sleep(0.2) logger.info(f&quot;[DISCARD] Returning redirect to /list_staged&quot;) return redirect(url_for(&quot;main.list_staged&quot;)) export_portal_updates() Export all portal update log entries as a CSV file. Returns: Response( Response ) \u2013 CSV file download of portal update logs. Notes Respects filters set in the /portal_updates page. Uses standard CSV headers and UTF-8 encoding. Source code in arb\\portal\\routes.py 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939@main.route(&quot;/portal_updates/export&quot;) def export_portal_updates() -&gt; Response: &quot;&quot;&quot; Export all portal update log entries as a CSV file. Returns: Response: CSV file download of portal update logs. Notes: - Respects filters set in the `/portal_updates` page. - Uses standard CSV headers and UTF-8 encoding. &quot;&quot;&quot; logger.info(f&quot;route called: export_portal_updates&quot;) query = db.session.query(PortalUpdate) query = apply_portal_update_filters(query, PortalUpdate, request.args) updates = query.order_by(PortalUpdate.timestamp.desc()).all() si = StringIO() writer = csv.writer(si) writer.writerow([&quot;timestamp&quot;, &quot;key&quot;, &quot;old_value&quot;, &quot;new_value&quot;, &quot;user&quot;, &quot;comments&quot;, &quot;id_incidence&quot;]) for u in updates: writer.writerow([ u.timestamp, u.key, u.old_value, u.new_value, u.user, u.comments, u.id_incidence or &quot;&quot; ]) return Response( si.getvalue(), mimetype=&quot;text/csv&quot;, headers={&quot;Content-Disposition&quot;: &quot;attachment; filename=portal_updates_export.csv&quot;} ) incidence_delete(id_) Delete a specified incidence from the database. Parameters: id_ (int) \u2013 Primary key of the incidence to delete. Returns: Response( ResponseReturnValue ) \u2013 Redirect to the homepage after deletion. Examples: In browser: POST /incidence_delete/123/ Redirects to: / Notes Future: consider adding authorization (e.g., CARB password) to restrict access. Source code in arb\\portal\\routes.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247@main.post(&#39;/incidence_delete/&lt;int:id_&gt;/&#39;) def incidence_delete(id_: int) -&gt; ResponseReturnValue: &quot;&quot;&quot; Delete a specified incidence from the database. Args: id_ (int): Primary key of the incidence to delete. Returns: Response: Redirect to the homepage after deletion. Examples: # In browser: POST /incidence_delete/123/ # Redirects to: / Notes: - Future: consider adding authorization (e.g., CARB password) to restrict access. &quot;&quot;&quot; logger.info(f&quot;route called: incidence_delete with id= {id_}&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] table_name = &#39;incidences&#39; table = get_class_from_table_name(base, table_name) if table is None: abort(500, description=&quot;Could not get table class for incidences&quot;) # Type cast to help with SQLAlchemy typing table_class = table # type: Any model_row = db.session.query(table_class).get_or_404(id_) # todo - ensure portal changes are properly updated arb.utils.sql_alchemy.delete_commit_and_log_model(db, model_row, comment=f&#39;Deleting incidence row {id_}&#39;) return redirect(url_for(&#39;main.index&#39;)) incidence_update(id_) Display and edit a specific incidence record by ID. Parameters: id_ (int) \u2013 Primary key of the incidence to edit. Returns: Union[str, Response] \u2013 str|Response: Rendered HTML of the feedback form for the selected incidence, or a redirect to the upload page if the ID is missing. Raises: 500 Internal Server Error \u2013 If multiple records are found for the same ID. Examples: In browser: GET /incidence_update/123/ Returns: HTML form for editing incidence 123 Notes Redirects if the ID is not found in the database. Assumes each incidence ID is unique. Source code in arb\\portal\\routes.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141@main.route(&#39;/incidence_update/&lt;int:id_&gt;/&#39;, methods=(&#39;GET&#39;, &#39;POST&#39;)) def incidence_update(id_: int) -&gt; Union[str, Response]: &quot;&quot;&quot; Display and edit a specific incidence record by ID. Args: id_ (int): Primary key of the incidence to edit. Returns: str|Response: Rendered HTML of the feedback form for the selected incidence, or a redirect to the upload page if the ID is missing. Raises: 500 Internal Server Error: If multiple records are found for the same ID. Examples: # In browser: GET /incidence_update/123/ # Returns: HTML form for editing incidence 123 Notes: - Redirects if the ID is not found in the database. - Assumes each incidence ID is unique. &quot;&quot;&quot; logger.info(f&quot;route called: incidence_update with id= {id_}.&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] table_name = &#39;incidences&#39; table = get_class_from_table_name(base, table_name) # get_or_404 uses the tables primary key # model_row = db.session.query(table).get_or_404(id_) # todo turn this into a get and if it is null, then redirect? to the spreadsheet upload # todo consider turning into one_or_none and have error handling if table is None: abort(500, description=&quot;Could not get table class for incidences&quot;) # Type cast to help with SQLAlchemy typing table_class = table # type: Any rows = db.session.query(table_class).filter_by(id_incidence=id_).all() if not rows: message = f&quot;A request was made to edit a non-existent id_incidence ({id_}). Consider uploading the incidence by importing a spreadsheet.&quot; return redirect(url_for(&#39;main.upload_file&#39;, message=message)) if len(rows) &gt; 1: abort(500, description=f&quot;Multiple rows found for id={id_}&quot;) model_row = rows[0] sector, sector_type = get_sector_info(db, base, id_) logger.debug(f&quot;calling incidence_prep()&quot;) return incidence_prep(model_row, crud_type=&#39;update&#39;, sector_type=sector_type, default_dropdown=PLEASE_SELECT) index() Display the homepage with a list of all existing incidence records. Returns: str( str ) \u2013 Rendered HTML for the homepage with incidence records. Examples: In browser: GET / Returns: HTML page with table of incidences Source code in arb\\portal\\routes.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86@main.route(&#39;/&#39;) def index() -&gt; str: &quot;&quot;&quot; Display the homepage with a list of all existing incidence records. Returns: str: Rendered HTML for the homepage with incidence records. Examples: # In browser: GET / # Returns: HTML page with table of incidences &quot;&quot;&quot; logger.info(f&quot;route called: index.&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] table_name = &#39;incidences&#39; colum_name_pk = &#39;id_incidence&#39; rows = get_rows_by_table_name(db, base, table_name, colum_name_pk, ascending=False) return render_template(&#39;index.html&#39;, model_rows=rows) java_script_diagnostic_test() Render a simple page for testing JavaScript diagnostics logging (frontend and backend). NOTE: This is a developer-only route, not covered by E2E tests by design. Source code in arb\\portal\\routes.py 1281 1282 1283 1284 1285 1286 1287 1288 1289@main.route(&#39;/java_script_diagnostic_test&#39;) def java_script_diagnostic_test(): &quot;&quot;&quot; Render a simple page for testing JavaScript diagnostics logging (frontend and backend). NOTE: This is a developer-only route, not covered by E2E tests by design. &quot;&quot;&quot; logger.info(f&quot;route called: java_script_diagnostic_test&quot;) return render_template(&#39;java_script_diagnostic_test.html&#39;) js_diagnostic_log() JavaScript Diagnostics Logging Endpoint NOTE: This is a developer-only route, not covered by E2E tests by design. Purpose This route allows frontend JavaScript code to send diagnostic or debug messages directly to the backend server. These messages are then written to the backend log file, making it possible to correlate frontend/browser events with backend activity, errors, or user actions. This is especially useful for debugging issues that are hard to capture with browser console logs alone (e.g., page reloads, E2E tests, or user-reported problems). How to Use (Frontend JavaScript): Use the provided JS helper function (see below) to send a POST request to this endpoint: Example JS function: function sendJsDiagnostic(msg, extra) { fetch('/js_diagnostic_log', { method: 'POST', headers: {'Content-Type': 'application/json'}, body: JSON.stringify({msg, ts: new Date().toISOString(), ...extra}) }); } Example usage: sendJsDiagnostic('User clicked discard', {action: '/discard_staged_update/0/file.json', userId: 123}); Expected Payload (JSON): { \"msg\": \"A human-readable message (required)\", \"ts\": \"ISO timestamp (optional, will be included in log if present)\", ... any other key-value pairs (optional, will be logged as 'extra') } What Happens The backend receives the POST request and parses the JSON payload. It extracts the 'msg' (message), 'ts' (timestamp), and any other fields (as 'extra'). It writes a log entry to the backend log file in the format: JS_DIAG message | extra: {...} Returns a simple JSON response: {\"status\": \"ok\"} Why Use This Console logs in the browser are lost on navigation/reload and are not visible to backend developers. This route allows you to persistently log important frontend events, errors, or user actions for later analysis. Especially useful for E2E testing, debugging user issues, or tracking hard-to-reproduce bugs. Security Note This endpoint is for diagnostics only. Do not send sensitive user data. Rate limiting or authentication can be added if needed for production. Source code in arb\\portal\\routes.py 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278@main.route(&#39;/js_diagnostic_log&#39;, methods=[&#39;POST&#39;]) def js_diagnostic_log(): &quot;&quot;&quot; JavaScript Diagnostics Logging Endpoint ======================================= NOTE: This is a developer-only route, not covered by E2E tests by design. Purpose: This route allows frontend JavaScript code to send diagnostic or debug messages directly to the backend server. These messages are then written to the backend log file, making it possible to correlate frontend/browser events with backend activity, errors, or user actions. This is especially useful for debugging issues that are hard to capture with browser console logs alone (e.g., page reloads, E2E tests, or user-reported problems). How to Use (Frontend JavaScript): Use the provided JS helper function (see below) to send a POST request to this endpoint: Example JS function: function sendJsDiagnostic(msg, extra) { fetch(&#39;/js_diagnostic_log&#39;, { method: &#39;POST&#39;, headers: {&#39;Content-Type&#39;: &#39;application/json&#39;}, body: JSON.stringify({msg, ts: new Date().toISOString(), ...extra}) }); } Example usage: sendJsDiagnostic(&#39;User clicked discard&#39;, {action: &#39;/discard_staged_update/0/file.json&#39;, userId: 123}); Expected Payload (JSON): { &quot;msg&quot;: &quot;A human-readable message (required)&quot;, &quot;ts&quot;: &quot;ISO timestamp (optional, will be included in log if present)&quot;, ... any other key-value pairs (optional, will be logged as &#39;extra&#39;) } What Happens: - The backend receives the POST request and parses the JSON payload. - It extracts the &#39;msg&#39; (message), &#39;ts&#39; (timestamp), and any other fields (as &#39;extra&#39;). - It writes a log entry to the backend log file in the format: [JS_DIAG][timestamp] message | extra: {...} - Returns a simple JSON response: {&quot;status&quot;: &quot;ok&quot;} Why Use This: - Console logs in the browser are lost on navigation/reload and are not visible to backend developers. - This route allows you to persistently log important frontend events, errors, or user actions for later analysis. - Especially useful for E2E testing, debugging user issues, or tracking hard-to-reproduce bugs. Security Note: - This endpoint is for diagnostics only. Do not send sensitive user data. - Rate limiting or authentication can be added if needed for production. &quot;&quot;&quot; logger.info(f&quot;route called: js_diagnostic_log&quot;) data = request.get_json(force=True, silent=True) or {} msg = data.get(&#39;msg&#39;, &#39;[NO MSG]&#39;) ts = data.get(&#39;ts&#39;) extra = {k: v for k, v in data.items() if k not in (&#39;msg&#39;, &#39;ts&#39;)} log_line = f&quot;[JS_DIAG]{&#39;[&#39; + ts + &#39;]&#39; if ts else &#39;&#39;} {msg}&quot; if extra: log_line += f&quot; | extra: {extra}&quot; logger.info(log_line) return jsonify({&#39;status&#39;: &#39;ok&#39;}) landfill_incidence_create() Create a new dummy Landfill incidence and redirect to its edit form. Returns: Response( Response ) \u2013 Redirect to the incidence_update page for the newly created ID. Examples: In browser: POST /landfill_incidence_create/ Redirects to: /incidence_update// Notes Dummy data is loaded from db_hardcoded.get_landfill_dummy_form_data(). Source code in arb\\portal\\routes.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209@main.route(&#39;/landfill_incidence_create/&#39;, methods=(&#39;GET&#39;, &#39;POST&#39;)) def landfill_incidence_create() -&gt; Response: &quot;&quot;&quot; Create a new dummy Landfill incidence and redirect to its edit form. Returns: Response: Redirect to the `incidence_update` page for the newly created ID. Examples: # In browser: POST /landfill_incidence_create/ # Redirects to: /incidence_update/&lt;new_id&gt;/ Notes: - Dummy data is loaded from `db_hardcoded.get_landfill_dummy_form_data()`. &quot;&quot;&quot; logger.info(f&quot;route called: landfill_incidence_create.&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] table_name = &#39;incidences&#39; col_name = &#39;misc_json&#39; data_dict = arb.portal.db_hardcoded.get_landfill_dummy_form_data() id_ = dict_to_database(db, base, data_dict, table_name=table_name, json_field=col_name, ) logger.debug(f&quot;landfill_incidence_create() - leaving.&quot;) return redirect(url_for(&#39;main.incidence_update&#39;, id_=id_)) list_staged() List all staged files available for review or processing. Returns: str( ResponseReturnValue ) \u2013 Rendered HTML showing all staged files. Source code in arb\\portal\\routes.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347@main.route(&#39;/list_staged&#39;) def list_staged() -&gt; ResponseReturnValue: &quot;&quot;&quot; List all staged files available for review or processing. Returns: str: Rendered HTML showing all staged files. &quot;&quot;&quot; logger.info(f&quot;route called: list_staged&quot;) logger.info(&quot;[LIST_STAGED] Route called&quot;) logger.debug(&quot;list_staged route called&quot;) logger.warning(&#39;[DEBUG] /list_staged route called&#39;) staging_dir = Path(get_upload_folder()) / &quot;staging&quot; staged_files = [] malformed_files = [] if staging_dir.exists(): for file_path in staging_dir.glob(&quot;*.json&quot;): filename = file_path.name id_incidence = None sector = &quot;Unknown&quot; try: # Try to extract ID from filename (format: id_XXXX_ts_YYYYMMDD_HHMMSS.json) if filename.startswith(&quot;id_&quot;) and &quot;_ts_&quot; in filename: id_part = filename.split(&quot;_ts_&quot;)[0] id_incidence = int(id_part.replace(&quot;id_&quot;, &quot;&quot;)) # Try to load metadata to get sector and check for required fields try: json_data, metadata = json_load_with_meta(file_path) base_misc_json = metadata.get(&quot;base_misc_json&quot;, {}) sector = base_misc_json.get(&quot;sector&quot;, &quot;Unknown&quot;) # Check for required field: id_incidence must be a positive integer id_candidate = None # Try to get id_incidence from JSON if not from filename if id_incidence is None: id_candidate = json_data.get(&quot;id_incidence&quot;) if isinstance(id_candidate, int) and id_candidate &gt; 0: id_incidence = id_candidate # If still not valid, treat as malformed if not (isinstance(id_incidence, int) and id_incidence &gt; 0): raise ValueError(&quot;Missing or invalid id_incidence&quot;) staged_files.append({ &#39;filename&#39;: filename, &#39;id_incidence&#39;: id_incidence, &#39;sector&#39;: sector, &#39;file_size&#39;: file_path.stat().st_size, &#39;modified_time&#39;: datetime.datetime.fromtimestamp(file_path.stat().st_mtime), &#39;malformed&#39;: False }) except Exception as meta_exc: # If JSON loads but required fields are missing, treat as malformed logger.warning(f&quot;Malformed staged file (missing fields) {file_path}: {meta_exc}&quot;) malformed_files.append({ &#39;filename&#39;: filename, &#39;file_size&#39;: file_path.stat().st_size, &#39;modified_time&#39;: datetime.datetime.fromtimestamp(file_path.stat().st_mtime), &#39;error&#39;: f&quot;Missing required fields: {meta_exc}&quot; }) except Exception as e: logger.warning(f&quot;Could not process staged file {file_path}: {e}&quot;) malformed_files.append({ &#39;filename&#39;: filename, &#39;file_size&#39;: file_path.stat().st_size, &#39;modified_time&#39;: datetime.datetime.fromtimestamp(file_path.stat().st_mtime), &#39;error&#39;: str(e) }) # Sort by modification time (newest first) staged_files.sort(key=lambda x: x[&#39;modified_time&#39;], reverse=True) malformed_files.sort(key=lambda x: x[&#39;modified_time&#39;], reverse=True) logger.warning(f&#39;[DEBUG] /list_staged rendering {len(staged_files)} staged, {len(malformed_files)} malformed files&#39;) # Always pass both variables, even if empty return render_template(&#39;staged_list.html&#39;, staged_files=staged_files, malformed_files=malformed_files) list_uploads() List all files in the upload directory. Returns: str( str ) \u2013 Rendered HTML showing all uploaded Excel files available on disk. Examples: In browser: GET /list_uploads Returns: HTML page listing uploaded files Source code in arb\\portal\\routes.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270@main.route(&#39;/list_uploads&#39;) def list_uploads() -&gt; str: &quot;&quot;&quot; List all files in the upload directory. Returns: str: Rendered HTML showing all uploaded Excel files available on disk. Examples: # In browser: GET /list_uploads # Returns: HTML page listing uploaded files &quot;&quot;&quot; logger.info(f&quot;route called: list_uploads&quot;) upload_folder = get_upload_folder() # up_dir = Path(&quot;portal/static/uploads&quot;) # print(f&quot;{type(up_dir)=}: {up_dir=}&quot;) files = [x.name for x in upload_folder.iterdir() if x.is_file()] logger.debug(f&quot;{files=}&quot;) return render_template(&#39;uploads_list.html&#39;, files=files) og_incidence_create() Create a new dummy Oil &amp; Gas incidence and redirect to its edit form. Returns: Response( Response ) \u2013 Redirect to the incidence_update page for the newly created ID. Examples: In browser: POST /og_incidence_create/ Redirects to: /incidence_update// Notes Dummy data is loaded from db_hardcoded.get_og_dummy_form_data(). Source code in arb\\portal\\routes.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175@main.route(&#39;/og_incidence_create/&#39;, methods=(&#39;GET&#39;, &#39;POST&#39;)) def og_incidence_create() -&gt; Response: &quot;&quot;&quot; Create a new dummy Oil &amp; Gas incidence and redirect to its edit form. Returns: Response: Redirect to the `incidence_update` page for the newly created ID. Examples: # In browser: POST /og_incidence_create/ # Redirects to: /incidence_update/&lt;new_id&gt;/ Notes: - Dummy data is loaded from `db_hardcoded.get_og_dummy_form_data()`. &quot;&quot;&quot; logger.info(f&quot;route called: og_incidence_create.&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] table_name = &#39;incidences&#39; col_name = &#39;misc_json&#39; data_dict = arb.portal.db_hardcoded.get_og_dummy_form_data() id_ = dict_to_database(db, base, data_dict, table_name=table_name, json_field=col_name, ) logger.debug(f&quot;og_incidence_create() - leaving.&quot;) return redirect(url_for(&#39;main.incidence_update&#39;, id_=id_)) review_staged(id_, filename) Review the contents of a staged file for a specific incidence ID. Parameters: id_ (int) \u2013 Incidence ID associated with the staged file. filename (str) \u2013 Name of the staged file to review. Returns: str | Response \u2013 str|Response: Rendered HTML for file review, or redirect if not found. Examples: In browser: GET /review_staged/123/myfile.xlsx Returns: HTML review page for the file Source code in arb\\portal\\routes.py 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615@main.route(&quot;/review_staged/&lt;int:id_&gt;/&lt;filename&gt;&quot;, methods=[&quot;GET&quot;]) def review_staged(id_: int, filename: str) -&gt; str | Response: &quot;&quot;&quot; Review the contents of a staged file for a specific incidence ID. Args: id_ (int): Incidence ID associated with the staged file. filename (str): Name of the staged file to review. Returns: str|Response: Rendered HTML for file review, or redirect if not found. Examples: # In browser: GET /review_staged/123/myfile.xlsx # Returns: HTML review page for the file &quot;&quot;&quot; logger.info(f&quot;route called: review_staged with id_: {id_} and filename: {filename}&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] staging_dir = Path(get_upload_folder()) / &quot;staging&quot; staged_json_path = staging_dir / filename if not staged_json_path.exists(): logger.warning(f&quot;Staged JSON file not found: {staged_json_path}&quot;) return render_template(&quot;review_staged.html&quot;, error=f&quot;No staged data found for ID {id_}.&quot;, is_new_row=False, id_incidence=id_, staged_fields=[], metadata={}, filename=filename) try: staged_data, metadata = json_load_with_meta(staged_json_path) staged_payload = extract_tab_and_sector(staged_data, tab_name=&quot;Feedback Form&quot;) except Exception: logger.exception(&quot;Error loading staged JSON&quot;) return render_template(&quot;review_staged.html&quot;, error=&quot;Could not load staged data.&quot;, is_new_row=False, id_incidence=id_, staged_fields=[], metadata={}, filename=filename) model, _, is_new_row = get_ensured_row( db=db, base=base, table_name=&quot;incidences&quot;, primary_key_name=&quot;id_incidence&quot;, id_=id_ ) db_json = getattr(model, &quot;misc_json&quot;, {}) or {} staged_fields = compute_field_differences(new_data=staged_payload, existing_data=db_json) if is_new_row: logger.info(f&quot;\u26a0\ufe0f Staged ID {id_} did not exist in DB. A blank row was created for review.&quot;) logger.debug(f&quot;Computed {sum(f[&#39;changed&#39;] for f in staged_fields)} changes across {len(staged_fields)} fields&quot;) return render_template( &quot;review_staged.html&quot;, id_incidence=id_, staged_fields=staged_fields, is_new_row=is_new_row, metadata=metadata, error=None, filename=filename, ) search() Search for incidences or updates in the portal database. Returns: str( str ) \u2013 Rendered HTML search results page. Notes Currently echoes the user-submitted query string. Source code in arb\\portal\\routes.py 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960@main.route(&#39;/search/&#39;, methods=(&#39;GET&#39;, &#39;POST&#39;)) def search() -&gt; str: &quot;&quot;&quot; Search for incidences or updates in the portal database. Returns: str: Rendered HTML search results page. Notes: - Currently echoes the user-submitted query string. &quot;&quot;&quot; logger.info(f&quot;route called: search&quot;) logger.debug(f&quot;{request.form=}&quot;) search_string = request.form.get(&#39;navbar_search&#39;) logger.debug(f&quot;{search_string=}&quot;) return render_template(&#39;search.html&#39;, search_string=search_string, ) serve_file(filename) Serve a file from the uploads directory. Parameters: filename (str) \u2013 Name of the file to serve. Returns: Response( Response ) \u2013 File response for download or viewing in browser. Examples: In browser: GET /serve_file/myfile.xlsx Returns: File download or inline view Source code in arb\\portal\\routes.py 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859@main.route(&quot;/serve_file/&lt;path:filename&gt;&quot;) def serve_file(filename) -&gt; Response: &quot;&quot;&quot; Serve a file from the uploads directory. Args: filename (str): Name of the file to serve. Returns: Response: File response for download or viewing in browser. Examples: # In browser: GET /serve_file/myfile.xlsx # Returns: File download or inline view &quot;&quot;&quot; logger.info(f&quot;route called: serve_file with filename: {filename}&quot;) upload_folder = get_upload_folder() file_path = os.path.join(upload_folder, filename) if not os.path.exists(file_path): abort(404) return send_from_directory(upload_folder, filename) show_database_structure() Show the structure of the portal database (tables, columns, types). Returns: str( str ) \u2013 Rendered HTML of database structure. Examples: In browser: GET /show_database_structure Returns: HTML with database structure Source code in arb\\portal\\routes.py 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041@main.route(&#39;/show_database_structure&#39;) def show_database_structure() -&gt; str: &quot;&quot;&quot; Show the structure of the portal database (tables, columns, types). Returns: str: Rendered HTML of database structure. Examples: # In browser: GET /show_database_structure # Returns: HTML with database structure &quot;&quot;&quot; logger.info(f&quot;route called: show_database_structure&quot;) result = obj_to_html(Globals.db_column_types) result = f&quot;&lt;p&gt;&lt;strong&gt;Postgres Database Structure=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result}&lt;/p&gt;&quot; return render_template(&#39;diagnostics.html&#39;, header=&quot;Database Structure Overview&quot;, subheader=&quot;Reflecting SQLAlchemy model metadata.&quot;, html_content=result, ) show_dropdown_dict() Show the current dropdown dictionary used in forms. Returns: str( str ) \u2013 Rendered HTML of dropdown dictionary. Notes Useful for verifying dropdown contents used in WTForms. Source code in arb\\portal\\routes.py 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018@main.route(&#39;/show_dropdown_dict&#39;) def show_dropdown_dict() -&gt; str: &quot;&quot;&quot; Show the current dropdown dictionary used in forms. Returns: str: Rendered HTML of dropdown dictionary. Notes: - Useful for verifying dropdown contents used in WTForms. &quot;&quot;&quot; logger.info(f&quot;route called: show_dropdown_dict&quot;) # update drop-down tables Globals.load_drop_downs(current_app, db) result1 = obj_to_html(Globals.drop_downs) result2 = obj_to_html(Globals.drop_downs_contingent) result = (f&quot;&lt;p&gt;&lt;strong&gt;Globals.drop_downs=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result1}&lt;/p&gt;&quot; f&quot;&lt;p&gt;&lt;strong&gt;Globals.drop_downs_contingent=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result2}&lt;/p&gt;&quot;) return render_template(&#39;diagnostics.html&#39;, header=&quot;Dropdown Dictionaries&quot;, subheader=&quot;Loaded dropdown values and contingent mappings.&quot;, html_content=result, ) show_feedback_form_structure() Show the structure of the feedback form (fields, types, validators). Returns: str( str ) \u2013 Rendered HTML of feedback form structure. Examples: In browser: GET /show_feedback_form_structure Returns: HTML with feedback form structure Source code in arb\\portal\\routes.py 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073@main.route(&#39;/show_feedback_form_structure&#39;) def show_feedback_form_structure() -&gt; str: &quot;&quot;&quot; Show the structure of the feedback form (fields, types, validators). Returns: str: Rendered HTML of feedback form structure. Examples: # In browser: GET /show_feedback_form_structure # Returns: HTML with feedback form structure &quot;&quot;&quot; logger.info(f&quot;route called: show_feedback_form_structure&quot;) form1 = OGFeedback() fields1 = get_wtforms_fields(form1) result1 = obj_to_html(fields1) form2 = LandfillFeedback() fields2 = get_wtforms_fields(form2) result2 = obj_to_html(fields2) result = (f&quot;&lt;p&gt;&lt;strong&gt;WTF OGFeedback Form Structure=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result1}&lt;/p&gt;&quot; f&quot;&lt;p&gt;&lt;strong&gt;WTF LandfillFeedback Form Structure=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result2}&lt;/p&gt;&quot;) return render_template(&#39;diagnostics.html&#39;, header=&quot;WTForms Feedback Form Structure&quot;, subheader=&quot;Inspecting field mappings in Oil &amp; Gas and Landfill feedback forms.&quot;, html_content=result, ) show_log_file() Display the last N lines of the portal log file. NOTE: This is a developer-only route, not covered by E2E tests by design. Query Parameters lines (int, optional): Number of lines to show from the end of the log file. Defaults to 1000 if not provided or invalid. Args: None Returns: str( str ) \u2013 Rendered HTML with the log file content shown inside a block. Example Usage /show_log_file?lines=500 Notes Useful for debugging in development or staging. Efficient for large files using read_file_reverse(). Source code in arb\\portal\\routes.py 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144@main.route(&#39;/show_log_file&#39;) def show_log_file() -&gt; str: &quot;&quot;&quot; Display the last N lines of the portal log file. NOTE: This is a developer-only route, not covered by E2E tests by design. Query Parameters: lines (int, optional): Number of lines to show from the end of the log file. Defaults to 1000 if not provided or invalid. Args: None Returns: str: Rendered HTML with the log file content shown inside a &lt;pre&gt; block. Example Usage: /show_log_file?lines=500 Notes: - Useful for debugging in development or staging. - Efficient for large files using read_file_reverse(). &quot;&quot;&quot; logger.info(f&quot;route called: show_log_file&quot;) default_lines = 1000 try: num_lines = int(request.args.get(&#39;lines&#39;, default_lines)) if num_lines &lt; 1: raise ValueError except ValueError: num_lines = default_lines logger.info(f&quot;Displaying the last {num_lines} lines of the log file as a diagnostic&quot;) lines = read_file_reverse(LOG_FILE, n=num_lines) file_content = &#39;\\n&#39;.join(lines) result = f&quot;&lt;p&gt;&lt;strong&gt;Last {num_lines} lines of logger file:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;pre&gt;{file_content}&lt;/pre&gt;&lt;/p&gt;&quot; return render_template( &#39;diagnostics.html&#39;, header=&quot;Log File Contents&quot;, html_content=result, ) upload_file(message=None) Handle file upload form and process uploaded Excel files. Parameters: message (str | None, default: None ) \u2013 Optional message to display on the upload page. Returns: Union[str, Response] \u2013 str|Response: Rendered HTML for the upload form, or redirect after upload. Examples: In browser: GET /upload Returns: HTML upload form In browser: POST /upload Redirects to: /list_uploads or error page Source code in arb\\portal\\routes.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441@main.route(&#39;/upload&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) @main.route(&#39;/upload/&lt;message&gt;&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) def upload_file(message: str | None = None) -&gt; Union[str, Response]: &quot;&quot;&quot; Handle file upload form and process uploaded Excel files. Args: message (str | None): Optional message to display on the upload page. Returns: str|Response: Rendered HTML for the upload form, or redirect after upload. Examples: # In browser: GET /upload # Returns: HTML upload form # In browser: POST /upload # Redirects to: /list_uploads or error page &quot;&quot;&quot; logger.info(f&quot;route called: upload_file with message: {message}&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] form = UploadForm() # Decode redirect message, if present if message: message = unquote(message) logger.debug(f&quot;Received redirect message: {message}&quot;) upload_folder = get_upload_folder() logger.debug(f&quot;Files received: {list(request.files.keys())}, upload_folder={upload_folder}&quot;) if request.method == &#39;POST&#39;: try: request_file = request.files.get(&#39;file&#39;) if not request_file or not request_file.filename: logger.warning(&quot;POST received with no file selected.&quot;) return render_template( &#39;upload.html&#39;, form=form, upload_message=&quot;No file selected. Please choose a file.&quot; ) logger.debug(f&quot;Received uploaded file: {request_file.filename}&quot;) # Step 1: Save file and attempt DB ingest file_path, id_, sector = upload_and_update_db(db, upload_folder, request_file, base) if id_: logger.debug(f&quot;Upload successful: id={id_}, sector={sector}. Redirecting to update page.&quot;) return redirect(url_for(&#39;main.incidence_update&#39;, id_=id_)) # If id_ is None, check if likely blocked due to missing/invalid id_incidence if file_path and (file_path.exists() if hasattr(file_path, &#39;exists&#39;) else True): # Check log message or just show the message if id_ is None logger.warning(f&quot;Upload blocked: missing or invalid id_incidence in {file_path.name}&quot;) return render_template( &#39;upload.html&#39;, form=form, upload_message=( &quot;This file is missing a valid &#39;Incidence/Emission ID&#39; (id_incidence). &quot; &quot;Please add a positive integer id_incidence to your spreadsheet before uploading.&quot; ) ) # Step 2: Handle schema recognition failure with enhanced diagnostics logger.warning(f&quot;Upload failed schema recognition: {file_path=}&quot;) error_details = generate_upload_diagnostics(request_file, file_path) detailed_message = format_diagnostic_message(error_details, &quot;Uploaded file format not recognized.&quot;) return render_template( &#39;upload.html&#39;, form=form, upload_message=detailed_message ) except Exception as e: logger.exception(&quot;Exception occurred during upload or parsing.&quot;) # Enhanced error handling with diagnostic information error_details = generate_upload_diagnostics(request_file, file_path if &#39;file_path&#39; in locals() else None) detailed_message = format_diagnostic_message(error_details) return render_template( &#39;upload.html&#39;, form=form, upload_message=detailed_message ) # GET request: display form return render_template(&#39;upload.html&#39;, form=form, upload_message=message) upload_file_staged(message=None) Handle staged file upload form and process staged Excel files. Parameters: message (str | None, default: None ) \u2013 Optional message to display on the staged upload page. Returns: Union[str, Response] \u2013 str|Response: Rendered HTML for the staged upload form, or redirect after upload. Examples: In browser: GET /upload_staged Returns: HTML staged upload form In browser: POST /upload_staged Redirects to: /list_staged or error page Source code in arb\\portal\\routes.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543@main.route(&#39;/upload_staged&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) @main.route(&#39;/upload_staged/&lt;message&gt;&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) def upload_file_staged(message: str | None = None) -&gt; Union[str, Response]: &quot;&quot;&quot; Handle staged file upload form and process staged Excel files. Args: message (str | None): Optional message to display on the staged upload page. Returns: str|Response: Rendered HTML for the staged upload form, or redirect after upload. Examples: # In browser: GET /upload_staged # Returns: HTML staged upload form # In browser: POST /upload_staged # Redirects to: /list_staged or error page &quot;&quot;&quot; logger.info(f&quot;route called: upload_file_staged with message: {message}&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] form = UploadForm() # Decode optional redirect message if message: message = unquote(message) logger.debug(f&quot;Received redirect message: {message}&quot;) upload_folder = get_upload_folder() logger.debug(f&quot;Request received with files: {list(request.files.keys())}, upload_folder={upload_folder}&quot;) if request.method == &#39;POST&#39;: try: request_file = request.files.get(&#39;file&#39;) if not request_file or not request_file.filename: logger.warning(&quot;POST received with no file selected.&quot;) return render_template(&#39;upload_staged.html&#39;, form=form, upload_message=&quot;No file selected. Please choose a file.&quot;) logger.debug(f&quot;Received uploaded file: {request_file.filename}&quot;) # Save and stage (no DB commit) file_path, id_, sector, json_data, staged_filename = upload_and_stage_only(db, upload_folder, request_file, base) if id_ and staged_filename: logger.debug(f&quot;Staged upload successful: id={id_}, sector={sector}, filename={staged_filename}. Redirecting to review page.&quot;) # Enhanced success feedback with staging details success_message = ( f&quot;\u2705 File &#39;{request_file.filename}&#39; staged successfully!\\n&quot; f&quot;\ud83d\udccb ID: {id_}\\n&quot; f&quot;\ud83c\udfed Sector: {sector}\\n&quot; f&quot;\ud83d\udcc1 Staged as: {staged_filename}\\n&quot; f&quot;\ud83d\udd0d Ready for review and confirmation.&quot; ) flash(success_message, &quot;success&quot;) return redirect(url_for(&#39;main.review_staged&#39;, id_=id_, filename=staged_filename)) # If id_ is None or not staged, check if likely blocked due to missing/invalid id_incidence if file_path and (file_path.exists() if hasattr(file_path, &#39;exists&#39;) else True): logger.warning(f&quot;Staging blocked: missing or invalid id_incidence in {file_path.name}&quot;) return render_template( &#39;upload_staged.html&#39;, form=form, upload_message=( &quot;This file is missing a valid &#39;Incidence/Emission ID&#39; (id_incidence). &quot; &quot;Please add a positive integer id_incidence to your spreadsheet before uploading.&quot; ) ) # Fallback: schema recognition failure or other error logger.warning(f&quot;Staging failed: missing or invalid id_incidence in {file_path.name}&quot;) return render_template( &#39;upload_staged.html&#39;, form=form, upload_message=&quot;This file is missing a valid &#39;Incidence/Emission ID&#39; (id_incidence). &quot; &quot;Please verify the spreadsheet includes that field and try again.&quot; ) except Exception as e: logger.exception(&quot;Exception occurred during staged upload.&quot;) # Enhanced error handling with staging-specific diagnostic information error_details = generate_staging_diagnostics( request_file, file_path if &#39;file_path&#39; in locals() else None, staged_filename if &#39;staged_filename&#39; in locals() else None, id_ if &#39;id_&#39; in locals() else None, sector if &#39;sector&#39; in locals() else None ) detailed_message = format_diagnostic_message(error_details, &quot;Staged upload processing failed.&quot;) return render_template( &#39;upload_staged.html&#39;, form=form, upload_message=detailed_message ) # GET request: display form return render_template(&#39;upload_staged.html&#39;, form=form, upload_message=message) view_portal_updates() Display a table of all portal update log entries. Returns: str( str ) \u2013 Rendered HTML table of portal update logs. Notes Supports pagination, filtering, and sorting via query parameters. Default sort is descending by timestamp. Source code in arb\\portal\\routes.py 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898@main.route(&quot;/portal_updates&quot;) def view_portal_updates() -&gt; str: &quot;&quot;&quot; Display a table of all portal update log entries. Returns: str: Rendered HTML table of portal update logs. Notes: - Supports pagination, filtering, and sorting via query parameters. - Default sort is descending by timestamp. &quot;&quot;&quot; sort_by = request.args.get(&quot;sort_by&quot;, &quot;timestamp&quot;) direction = request.args.get(&quot;direction&quot;, &quot;desc&quot;) page = int(request.args.get(&quot;page&quot;, 1)) per_page = int(request.args.get(&quot;per_page&quot;, 100)) query = db.session.query(PortalUpdate) query = apply_portal_update_filters(query, PortalUpdate, request.args) updates = query.order_by(PortalUpdate.timestamp.desc()).all() return render_template( &quot;portal_updates.html&quot;, updates=updates, sort_by=sort_by, direction=direction, page=page, per_page=per_page, total_pages=1, filter_key=request.args.get(&quot;filter_key&quot;, &quot;&quot;).strip(), filter_user=request.args.get(&quot;filter_user&quot;, &quot;&quot;).strip(), filter_comments=request.args.get(&quot;filter_comments&quot;, &quot;&quot;).strip(), filter_id_incidence=request.args.get(&quot;filter_id_incidence&quot;, &quot;&quot;).strip(), start_date=request.args.get(&quot;start_date&quot;, &quot;&quot;).strip(), end_date=request.args.get(&quot;end_date&quot;, &quot;&quot;).strip(), ) /"},{"location":"reference/arb/portal/routes/#arb.portal.routes.list_staged","text":"List all staged files available for review or processing. Returns: str ( ResponseReturnValue ) \u2013 Rendered HTML showing all staged files. Source code in arb\\portal\\routes.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 @main . route ( '/list_staged' ) def list_staged () -> ResponseReturnValue : \"\"\" List all staged files available for review or processing. Returns: str: Rendered HTML showing all staged files. \"\"\" logger . info ( f \"route called: list_staged\" ) logger . info ( \"[LIST_STAGED] Route called\" ) logger . debug ( \"list_staged route called\" ) logger . warning ( '[DEBUG] /list_staged route called' ) staging_dir = Path ( get_upload_folder ()) / \"staging\" staged_files = [] malformed_files = [] if staging_dir . exists (): for file_path in staging_dir . glob ( \"*.json\" ): filename = file_path . name id_incidence = None sector = \"Unknown\" try : # Try to extract ID from filename (format: id_XXXX_ts_YYYYMMDD_HHMMSS.json) if filename . startswith ( \"id_\" ) and \"_ts_\" in filename : id_part = filename . split ( \"_ts_\" )[ 0 ] id_incidence = int ( id_part . replace ( \"id_\" , \"\" )) # Try to load metadata to get sector and check for required fields try : json_data , metadata = json_load_with_meta ( file_path ) base_misc_json = metadata . get ( \"base_misc_json\" , {}) sector = base_misc_json . get ( \"sector\" , \"Unknown\" ) # Check for required field: id_incidence must be a positive integer id_candidate = None # Try to get id_incidence from JSON if not from filename if id_incidence is None : id_candidate = json_data . get ( \"id_incidence\" ) if isinstance ( id_candidate , int ) and id_candidate > 0 : id_incidence = id_candidate # If still not valid, treat as malformed if not ( isinstance ( id_incidence , int ) and id_incidence > 0 ): raise ValueError ( \"Missing or invalid id_incidence\" ) staged_files . append ({ 'filename' : filename , 'id_incidence' : id_incidence , 'sector' : sector , 'file_size' : file_path . stat () . st_size , 'modified_time' : datetime . datetime . fromtimestamp ( file_path . stat () . st_mtime ), 'malformed' : False }) except Exception as meta_exc : # If JSON loads but required fields are missing, treat as malformed logger . warning ( f \"Malformed staged file (missing fields) { file_path } : { meta_exc } \" ) malformed_files . append ({ 'filename' : filename , 'file_size' : file_path . stat () . st_size , 'modified_time' : datetime . datetime . fromtimestamp ( file_path . stat () . st_mtime ), 'error' : f \"Missing required fields: { meta_exc } \" }) except Exception as e : logger . warning ( f \"Could not process staged file { file_path } : { e } \" ) malformed_files . append ({ 'filename' : filename , 'file_size' : file_path . stat () . st_size , 'modified_time' : datetime . datetime . fromtimestamp ( file_path . stat () . st_mtime ), 'error' : str ( e ) }) # Sort by modification time (newest first) staged_files . sort ( key = lambda x : x [ 'modified_time' ], reverse = True ) malformed_files . sort ( key = lambda x : x [ 'modified_time' ], reverse = True ) logger . warning ( f '[DEBUG] /list_staged rendering { len ( staged_files ) } staged, { len ( malformed_files ) } malformed files' ) # Always pass both variables, even if empty return render_template ( 'staged_list.html' , staged_files = staged_files , malformed_files = malformed_files )","title":"list_staged"},{"location":"reference/arb/portal/routes/#arb.portal.routes.list_uploads","text":"List all files in the upload directory. Returns: str ( str ) \u2013 Rendered HTML showing all uploaded Excel files available on disk. Examples:","title":"list_uploads"},{"location":"reference/arb/portal/routes/#arb.portal.routes.list_uploads--in-browser-get-list_uploads","text":"","title":"In browser: GET /list_uploads"},{"location":"reference/arb/portal/routes/#arb.portal.routes.list_uploads--returns-html-page-listing-uploaded-files","text":"Source code in arb\\portal\\routes.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 @main . route ( '/list_uploads' ) def list_uploads () -> str : \"\"\" List all files in the upload directory. Returns: str: Rendered HTML showing all uploaded Excel files available on disk. Examples: # In browser: GET /list_uploads # Returns: HTML page listing uploaded files \"\"\" logger . info ( f \"route called: list_uploads\" ) upload_folder = get_upload_folder () # up_dir = Path(\"portal/static/uploads\") # print(f\"{type(up_dir)=}: {up_dir=}\") files = [ x . name for x in upload_folder . iterdir () if x . is_file ()] logger . debug ( f \" { files =} \" ) return render_template ( 'uploads_list.html' , files = files )","title":"Returns: HTML page listing uploaded files"},{"location":"reference/arb/portal/routes/#arb.portal.routes.og_incidence_create","text":"Create a new dummy Oil & Gas incidence and redirect to its edit form. Returns: Response ( Response ) \u2013 Redirect to the incidence_update page for the newly created ID. Examples:","title":"og_incidence_create"},{"location":"reference/arb/portal/routes/#arb.portal.routes.og_incidence_create--in-browser-post-og_incidence_create","text":"","title":"In browser: POST /og_incidence_create/"},{"location":"reference/arb/portal/routes/#arb.portal.routes.og_incidence_create--redirects-to-incidence_update","text":"Notes Dummy data is loaded from db_hardcoded.get_og_dummy_form_data() . Source code in arb\\portal\\routes.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 @main . route ( '/og_incidence_create/' , methods = ( 'GET' , 'POST' )) def og_incidence_create () -> Response : \"\"\" Create a new dummy Oil & Gas incidence and redirect to its edit form. Returns: Response: Redirect to the `incidence_update` page for the newly created ID. Examples: # In browser: POST /og_incidence_create/ # Redirects to: /incidence_update/<new_id>/ Notes: - Dummy data is loaded from `db_hardcoded.get_og_dummy_form_data()`. \"\"\" logger . info ( f \"route called: og_incidence_create.\" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] table_name = 'incidences' col_name = 'misc_json' data_dict = arb . portal . db_hardcoded . get_og_dummy_form_data () id_ = dict_to_database ( db , base , data_dict , table_name = table_name , json_field = col_name , ) logger . debug ( f \"og_incidence_create() - leaving.\" ) return redirect ( url_for ( 'main.incidence_update' , id_ = id_ ))","title":"Redirects to: /incidence_update/ Blueprint-based route definitions for the ARB Feedback Portal. This module defines all Flask routes originally found in app.py, now organized under the main Blueprint for modularity. Module_Attributes main (Blueprint): Flask Blueprint for all portal routes. logger (logging.Logger): Logger instance for this module. Examples: from arb.portal.routes import main app.register_blueprint(main) Notes All routes assume that create_app() registers the main Blueprint. Developer diagnostics are inlined near the end of the module. apply_staged_update(id_) Apply a staged update to the database for a specific incidence ID. Parameters: id_ (int) \u2013 Incidence ID to apply the staged update to. Returns: Response( Response ) \u2013 Redirect to the incidence update page after applying the update. Examples: In browser: POST /apply_staged_update/123 Redirects to: /incidence_update/123/ Source code in arb\\portal\\routes.py 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833@main.route(&#39;/apply_staged_update/&lt;int:id_&gt;&#39;, methods=[&#39;POST&#39;]) def apply_staged_update(id_: int) -&gt; Response: &quot;&quot;&quot; Apply a staged update to the database for a specific incidence ID. Args: id_ (int): Incidence ID to apply the staged update to. Returns: Response: Redirect to the incidence update page after applying the update. Examples: # In browser: POST /apply_staged_update/123 # Redirects to: /incidence_update/123/ &quot;&quot;&quot; logger.info(f&quot;route called: apply_staged_update with id_: {id_}&quot;) try: # staging_dir = Path(current_app.config[&quot;UPLOAD_STAGING_FOLDER&quot;]) staging_dir = Path(get_upload_folder()) / &quot;staging&quot; staged_file = staging_dir / f&quot;{id_}.json&quot; if not staged_file.exists(): logger.error(f&quot;Staged file does not exist: {staged_file}&quot;) flash(&quot;Staged file not found.&quot;, &quot;danger&quot;) return redirect(url_for(&quot;main.upload_file_staged&quot;)) xl_dict, _ = json_load_with_meta(staged_file) base = current_app.base # type: ignore[attr-defined] final_id, sector = xl_dict_to_database(db, base, xl_dict) logger.info(f&quot;Applied staged update for id={final_id}, sector={sector}&quot;) try: staged_file.unlink() logger.debug(f&quot;Deleted staged file: {staged_file}&quot;) except Exception as delete_error: logger.warning(f&quot;Could not delete staged file: {delete_error}&quot;) return redirect(url_for(&quot;main.incidence_update&quot;, id_=final_id)) except Exception as e: logger.exception(&quot;Failed to apply staged update.&quot;) flash(&quot;Error applying update. Please try again.&quot;, &quot;danger&quot;) return redirect(url_for(&quot;main.upload_file_staged&quot;)) confirm_staged(id_, filename) Confirm and apply a staged update for a specific incidence ID and file. Parameters: id_ (int) \u2013 Incidence ID to update. filename (str) \u2013 Name of the staged file to confirm. Returns: Response( ResponseReturnValue ) \u2013 Redirect to the incidence update page after applying the update. Examples: In browser: POST /confirm_staged/123/myfile.xlsx Redirects to: /incidence_update/123/ Source code in arb\\portal\\routes.py 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752@main.route(&quot;/confirm_staged/&lt;int:id_&gt;/&lt;filename&gt;&quot;, methods=[&quot;POST&quot;]) def confirm_staged(id_: int, filename: str) -&gt; ResponseReturnValue: &quot;&quot;&quot; Confirm and apply a staged update for a specific incidence ID and file. Args: id_ (int): Incidence ID to update. filename (str): Name of the staged file to confirm. Returns: Response: Redirect to the incidence update page after applying the update. Examples: # In browser: POST /confirm_staged/123/myfile.xlsx # Redirects to: /incidence_update/123/ &quot;&quot;&quot; import shutil logger.info(f&quot;route called: confirm_staged with id_: {id_} and filename: {filename}&quot;) # Resolve paths root = get_upload_folder() staged_path = os.path.join(root, &quot;staging&quot;, filename) processed_dir = os.path.join(root, &quot;processed&quot;) os.makedirs(processed_dir, exist_ok=True) processed_path = os.path.join(processed_dir, filename) # Load staged payload and metadata try: staged_data, staged_meta = json_load_with_meta(Path(staged_path)) base_misc_json = staged_meta.get(&quot;base_misc_json&quot;, {}) except Exception as e: flash(f&quot;Failed to load staged file for ID {id_}: {e}&quot;, &quot;danger&quot;) return redirect(url_for(&quot;main.upload_file_staged&quot;)) # Extract form data from the staged JSON structure # staged_data contains: {&#39;metadata&#39;: {...}, &#39;schemas&#39;: {...}, &#39;tab_contents&#39;: {&#39;Feedback Form&#39;: {...}}} # We need to extract just the form data from tab_contents and include sector from metadata form_data = extract_tab_and_sector(staged_data, tab_name=&quot;Feedback Form&quot;) if not form_data: flash(f&quot;Failed to extract form data from staged file for ID {id_}&quot;, &quot;danger&quot;) return redirect(url_for(&quot;main.upload_file_staged&quot;)) # Get the database model base: AutomapBase = current_app.base # type: ignore[attr-defined] table_name = &#39;incidences&#39; table = get_class_from_table_name(base, table_name) # Get or create the model row logger.info(f&quot;[confirm_staged] Getting/creating model row for id_incidence={id_}&quot;) model_row, _, is_new_row = get_ensured_row( db=db, base=base, table_name=table_name, primary_key_name=&quot;id_incidence&quot;, id_=id_, add_to_session=True ) logger.info(f&quot;[confirm_staged] Model row result: type={type(model_row)}, &quot; f&quot;id_incidence={getattr(model_row, &#39;id_incidence&#39;, &#39;N/A&#39;)}, &quot; f&quot;is_new_row={is_new_row}&quot;) # Check for concurrent DB changes current_misc_json = getattr(model_row, &quot;misc_json&quot;, {}) or {} logger.info(f&quot;[confirm_staged] Current misc_json: {current_misc_json}&quot;) logger.info(f&quot;[confirm_staged] Base misc_json from staging: {base_misc_json}&quot;) if current_misc_json != base_misc_json: logger.warning(f&quot;[confirm_staged] Concurrent DB changes detected! &quot; f&quot;current_misc_json != base_misc_json&quot;) flash( &quot;\u26a0\ufe0f The database was changed by another user before your updates were confirmed. Please review the new database state and reconfirm which fields you wish to update.&quot;, &quot;warning&quot;) return redirect(url_for(&quot;main.review_staged&quot;, id_=id_, filename=filename)) # Build update patch only for fields user confirmed patch: dict = {} logger.info(f&quot;[confirm_staged] Building patch from {len(form_data)} form fields&quot;) for key in form_data: checkbox_name = f&quot;confirm_overwrite_{key}&quot; confirmed = checkbox_name in request.form new_val = form_data[key] # Ensure we have a dictionary to work with, even if misc_json is None misc_json = getattr(model_row, &quot;misc_json&quot;, {}) or {} old_val = misc_json.get(key) if confirmed: patch[key] = new_val logger.debug(f&quot;[confirm_staged] Added to patch: {key}={new_val} (confirmed={confirmed})&quot;) logger.info(f&quot;[confirm_staged] Final patch contains {len(patch)} fields: {list(patch.keys())}&quot;) if not patch: logger.warning(f&quot;[confirm_staged] No fields in patch - no changes to save&quot;) flash(&quot;No fields were confirmed for update. No changes saved.&quot;, &quot;warning&quot;) return redirect(url_for(&quot;main.upload_file_staged&quot;)) # \ud83c\udd95 Prepare patch for JSON serialization (type coercion, datetime conversion, etc.) patch = prep_payload_for_json(patch) logger.info(f&quot;[confirm_staged] Prepared patch for JSON: {patch}&quot;) # Apply patch to the database model try: logger.info(f&quot;[confirm_staged] About to call apply_json_patch_and_log with {len(patch)} fields&quot;) apply_json_patch_and_log( model=model_row, updates=patch, json_field=&quot;misc_json&quot;, user=&quot;anonymous&quot;, comments=f&quot;Staged update confirmed for ID {id_}&quot; ) logger.info(f&quot;[confirm_staged] \u2705 apply_json_patch_and_log completed successfully&quot;) # \ud83c\udd95 Commit the database transaction to persist changes logger.info(f&quot;[confirm_staged] About to commit database session&quot;) db.session.commit() logger.info(f&quot;[confirm_staged] \u2705 Database session committed successfully&quot;) # Move the staged JSON file to the processed directory shutil.move(staged_path, processed_path) logger.info(f&quot;[confirm_staged] \u2705 Moved staged file to processed: {processed_path}&quot;) flash(f&quot;\u2705 Successfully updated record {id_}. {len(patch)} fields changed. Staged file moved to processed directory.&quot;, &quot;success&quot;) except Exception as e: # Rollback on error to prevent partial commits logger.error(f&quot;[confirm_staged] \u274c Error during database update: {e}&quot;) logger.exception(f&quot;[confirm_staged] Full exception details:&quot;) db.session.rollback() flash(f&quot;\u274c Error applying updates for ID {id_}: {e}&quot;, &quot;danger&quot;) return redirect(url_for(&quot;main.upload_file_staged&quot;)) return redirect(url_for(&quot;main.upload_file_staged&quot;)) delete_testing_range() Developer utility: Delete testing rows in a specified id_incidence range. NOTE: This is a developer/destructive route, not covered by E2E tests by design. GET: Show form to specify min_id, max_id, and dry_run. POST: Run delete_testing_rows and show summary/results, including id_incidences if dry run. Source code in arb\\portal\\routes.py 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213@main.route(&#39;/delete_testing_range&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) def delete_testing_range() -&gt; str: &quot;&quot;&quot; Developer utility: Delete testing rows in a specified id_incidence range. NOTE: This is a developer/destructive route, not covered by E2E tests by design. GET: Show form to specify min_id, max_id, and dry_run. POST: Run delete_testing_rows and show summary/results, including id_incidences if dry run. &quot;&quot;&quot; from flask import current_app logger.info(f&quot;route called: delete_testing_range&quot;) base = current_app.base # type: ignore[attr-defined] error = None result = None min_id = 1000000 max_id = 2000000 dry_run = True submitted = False portal_updates_ids = [] incidences_ids = [] if request.method == &#39;POST&#39;: try: min_id = int(request.form.get(&#39;min_id&#39;, 1000000)) max_id = int(request.form.get(&#39;max_id&#39;, 2000000)) dry_run = bool(request.form.get(&#39;dry_run&#39;)) submitted = True if min_id &lt; 1000000 or max_id &lt; 1000000: error = &quot;Both min and max id_incidence must be at least 1000000.&quot; elif min_id &gt; max_id: error = &quot;min_id cannot be greater than max_id.&quot; else: if dry_run: # Get the IDs that would be deleted preview = list_testing_rows(db, base, min_id, max_id) portal_updates_ids = sorted(set(row[&#39;id_incidence&#39;] for row in preview[&#39;portal_updates&#39;])) incidences_ids = sorted(row[&#39;id_incidence&#39;] for row in preview[&#39;incidences&#39;]) result = delete_testing_rows(db, base, min_id, max_id, dry_run=dry_run) except Exception as e: error = f&quot;Error: {e}&quot; instructions = ( &quot;&lt;ul&gt;&quot; &quot;&lt;li&gt;&lt;b&gt;Use this tool to delete test rows from the portal_updates and incidences tables.&lt;/b&gt;&lt;/li&gt;&quot; &quot;&lt;li&gt;Specify a min and max id_incidence (both must be at least 1000000).&lt;/li&gt;&quot; &quot;&lt;li&gt;Check &#39;Dry Run&#39; to preview what would be deleted without making changes.&lt;/li&gt;&quot; &quot;&lt;li&gt;&lt;b&gt;Warning:&lt;/b&gt; This cannot delete real data (id_incidence &lt; 1000000 is not allowed).&lt;/li&gt;&quot; &quot;&lt;li&gt;For safety, always do a dry run first!&lt;/li&gt;&quot; &quot;&lt;/ul&gt;&quot; ) # Remove summarize_ids and always pass full lists return render_template( &#39;delete_testing_range.html&#39;, min_id=min_id, max_id=max_id, dry_run=dry_run, error=error, result=result, submitted=submitted, instructions=instructions, portal_updates_ids=portal_updates_ids, incidences_ids=incidences_ids ) diagnostics() Display developer diagnostics and runtime information. NOTE: This is a developer-only route, not covered by E2E tests by design. Returns: str( str ) \u2013 Rendered HTML diagnostics page. Examples: In browser: GET /diagnostics Returns: HTML diagnostics info Source code in arb\\portal\\routes.py 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992@main.route(&#39;/diagnostics&#39;) def diagnostics() -&gt; str: &quot;&quot;&quot; Display developer diagnostics and runtime information. NOTE: This is a developer-only route, not covered by E2E tests by design. Returns: str: Rendered HTML diagnostics page. Examples: # In browser: GET /diagnostics # Returns: HTML diagnostics info &quot;&quot;&quot; logger.info(f&quot;route called: diagnostics&quot;) result = find_auto_increment_value(db, &quot;incidences&quot;, &quot;id_incidence&quot;) html_content = f&quot;&lt;p&gt;&lt;strong&gt;Diagnostic Results=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result}&lt;/p&gt;&quot; return render_template(&#39;diagnostics.html&#39;, header=&quot;Auto-Increment Check&quot;, subheader=&quot;Next available ID value in the &#39;incidences&#39; table.&quot;, html_content=html_content, modal_title=&quot;Success&quot;, modal_message=&quot;Diagnostics completed successfully.&quot;, ) discard_staged_update(id_, filename) Discard a staged update for a specific incidence ID and filename. CSRF is disabled for this route. Source code in arb\\portal\\routes.py 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785@csrf.exempt @main.route(&quot;/discard_staged_update/&lt;int:id_&gt;/&lt;filename&gt;&quot;, methods=[&quot;POST&quot;]) def discard_staged_update(id_: int, filename: str) -&gt; ResponseReturnValue: &quot;&quot;&quot; Discard a staged update for a specific incidence ID and filename. CSRF is disabled for this route. &quot;&quot;&quot; logger.info(f&quot;route called: discard_staged_update with id_: {id_} and filename: {filename}&quot;) import unicodedata import time staging_dir = Path(get_upload_folder()) / &quot;staging&quot; # Normalize filename for cross-platform compatibility safe_filename = unicodedata.normalize(&#39;NFC&#39;, filename.strip()) staged_file = staging_dir / safe_filename logger.info(f&quot;[DISCARD] Route called: id_={id_}, filename=&#39;{filename}&#39;, safe_filename=&#39;{safe_filename}&#39;&quot;) logger.info(f&quot;[DISCARD] Starting deletion for: {staged_file}&quot;) logger.info(f&quot;[DISCARD] File exists before deletion: {staged_file.exists()}&quot;) try: if staged_file.exists(): staged_file.unlink() logger.info(f&quot;[DISCARD] File deleted: {staged_file}&quot;) else: logger.info(f&quot;[DISCARD] File not found for deletion: {staged_file}&quot;) except Exception as e: logger.error(f&quot;[DISCARD] Exception during file deletion: {e}&quot;) logger.info(f&quot;[DISCARD] File exists after deletion: {staged_file.exists()}&quot;) logger.info(f&quot;[DISCARD] Completed discard_staged_update for: {staged_file}&quot;) # Sleep briefly to ensure filesystem updates propagate (for test timing) time.sleep(0.2) logger.info(f&quot;[DISCARD] Returning redirect to /list_staged&quot;) return redirect(url_for(&quot;main.list_staged&quot;)) export_portal_updates() Export all portal update log entries as a CSV file. Returns: Response( Response ) \u2013 CSV file download of portal update logs. Notes Respects filters set in the /portal_updates page. Uses standard CSV headers and UTF-8 encoding. Source code in arb\\portal\\routes.py 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939@main.route(&quot;/portal_updates/export&quot;) def export_portal_updates() -&gt; Response: &quot;&quot;&quot; Export all portal update log entries as a CSV file. Returns: Response: CSV file download of portal update logs. Notes: - Respects filters set in the `/portal_updates` page. - Uses standard CSV headers and UTF-8 encoding. &quot;&quot;&quot; logger.info(f&quot;route called: export_portal_updates&quot;) query = db.session.query(PortalUpdate) query = apply_portal_update_filters(query, PortalUpdate, request.args) updates = query.order_by(PortalUpdate.timestamp.desc()).all() si = StringIO() writer = csv.writer(si) writer.writerow([&quot;timestamp&quot;, &quot;key&quot;, &quot;old_value&quot;, &quot;new_value&quot;, &quot;user&quot;, &quot;comments&quot;, &quot;id_incidence&quot;]) for u in updates: writer.writerow([ u.timestamp, u.key, u.old_value, u.new_value, u.user, u.comments, u.id_incidence or &quot;&quot; ]) return Response( si.getvalue(), mimetype=&quot;text/csv&quot;, headers={&quot;Content-Disposition&quot;: &quot;attachment; filename=portal_updates_export.csv&quot;} ) incidence_delete(id_) Delete a specified incidence from the database. Parameters: id_ (int) \u2013 Primary key of the incidence to delete. Returns: Response( ResponseReturnValue ) \u2013 Redirect to the homepage after deletion. Examples: In browser: POST /incidence_delete/123/ Redirects to: / Notes Future: consider adding authorization (e.g., CARB password) to restrict access. Source code in arb\\portal\\routes.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247@main.post(&#39;/incidence_delete/&lt;int:id_&gt;/&#39;) def incidence_delete(id_: int) -&gt; ResponseReturnValue: &quot;&quot;&quot; Delete a specified incidence from the database. Args: id_ (int): Primary key of the incidence to delete. Returns: Response: Redirect to the homepage after deletion. Examples: # In browser: POST /incidence_delete/123/ # Redirects to: / Notes: - Future: consider adding authorization (e.g., CARB password) to restrict access. &quot;&quot;&quot; logger.info(f&quot;route called: incidence_delete with id= {id_}&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] table_name = &#39;incidences&#39; table = get_class_from_table_name(base, table_name) if table is None: abort(500, description=&quot;Could not get table class for incidences&quot;) # Type cast to help with SQLAlchemy typing table_class = table # type: Any model_row = db.session.query(table_class).get_or_404(id_) # todo - ensure portal changes are properly updated arb.utils.sql_alchemy.delete_commit_and_log_model(db, model_row, comment=f&#39;Deleting incidence row {id_}&#39;) return redirect(url_for(&#39;main.index&#39;)) incidence_update(id_) Display and edit a specific incidence record by ID. Parameters: id_ (int) \u2013 Primary key of the incidence to edit. Returns: Union[str, Response] \u2013 str|Response: Rendered HTML of the feedback form for the selected incidence, or a redirect to the upload page if the ID is missing. Raises: 500 Internal Server Error \u2013 If multiple records are found for the same ID. Examples: In browser: GET /incidence_update/123/ Returns: HTML form for editing incidence 123 Notes Redirects if the ID is not found in the database. Assumes each incidence ID is unique. Source code in arb\\portal\\routes.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141@main.route(&#39;/incidence_update/&lt;int:id_&gt;/&#39;, methods=(&#39;GET&#39;, &#39;POST&#39;)) def incidence_update(id_: int) -&gt; Union[str, Response]: &quot;&quot;&quot; Display and edit a specific incidence record by ID. Args: id_ (int): Primary key of the incidence to edit. Returns: str|Response: Rendered HTML of the feedback form for the selected incidence, or a redirect to the upload page if the ID is missing. Raises: 500 Internal Server Error: If multiple records are found for the same ID. Examples: # In browser: GET /incidence_update/123/ # Returns: HTML form for editing incidence 123 Notes: - Redirects if the ID is not found in the database. - Assumes each incidence ID is unique. &quot;&quot;&quot; logger.info(f&quot;route called: incidence_update with id= {id_}.&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] table_name = &#39;incidences&#39; table = get_class_from_table_name(base, table_name) # get_or_404 uses the tables primary key # model_row = db.session.query(table).get_or_404(id_) # todo turn this into a get and if it is null, then redirect? to the spreadsheet upload # todo consider turning into one_or_none and have error handling if table is None: abort(500, description=&quot;Could not get table class for incidences&quot;) # Type cast to help with SQLAlchemy typing table_class = table # type: Any rows = db.session.query(table_class).filter_by(id_incidence=id_).all() if not rows: message = f&quot;A request was made to edit a non-existent id_incidence ({id_}). Consider uploading the incidence by importing a spreadsheet.&quot; return redirect(url_for(&#39;main.upload_file&#39;, message=message)) if len(rows) &gt; 1: abort(500, description=f&quot;Multiple rows found for id={id_}&quot;) model_row = rows[0] sector, sector_type = get_sector_info(db, base, id_) logger.debug(f&quot;calling incidence_prep()&quot;) return incidence_prep(model_row, crud_type=&#39;update&#39;, sector_type=sector_type, default_dropdown=PLEASE_SELECT) index() Display the homepage with a list of all existing incidence records. Returns: str( str ) \u2013 Rendered HTML for the homepage with incidence records. Examples: In browser: GET / Returns: HTML page with table of incidences Source code in arb\\portal\\routes.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86@main.route(&#39;/&#39;) def index() -&gt; str: &quot;&quot;&quot; Display the homepage with a list of all existing incidence records. Returns: str: Rendered HTML for the homepage with incidence records. Examples: # In browser: GET / # Returns: HTML page with table of incidences &quot;&quot;&quot; logger.info(f&quot;route called: index.&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] table_name = &#39;incidences&#39; colum_name_pk = &#39;id_incidence&#39; rows = get_rows_by_table_name(db, base, table_name, colum_name_pk, ascending=False) return render_template(&#39;index.html&#39;, model_rows=rows) java_script_diagnostic_test() Render a simple page for testing JavaScript diagnostics logging (frontend and backend). NOTE: This is a developer-only route, not covered by E2E tests by design. Source code in arb\\portal\\routes.py 1281 1282 1283 1284 1285 1286 1287 1288 1289@main.route(&#39;/java_script_diagnostic_test&#39;) def java_script_diagnostic_test(): &quot;&quot;&quot; Render a simple page for testing JavaScript diagnostics logging (frontend and backend). NOTE: This is a developer-only route, not covered by E2E tests by design. &quot;&quot;&quot; logger.info(f&quot;route called: java_script_diagnostic_test&quot;) return render_template(&#39;java_script_diagnostic_test.html&#39;) js_diagnostic_log() JavaScript Diagnostics Logging Endpoint NOTE: This is a developer-only route, not covered by E2E tests by design. Purpose This route allows frontend JavaScript code to send diagnostic or debug messages directly to the backend server. These messages are then written to the backend log file, making it possible to correlate frontend/browser events with backend activity, errors, or user actions. This is especially useful for debugging issues that are hard to capture with browser console logs alone (e.g., page reloads, E2E tests, or user-reported problems). How to Use (Frontend JavaScript): Use the provided JS helper function (see below) to send a POST request to this endpoint: Example JS function: function sendJsDiagnostic(msg, extra) { fetch('/js_diagnostic_log', { method: 'POST', headers: {'Content-Type': 'application/json'}, body: JSON.stringify({msg, ts: new Date().toISOString(), ...extra}) }); } Example usage: sendJsDiagnostic('User clicked discard', {action: '/discard_staged_update/0/file.json', userId: 123}); Expected Payload (JSON): { \"msg\": \"A human-readable message (required)\", \"ts\": \"ISO timestamp (optional, will be included in log if present)\", ... any other key-value pairs (optional, will be logged as 'extra') } What Happens The backend receives the POST request and parses the JSON payload. It extracts the 'msg' (message), 'ts' (timestamp), and any other fields (as 'extra'). It writes a log entry to the backend log file in the format: JS_DIAG message | extra: {...} Returns a simple JSON response: {\"status\": \"ok\"} Why Use This Console logs in the browser are lost on navigation/reload and are not visible to backend developers. This route allows you to persistently log important frontend events, errors, or user actions for later analysis. Especially useful for E2E testing, debugging user issues, or tracking hard-to-reproduce bugs. Security Note This endpoint is for diagnostics only. Do not send sensitive user data. Rate limiting or authentication can be added if needed for production. Source code in arb\\portal\\routes.py 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278@main.route(&#39;/js_diagnostic_log&#39;, methods=[&#39;POST&#39;]) def js_diagnostic_log(): &quot;&quot;&quot; JavaScript Diagnostics Logging Endpoint ======================================= NOTE: This is a developer-only route, not covered by E2E tests by design. Purpose: This route allows frontend JavaScript code to send diagnostic or debug messages directly to the backend server. These messages are then written to the backend log file, making it possible to correlate frontend/browser events with backend activity, errors, or user actions. This is especially useful for debugging issues that are hard to capture with browser console logs alone (e.g., page reloads, E2E tests, or user-reported problems). How to Use (Frontend JavaScript): Use the provided JS helper function (see below) to send a POST request to this endpoint: Example JS function: function sendJsDiagnostic(msg, extra) { fetch(&#39;/js_diagnostic_log&#39;, { method: &#39;POST&#39;, headers: {&#39;Content-Type&#39;: &#39;application/json&#39;}, body: JSON.stringify({msg, ts: new Date().toISOString(), ...extra}) }); } Example usage: sendJsDiagnostic(&#39;User clicked discard&#39;, {action: &#39;/discard_staged_update/0/file.json&#39;, userId: 123}); Expected Payload (JSON): { &quot;msg&quot;: &quot;A human-readable message (required)&quot;, &quot;ts&quot;: &quot;ISO timestamp (optional, will be included in log if present)&quot;, ... any other key-value pairs (optional, will be logged as &#39;extra&#39;) } What Happens: - The backend receives the POST request and parses the JSON payload. - It extracts the &#39;msg&#39; (message), &#39;ts&#39; (timestamp), and any other fields (as &#39;extra&#39;). - It writes a log entry to the backend log file in the format: [JS_DIAG][timestamp] message | extra: {...} - Returns a simple JSON response: {&quot;status&quot;: &quot;ok&quot;} Why Use This: - Console logs in the browser are lost on navigation/reload and are not visible to backend developers. - This route allows you to persistently log important frontend events, errors, or user actions for later analysis. - Especially useful for E2E testing, debugging user issues, or tracking hard-to-reproduce bugs. Security Note: - This endpoint is for diagnostics only. Do not send sensitive user data. - Rate limiting or authentication can be added if needed for production. &quot;&quot;&quot; logger.info(f&quot;route called: js_diagnostic_log&quot;) data = request.get_json(force=True, silent=True) or {} msg = data.get(&#39;msg&#39;, &#39;[NO MSG]&#39;) ts = data.get(&#39;ts&#39;) extra = {k: v for k, v in data.items() if k not in (&#39;msg&#39;, &#39;ts&#39;)} log_line = f&quot;[JS_DIAG]{&#39;[&#39; + ts + &#39;]&#39; if ts else &#39;&#39;} {msg}&quot; if extra: log_line += f&quot; | extra: {extra}&quot; logger.info(log_line) return jsonify({&#39;status&#39;: &#39;ok&#39;}) landfill_incidence_create() Create a new dummy Landfill incidence and redirect to its edit form. Returns: Response( Response ) \u2013 Redirect to the incidence_update page for the newly created ID. Examples: In browser: POST /landfill_incidence_create/ Redirects to: /incidence_update// Notes Dummy data is loaded from db_hardcoded.get_landfill_dummy_form_data(). Source code in arb\\portal\\routes.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209@main.route(&#39;/landfill_incidence_create/&#39;, methods=(&#39;GET&#39;, &#39;POST&#39;)) def landfill_incidence_create() -&gt; Response: &quot;&quot;&quot; Create a new dummy Landfill incidence and redirect to its edit form. Returns: Response: Redirect to the `incidence_update` page for the newly created ID. Examples: # In browser: POST /landfill_incidence_create/ # Redirects to: /incidence_update/&lt;new_id&gt;/ Notes: - Dummy data is loaded from `db_hardcoded.get_landfill_dummy_form_data()`. &quot;&quot;&quot; logger.info(f&quot;route called: landfill_incidence_create.&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] table_name = &#39;incidences&#39; col_name = &#39;misc_json&#39; data_dict = arb.portal.db_hardcoded.get_landfill_dummy_form_data() id_ = dict_to_database(db, base, data_dict, table_name=table_name, json_field=col_name, ) logger.debug(f&quot;landfill_incidence_create() - leaving.&quot;) return redirect(url_for(&#39;main.incidence_update&#39;, id_=id_)) list_staged() List all staged files available for review or processing. Returns: str( ResponseReturnValue ) \u2013 Rendered HTML showing all staged files. Source code in arb\\portal\\routes.py 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347@main.route(&#39;/list_staged&#39;) def list_staged() -&gt; ResponseReturnValue: &quot;&quot;&quot; List all staged files available for review or processing. Returns: str: Rendered HTML showing all staged files. &quot;&quot;&quot; logger.info(f&quot;route called: list_staged&quot;) logger.info(&quot;[LIST_STAGED] Route called&quot;) logger.debug(&quot;list_staged route called&quot;) logger.warning(&#39;[DEBUG] /list_staged route called&#39;) staging_dir = Path(get_upload_folder()) / &quot;staging&quot; staged_files = [] malformed_files = [] if staging_dir.exists(): for file_path in staging_dir.glob(&quot;*.json&quot;): filename = file_path.name id_incidence = None sector = &quot;Unknown&quot; try: # Try to extract ID from filename (format: id_XXXX_ts_YYYYMMDD_HHMMSS.json) if filename.startswith(&quot;id_&quot;) and &quot;_ts_&quot; in filename: id_part = filename.split(&quot;_ts_&quot;)[0] id_incidence = int(id_part.replace(&quot;id_&quot;, &quot;&quot;)) # Try to load metadata to get sector and check for required fields try: json_data, metadata = json_load_with_meta(file_path) base_misc_json = metadata.get(&quot;base_misc_json&quot;, {}) sector = base_misc_json.get(&quot;sector&quot;, &quot;Unknown&quot;) # Check for required field: id_incidence must be a positive integer id_candidate = None # Try to get id_incidence from JSON if not from filename if id_incidence is None: id_candidate = json_data.get(&quot;id_incidence&quot;) if isinstance(id_candidate, int) and id_candidate &gt; 0: id_incidence = id_candidate # If still not valid, treat as malformed if not (isinstance(id_incidence, int) and id_incidence &gt; 0): raise ValueError(&quot;Missing or invalid id_incidence&quot;) staged_files.append({ &#39;filename&#39;: filename, &#39;id_incidence&#39;: id_incidence, &#39;sector&#39;: sector, &#39;file_size&#39;: file_path.stat().st_size, &#39;modified_time&#39;: datetime.datetime.fromtimestamp(file_path.stat().st_mtime), &#39;malformed&#39;: False }) except Exception as meta_exc: # If JSON loads but required fields are missing, treat as malformed logger.warning(f&quot;Malformed staged file (missing fields) {file_path}: {meta_exc}&quot;) malformed_files.append({ &#39;filename&#39;: filename, &#39;file_size&#39;: file_path.stat().st_size, &#39;modified_time&#39;: datetime.datetime.fromtimestamp(file_path.stat().st_mtime), &#39;error&#39;: f&quot;Missing required fields: {meta_exc}&quot; }) except Exception as e: logger.warning(f&quot;Could not process staged file {file_path}: {e}&quot;) malformed_files.append({ &#39;filename&#39;: filename, &#39;file_size&#39;: file_path.stat().st_size, &#39;modified_time&#39;: datetime.datetime.fromtimestamp(file_path.stat().st_mtime), &#39;error&#39;: str(e) }) # Sort by modification time (newest first) staged_files.sort(key=lambda x: x[&#39;modified_time&#39;], reverse=True) malformed_files.sort(key=lambda x: x[&#39;modified_time&#39;], reverse=True) logger.warning(f&#39;[DEBUG] /list_staged rendering {len(staged_files)} staged, {len(malformed_files)} malformed files&#39;) # Always pass both variables, even if empty return render_template(&#39;staged_list.html&#39;, staged_files=staged_files, malformed_files=malformed_files) list_uploads() List all files in the upload directory. Returns: str( str ) \u2013 Rendered HTML showing all uploaded Excel files available on disk. Examples: In browser: GET /list_uploads Returns: HTML page listing uploaded files Source code in arb\\portal\\routes.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270@main.route(&#39;/list_uploads&#39;) def list_uploads() -&gt; str: &quot;&quot;&quot; List all files in the upload directory. Returns: str: Rendered HTML showing all uploaded Excel files available on disk. Examples: # In browser: GET /list_uploads # Returns: HTML page listing uploaded files &quot;&quot;&quot; logger.info(f&quot;route called: list_uploads&quot;) upload_folder = get_upload_folder() # up_dir = Path(&quot;portal/static/uploads&quot;) # print(f&quot;{type(up_dir)=}: {up_dir=}&quot;) files = [x.name for x in upload_folder.iterdir() if x.is_file()] logger.debug(f&quot;{files=}&quot;) return render_template(&#39;uploads_list.html&#39;, files=files) og_incidence_create() Create a new dummy Oil &amp; Gas incidence and redirect to its edit form. Returns: Response( Response ) \u2013 Redirect to the incidence_update page for the newly created ID. Examples: In browser: POST /og_incidence_create/ Redirects to: /incidence_update// Notes Dummy data is loaded from db_hardcoded.get_og_dummy_form_data(). Source code in arb\\portal\\routes.py 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175@main.route(&#39;/og_incidence_create/&#39;, methods=(&#39;GET&#39;, &#39;POST&#39;)) def og_incidence_create() -&gt; Response: &quot;&quot;&quot; Create a new dummy Oil &amp; Gas incidence and redirect to its edit form. Returns: Response: Redirect to the `incidence_update` page for the newly created ID. Examples: # In browser: POST /og_incidence_create/ # Redirects to: /incidence_update/&lt;new_id&gt;/ Notes: - Dummy data is loaded from `db_hardcoded.get_og_dummy_form_data()`. &quot;&quot;&quot; logger.info(f&quot;route called: og_incidence_create.&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] table_name = &#39;incidences&#39; col_name = &#39;misc_json&#39; data_dict = arb.portal.db_hardcoded.get_og_dummy_form_data() id_ = dict_to_database(db, base, data_dict, table_name=table_name, json_field=col_name, ) logger.debug(f&quot;og_incidence_create() - leaving.&quot;) return redirect(url_for(&#39;main.incidence_update&#39;, id_=id_)) review_staged(id_, filename) Review the contents of a staged file for a specific incidence ID. Parameters: id_ (int) \u2013 Incidence ID associated with the staged file. filename (str) \u2013 Name of the staged file to review. Returns: str | Response \u2013 str|Response: Rendered HTML for file review, or redirect if not found. Examples: In browser: GET /review_staged/123/myfile.xlsx Returns: HTML review page for the file Source code in arb\\portal\\routes.py 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615@main.route(&quot;/review_staged/&lt;int:id_&gt;/&lt;filename&gt;&quot;, methods=[&quot;GET&quot;]) def review_staged(id_: int, filename: str) -&gt; str | Response: &quot;&quot;&quot; Review the contents of a staged file for a specific incidence ID. Args: id_ (int): Incidence ID associated with the staged file. filename (str): Name of the staged file to review. Returns: str|Response: Rendered HTML for file review, or redirect if not found. Examples: # In browser: GET /review_staged/123/myfile.xlsx # Returns: HTML review page for the file &quot;&quot;&quot; logger.info(f&quot;route called: review_staged with id_: {id_} and filename: {filename}&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] staging_dir = Path(get_upload_folder()) / &quot;staging&quot; staged_json_path = staging_dir / filename if not staged_json_path.exists(): logger.warning(f&quot;Staged JSON file not found: {staged_json_path}&quot;) return render_template(&quot;review_staged.html&quot;, error=f&quot;No staged data found for ID {id_}.&quot;, is_new_row=False, id_incidence=id_, staged_fields=[], metadata={}, filename=filename) try: staged_data, metadata = json_load_with_meta(staged_json_path) staged_payload = extract_tab_and_sector(staged_data, tab_name=&quot;Feedback Form&quot;) except Exception: logger.exception(&quot;Error loading staged JSON&quot;) return render_template(&quot;review_staged.html&quot;, error=&quot;Could not load staged data.&quot;, is_new_row=False, id_incidence=id_, staged_fields=[], metadata={}, filename=filename) model, _, is_new_row = get_ensured_row( db=db, base=base, table_name=&quot;incidences&quot;, primary_key_name=&quot;id_incidence&quot;, id_=id_ ) db_json = getattr(model, &quot;misc_json&quot;, {}) or {} staged_fields = compute_field_differences(new_data=staged_payload, existing_data=db_json) if is_new_row: logger.info(f&quot;\u26a0\ufe0f Staged ID {id_} did not exist in DB. A blank row was created for review.&quot;) logger.debug(f&quot;Computed {sum(f[&#39;changed&#39;] for f in staged_fields)} changes across {len(staged_fields)} fields&quot;) return render_template( &quot;review_staged.html&quot;, id_incidence=id_, staged_fields=staged_fields, is_new_row=is_new_row, metadata=metadata, error=None, filename=filename, ) search() Search for incidences or updates in the portal database. Returns: str( str ) \u2013 Rendered HTML search results page. Notes Currently echoes the user-submitted query string. Source code in arb\\portal\\routes.py 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960@main.route(&#39;/search/&#39;, methods=(&#39;GET&#39;, &#39;POST&#39;)) def search() -&gt; str: &quot;&quot;&quot; Search for incidences or updates in the portal database. Returns: str: Rendered HTML search results page. Notes: - Currently echoes the user-submitted query string. &quot;&quot;&quot; logger.info(f&quot;route called: search&quot;) logger.debug(f&quot;{request.form=}&quot;) search_string = request.form.get(&#39;navbar_search&#39;) logger.debug(f&quot;{search_string=}&quot;) return render_template(&#39;search.html&#39;, search_string=search_string, ) serve_file(filename) Serve a file from the uploads directory. Parameters: filename (str) \u2013 Name of the file to serve. Returns: Response( Response ) \u2013 File response for download or viewing in browser. Examples: In browser: GET /serve_file/myfile.xlsx Returns: File download or inline view Source code in arb\\portal\\routes.py 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859@main.route(&quot;/serve_file/&lt;path:filename&gt;&quot;) def serve_file(filename) -&gt; Response: &quot;&quot;&quot; Serve a file from the uploads directory. Args: filename (str): Name of the file to serve. Returns: Response: File response for download or viewing in browser. Examples: # In browser: GET /serve_file/myfile.xlsx # Returns: File download or inline view &quot;&quot;&quot; logger.info(f&quot;route called: serve_file with filename: {filename}&quot;) upload_folder = get_upload_folder() file_path = os.path.join(upload_folder, filename) if not os.path.exists(file_path): abort(404) return send_from_directory(upload_folder, filename) show_database_structure() Show the structure of the portal database (tables, columns, types). Returns: str( str ) \u2013 Rendered HTML of database structure. Examples: In browser: GET /show_database_structure Returns: HTML with database structure Source code in arb\\portal\\routes.py 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041@main.route(&#39;/show_database_structure&#39;) def show_database_structure() -&gt; str: &quot;&quot;&quot; Show the structure of the portal database (tables, columns, types). Returns: str: Rendered HTML of database structure. Examples: # In browser: GET /show_database_structure # Returns: HTML with database structure &quot;&quot;&quot; logger.info(f&quot;route called: show_database_structure&quot;) result = obj_to_html(Globals.db_column_types) result = f&quot;&lt;p&gt;&lt;strong&gt;Postgres Database Structure=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result}&lt;/p&gt;&quot; return render_template(&#39;diagnostics.html&#39;, header=&quot;Database Structure Overview&quot;, subheader=&quot;Reflecting SQLAlchemy model metadata.&quot;, html_content=result, ) show_dropdown_dict() Show the current dropdown dictionary used in forms. Returns: str( str ) \u2013 Rendered HTML of dropdown dictionary. Notes Useful for verifying dropdown contents used in WTForms. Source code in arb\\portal\\routes.py 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018@main.route(&#39;/show_dropdown_dict&#39;) def show_dropdown_dict() -&gt; str: &quot;&quot;&quot; Show the current dropdown dictionary used in forms. Returns: str: Rendered HTML of dropdown dictionary. Notes: - Useful for verifying dropdown contents used in WTForms. &quot;&quot;&quot; logger.info(f&quot;route called: show_dropdown_dict&quot;) # update drop-down tables Globals.load_drop_downs(current_app, db) result1 = obj_to_html(Globals.drop_downs) result2 = obj_to_html(Globals.drop_downs_contingent) result = (f&quot;&lt;p&gt;&lt;strong&gt;Globals.drop_downs=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result1}&lt;/p&gt;&quot; f&quot;&lt;p&gt;&lt;strong&gt;Globals.drop_downs_contingent=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result2}&lt;/p&gt;&quot;) return render_template(&#39;diagnostics.html&#39;, header=&quot;Dropdown Dictionaries&quot;, subheader=&quot;Loaded dropdown values and contingent mappings.&quot;, html_content=result, ) show_feedback_form_structure() Show the structure of the feedback form (fields, types, validators). Returns: str( str ) \u2013 Rendered HTML of feedback form structure. Examples: In browser: GET /show_feedback_form_structure Returns: HTML with feedback form structure Source code in arb\\portal\\routes.py 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073@main.route(&#39;/show_feedback_form_structure&#39;) def show_feedback_form_structure() -&gt; str: &quot;&quot;&quot; Show the structure of the feedback form (fields, types, validators). Returns: str: Rendered HTML of feedback form structure. Examples: # In browser: GET /show_feedback_form_structure # Returns: HTML with feedback form structure &quot;&quot;&quot; logger.info(f&quot;route called: show_feedback_form_structure&quot;) form1 = OGFeedback() fields1 = get_wtforms_fields(form1) result1 = obj_to_html(fields1) form2 = LandfillFeedback() fields2 = get_wtforms_fields(form2) result2 = obj_to_html(fields2) result = (f&quot;&lt;p&gt;&lt;strong&gt;WTF OGFeedback Form Structure=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result1}&lt;/p&gt;&quot; f&quot;&lt;p&gt;&lt;strong&gt;WTF LandfillFeedback Form Structure=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result2}&lt;/p&gt;&quot;) return render_template(&#39;diagnostics.html&#39;, header=&quot;WTForms Feedback Form Structure&quot;, subheader=&quot;Inspecting field mappings in Oil &amp; Gas and Landfill feedback forms.&quot;, html_content=result, ) show_log_file() Display the last N lines of the portal log file. NOTE: This is a developer-only route, not covered by E2E tests by design. Query Parameters lines (int, optional): Number of lines to show from the end of the log file. Defaults to 1000 if not provided or invalid. Args: None Returns: str( str ) \u2013 Rendered HTML with the log file content shown inside a block. Example Usage /show_log_file?lines=500 Notes Useful for debugging in development or staging. Efficient for large files using read_file_reverse(). Source code in arb\\portal\\routes.py 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144@main.route(&#39;/show_log_file&#39;) def show_log_file() -&gt; str: &quot;&quot;&quot; Display the last N lines of the portal log file. NOTE: This is a developer-only route, not covered by E2E tests by design. Query Parameters: lines (int, optional): Number of lines to show from the end of the log file. Defaults to 1000 if not provided or invalid. Args: None Returns: str: Rendered HTML with the log file content shown inside a &lt;pre&gt; block. Example Usage: /show_log_file?lines=500 Notes: - Useful for debugging in development or staging. - Efficient for large files using read_file_reverse(). &quot;&quot;&quot; logger.info(f&quot;route called: show_log_file&quot;) default_lines = 1000 try: num_lines = int(request.args.get(&#39;lines&#39;, default_lines)) if num_lines &lt; 1: raise ValueError except ValueError: num_lines = default_lines logger.info(f&quot;Displaying the last {num_lines} lines of the log file as a diagnostic&quot;) lines = read_file_reverse(LOG_FILE, n=num_lines) file_content = &#39;\\n&#39;.join(lines) result = f&quot;&lt;p&gt;&lt;strong&gt;Last {num_lines} lines of logger file:&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;pre&gt;{file_content}&lt;/pre&gt;&lt;/p&gt;&quot; return render_template( &#39;diagnostics.html&#39;, header=&quot;Log File Contents&quot;, html_content=result, ) upload_file(message=None) Handle file upload form and process uploaded Excel files. Parameters: message (str | None, default: None ) \u2013 Optional message to display on the upload page. Returns: Union[str, Response] \u2013 str|Response: Rendered HTML for the upload form, or redirect after upload. Examples: In browser: GET /upload Returns: HTML upload form In browser: POST /upload Redirects to: /list_uploads or error page Source code in arb\\portal\\routes.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441@main.route(&#39;/upload&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) @main.route(&#39;/upload/&lt;message&gt;&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) def upload_file(message: str | None = None) -&gt; Union[str, Response]: &quot;&quot;&quot; Handle file upload form and process uploaded Excel files. Args: message (str | None): Optional message to display on the upload page. Returns: str|Response: Rendered HTML for the upload form, or redirect after upload. Examples: # In browser: GET /upload # Returns: HTML upload form # In browser: POST /upload # Redirects to: /list_uploads or error page &quot;&quot;&quot; logger.info(f&quot;route called: upload_file with message: {message}&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] form = UploadForm() # Decode redirect message, if present if message: message = unquote(message) logger.debug(f&quot;Received redirect message: {message}&quot;) upload_folder = get_upload_folder() logger.debug(f&quot;Files received: {list(request.files.keys())}, upload_folder={upload_folder}&quot;) if request.method == &#39;POST&#39;: try: request_file = request.files.get(&#39;file&#39;) if not request_file or not request_file.filename: logger.warning(&quot;POST received with no file selected.&quot;) return render_template( &#39;upload.html&#39;, form=form, upload_message=&quot;No file selected. Please choose a file.&quot; ) logger.debug(f&quot;Received uploaded file: {request_file.filename}&quot;) # Step 1: Save file and attempt DB ingest file_path, id_, sector = upload_and_update_db(db, upload_folder, request_file, base) if id_: logger.debug(f&quot;Upload successful: id={id_}, sector={sector}. Redirecting to update page.&quot;) return redirect(url_for(&#39;main.incidence_update&#39;, id_=id_)) # If id_ is None, check if likely blocked due to missing/invalid id_incidence if file_path and (file_path.exists() if hasattr(file_path, &#39;exists&#39;) else True): # Check log message or just show the message if id_ is None logger.warning(f&quot;Upload blocked: missing or invalid id_incidence in {file_path.name}&quot;) return render_template( &#39;upload.html&#39;, form=form, upload_message=( &quot;This file is missing a valid &#39;Incidence/Emission ID&#39; (id_incidence). &quot; &quot;Please add a positive integer id_incidence to your spreadsheet before uploading.&quot; ) ) # Step 2: Handle schema recognition failure with enhanced diagnostics logger.warning(f&quot;Upload failed schema recognition: {file_path=}&quot;) error_details = generate_upload_diagnostics(request_file, file_path) detailed_message = format_diagnostic_message(error_details, &quot;Uploaded file format not recognized.&quot;) return render_template( &#39;upload.html&#39;, form=form, upload_message=detailed_message ) except Exception as e: logger.exception(&quot;Exception occurred during upload or parsing.&quot;) # Enhanced error handling with diagnostic information error_details = generate_upload_diagnostics(request_file, file_path if &#39;file_path&#39; in locals() else None) detailed_message = format_diagnostic_message(error_details) return render_template( &#39;upload.html&#39;, form=form, upload_message=detailed_message ) # GET request: display form return render_template(&#39;upload.html&#39;, form=form, upload_message=message) upload_file_staged(message=None) Handle staged file upload form and process staged Excel files. Parameters: message (str | None, default: None ) \u2013 Optional message to display on the staged upload page. Returns: Union[str, Response] \u2013 str|Response: Rendered HTML for the staged upload form, or redirect after upload. Examples: In browser: GET /upload_staged Returns: HTML staged upload form In browser: POST /upload_staged Redirects to: /list_staged or error page Source code in arb\\portal\\routes.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543@main.route(&#39;/upload_staged&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) @main.route(&#39;/upload_staged/&lt;message&gt;&#39;, methods=[&#39;GET&#39;, &#39;POST&#39;]) def upload_file_staged(message: str | None = None) -&gt; Union[str, Response]: &quot;&quot;&quot; Handle staged file upload form and process staged Excel files. Args: message (str | None): Optional message to display on the staged upload page. Returns: str|Response: Rendered HTML for the staged upload form, or redirect after upload. Examples: # In browser: GET /upload_staged # Returns: HTML staged upload form # In browser: POST /upload_staged # Redirects to: /list_staged or error page &quot;&quot;&quot; logger.info(f&quot;route called: upload_file_staged with message: {message}&quot;) base: AutomapBase = current_app.base # type: ignore[attr-defined] form = UploadForm() # Decode optional redirect message if message: message = unquote(message) logger.debug(f&quot;Received redirect message: {message}&quot;) upload_folder = get_upload_folder() logger.debug(f&quot;Request received with files: {list(request.files.keys())}, upload_folder={upload_folder}&quot;) if request.method == &#39;POST&#39;: try: request_file = request.files.get(&#39;file&#39;) if not request_file or not request_file.filename: logger.warning(&quot;POST received with no file selected.&quot;) return render_template(&#39;upload_staged.html&#39;, form=form, upload_message=&quot;No file selected. Please choose a file.&quot;) logger.debug(f&quot;Received uploaded file: {request_file.filename}&quot;) # Save and stage (no DB commit) file_path, id_, sector, json_data, staged_filename = upload_and_stage_only(db, upload_folder, request_file, base) if id_ and staged_filename: logger.debug(f&quot;Staged upload successful: id={id_}, sector={sector}, filename={staged_filename}. Redirecting to review page.&quot;) # Enhanced success feedback with staging details success_message = ( f&quot;\u2705 File &#39;{request_file.filename}&#39; staged successfully!\\n&quot; f&quot;\ud83d\udccb ID: {id_}\\n&quot; f&quot;\ud83c\udfed Sector: {sector}\\n&quot; f&quot;\ud83d\udcc1 Staged as: {staged_filename}\\n&quot; f&quot;\ud83d\udd0d Ready for review and confirmation.&quot; ) flash(success_message, &quot;success&quot;) return redirect(url_for(&#39;main.review_staged&#39;, id_=id_, filename=staged_filename)) # If id_ is None or not staged, check if likely blocked due to missing/invalid id_incidence if file_path and (file_path.exists() if hasattr(file_path, &#39;exists&#39;) else True): logger.warning(f&quot;Staging blocked: missing or invalid id_incidence in {file_path.name}&quot;) return render_template( &#39;upload_staged.html&#39;, form=form, upload_message=( &quot;This file is missing a valid &#39;Incidence/Emission ID&#39; (id_incidence). &quot; &quot;Please add a positive integer id_incidence to your spreadsheet before uploading.&quot; ) ) # Fallback: schema recognition failure or other error logger.warning(f&quot;Staging failed: missing or invalid id_incidence in {file_path.name}&quot;) return render_template( &#39;upload_staged.html&#39;, form=form, upload_message=&quot;This file is missing a valid &#39;Incidence/Emission ID&#39; (id_incidence). &quot; &quot;Please verify the spreadsheet includes that field and try again.&quot; ) except Exception as e: logger.exception(&quot;Exception occurred during staged upload.&quot;) # Enhanced error handling with staging-specific diagnostic information error_details = generate_staging_diagnostics( request_file, file_path if &#39;file_path&#39; in locals() else None, staged_filename if &#39;staged_filename&#39; in locals() else None, id_ if &#39;id_&#39; in locals() else None, sector if &#39;sector&#39; in locals() else None ) detailed_message = format_diagnostic_message(error_details, &quot;Staged upload processing failed.&quot;) return render_template( &#39;upload_staged.html&#39;, form=form, upload_message=detailed_message ) # GET request: display form return render_template(&#39;upload_staged.html&#39;, form=form, upload_message=message) view_portal_updates() Display a table of all portal update log entries. Returns: str( str ) \u2013 Rendered HTML table of portal update logs. Notes Supports pagination, filtering, and sorting via query parameters. Default sort is descending by timestamp. Source code in arb\\portal\\routes.py 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898@main.route(&quot;/portal_updates&quot;) def view_portal_updates() -&gt; str: &quot;&quot;&quot; Display a table of all portal update log entries. Returns: str: Rendered HTML table of portal update logs. Notes: - Supports pagination, filtering, and sorting via query parameters. - Default sort is descending by timestamp. &quot;&quot;&quot; sort_by = request.args.get(&quot;sort_by&quot;, &quot;timestamp&quot;) direction = request.args.get(&quot;direction&quot;, &quot;desc&quot;) page = int(request.args.get(&quot;page&quot;, 1)) per_page = int(request.args.get(&quot;per_page&quot;, 100)) query = db.session.query(PortalUpdate) query = apply_portal_update_filters(query, PortalUpdate, request.args) updates = query.order_by(PortalUpdate.timestamp.desc()).all() return render_template( &quot;portal_updates.html&quot;, updates=updates, sort_by=sort_by, direction=direction, page=page, per_page=per_page, total_pages=1, filter_key=request.args.get(&quot;filter_key&quot;, &quot;&quot;).strip(), filter_user=request.args.get(&quot;filter_user&quot;, &quot;&quot;).strip(), filter_comments=request.args.get(&quot;filter_comments&quot;, &quot;&quot;).strip(), filter_id_incidence=request.args.get(&quot;filter_id_incidence&quot;, &quot;&quot;).strip(), start_date=request.args.get(&quot;start_date&quot;, &quot;&quot;).strip(), end_date=request.args.get(&quot;end_date&quot;, &quot;&quot;).strip(), ) /"},{"location":"reference/arb/portal/routes/#arb.portal.routes.review_staged","text":"Review the contents of a staged file for a specific incidence ID. Parameters: id_ ( int ) \u2013 Incidence ID associated with the staged file. filename ( str ) \u2013 Name of the staged file to review. Returns: str | Response \u2013 str|Response: Rendered HTML for file review, or redirect if not found. Examples:","title":"review_staged"},{"location":"reference/arb/portal/routes/#arb.portal.routes.review_staged--in-browser-get-review_staged123myfilexlsx","text":"","title":"In browser: GET /review_staged/123/myfile.xlsx"},{"location":"reference/arb/portal/routes/#arb.portal.routes.review_staged--returns-html-review-page-for-the-file","text":"Source code in arb\\portal\\routes.py 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 @main . route ( \"/review_staged/<int:id_>/<filename>\" , methods = [ \"GET\" ]) def review_staged ( id_ : int , filename : str ) -> str | Response : \"\"\" Review the contents of a staged file for a specific incidence ID. Args: id_ (int): Incidence ID associated with the staged file. filename (str): Name of the staged file to review. Returns: str|Response: Rendered HTML for file review, or redirect if not found. Examples: # In browser: GET /review_staged/123/myfile.xlsx # Returns: HTML review page for the file \"\"\" logger . info ( f \"route called: review_staged with id_: { id_ } and filename: { filename } \" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] staging_dir = Path ( get_upload_folder ()) / \"staging\" staged_json_path = staging_dir / filename if not staged_json_path . exists (): logger . warning ( f \"Staged JSON file not found: { staged_json_path } \" ) return render_template ( \"review_staged.html\" , error = f \"No staged data found for ID { id_ } .\" , is_new_row = False , id_incidence = id_ , staged_fields = [], metadata = {}, filename = filename ) try : staged_data , metadata = json_load_with_meta ( staged_json_path ) staged_payload = extract_tab_and_sector ( staged_data , tab_name = \"Feedback Form\" ) except Exception : logger . exception ( \"Error loading staged JSON\" ) return render_template ( \"review_staged.html\" , error = \"Could not load staged data.\" , is_new_row = False , id_incidence = id_ , staged_fields = [], metadata = {}, filename = filename ) model , _ , is_new_row = get_ensured_row ( db = db , base = base , table_name = \"incidences\" , primary_key_name = \"id_incidence\" , id_ = id_ ) db_json = getattr ( model , \"misc_json\" , {}) or {} staged_fields = compute_field_differences ( new_data = staged_payload , existing_data = db_json ) if is_new_row : logger . info ( f \"\u26a0\ufe0f Staged ID { id_ } did not exist in DB. A blank row was created for review.\" ) logger . debug ( f \"Computed { sum ( f [ 'changed' ] for f in staged_fields ) } changes across { len ( staged_fields ) } fields\" ) return render_template ( \"review_staged.html\" , id_incidence = id_ , staged_fields = staged_fields , is_new_row = is_new_row , metadata = metadata , error = None , filename = filename , )","title":"Returns: HTML review page for the file"},{"location":"reference/arb/portal/routes/#arb.portal.routes.search","text":"Search for incidences or updates in the portal database. Returns: str ( str ) \u2013 Rendered HTML search results page. Notes Currently echoes the user-submitted query string. Source code in arb\\portal\\routes.py 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 @main . route ( '/search/' , methods = ( 'GET' , 'POST' )) def search () -> str : \"\"\" Search for incidences or updates in the portal database. Returns: str: Rendered HTML search results page. Notes: - Currently echoes the user-submitted query string. \"\"\" logger . info ( f \"route called: search\" ) logger . debug ( f \" { request . form =} \" ) search_string = request . form . get ( 'navbar_search' ) logger . debug ( f \" { search_string =} \" ) return render_template ( 'search.html' , search_string = search_string , )","title":"search"},{"location":"reference/arb/portal/routes/#arb.portal.routes.serve_file","text":"Serve a file from the uploads directory. Parameters: filename ( str ) \u2013 Name of the file to serve. Returns: Response ( Response ) \u2013 File response for download or viewing in browser. Examples:","title":"serve_file"},{"location":"reference/arb/portal/routes/#arb.portal.routes.serve_file--in-browser-get-serve_filemyfilexlsx","text":"","title":"In browser: GET /serve_file/myfile.xlsx"},{"location":"reference/arb/portal/routes/#arb.portal.routes.serve_file--returns-file-download-or-inline-view","text":"Source code in arb\\portal\\routes.py 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 @main . route ( \"/serve_file/<path:filename>\" ) def serve_file ( filename ) -> Response : \"\"\" Serve a file from the uploads directory. Args: filename (str): Name of the file to serve. Returns: Response: File response for download or viewing in browser. Examples: # In browser: GET /serve_file/myfile.xlsx # Returns: File download or inline view \"\"\" logger . info ( f \"route called: serve_file with filename: { filename } \" ) upload_folder = get_upload_folder () file_path = os . path . join ( upload_folder , filename ) if not os . path . exists ( file_path ): abort ( 404 ) return send_from_directory ( upload_folder , filename )","title":"Returns: File download or inline view"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_database_structure","text":"Show the structure of the portal database (tables, columns, types). Returns: str ( str ) \u2013 Rendered HTML of database structure. Examples:","title":"show_database_structure"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_database_structure--in-browser-get-show_database_structure","text":"","title":"In browser: GET /show_database_structure"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_database_structure--returns-html-with-database-structure","text":"Source code in arb\\portal\\routes.py 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 @main . route ( '/show_database_structure' ) def show_database_structure () -> str : \"\"\" Show the structure of the portal database (tables, columns, types). Returns: str: Rendered HTML of database structure. Examples: # In browser: GET /show_database_structure # Returns: HTML with database structure \"\"\" logger . info ( f \"route called: show_database_structure\" ) result = obj_to_html ( Globals . db_column_types ) result = f \"<p><strong>Postgres Database Structure=</strong></p> <p> { result } </p>\" return render_template ( 'diagnostics.html' , header = \"Database Structure Overview\" , subheader = \"Reflecting SQLAlchemy model metadata.\" , html_content = result , )","title":"Returns: HTML with database structure"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_dropdown_dict","text":"Show the current dropdown dictionary used in forms. Returns: str ( str ) \u2013 Rendered HTML of dropdown dictionary. Notes Useful for verifying dropdown contents used in WTForms. Source code in arb\\portal\\routes.py 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 @main . route ( '/show_dropdown_dict' ) def show_dropdown_dict () -> str : \"\"\" Show the current dropdown dictionary used in forms. Returns: str: Rendered HTML of dropdown dictionary. Notes: - Useful for verifying dropdown contents used in WTForms. \"\"\" logger . info ( f \"route called: show_dropdown_dict\" ) # update drop-down tables Globals . load_drop_downs ( current_app , db ) result1 = obj_to_html ( Globals . drop_downs ) result2 = obj_to_html ( Globals . drop_downs_contingent ) result = ( f \"<p><strong>Globals.drop_downs=</strong></p> <p> { result1 } </p>\" f \"<p><strong>Globals.drop_downs_contingent=</strong></p> <p> { result2 } </p>\" ) return render_template ( 'diagnostics.html' , header = \"Dropdown Dictionaries\" , subheader = \"Loaded dropdown values and contingent mappings.\" , html_content = result , )","title":"show_dropdown_dict"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_feedback_form_structure","text":"Show the structure of the feedback form (fields, types, validators). Returns: str ( str ) \u2013 Rendered HTML of feedback form structure. Examples:","title":"show_feedback_form_structure"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_feedback_form_structure--in-browser-get-show_feedback_form_structure","text":"","title":"In browser: GET /show_feedback_form_structure"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_feedback_form_structure--returns-html-with-feedback-form-structure","text":"Source code in arb\\portal\\routes.py 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 @main . route ( '/show_feedback_form_structure' ) def show_feedback_form_structure () -> str : \"\"\" Show the structure of the feedback form (fields, types, validators). Returns: str: Rendered HTML of feedback form structure. Examples: # In browser: GET /show_feedback_form_structure # Returns: HTML with feedback form structure \"\"\" logger . info ( f \"route called: show_feedback_form_structure\" ) form1 = OGFeedback () fields1 = get_wtforms_fields ( form1 ) result1 = obj_to_html ( fields1 ) form2 = LandfillFeedback () fields2 = get_wtforms_fields ( form2 ) result2 = obj_to_html ( fields2 ) result = ( f \"<p><strong>WTF OGFeedback Form Structure=</strong></p> <p> { result1 } </p>\" f \"<p><strong>WTF LandfillFeedback Form Structure=</strong></p> <p> { result2 } </p>\" ) return render_template ( 'diagnostics.html' , header = \"WTForms Feedback Form Structure\" , subheader = \"Inspecting field mappings in Oil & Gas and Landfill feedback forms.\" , html_content = result , )","title":"Returns: HTML with feedback form structure"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_log_file","text":"Display the last N lines of the portal log file. NOTE: This is a developer-only route, not covered by E2E tests by design. Query Parameters lines (int, optional): Number of lines to show from the end of the log file. Defaults to 1000 if not provided or invalid. Args: None Returns: str ( str ) \u2013 Rendered HTML with the log file content shown inside a block. Example Usage /show_log_file?lines=500 Notes Useful for debugging in development or staging. Efficient for large files using read_file_reverse(). Source code in arb\\portal\\routes.py 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 @main . route ( '/show_log_file' ) def show_log_file () -> str : \"\"\" Display the last N lines of the portal log file. NOTE: This is a developer-only route, not covered by E2E tests by design. Query Parameters: lines (int, optional): Number of lines to show from the end of the log file. Defaults to 1000 if not provided or invalid. Args: None Returns: str: Rendered HTML with the log file content shown inside a <pre> block. Example Usage: /show_log_file?lines=500 Notes: - Useful for debugging in development or staging. - Efficient for large files using read_file_reverse(). \"\"\" logger . info ( f \"route called: show_log_file\" ) default_lines = 1000 try : num_lines = int ( request . args . get ( 'lines' , default_lines )) if num_lines < 1 : raise ValueError except ValueError : num_lines = default_lines logger . info ( f \"Displaying the last { num_lines } lines of the log file as a diagnostic\" ) lines = read_file_reverse ( LOG_FILE , n = num_lines ) file_content = ' \\n ' . join ( lines ) result = f \"<p><strong>Last { num_lines } lines of logger file:</strong></p><p><pre> { file_content } </pre></p>\" return render_template ( 'diagnostics.html' , header = \"Log File Contents\" , html_content = result , )","title":"show_log_file"},{"location":"reference/arb/portal/routes/#arb.portal.routes.upload_file","text":"Handle file upload form and process uploaded Excel files. Parameters: message ( str | None , default: None ) \u2013 Optional message to display on the upload page. Returns: Union [ str , Response ] \u2013 str|Response: Rendered HTML for the upload form, or redirect after upload. Examples:","title":"upload_file"},{"location":"reference/arb/portal/routes/#arb.portal.routes.upload_file--in-browser-get-upload","text":"","title":"In browser: GET /upload"},{"location":"reference/arb/portal/routes/#arb.portal.routes.upload_file--returns-html-upload-form","text":"","title":"Returns: HTML upload form"},{"location":"reference/arb/portal/routes/#arb.portal.routes.upload_file--in-browser-post-upload","text":"","title":"In browser: POST /upload"},{"location":"reference/arb/portal/routes/#arb.portal.routes.upload_file--redirects-to-list_uploads-or-error-page","text":"Source code in arb\\portal\\routes.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 @main . route ( '/upload' , methods = [ 'GET' , 'POST' ]) @main . route ( '/upload/<message>' , methods = [ 'GET' , 'POST' ]) def upload_file ( message : str | None = None ) -> Union [ str , Response ]: \"\"\" Handle file upload form and process uploaded Excel files. Args: message (str | None): Optional message to display on the upload page. Returns: str|Response: Rendered HTML for the upload form, or redirect after upload. Examples: # In browser: GET /upload # Returns: HTML upload form # In browser: POST /upload # Redirects to: /list_uploads or error page \"\"\" logger . info ( f \"route called: upload_file with message: { message } \" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] form = UploadForm () # Decode redirect message, if present if message : message = unquote ( message ) logger . debug ( f \"Received redirect message: { message } \" ) upload_folder = get_upload_folder () logger . debug ( f \"Files received: { list ( request . files . keys ()) } , upload_folder= { upload_folder } \" ) if request . method == 'POST' : try : request_file = request . files . get ( 'file' ) if not request_file or not request_file . filename : logger . warning ( \"POST received with no file selected.\" ) return render_template ( 'upload.html' , form = form , upload_message = \"No file selected. Please choose a file.\" ) logger . debug ( f \"Received uploaded file: { request_file . filename } \" ) # Step 1: Save file and attempt DB ingest file_path , id_ , sector = upload_and_update_db ( db , upload_folder , request_file , base ) if id_ : logger . debug ( f \"Upload successful: id= { id_ } , sector= { sector } . Redirecting to update page.\" ) return redirect ( url_for ( 'main.incidence_update' , id_ = id_ )) # If id_ is None, check if likely blocked due to missing/invalid id_incidence if file_path and ( file_path . exists () if hasattr ( file_path , 'exists' ) else True ): # Check log message or just show the message if id_ is None logger . warning ( f \"Upload blocked: missing or invalid id_incidence in { file_path . name } \" ) return render_template ( 'upload.html' , form = form , upload_message = ( \"This file is missing a valid 'Incidence/Emission ID' (id_incidence). \" \"Please add a positive integer id_incidence to your spreadsheet before uploading.\" ) ) # Step 2: Handle schema recognition failure with enhanced diagnostics logger . warning ( f \"Upload failed schema recognition: { file_path =} \" ) error_details = generate_upload_diagnostics ( request_file , file_path ) detailed_message = format_diagnostic_message ( error_details , \"Uploaded file format not recognized.\" ) return render_template ( 'upload.html' , form = form , upload_message = detailed_message ) except Exception as e : logger . exception ( \"Exception occurred during upload or parsing.\" ) # Enhanced error handling with diagnostic information error_details = generate_upload_diagnostics ( request_file , file_path if 'file_path' in locals () else None ) detailed_message = format_diagnostic_message ( error_details ) return render_template ( 'upload.html' , form = form , upload_message = detailed_message ) # GET request: display form return render_template ( 'upload.html' , form = form , upload_message = message )","title":"Redirects to: /list_uploads or error page"},{"location":"reference/arb/portal/routes/#arb.portal.routes.upload_file_staged","text":"Handle staged file upload form and process staged Excel files. Parameters: message ( str | None , default: None ) \u2013 Optional message to display on the staged upload page. Returns: Union [ str , Response ] \u2013 str|Response: Rendered HTML for the staged upload form, or redirect after upload. Examples:","title":"upload_file_staged"},{"location":"reference/arb/portal/routes/#arb.portal.routes.upload_file_staged--in-browser-get-upload_staged","text":"","title":"In browser: GET /upload_staged"},{"location":"reference/arb/portal/routes/#arb.portal.routes.upload_file_staged--returns-html-staged-upload-form","text":"","title":"Returns: HTML staged upload form"},{"location":"reference/arb/portal/routes/#arb.portal.routes.upload_file_staged--in-browser-post-upload_staged","text":"","title":"In browser: POST /upload_staged"},{"location":"reference/arb/portal/routes/#arb.portal.routes.upload_file_staged--redirects-to-list_staged-or-error-page","text":"Source code in arb\\portal\\routes.py 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 @main . route ( '/upload_staged' , methods = [ 'GET' , 'POST' ]) @main . route ( '/upload_staged/<message>' , methods = [ 'GET' , 'POST' ]) def upload_file_staged ( message : str | None = None ) -> Union [ str , Response ]: \"\"\" Handle staged file upload form and process staged Excel files. Args: message (str | None): Optional message to display on the staged upload page. Returns: str|Response: Rendered HTML for the staged upload form, or redirect after upload. Examples: # In browser: GET /upload_staged # Returns: HTML staged upload form # In browser: POST /upload_staged # Redirects to: /list_staged or error page \"\"\" logger . info ( f \"route called: upload_file_staged with message: { message } \" ) base : AutomapBase = current_app . base # type: ignore[attr-defined] form = UploadForm () # Decode optional redirect message if message : message = unquote ( message ) logger . debug ( f \"Received redirect message: { message } \" ) upload_folder = get_upload_folder () logger . debug ( f \"Request received with files: { list ( request . files . keys ()) } , upload_folder= { upload_folder } \" ) if request . method == 'POST' : try : request_file = request . files . get ( 'file' ) if not request_file or not request_file . filename : logger . warning ( \"POST received with no file selected.\" ) return render_template ( 'upload_staged.html' , form = form , upload_message = \"No file selected. Please choose a file.\" ) logger . debug ( f \"Received uploaded file: { request_file . filename } \" ) # Save and stage (no DB commit) file_path , id_ , sector , json_data , staged_filename = upload_and_stage_only ( db , upload_folder , request_file , base ) if id_ and staged_filename : logger . debug ( f \"Staged upload successful: id= { id_ } , sector= { sector } , filename= { staged_filename } . Redirecting to review page.\" ) # Enhanced success feedback with staging details success_message = ( f \"\u2705 File ' { request_file . filename } ' staged successfully! \\n \" f \"\ud83d\udccb ID: { id_ } \\n \" f \"\ud83c\udfed Sector: { sector } \\n \" f \"\ud83d\udcc1 Staged as: { staged_filename } \\n \" f \"\ud83d\udd0d Ready for review and confirmation.\" ) flash ( success_message , \"success\" ) return redirect ( url_for ( 'main.review_staged' , id_ = id_ , filename = staged_filename )) # If id_ is None or not staged, check if likely blocked due to missing/invalid id_incidence if file_path and ( file_path . exists () if hasattr ( file_path , 'exists' ) else True ): logger . warning ( f \"Staging blocked: missing or invalid id_incidence in { file_path . name } \" ) return render_template ( 'upload_staged.html' , form = form , upload_message = ( \"This file is missing a valid 'Incidence/Emission ID' (id_incidence). \" \"Please add a positive integer id_incidence to your spreadsheet before uploading.\" ) ) # Fallback: schema recognition failure or other error logger . warning ( f \"Staging failed: missing or invalid id_incidence in { file_path . name } \" ) return render_template ( 'upload_staged.html' , form = form , upload_message = \"This file is missing a valid 'Incidence/Emission ID' (id_incidence). \" \"Please verify the spreadsheet includes that field and try again.\" ) except Exception as e : logger . exception ( \"Exception occurred during staged upload.\" ) # Enhanced error handling with staging-specific diagnostic information error_details = generate_staging_diagnostics ( request_file , file_path if 'file_path' in locals () else None , staged_filename if 'staged_filename' in locals () else None , id_ if 'id_' in locals () else None , sector if 'sector' in locals () else None ) detailed_message = format_diagnostic_message ( error_details , \"Staged upload processing failed.\" ) return render_template ( 'upload_staged.html' , form = form , upload_message = detailed_message ) # GET request: display form return render_template ( 'upload_staged.html' , form = form , upload_message = message )","title":"Redirects to: /list_staged or error page"},{"location":"reference/arb/portal/routes/#arb.portal.routes.view_portal_updates","text":"Display a table of all portal update log entries. Returns: str ( str ) \u2013 Rendered HTML table of portal update logs. Notes Supports pagination, filtering, and sorting via query parameters. Default sort is descending by timestamp. Source code in arb\\portal\\routes.py 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 @main . route ( \"/portal_updates\" ) def view_portal_updates () -> str : \"\"\" Display a table of all portal update log entries. Returns: str: Rendered HTML table of portal update logs. Notes: - Supports pagination, filtering, and sorting via query parameters. - Default sort is descending by timestamp. \"\"\" sort_by = request . args . get ( \"sort_by\" , \"timestamp\" ) direction = request . args . get ( \"direction\" , \"desc\" ) page = int ( request . args . get ( \"page\" , 1 )) per_page = int ( request . args . get ( \"per_page\" , 100 )) query = db . session . query ( PortalUpdate ) query = apply_portal_update_filters ( query , PortalUpdate , request . args ) updates = query . order_by ( PortalUpdate . timestamp . desc ()) . all () return render_template ( \"portal_updates.html\" , updates = updates , sort_by = sort_by , direction = direction , page = page , per_page = per_page , total_pages = 1 , filter_key = request . args . get ( \"filter_key\" , \"\" ) . strip (), filter_user = request . args . get ( \"filter_user\" , \"\" ) . strip (), filter_comments = request . args . get ( \"filter_comments\" , \"\" ) . strip (), filter_id_incidence = request . args . get ( \"filter_id_incidence\" , \"\" ) . strip (), start_date = request . args . get ( \"start_date\" , \"\" ) . strip (), end_date = request . args . get ( \"end_date\" , \"\" ) . strip (), )","title":"view_portal_updates"},{"location":"reference/arb/portal/sqla_models/","text":"arb.portal.sqla_models SQLAlchemy model definitions for the ARB Feedback Portal. This module defines ORM classes that map to key tables in the database, including uploaded file metadata and portal JSON update logs. Module_Attributes UploadedFile (type): SQLAlchemy model for uploaded file metadata. PortalUpdate (type): SQLAlchemy model for portal update logs. logger (logging.Logger): Logger instance for this module. Examples: file = UploadedFile(path=\"uploads/report.xlsx\", status=\"pending\") db.session.add(file) db.session.commit() file is inserted into the uploaded_files table with timestamps autopopulated Notes Only models explicitly defined here will be created by SQLAlchemy via db.create_all() . Most schema inspection and data access for incidences is handled dynamically via reflection. Timezone-aware UTC timestamps are used on all tracked models. All models inherit from db.Model , and can be directly queried with SQLAlchemy syntax. The logger emits a debug message when this file is loaded. PortalUpdate Bases: Model SQLAlchemy model tracking updates to the misc_json field on incidence records. Table Name portal_updates Attributes: id ( int ) \u2013 Primary key. timestamp ( datetime ) \u2013 UTC time when the change was logged. key ( str ) \u2013 JSON key that was modified. old_value ( str | None ) \u2013 Previous value (nullable). new_value ( str ) \u2013 New value. user ( str ) \u2013 Username or identifier of the user making the change. comments ( str ) \u2013 Optional explanatory comment. id_incidence ( int | None ) \u2013 Foreign key to the modified incidence (nullable). Examples: update = PortalUpdate(key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\", id_incidence=1) db.session.add(update) db.session.commit() update appears in the portal_updates table Notes Automatically populated by apply_json_patch_and_log() . Used for rendering the portal_updates.html table. Source code in arb\\portal\\sqla_models.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class PortalUpdate ( db . Model ): \"\"\" SQLAlchemy model tracking updates to the misc_json field on incidence records. Table Name: portal_updates Attributes: id (int): Primary key. timestamp (datetime): UTC time when the change was logged. key (str): JSON key that was modified. old_value (str | None): Previous value (nullable). new_value (str): New value. user (str): Username or identifier of the user making the change. comments (str): Optional explanatory comment. id_incidence (int | None): Foreign key to the modified incidence (nullable). Examples: update = PortalUpdate(key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\", id_incidence=1) db.session.add(update) db.session.commit() # update appears in the portal_updates table Notes: - Automatically populated by `apply_json_patch_and_log()`. - Used for rendering the `portal_updates.html` table. \"\"\" __tablename__ = \"portal_updates\" id = Column ( Integer , primary_key = True ) timestamp = Column ( DateTime ( timezone = True ), nullable = False , server_default = func . now ()) key = Column ( String ( 255 ), nullable = False ) old_value = Column ( Text , nullable = True ) new_value = Column ( Text , nullable = False ) user = Column ( String ( 255 ), nullable = False , default = \"anonymous\" ) comments = Column ( Text , nullable = False , default = \"\" ) id_incidence = Column ( Integer , nullable = True ) def __repr__ ( self ) -> str : \"\"\" Return a human-readable string representation of the portal update record. Returns: str: Summary string showing the update ID, key, old/new values, user, and timestamp. Examples: update = PortalUpdate(id=1, key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\") print(repr(update)) # Output: '<PortalUpdate id=1 key='field1' old='A' new='B' user='alice' at=...>' \"\"\" return ( f \"<PortalUpdate id= { self . id } key= { self . key !r} old= { self . old_value !r} \" f \"new= { self . new_value !r} user= { self . user !r} at= { self . timestamp } >\" ) __repr__ () Return a human-readable string representation of the portal update record. Returns: str ( str ) \u2013 Summary string showing the update ID, key, old/new values, user, and timestamp. Examples: update = PortalUpdate(id=1, key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\") print(repr(update)) Output: ' ' Source code in arb\\portal\\sqla_models.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def __repr__ ( self ) -> str : \"\"\" Return a human-readable string representation of the portal update record. Returns: str: Summary string showing the update ID, key, old/new values, user, and timestamp. Examples: update = PortalUpdate(id=1, key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\") print(repr(update)) # Output: '<PortalUpdate id=1 key='field1' old='A' new='B' user='alice' at=...>' \"\"\" return ( f \"<PortalUpdate id= { self . id } key= { self . key !r} old= { self . old_value !r} \" f \"new= { self . new_value !r} user= { self . user !r} at= { self . timestamp } >\" ) UploadedFile Bases: Model SQLAlchemy model representing a user-uploaded file. Table Name uploaded_files Attributes: id_ ( int ) \u2013 Primary key. path ( str ) \u2013 Filesystem path to the uploaded file. description ( str | None ) \u2013 Optional human-friendly explanation. status ( str | None ) \u2013 Upload status, e.g., 'pending', 'processed', or 'error'. created_timestamp ( datetime ) \u2013 UTC timestamp of initial creation. modified_timestamp ( datetime ) \u2013 UTC timestamp of last update. Examples: file = UploadedFile(path=\"uploads/test.xlsx\", status=\"pending\") db.session.add(file) db.session.commit() file appears in the uploaded_files table with 'pending' status Notes Timestamps use UTC and are timezone-aware. This table is managed by SQLAlchemy directly (not introspected). Source code in arb\\portal\\sqla_models.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class UploadedFile ( db . Model ): \"\"\" SQLAlchemy model representing a user-uploaded file. Table Name: uploaded_files Attributes: id_ (int): Primary key. path (str): Filesystem path to the uploaded file. description (str | None): Optional human-friendly explanation. status (str | None): Upload status, e.g., 'pending', 'processed', or 'error'. created_timestamp (datetime): UTC timestamp of initial creation. modified_timestamp (datetime): UTC timestamp of last update. Examples: file = UploadedFile(path=\"uploads/test.xlsx\", status=\"pending\") db.session.add(file) db.session.commit() # file appears in the uploaded_files table with 'pending' status Notes: - Timestamps use UTC and are timezone-aware. - This table is managed by SQLAlchemy directly (not introspected). \"\"\" __tablename__ = \"uploaded_files\" id_ = db . Column ( db . Integer , primary_key = True ) path = db . Column ( db . Text , nullable = False ) description = db . Column ( db . Text , nullable = True ) status = db . Column ( db . Text , nullable = True ) created_timestamp = db . Column ( db . DateTime ( timezone = True ), server_default = func . now () ) modified_timestamp = db . Column ( db . DateTime ( timezone = True ), server_default = func . now () ) def __repr__ ( self ) -> str : \"\"\" Return a human-readable string representation of the uploaded file record. Returns: str: Summary string showing the ID, path, description, and status. Examples: file = UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\") print(repr(file)) # Output: '<Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done>' \"\"\" return ( f '<Uploaded File: { self . id_ } , Path: { self . path } , ' f 'Description: { self . description } , Status: { self . status } >' ) __repr__ () Return a human-readable string representation of the uploaded file record. Returns: str ( str ) \u2013 Summary string showing the ID, path, description, and status. Examples: file = UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\") print(repr(file)) Output: ' ' Source code in arb\\portal\\sqla_models.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __repr__ ( self ) -> str : \"\"\" Return a human-readable string representation of the uploaded file record. Returns: str: Summary string showing the ID, path, description, and status. Examples: file = UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\") print(repr(file)) # Output: '<Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done>' \"\"\" return ( f '<Uploaded File: { self . id_ } , Path: { self . path } , ' f 'Description: { self . description } , Status: { self . status } >' ) run_diagnostics () Run a test transaction to validate UploadedFile model functionality. Raises: RuntimeError \u2013 If database access or fetch fails. Examples: run_diagnostics() Logs diagnostic info and rolls back the transaction Notes Meant for developer use in test environments only. This function leaves no data in the database due to rollback. Logs diagnostic information using the project logger. LIKELY OBSOLETE: This function is not covered by unit tests and is likely obsolete. It requires a real database connection and is not robustly testable with mocks or in CI environments. Change with caution. See documentation/docstring_update_for_testing.md for details. Source code in arb\\portal\\sqla_models.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def run_diagnostics () -> None : \"\"\" Run a test transaction to validate UploadedFile model functionality. Raises: RuntimeError: If database access or fetch fails. Examples: run_diagnostics() # Logs diagnostic info and rolls back the transaction Notes: - Meant for developer use in test environments only. - This function leaves no data in the database due to rollback. - Logs diagnostic information using the project logger. - LIKELY OBSOLETE: This function is not covered by unit tests and is likely obsolete. It requires a real database connection and is not robustly testable with mocks or in CI environments. Change with caution. See documentation/docstring_update_for_testing.md for details. \"\"\" logger . info ( f \"Running UploadedFile diagnostics...\" ) try : logger . debug ( f \"Beginning diagnostic transaction...\" ) test_file = UploadedFile ( path = \"uploads/test_file.xlsx\" , description = \"Diagnostic test file\" , status = \"testing\" ) db . session . add ( test_file ) db . session . flush () # Ensures test_file.id_ is populated logger . info ( f \"Inserted test file with ID: { test_file . id_ } \" ) fetched = UploadedFile . query . get ( test_file . id_ ) if fetched is None : raise RuntimeError ( \"Failed to retrieve inserted UploadedFile instance.\" ) logger . info ( f \"Fetched file: { fetched } \" ) logger . debug ( f \"repr: { repr ( fetched ) } \" ) logger . debug ( f \"created_timestamp: { fetched . created_timestamp } \" ) except SQLAlchemyError as e : logger . exception ( \"SQLAlchemy error during diagnostics.\" ) raise RuntimeError ( \"Database error during UploadedFile diagnostics.\" ) from e finally : logger . debug ( f \"Rolling back diagnostic transaction.\" ) db . session . rollback () logger . info ( f \"Diagnostics completed and transaction rolled back.\" )","title":"arb.portal.sqla_models"},{"location":"reference/arb/portal/sqla_models/#arbportalsqla_models","text":"SQLAlchemy model definitions for the ARB Feedback Portal. This module defines ORM classes that map to key tables in the database, including uploaded file metadata and portal JSON update logs. Module_Attributes UploadedFile (type): SQLAlchemy model for uploaded file metadata. PortalUpdate (type): SQLAlchemy model for portal update logs. logger (logging.Logger): Logger instance for this module. Examples: file = UploadedFile(path=\"uploads/report.xlsx\", status=\"pending\") db.session.add(file) db.session.commit()","title":"arb.portal.sqla_models"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models--file-is-inserted-into-the-uploaded_files-table-with-timestamps-autopopulated","text":"Notes Only models explicitly defined here will be created by SQLAlchemy via db.create_all() . Most schema inspection and data access for incidences is handled dynamically via reflection. Timezone-aware UTC timestamps are used on all tracked models. All models inherit from db.Model , and can be directly queried with SQLAlchemy syntax. The logger emits a debug message when this file is loaded.","title":"file is inserted into the uploaded_files table with timestamps autopopulated"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.PortalUpdate","text":"Bases: Model SQLAlchemy model tracking updates to the misc_json field on incidence records. Table Name portal_updates Attributes: id ( int ) \u2013 Primary key. timestamp ( datetime ) \u2013 UTC time when the change was logged. key ( str ) \u2013 JSON key that was modified. old_value ( str | None ) \u2013 Previous value (nullable). new_value ( str ) \u2013 New value. user ( str ) \u2013 Username or identifier of the user making the change. comments ( str ) \u2013 Optional explanatory comment. id_incidence ( int | None ) \u2013 Foreign key to the modified incidence (nullable). Examples: update = PortalUpdate(key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\", id_incidence=1) db.session.add(update) db.session.commit()","title":"PortalUpdate"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.PortalUpdate--update-appears-in-the-portal_updates-table","text":"Notes Automatically populated by apply_json_patch_and_log() . Used for rendering the portal_updates.html table. Source code in arb\\portal\\sqla_models.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class PortalUpdate ( db . Model ): \"\"\" SQLAlchemy model tracking updates to the misc_json field on incidence records. Table Name: portal_updates Attributes: id (int): Primary key. timestamp (datetime): UTC time when the change was logged. key (str): JSON key that was modified. old_value (str | None): Previous value (nullable). new_value (str): New value. user (str): Username or identifier of the user making the change. comments (str): Optional explanatory comment. id_incidence (int | None): Foreign key to the modified incidence (nullable). Examples: update = PortalUpdate(key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\", id_incidence=1) db.session.add(update) db.session.commit() # update appears in the portal_updates table Notes: - Automatically populated by `apply_json_patch_and_log()`. - Used for rendering the `portal_updates.html` table. \"\"\" __tablename__ = \"portal_updates\" id = Column ( Integer , primary_key = True ) timestamp = Column ( DateTime ( timezone = True ), nullable = False , server_default = func . now ()) key = Column ( String ( 255 ), nullable = False ) old_value = Column ( Text , nullable = True ) new_value = Column ( Text , nullable = False ) user = Column ( String ( 255 ), nullable = False , default = \"anonymous\" ) comments = Column ( Text , nullable = False , default = \"\" ) id_incidence = Column ( Integer , nullable = True ) def __repr__ ( self ) -> str : \"\"\" Return a human-readable string representation of the portal update record. Returns: str: Summary string showing the update ID, key, old/new values, user, and timestamp. Examples: update = PortalUpdate(id=1, key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\") print(repr(update)) # Output: '<PortalUpdate id=1 key='field1' old='A' new='B' user='alice' at=...>' \"\"\" return ( f \"<PortalUpdate id= { self . id } key= { self . key !r} old= { self . old_value !r} \" f \"new= { self . new_value !r} user= { self . user !r} at= { self . timestamp } >\" )","title":"update appears in the portal_updates table"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.PortalUpdate.__repr__","text":"Return a human-readable string representation of the portal update record. Returns: str ( str ) \u2013 Summary string showing the update ID, key, old/new values, user, and timestamp. Examples: update = PortalUpdate(id=1, key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\") print(repr(update))","title":"__repr__"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.PortalUpdate.__repr__--output","text":"Source code in arb\\portal\\sqla_models.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def __repr__ ( self ) -> str : \"\"\" Return a human-readable string representation of the portal update record. Returns: str: Summary string showing the update ID, key, old/new values, user, and timestamp. Examples: update = PortalUpdate(id=1, key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\") print(repr(update)) # Output: '<PortalUpdate id=1 key='field1' old='A' new='B' user='alice' at=...>' \"\"\" return ( f \"<PortalUpdate id= { self . id } key= { self . key !r} old= { self . old_value !r} \" f \"new= { self . new_value !r} user= { self . user !r} at= { self . timestamp } >\" )","title":"Output: ' SQLAlchemy model definitions for the ARB Feedback Portal. This module defines ORM classes that map to key tables in the database, including uploaded file metadata and portal JSON update logs. Module_Attributes UploadedFile (type): SQLAlchemy model for uploaded file metadata. PortalUpdate (type): SQLAlchemy model for portal update logs. logger (logging.Logger): Logger instance for this module. Examples: file = UploadedFile(path=\"uploads/report.xlsx\", status=\"pending\") db.session.add(file) db.session.commit() file is inserted into the uploaded_files table with timestamps autopopulated Notes Only models explicitly defined here will be created by SQLAlchemy via db.create_all(). Most schema inspection and data access for incidences is handled dynamically via reflection. Timezone-aware UTC timestamps are used on all tracked models. All models inherit from db.Model, and can be directly queried with SQLAlchemy syntax. The logger emits a debug message when this file is loaded. PortalUpdate Bases: Model SQLAlchemy model tracking updates to the misc_json field on incidence records. Table Name portal_updates Attributes: id (int) \u2013 Primary key. timestamp (datetime) \u2013 UTC time when the change was logged. key (str) \u2013 JSON key that was modified. old_value (str | None) \u2013 Previous value (nullable). new_value (str) \u2013 New value. user (str) \u2013 Username or identifier of the user making the change. comments (str) \u2013 Optional explanatory comment. id_incidence (int | None) \u2013 Foreign key to the modified incidence (nullable). Examples: update = PortalUpdate(key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\", id_incidence=1) db.session.add(update) db.session.commit() update appears in the portal_updates table Notes Automatically populated by apply_json_patch_and_log(). Used for rendering the portal_updates.html table. Source code in arb\\portal\\sqla_models.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153class PortalUpdate(db.Model): &quot;&quot;&quot; SQLAlchemy model tracking updates to the misc_json field on incidence records. Table Name: portal_updates Attributes: id (int): Primary key. timestamp (datetime): UTC time when the change was logged. key (str): JSON key that was modified. old_value (str | None): Previous value (nullable). new_value (str): New value. user (str): Username or identifier of the user making the change. comments (str): Optional explanatory comment. id_incidence (int | None): Foreign key to the modified incidence (nullable). Examples: update = PortalUpdate(key=&quot;field1&quot;, old_value=&quot;A&quot;, new_value=&quot;B&quot;, user=&quot;alice&quot;, id_incidence=1) db.session.add(update) db.session.commit() # update appears in the portal_updates table Notes: - Automatically populated by `apply_json_patch_and_log()`. - Used for rendering the `portal_updates.html` table. &quot;&quot;&quot; __tablename__ = &quot;portal_updates&quot; id = Column(Integer, primary_key=True) timestamp = Column(DateTime(timezone=True), nullable=False, server_default=func.now()) key = Column(String(255), nullable=False) old_value = Column(Text, nullable=True) new_value = Column(Text, nullable=False) user = Column(String(255), nullable=False, default=&quot;anonymous&quot;) comments = Column(Text, nullable=False, default=&quot;&quot;) id_incidence = Column(Integer, nullable=True) def __repr__(self) -&gt; str: &quot;&quot;&quot; Return a human-readable string representation of the portal update record. Returns: str: Summary string showing the update ID, key, old/new values, user, and timestamp. Examples: update = PortalUpdate(id=1, key=&quot;field1&quot;, old_value=&quot;A&quot;, new_value=&quot;B&quot;, user=&quot;alice&quot;) print(repr(update)) # Output: &#39;&lt;PortalUpdate id=1 key=&#39;field1&#39; old=&#39;A&#39; new=&#39;B&#39; user=&#39;alice&#39; at=...&gt;&#39; &quot;&quot;&quot; return ( f&quot;&lt;PortalUpdate id={self.id} key={self.key!r} old={self.old_value!r} &quot; f&quot;new={self.new_value!r} user={self.user!r} at={self.timestamp}&gt;&quot; ) __repr__() Return a human-readable string representation of the portal update record. Returns: str( str ) \u2013 Summary string showing the update ID, key, old/new values, user, and timestamp. Examples: update = PortalUpdate(id=1, key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\") print(repr(update)) Output: '' Source code in arb\\portal\\sqla_models.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153def __repr__(self) -&gt; str: &quot;&quot;&quot; Return a human-readable string representation of the portal update record. Returns: str: Summary string showing the update ID, key, old/new values, user, and timestamp. Examples: update = PortalUpdate(id=1, key=&quot;field1&quot;, old_value=&quot;A&quot;, new_value=&quot;B&quot;, user=&quot;alice&quot;) print(repr(update)) # Output: &#39;&lt;PortalUpdate id=1 key=&#39;field1&#39; old=&#39;A&#39; new=&#39;B&#39; user=&#39;alice&#39; at=...&gt;&#39; &quot;&quot;&quot; return ( f&quot;&lt;PortalUpdate id={self.id} key={self.key!r} old={self.old_value!r} &quot; f&quot;new={self.new_value!r} user={self.user!r} at={self.timestamp}&gt;&quot; ) UploadedFile Bases: Model SQLAlchemy model representing a user-uploaded file. Table Name uploaded_files Attributes: id_ (int) \u2013 Primary key. path (str) \u2013 Filesystem path to the uploaded file. description (str | None) \u2013 Optional human-friendly explanation. status (str | None) \u2013 Upload status, e.g., 'pending', 'processed', or 'error'. created_timestamp (datetime) \u2013 UTC timestamp of initial creation. modified_timestamp (datetime) \u2013 UTC timestamp of last update. Examples: file = UploadedFile(path=\"uploads/test.xlsx\", status=\"pending\") db.session.add(file) db.session.commit() file appears in the uploaded_files table with 'pending' status Notes Timestamps use UTC and are timezone-aware. This table is managed by SQLAlchemy directly (not introspected). Source code in arb\\portal\\sqla_models.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95class UploadedFile(db.Model): &quot;&quot;&quot; SQLAlchemy model representing a user-uploaded file. Table Name: uploaded_files Attributes: id_ (int): Primary key. path (str): Filesystem path to the uploaded file. description (str | None): Optional human-friendly explanation. status (str | None): Upload status, e.g., &#39;pending&#39;, &#39;processed&#39;, or &#39;error&#39;. created_timestamp (datetime): UTC timestamp of initial creation. modified_timestamp (datetime): UTC timestamp of last update. Examples: file = UploadedFile(path=&quot;uploads/test.xlsx&quot;, status=&quot;pending&quot;) db.session.add(file) db.session.commit() # file appears in the uploaded_files table with &#39;pending&#39; status Notes: - Timestamps use UTC and are timezone-aware. - This table is managed by SQLAlchemy directly (not introspected). &quot;&quot;&quot; __tablename__ = &quot;uploaded_files&quot; id_ = db.Column(db.Integer, primary_key=True) path = db.Column(db.Text, nullable=False) description = db.Column(db.Text, nullable=True) status = db.Column(db.Text, nullable=True) created_timestamp = db.Column( db.DateTime(timezone=True), server_default=func.now() ) modified_timestamp = db.Column( db.DateTime(timezone=True), server_default=func.now() ) def __repr__(self) -&gt; str: &quot;&quot;&quot; Return a human-readable string representation of the uploaded file record. Returns: str: Summary string showing the ID, path, description, and status. Examples: file = UploadedFile(id_=3, path=&quot;uploads/data.csv&quot;, description=&quot;Data&quot;, status=&quot;done&quot;) print(repr(file)) # Output: &#39;&lt;Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done&gt;&#39; &quot;&quot;&quot; return ( f&#39;&lt;Uploaded File: {self.id_}, Path: {self.path}, &#39; f&#39;Description: {self.description}, Status: {self.status}&gt;&#39; ) __repr__() Return a human-readable string representation of the uploaded file record. Returns: str( str ) \u2013 Summary string showing the ID, path, description, and status. Examples: file = UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\") print(repr(file)) Output: '' Source code in arb\\portal\\sqla_models.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95def __repr__(self) -&gt; str: &quot;&quot;&quot; Return a human-readable string representation of the uploaded file record. Returns: str: Summary string showing the ID, path, description, and status. Examples: file = UploadedFile(id_=3, path=&quot;uploads/data.csv&quot;, description=&quot;Data&quot;, status=&quot;done&quot;) print(repr(file)) # Output: &#39;&lt;Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done&gt;&#39; &quot;&quot;&quot; return ( f&#39;&lt;Uploaded File: {self.id_}, Path: {self.path}, &#39; f&#39;Description: {self.description}, Status: {self.status}&gt;&#39; ) run_diagnostics() Run a test transaction to validate UploadedFile model functionality. Raises: RuntimeError \u2013 If database access or fetch fails. Examples: run_diagnostics() Logs diagnostic info and rolls back the transaction Notes Meant for developer use in test environments only. This function leaves no data in the database due to rollback. Logs diagnostic information using the project logger. LIKELY OBSOLETE: This function is not covered by unit tests and is likely obsolete. It requires a real database connection and is not robustly testable with mocks or in CI environments. Change with caution. See documentation/docstring_update_for_testing.md for details. Source code in arb\\portal\\sqla_models.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203def run_diagnostics() -&gt; None: &quot;&quot;&quot; Run a test transaction to validate UploadedFile model functionality. Raises: RuntimeError: If database access or fetch fails. Examples: run_diagnostics() # Logs diagnostic info and rolls back the transaction Notes: - Meant for developer use in test environments only. - This function leaves no data in the database due to rollback. - Logs diagnostic information using the project logger. - LIKELY OBSOLETE: This function is not covered by unit tests and is likely obsolete. It requires a real database connection and is not robustly testable with mocks or in CI environments. Change with caution. See documentation/docstring_update_for_testing.md for details. &quot;&quot;&quot; logger.info(f&quot;Running UploadedFile diagnostics...&quot;) try: logger.debug(f&quot;Beginning diagnostic transaction...&quot;) test_file = UploadedFile( path=&quot;uploads/test_file.xlsx&quot;, description=&quot;Diagnostic test file&quot;, status=&quot;testing&quot; ) db.session.add(test_file) db.session.flush() # Ensures test_file.id_ is populated logger.info(f&quot;Inserted test file with ID: {test_file.id_}&quot;) fetched = UploadedFile.query.get(test_file.id_) if fetched is None: raise RuntimeError(&quot;Failed to retrieve inserted UploadedFile instance.&quot;) logger.info(f&quot;Fetched file: {fetched}&quot;) logger.debug(f&quot;repr: {repr(fetched)}&quot;) logger.debug(f&quot;created_timestamp: {fetched.created_timestamp}&quot;) except SQLAlchemyError as e: logger.exception(&quot;SQLAlchemy error during diagnostics.&quot;) raise RuntimeError(&quot;Database error during UploadedFile diagnostics.&quot;) from e finally: logger.debug(f&quot;Rolling back diagnostic transaction.&quot;) db.session.rollback() logger.info(f&quot;Diagnostics completed and transaction rolled back.&quot;) '"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.UploadedFile","text":"Bases: Model SQLAlchemy model representing a user-uploaded file. Table Name uploaded_files Attributes: id_ ( int ) \u2013 Primary key. path ( str ) \u2013 Filesystem path to the uploaded file. description ( str | None ) \u2013 Optional human-friendly explanation. status ( str | None ) \u2013 Upload status, e.g., 'pending', 'processed', or 'error'. created_timestamp ( datetime ) \u2013 UTC timestamp of initial creation. modified_timestamp ( datetime ) \u2013 UTC timestamp of last update. Examples: file = UploadedFile(path=\"uploads/test.xlsx\", status=\"pending\") db.session.add(file) db.session.commit()","title":"UploadedFile"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.UploadedFile--file-appears-in-the-uploaded_files-table-with-pending-status","text":"Notes Timestamps use UTC and are timezone-aware. This table is managed by SQLAlchemy directly (not introspected). Source code in arb\\portal\\sqla_models.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class UploadedFile ( db . Model ): \"\"\" SQLAlchemy model representing a user-uploaded file. Table Name: uploaded_files Attributes: id_ (int): Primary key. path (str): Filesystem path to the uploaded file. description (str | None): Optional human-friendly explanation. status (str | None): Upload status, e.g., 'pending', 'processed', or 'error'. created_timestamp (datetime): UTC timestamp of initial creation. modified_timestamp (datetime): UTC timestamp of last update. Examples: file = UploadedFile(path=\"uploads/test.xlsx\", status=\"pending\") db.session.add(file) db.session.commit() # file appears in the uploaded_files table with 'pending' status Notes: - Timestamps use UTC and are timezone-aware. - This table is managed by SQLAlchemy directly (not introspected). \"\"\" __tablename__ = \"uploaded_files\" id_ = db . Column ( db . Integer , primary_key = True ) path = db . Column ( db . Text , nullable = False ) description = db . Column ( db . Text , nullable = True ) status = db . Column ( db . Text , nullable = True ) created_timestamp = db . Column ( db . DateTime ( timezone = True ), server_default = func . now () ) modified_timestamp = db . Column ( db . DateTime ( timezone = True ), server_default = func . now () ) def __repr__ ( self ) -> str : \"\"\" Return a human-readable string representation of the uploaded file record. Returns: str: Summary string showing the ID, path, description, and status. Examples: file = UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\") print(repr(file)) # Output: '<Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done>' \"\"\" return ( f '<Uploaded File: { self . id_ } , Path: { self . path } , ' f 'Description: { self . description } , Status: { self . status } >' )","title":"file appears in the uploaded_files table with 'pending' status"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.UploadedFile.__repr__","text":"Return a human-readable string representation of the uploaded file record. Returns: str ( str ) \u2013 Summary string showing the ID, path, description, and status. Examples: file = UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\") print(repr(file))","title":"__repr__"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.UploadedFile.__repr__--output","text":"Source code in arb\\portal\\sqla_models.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __repr__ ( self ) -> str : \"\"\" Return a human-readable string representation of the uploaded file record. Returns: str: Summary string showing the ID, path, description, and status. Examples: file = UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\") print(repr(file)) # Output: '<Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done>' \"\"\" return ( f '<Uploaded File: { self . id_ } , Path: { self . path } , ' f 'Description: { self . description } , Status: { self . status } >' )","title":"Output: ' SQLAlchemy model definitions for the ARB Feedback Portal. This module defines ORM classes that map to key tables in the database, including uploaded file metadata and portal JSON update logs. Module_Attributes UploadedFile (type): SQLAlchemy model for uploaded file metadata. PortalUpdate (type): SQLAlchemy model for portal update logs. logger (logging.Logger): Logger instance for this module. Examples: file = UploadedFile(path=\"uploads/report.xlsx\", status=\"pending\") db.session.add(file) db.session.commit() file is inserted into the uploaded_files table with timestamps autopopulated Notes Only models explicitly defined here will be created by SQLAlchemy via db.create_all(). Most schema inspection and data access for incidences is handled dynamically via reflection. Timezone-aware UTC timestamps are used on all tracked models. All models inherit from db.Model, and can be directly queried with SQLAlchemy syntax. The logger emits a debug message when this file is loaded. PortalUpdate Bases: Model SQLAlchemy model tracking updates to the misc_json field on incidence records. Table Name portal_updates Attributes: id (int) \u2013 Primary key. timestamp (datetime) \u2013 UTC time when the change was logged. key (str) \u2013 JSON key that was modified. old_value (str | None) \u2013 Previous value (nullable). new_value (str) \u2013 New value. user (str) \u2013 Username or identifier of the user making the change. comments (str) \u2013 Optional explanatory comment. id_incidence (int | None) \u2013 Foreign key to the modified incidence (nullable). Examples: update = PortalUpdate(key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\", id_incidence=1) db.session.add(update) db.session.commit() update appears in the portal_updates table Notes Automatically populated by apply_json_patch_and_log(). Used for rendering the portal_updates.html table. Source code in arb\\portal\\sqla_models.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153class PortalUpdate(db.Model): &quot;&quot;&quot; SQLAlchemy model tracking updates to the misc_json field on incidence records. Table Name: portal_updates Attributes: id (int): Primary key. timestamp (datetime): UTC time when the change was logged. key (str): JSON key that was modified. old_value (str | None): Previous value (nullable). new_value (str): New value. user (str): Username or identifier of the user making the change. comments (str): Optional explanatory comment. id_incidence (int | None): Foreign key to the modified incidence (nullable). Examples: update = PortalUpdate(key=&quot;field1&quot;, old_value=&quot;A&quot;, new_value=&quot;B&quot;, user=&quot;alice&quot;, id_incidence=1) db.session.add(update) db.session.commit() # update appears in the portal_updates table Notes: - Automatically populated by `apply_json_patch_and_log()`. - Used for rendering the `portal_updates.html` table. &quot;&quot;&quot; __tablename__ = &quot;portal_updates&quot; id = Column(Integer, primary_key=True) timestamp = Column(DateTime(timezone=True), nullable=False, server_default=func.now()) key = Column(String(255), nullable=False) old_value = Column(Text, nullable=True) new_value = Column(Text, nullable=False) user = Column(String(255), nullable=False, default=&quot;anonymous&quot;) comments = Column(Text, nullable=False, default=&quot;&quot;) id_incidence = Column(Integer, nullable=True) def __repr__(self) -&gt; str: &quot;&quot;&quot; Return a human-readable string representation of the portal update record. Returns: str: Summary string showing the update ID, key, old/new values, user, and timestamp. Examples: update = PortalUpdate(id=1, key=&quot;field1&quot;, old_value=&quot;A&quot;, new_value=&quot;B&quot;, user=&quot;alice&quot;) print(repr(update)) # Output: &#39;&lt;PortalUpdate id=1 key=&#39;field1&#39; old=&#39;A&#39; new=&#39;B&#39; user=&#39;alice&#39; at=...&gt;&#39; &quot;&quot;&quot; return ( f&quot;&lt;PortalUpdate id={self.id} key={self.key!r} old={self.old_value!r} &quot; f&quot;new={self.new_value!r} user={self.user!r} at={self.timestamp}&gt;&quot; ) __repr__() Return a human-readable string representation of the portal update record. Returns: str( str ) \u2013 Summary string showing the update ID, key, old/new values, user, and timestamp. Examples: update = PortalUpdate(id=1, key=\"field1\", old_value=\"A\", new_value=\"B\", user=\"alice\") print(repr(update)) Output: '' Source code in arb\\portal\\sqla_models.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153def __repr__(self) -&gt; str: &quot;&quot;&quot; Return a human-readable string representation of the portal update record. Returns: str: Summary string showing the update ID, key, old/new values, user, and timestamp. Examples: update = PortalUpdate(id=1, key=&quot;field1&quot;, old_value=&quot;A&quot;, new_value=&quot;B&quot;, user=&quot;alice&quot;) print(repr(update)) # Output: &#39;&lt;PortalUpdate id=1 key=&#39;field1&#39; old=&#39;A&#39; new=&#39;B&#39; user=&#39;alice&#39; at=...&gt;&#39; &quot;&quot;&quot; return ( f&quot;&lt;PortalUpdate id={self.id} key={self.key!r} old={self.old_value!r} &quot; f&quot;new={self.new_value!r} user={self.user!r} at={self.timestamp}&gt;&quot; ) UploadedFile Bases: Model SQLAlchemy model representing a user-uploaded file. Table Name uploaded_files Attributes: id_ (int) \u2013 Primary key. path (str) \u2013 Filesystem path to the uploaded file. description (str | None) \u2013 Optional human-friendly explanation. status (str | None) \u2013 Upload status, e.g., 'pending', 'processed', or 'error'. created_timestamp (datetime) \u2013 UTC timestamp of initial creation. modified_timestamp (datetime) \u2013 UTC timestamp of last update. Examples: file = UploadedFile(path=\"uploads/test.xlsx\", status=\"pending\") db.session.add(file) db.session.commit() file appears in the uploaded_files table with 'pending' status Notes Timestamps use UTC and are timezone-aware. This table is managed by SQLAlchemy directly (not introspected). Source code in arb\\portal\\sqla_models.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95class UploadedFile(db.Model): &quot;&quot;&quot; SQLAlchemy model representing a user-uploaded file. Table Name: uploaded_files Attributes: id_ (int): Primary key. path (str): Filesystem path to the uploaded file. description (str | None): Optional human-friendly explanation. status (str | None): Upload status, e.g., &#39;pending&#39;, &#39;processed&#39;, or &#39;error&#39;. created_timestamp (datetime): UTC timestamp of initial creation. modified_timestamp (datetime): UTC timestamp of last update. Examples: file = UploadedFile(path=&quot;uploads/test.xlsx&quot;, status=&quot;pending&quot;) db.session.add(file) db.session.commit() # file appears in the uploaded_files table with &#39;pending&#39; status Notes: - Timestamps use UTC and are timezone-aware. - This table is managed by SQLAlchemy directly (not introspected). &quot;&quot;&quot; __tablename__ = &quot;uploaded_files&quot; id_ = db.Column(db.Integer, primary_key=True) path = db.Column(db.Text, nullable=False) description = db.Column(db.Text, nullable=True) status = db.Column(db.Text, nullable=True) created_timestamp = db.Column( db.DateTime(timezone=True), server_default=func.now() ) modified_timestamp = db.Column( db.DateTime(timezone=True), server_default=func.now() ) def __repr__(self) -&gt; str: &quot;&quot;&quot; Return a human-readable string representation of the uploaded file record. Returns: str: Summary string showing the ID, path, description, and status. Examples: file = UploadedFile(id_=3, path=&quot;uploads/data.csv&quot;, description=&quot;Data&quot;, status=&quot;done&quot;) print(repr(file)) # Output: &#39;&lt;Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done&gt;&#39; &quot;&quot;&quot; return ( f&#39;&lt;Uploaded File: {self.id_}, Path: {self.path}, &#39; f&#39;Description: {self.description}, Status: {self.status}&gt;&#39; ) __repr__() Return a human-readable string representation of the uploaded file record. Returns: str( str ) \u2013 Summary string showing the ID, path, description, and status. Examples: file = UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\") print(repr(file)) Output: '' Source code in arb\\portal\\sqla_models.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95def __repr__(self) -&gt; str: &quot;&quot;&quot; Return a human-readable string representation of the uploaded file record. Returns: str: Summary string showing the ID, path, description, and status. Examples: file = UploadedFile(id_=3, path=&quot;uploads/data.csv&quot;, description=&quot;Data&quot;, status=&quot;done&quot;) print(repr(file)) # Output: &#39;&lt;Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done&gt;&#39; &quot;&quot;&quot; return ( f&#39;&lt;Uploaded File: {self.id_}, Path: {self.path}, &#39; f&#39;Description: {self.description}, Status: {self.status}&gt;&#39; ) run_diagnostics() Run a test transaction to validate UploadedFile model functionality. Raises: RuntimeError \u2013 If database access or fetch fails. Examples: run_diagnostics() Logs diagnostic info and rolls back the transaction Notes Meant for developer use in test environments only. This function leaves no data in the database due to rollback. Logs diagnostic information using the project logger. LIKELY OBSOLETE: This function is not covered by unit tests and is likely obsolete. It requires a real database connection and is not robustly testable with mocks or in CI environments. Change with caution. See documentation/docstring_update_for_testing.md for details. Source code in arb\\portal\\sqla_models.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203def run_diagnostics() -&gt; None: &quot;&quot;&quot; Run a test transaction to validate UploadedFile model functionality. Raises: RuntimeError: If database access or fetch fails. Examples: run_diagnostics() # Logs diagnostic info and rolls back the transaction Notes: - Meant for developer use in test environments only. - This function leaves no data in the database due to rollback. - Logs diagnostic information using the project logger. - LIKELY OBSOLETE: This function is not covered by unit tests and is likely obsolete. It requires a real database connection and is not robustly testable with mocks or in CI environments. Change with caution. See documentation/docstring_update_for_testing.md for details. &quot;&quot;&quot; logger.info(f&quot;Running UploadedFile diagnostics...&quot;) try: logger.debug(f&quot;Beginning diagnostic transaction...&quot;) test_file = UploadedFile( path=&quot;uploads/test_file.xlsx&quot;, description=&quot;Diagnostic test file&quot;, status=&quot;testing&quot; ) db.session.add(test_file) db.session.flush() # Ensures test_file.id_ is populated logger.info(f&quot;Inserted test file with ID: {test_file.id_}&quot;) fetched = UploadedFile.query.get(test_file.id_) if fetched is None: raise RuntimeError(&quot;Failed to retrieve inserted UploadedFile instance.&quot;) logger.info(f&quot;Fetched file: {fetched}&quot;) logger.debug(f&quot;repr: {repr(fetched)}&quot;) logger.debug(f&quot;created_timestamp: {fetched.created_timestamp}&quot;) except SQLAlchemyError as e: logger.exception(&quot;SQLAlchemy error during diagnostics.&quot;) raise RuntimeError(&quot;Database error during UploadedFile diagnostics.&quot;) from e finally: logger.debug(f&quot;Rolling back diagnostic transaction.&quot;) db.session.rollback() logger.info(f&quot;Diagnostics completed and transaction rolled back.&quot;) '"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.run_diagnostics","text":"Run a test transaction to validate UploadedFile model functionality. Raises: RuntimeError \u2013 If database access or fetch fails. Examples: run_diagnostics()","title":"run_diagnostics"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.run_diagnostics--logs-diagnostic-info-and-rolls-back-the-transaction","text":"Notes Meant for developer use in test environments only. This function leaves no data in the database due to rollback. Logs diagnostic information using the project logger. LIKELY OBSOLETE: This function is not covered by unit tests and is likely obsolete. It requires a real database connection and is not robustly testable with mocks or in CI environments. Change with caution. See documentation/docstring_update_for_testing.md for details. Source code in arb\\portal\\sqla_models.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def run_diagnostics () -> None : \"\"\" Run a test transaction to validate UploadedFile model functionality. Raises: RuntimeError: If database access or fetch fails. Examples: run_diagnostics() # Logs diagnostic info and rolls back the transaction Notes: - Meant for developer use in test environments only. - This function leaves no data in the database due to rollback. - Logs diagnostic information using the project logger. - LIKELY OBSOLETE: This function is not covered by unit tests and is likely obsolete. It requires a real database connection and is not robustly testable with mocks or in CI environments. Change with caution. See documentation/docstring_update_for_testing.md for details. \"\"\" logger . info ( f \"Running UploadedFile diagnostics...\" ) try : logger . debug ( f \"Beginning diagnostic transaction...\" ) test_file = UploadedFile ( path = \"uploads/test_file.xlsx\" , description = \"Diagnostic test file\" , status = \"testing\" ) db . session . add ( test_file ) db . session . flush () # Ensures test_file.id_ is populated logger . info ( f \"Inserted test file with ID: { test_file . id_ } \" ) fetched = UploadedFile . query . get ( test_file . id_ ) if fetched is None : raise RuntimeError ( \"Failed to retrieve inserted UploadedFile instance.\" ) logger . info ( f \"Fetched file: { fetched } \" ) logger . debug ( f \"repr: { repr ( fetched ) } \" ) logger . debug ( f \"created_timestamp: { fetched . created_timestamp } \" ) except SQLAlchemyError as e : logger . exception ( \"SQLAlchemy error during diagnostics.\" ) raise RuntimeError ( \"Database error during UploadedFile diagnostics.\" ) from e finally : logger . debug ( f \"Rolling back diagnostic transaction.\" ) db . session . rollback () logger . info ( f \"Diagnostics completed and transaction rolled back.\" )","title":"Logs diagnostic info and rolls back the transaction"},{"location":"reference/arb/portal/wtf_landfill/","text":"arb.portal.wtf_landfill Landfill feedback form definition for the ARB Feedback Portal (WTForms). This module defines the LandfillFeedback class, a comprehensive WTForms-based HTML form for collecting information on methane emission inspections and responses at landfill sites. The form is organized into multiple logical sections and includes dynamic dropdown behavior, conditional validation, and cross-field logic. Module_Attributes LandfillFeedback (type): WTForms form class for landfill feedback data. logger (logging.Logger): Logger instance for this module. Examples: form = LandfillFeedback() form.process(request.form) if form.validate_on_submit(): save_landfill_feedback(form.data) Notes The update_contingent_selectors() method updates selector/contingent choices. The determine_contingent_fields() method enforces dynamic field-level validation. Intended for use with the landfill_incidence_update route and similar flows. General-purpose WTForms utilities are located in: arb.utils.wtf_forms_util.py The logger emits a debug message when this file is loaded. LandfillFeedback Bases: FlaskForm WTForms form class for collecting landfill feedback data. Captures user-submitted information about methane emissions, inspections, corrective actions, and contact details related to landfill facility operations. Notes All form fields are defined as class attributes below. Examples: form = LandfillFeedback() form.process(request.form) if form.validate_on_submit(): save_landfill_feedback(form.data) Notes Some fields are conditionally validated depending on selections. The form dynamically updates contingent dropdowns using update_contingent_selectors() . Final validation is enforced in the validate() method. Source code in arb\\portal\\wtf_landfill.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 class LandfillFeedback ( FlaskForm ): \"\"\" WTForms form class for collecting landfill feedback data. Captures user-submitted information about methane emissions, inspections, corrective actions, and contact details related to landfill facility operations. Notes: - All form fields are defined as class attributes below. Examples: form = LandfillFeedback() form.process(request.form) if form.validate_on_submit(): save_landfill_feedback(form.data) Notes: - Some fields are conditionally validated depending on selections. - The form dynamically updates contingent dropdowns using `update_contingent_selectors()`. - Final validation is enforced in the `validate()` method. \"\"\" # Section 2 # todo - likely have to change these to InputRequired(), Optional(), blank and removed # label = \"1. Incidence/Emission ID\" id_incidence = IntegerField ( \"Incidence/Emission ID\" , validators = [ Optional ()], render_kw = { \"readonly\" : True } ) label = \"2. Plume ID(s)\" id_plume = IntegerField ( label = label , validators = [ InputRequired (), NumberRange ( min = 1 , message = \"Plume ID must be a positive integer\" )], ) # REFERENCES plumes (id_plume) label = \"3. Plume Observation Date\" observation_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"4. Plume Origin CARB Estimated Latitude\" # I think lat/longs are failing because they were renamed ... lat_carb = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LATITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LATITUDE_VALIDATION )], ) label = \"5. Plume Origin CARB Estimated Longitude\" long_carb = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LONGITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LONGITUDE_VALIDATION )], ) label = \"6. CARB Message ID\" id_message = StringField ( label = label , validators = [ Optional ()], ) # Section 3 label = \"Q1. Facility Name\" facility_name = StringField ( label = label , validators = [ InputRequired ()], ) label = \"Q2. Facility SWIS ID\" id_arb_swis = StringField ( label = label , validators = [ Optional ()], ) label = \"Q3. Contact Name\" contact_name = StringField ( label = label , validators = [ InputRequired ()], ) # contact_phone = StringField(label=\"Contact Phone\", validators=[InputRequired()]) label = \"Q4. Contact Phone\" message = \"Invalid phone number. Phone number must be in format '(123) 456-7890' or '(123) 456-7890 x1234567'.\" contact_phone = StringField ( label = label , validators = [ InputRequired (), Regexp ( regex = r \"^\\(\\d {3} \\) \\d {3} -\\d {4} ( x\\d{1,7})?$\" , message = message ) ], ) label = \"Q5. Contact Email\" contact_email = EmailField ( label = label , validators = [ InputRequired (), Email ()]) # Section 4 label = \"Q6. Date of owner/operator's follow-up ground monitoring.\" inspection_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired (), ], format = HTML_LOCAL_TIME_FORMAT , ) label = \"Q7. Instrument used to locate the leak (e.g., Fisher Scientific TVA2020; RKI Multigas Analyzer Eagle 2; TDL).\" instrument = StringField ( label = label , validators = [ InputRequired ()]) label = \"Q8. Was a leak identified through prior knowledge or by follow-up monitoring after receipt of a CARB plume notice?\" emission_identified_flag_fk = SelectField ( label = label , choices = [], validators = [ InputRequired (), ], ) label = ( f \"Q9. If no leaks were found, please describe any events or activities that may have \" f \"contributed to the plume observed on the date provided in Section 2.\" ) additional_activities = TextAreaField ( label = label , validators = [ Optional ()], ) # Section 5 label = \"Q10: Maximum concentration of methane leak (in ppmv).\" initial_leak_concentration = DecimalField ( label = label , validators = [ InputRequired ()], ) label = \"Q11. Please provide a revised latitude if the leak location differs from CARB's estimate in Section 2.\" lat_revised = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LATITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LATITUDE_VALIDATION )], ) label = \"Q12. Please provide a revised longitude if the leak location differs from CARB's estimate in Section 2.\" long_revised = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LONGITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LONGITUDE_VALIDATION )], ) label = \"Q13: Please select from the drop-down menu which option best matches the description of the leak.\" emission_type_fk = SelectField ( label = label , choices = [], validators = [ InputRequired (), ], ) label = \"Q14. Please select from the drop-down menu which option best describes the location of the leak.\" emission_location = SelectField ( label = label , choices = [], validators = [ InputRequired (), ], ) label = ( f \"Q15. Please provide a more detailed description of the leak location, \" f \"including grid ID number or component name, if applicable.\" ) emission_location_notes = TextAreaField ( label = label , validators = [], ) label = \"Q16. Please select the most likely cause of the leak.\" emission_cause = SelectField ( label = label , choices = [], validators = [ InputRequired (), ], ) label = ( f \"Q17 (Optional). Please select an alternative cause (only if suspected). \" f \"This should not be the same as your Q16 response.\" ) emission_cause_secondary = SelectField ( label = label , choices = [], validators = [ Optional ()], ) label = ( f \"Q18 (Optional). Please select an alternative cause (only if suspected). \" f \"This should not be the same as your Q16 or Q17 responses.\" ) emission_cause_tertiary = SelectField ( label = label , choices = [], validators = [ Optional ()], ) label = ( f \"Q19. Please provide a more detailed description of the cause(s), \" f \"including the reason for and duration of any construction activity or downtime.\" ) emission_cause_notes = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = \"Q20. Describe any corrective actions taken.\" mitigation_actions = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = \"Q21. Repair date.\" mitigation_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q22. Re-monitored date.\" re_monitored_timestamp = DateTimeLocalField ( label = label , validators = [ Optional ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q23. Re-monitored methane concentration after repair (ppmv).\" re_monitored_concentration = DecimalField ( label = label , validators = [ InputRequired ()], ) label = ( f \"Q24. Was the leak location monitored in the most recent \" f \"prior quarterly/annual surface emissions or quarterly component leak monitoring event?\" ) included_in_last_lmr = SelectField ( label = label , choices = [], validators = [ InputRequired (), ], ) label = \"Q25. If 'No' to Q24, please explain why the area was excluded from monitoring.\" included_in_last_lmr_description = TextAreaField ( label = label , validators = [ InputRequired ()]) label = \"Q26. Is this grid/component planned for inclusion in the next quarterly/annual leak monitoring?\" planned_for_next_lmr = SelectField ( label = label , choices = [], validators = [ InputRequired (), ], ) label = \"Q27. If 'No' to Q26, please state why the area will not be monitored.\" planned_for_next_lmr_description = TextAreaField ( label = label , validators = [ InputRequired ()]) label = \"Q28. Date of most recent surface emissions monitoring event (prior to this notification).\" last_component_leak_monitoring_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q29. Date of most recent component leak monitoring event (prior to this notification).\" last_surface_monitoring_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q30. Additional notes or comments.\" additional_notes = TextAreaField ( label = label , validators = [], ) label = \"1. CARB internal notes\" carb_notes = TextAreaField ( label = label , validators = [], ) def __init__ ( self , * args : Any , ** kwargs : Any ): \"\"\" Initialize the LandfillFeedback form and set up contingent selectors. Args: *args: Positional arguments passed to FlaskForm. **kwargs: Keyword arguments passed to FlaskForm. Notes: - Calls update_contingent_selectors() to initialize dropdowns. \"\"\" super () . __init__ ( * args , ** kwargs ) self . emission_identified_flag_fk . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_identified_flag_fk\" )) self . emission_type_fk . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_type_fk\" )) self . emission_location . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_location\" )) self . emission_cause . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_cause\" )) self . emission_cause_secondary . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_cause_secondary\" )) self . emission_cause_tertiary . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_cause_tertiary\" )) self . included_in_last_lmr . choices = coerce_choices ( Globals . drop_downs . get ( \"included_in_last_lmr\" )) self . planned_for_next_lmr . choices = coerce_choices ( Globals . drop_downs . get ( \"planned_for_next_lmr\" )) def update_contingent_selectors ( self ) -> None : \"\"\" Update contingent dropdown selectors based on current form state. This method looks up selector/contingent relationships defined in `Globals.drop_downs_contingent` and dynamically modifies the `choices` for child fields when a selector field has a known dependency. This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of `self.emission_location`. It ensures valid dropdown options and clears invalid selections. Assumes: - `self.emission_location`, `self.emission_cause`, `self.emission_cause_secondary`, and `self.emission_cause_tertiary` are all `SelectField` instances. - `Globals.drop_downs_contingent` contains a nested dictionary of location-contingent dropdown options. Returns: None Notes: - Uses Globals.drop_downs_contingent to update choices. - Should be called whenever a parent dropdown value changes. \"\"\" # todo - update contingent dropdowns? logger . debug ( f \"Running update_contingent_selectors()\" ) emission_location = self . emission_location . data logger . debug ( f \"Selected emission_location: { emission_location !r} \" ) emission_cause_dict = Globals . drop_downs_contingent . get ( \"emission_cause_contingent_on_emission_location\" , {} ) choices_raw = emission_cause_dict . get ( emission_location , []) logger . debug ( f \"Available contingent causes: { choices_raw !r} \" ) # Define headers primary_header = [ ( PLEASE_SELECT , PLEASE_SELECT , { \"disabled\" : True }), ( \"Not applicable as no leak was detected\" , \"Not applicable as no leak was detected\" , {}), ] secondary_tertiary_header = primary_header + [ ( \"Not applicable as no additional leak cause suspected\" , \"Not applicable as no additional leak cause suspected\" , {}), ] # Build full choices primary_choices = build_choices ( primary_header , choices_raw ) secondary_tertiary_choices = build_choices ( secondary_tertiary_header , choices_raw ) # Update each field's choices self . emission_cause . choices = primary_choices self . emission_cause_secondary . choices = secondary_tertiary_choices self . emission_cause_tertiary . choices = secondary_tertiary_choices def validate ( self , extra_validators = None ) -> bool : \"\"\" Perform full-form validation, including dynamic and cross-field checks. Args: extra_validators (list | None): Additional validators to apply. Returns: bool: True if the form is valid, False otherwise. Notes: - Enforces conditional requirements based on user input. - Calls determine_contingent_fields() for dynamic validation. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with an alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () self . update_contingent_selectors () ################################################################################################### # Set selectors with values not in their choice's list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each field's individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(f\"in the validator before super\") obj_diagnostics ( self , message = \"in the validator before super\" ) _ = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### # todo - move field level validation to separate function if self . emission_identified_flag_fk . data == \"No leak was detected\" : valid_options = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data not in valid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data not in valid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) # Q8 and Q13 should be coupled to Operator-aware response elif self . emission_identified_flag_fk . data == \"Operator was aware of the leak prior to receiving the CARB plume notification\" : valid_options = [ PLEASE_SELECT , \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_identified_flag_fk . data != \"No leak was detected\" : invalid_options = [ \"Not applicable as no leak was detected\" , ] if self . emission_type_fk . data in invalid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data in invalid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data in invalid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data in invalid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data in invalid_options : self . emission_cause_tertiary . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) if self . inspection_timestamp . data and self . mitigation_timestamp . data : if self . mitigation_timestamp . data < self . inspection_timestamp . data : self . mitigation_timestamp . errors . append ( \"Date of mitigation cannot be prior to initial site inspection.\" ) # todo - add that 2nd and 3rd can't be repeats ignore_repeats = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if ( self . emission_cause_secondary . data not in ignore_repeats and self . emission_cause_secondary . data in [ self . emission_cause . data ]): self . emission_cause_secondary . errors . append ( f \"Q17 appears to be a repeat\" ) if ( self . emission_cause_tertiary . data not in ignore_repeats and self . emission_cause_tertiary . data in [ self . emission_cause . data , self . emission_cause_secondary . data ]): self . emission_cause_secondary . errors . append ( f \"Q18 appears to be a repeat\" ) # not sure if this test makes sense since they may have know about it prior to the plume (going to comment out) # if self.observation_timestamp.data and self.inspection_timestamp.data: # if self.inspection_timestamp.data < self.observation_timestamp.data: # self.inspection_timestamp.errors.append( # \"Date of inspection cannot be prior to date of initial plume observation.\") ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields with a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) return form_valid def determine_contingent_fields ( self ): \"\"\" Enforce dynamic field-level validation for contingent fields. Notes: - Adjusts validators for fields that depend on other field values. - Called during form validation to ensure correct requirements. \"\"\" # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_emission_identified = [ \"additional_activities\" , \"initial_leak_concentration\" , # \"lat_revised\", # \"long_revised\", \"emission_type_fk\" , \"emission_location\" , # \"emission_location_notes\", \"emission_cause\" , # \"emission_cause_secondary\", # \"emission_cause_tertiary\", \"emission_cause_notes\" , \"mitigation_actions\" , \"mitigation_timestamp\" , \"re_monitored_timestamp\" , \"re_monitored_concentration\" , \"included_in_last_lmr\" , \"included_in_last_lmr_description\" , \"planned_for_next_lmr\" , \"planned_for_next_lmr_description\" , \"last_surface_monitoring_timestamp\" , \"last_component_leak_monitoring_timestamp\" , \"additional_notes\" , ] # todo - update logic for new selectors emission_identified_test = self . emission_identified_flag_fk . data != \"No leak was detected\" # print(f\"{emission_identified_test=}\") change_validators_on_test ( self , emission_identified_test , required_if_emission_identified ) if emission_identified_test : lmr_included_test = self . included_in_last_lmr . data == \"No\" logger . debug ( f \" { lmr_included_test =} \" ) change_validators_on_test ( self , lmr_included_test , [ \"included_in_last_lmr_description\" ]) lmr_planned_test = self . planned_for_next_lmr . data == \"No\" logger . debug ( f \" { lmr_planned_test =} \" ) change_validators_on_test ( self , lmr_planned_test , [ \"planned_for_next_lmr_description\" ]) __init__ ( * args , ** kwargs ) Initialize the LandfillFeedback form and set up contingent selectors. Parameters: *args ( Any , default: () ) \u2013 Positional arguments passed to FlaskForm. **kwargs ( Any , default: {} ) \u2013 Keyword arguments passed to FlaskForm. Notes Calls update_contingent_selectors() to initialize dropdowns. Source code in arb\\portal\\wtf_landfill.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 def __init__ ( self , * args : Any , ** kwargs : Any ): \"\"\" Initialize the LandfillFeedback form and set up contingent selectors. Args: *args: Positional arguments passed to FlaskForm. **kwargs: Keyword arguments passed to FlaskForm. Notes: - Calls update_contingent_selectors() to initialize dropdowns. \"\"\" super () . __init__ ( * args , ** kwargs ) self . emission_identified_flag_fk . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_identified_flag_fk\" )) self . emission_type_fk . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_type_fk\" )) self . emission_location . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_location\" )) self . emission_cause . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_cause\" )) self . emission_cause_secondary . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_cause_secondary\" )) self . emission_cause_tertiary . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_cause_tertiary\" )) self . included_in_last_lmr . choices = coerce_choices ( Globals . drop_downs . get ( \"included_in_last_lmr\" )) self . planned_for_next_lmr . choices = coerce_choices ( Globals . drop_downs . get ( \"planned_for_next_lmr\" )) determine_contingent_fields () Enforce dynamic field-level validation for contingent fields. Notes Adjusts validators for fields that depend on other field values. Called during form validation to ensure correct requirements. Source code in arb\\portal\\wtf_landfill.py 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def determine_contingent_fields ( self ): \"\"\" Enforce dynamic field-level validation for contingent fields. Notes: - Adjusts validators for fields that depend on other field values. - Called during form validation to ensure correct requirements. \"\"\" # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_emission_identified = [ \"additional_activities\" , \"initial_leak_concentration\" , # \"lat_revised\", # \"long_revised\", \"emission_type_fk\" , \"emission_location\" , # \"emission_location_notes\", \"emission_cause\" , # \"emission_cause_secondary\", # \"emission_cause_tertiary\", \"emission_cause_notes\" , \"mitigation_actions\" , \"mitigation_timestamp\" , \"re_monitored_timestamp\" , \"re_monitored_concentration\" , \"included_in_last_lmr\" , \"included_in_last_lmr_description\" , \"planned_for_next_lmr\" , \"planned_for_next_lmr_description\" , \"last_surface_monitoring_timestamp\" , \"last_component_leak_monitoring_timestamp\" , \"additional_notes\" , ] # todo - update logic for new selectors emission_identified_test = self . emission_identified_flag_fk . data != \"No leak was detected\" # print(f\"{emission_identified_test=}\") change_validators_on_test ( self , emission_identified_test , required_if_emission_identified ) if emission_identified_test : lmr_included_test = self . included_in_last_lmr . data == \"No\" logger . debug ( f \" { lmr_included_test =} \" ) change_validators_on_test ( self , lmr_included_test , [ \"included_in_last_lmr_description\" ]) lmr_planned_test = self . planned_for_next_lmr . data == \"No\" logger . debug ( f \" { lmr_planned_test =} \" ) change_validators_on_test ( self , lmr_planned_test , [ \"planned_for_next_lmr_description\" ]) update_contingent_selectors () Update contingent dropdown selectors based on current form state. This method looks up selector/contingent relationships defined in Globals.drop_downs_contingent and dynamically modifies the choices for child fields when a selector field has a known dependency. This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of self.emission_location . It ensures valid dropdown options and clears invalid selections. Assumes self.emission_location , self.emission_cause , self.emission_cause_secondary , and self.emission_cause_tertiary are all SelectField instances. Globals.drop_downs_contingent contains a nested dictionary of location-contingent dropdown options. Returns: None \u2013 None Notes Uses Globals.drop_downs_contingent to update choices. Should be called whenever a parent dropdown value changes. Source code in arb\\portal\\wtf_landfill.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 def update_contingent_selectors ( self ) -> None : \"\"\" Update contingent dropdown selectors based on current form state. This method looks up selector/contingent relationships defined in `Globals.drop_downs_contingent` and dynamically modifies the `choices` for child fields when a selector field has a known dependency. This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of `self.emission_location`. It ensures valid dropdown options and clears invalid selections. Assumes: - `self.emission_location`, `self.emission_cause`, `self.emission_cause_secondary`, and `self.emission_cause_tertiary` are all `SelectField` instances. - `Globals.drop_downs_contingent` contains a nested dictionary of location-contingent dropdown options. Returns: None Notes: - Uses Globals.drop_downs_contingent to update choices. - Should be called whenever a parent dropdown value changes. \"\"\" # todo - update contingent dropdowns? logger . debug ( f \"Running update_contingent_selectors()\" ) emission_location = self . emission_location . data logger . debug ( f \"Selected emission_location: { emission_location !r} \" ) emission_cause_dict = Globals . drop_downs_contingent . get ( \"emission_cause_contingent_on_emission_location\" , {} ) choices_raw = emission_cause_dict . get ( emission_location , []) logger . debug ( f \"Available contingent causes: { choices_raw !r} \" ) # Define headers primary_header = [ ( PLEASE_SELECT , PLEASE_SELECT , { \"disabled\" : True }), ( \"Not applicable as no leak was detected\" , \"Not applicable as no leak was detected\" , {}), ] secondary_tertiary_header = primary_header + [ ( \"Not applicable as no additional leak cause suspected\" , \"Not applicable as no additional leak cause suspected\" , {}), ] # Build full choices primary_choices = build_choices ( primary_header , choices_raw ) secondary_tertiary_choices = build_choices ( secondary_tertiary_header , choices_raw ) # Update each field's choices self . emission_cause . choices = primary_choices self . emission_cause_secondary . choices = secondary_tertiary_choices self . emission_cause_tertiary . choices = secondary_tertiary_choices validate ( extra_validators = None ) Perform full-form validation, including dynamic and cross-field checks. Parameters: extra_validators ( list | None , default: None ) \u2013 Additional validators to apply. Returns: bool ( bool ) \u2013 True if the form is valid, False otherwise. Notes Enforces conditional requirements based on user input. Calls determine_contingent_fields() for dynamic validation. Source code in arb\\portal\\wtf_landfill.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 def validate ( self , extra_validators = None ) -> bool : \"\"\" Perform full-form validation, including dynamic and cross-field checks. Args: extra_validators (list | None): Additional validators to apply. Returns: bool: True if the form is valid, False otherwise. Notes: - Enforces conditional requirements based on user input. - Calls determine_contingent_fields() for dynamic validation. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with an alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () self . update_contingent_selectors () ################################################################################################### # Set selectors with values not in their choice's list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each field's individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(f\"in the validator before super\") obj_diagnostics ( self , message = \"in the validator before super\" ) _ = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### # todo - move field level validation to separate function if self . emission_identified_flag_fk . data == \"No leak was detected\" : valid_options = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data not in valid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data not in valid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) # Q8 and Q13 should be coupled to Operator-aware response elif self . emission_identified_flag_fk . data == \"Operator was aware of the leak prior to receiving the CARB plume notification\" : valid_options = [ PLEASE_SELECT , \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_identified_flag_fk . data != \"No leak was detected\" : invalid_options = [ \"Not applicable as no leak was detected\" , ] if self . emission_type_fk . data in invalid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data in invalid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data in invalid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data in invalid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data in invalid_options : self . emission_cause_tertiary . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) if self . inspection_timestamp . data and self . mitigation_timestamp . data : if self . mitigation_timestamp . data < self . inspection_timestamp . data : self . mitigation_timestamp . errors . append ( \"Date of mitigation cannot be prior to initial site inspection.\" ) # todo - add that 2nd and 3rd can't be repeats ignore_repeats = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if ( self . emission_cause_secondary . data not in ignore_repeats and self . emission_cause_secondary . data in [ self . emission_cause . data ]): self . emission_cause_secondary . errors . append ( f \"Q17 appears to be a repeat\" ) if ( self . emission_cause_tertiary . data not in ignore_repeats and self . emission_cause_tertiary . data in [ self . emission_cause . data , self . emission_cause_secondary . data ]): self . emission_cause_secondary . errors . append ( f \"Q18 appears to be a repeat\" ) # not sure if this test makes sense since they may have know about it prior to the plume (going to comment out) # if self.observation_timestamp.data and self.inspection_timestamp.data: # if self.inspection_timestamp.data < self.observation_timestamp.data: # self.inspection_timestamp.errors.append( # \"Date of inspection cannot be prior to date of initial plume observation.\") ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields with a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) return form_valid","title":"arb.portal.wtf_landfill"},{"location":"reference/arb/portal/wtf_landfill/#arbportalwtf_landfill","text":"Landfill feedback form definition for the ARB Feedback Portal (WTForms). This module defines the LandfillFeedback class, a comprehensive WTForms-based HTML form for collecting information on methane emission inspections and responses at landfill sites. The form is organized into multiple logical sections and includes dynamic dropdown behavior, conditional validation, and cross-field logic. Module_Attributes LandfillFeedback (type): WTForms form class for landfill feedback data. logger (logging.Logger): Logger instance for this module. Examples: form = LandfillFeedback() form.process(request.form) if form.validate_on_submit(): save_landfill_feedback(form.data) Notes The update_contingent_selectors() method updates selector/contingent choices. The determine_contingent_fields() method enforces dynamic field-level validation. Intended for use with the landfill_incidence_update route and similar flows. General-purpose WTForms utilities are located in: arb.utils.wtf_forms_util.py The logger emits a debug message when this file is loaded.","title":"arb.portal.wtf_landfill"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback","text":"Bases: FlaskForm WTForms form class for collecting landfill feedback data. Captures user-submitted information about methane emissions, inspections, corrective actions, and contact details related to landfill facility operations. Notes All form fields are defined as class attributes below. Examples: form = LandfillFeedback() form.process(request.form) if form.validate_on_submit(): save_landfill_feedback(form.data) Notes Some fields are conditionally validated depending on selections. The form dynamically updates contingent dropdowns using update_contingent_selectors() . Final validation is enforced in the validate() method. Source code in arb\\portal\\wtf_landfill.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 class LandfillFeedback ( FlaskForm ): \"\"\" WTForms form class for collecting landfill feedback data. Captures user-submitted information about methane emissions, inspections, corrective actions, and contact details related to landfill facility operations. Notes: - All form fields are defined as class attributes below. Examples: form = LandfillFeedback() form.process(request.form) if form.validate_on_submit(): save_landfill_feedback(form.data) Notes: - Some fields are conditionally validated depending on selections. - The form dynamically updates contingent dropdowns using `update_contingent_selectors()`. - Final validation is enforced in the `validate()` method. \"\"\" # Section 2 # todo - likely have to change these to InputRequired(), Optional(), blank and removed # label = \"1. Incidence/Emission ID\" id_incidence = IntegerField ( \"Incidence/Emission ID\" , validators = [ Optional ()], render_kw = { \"readonly\" : True } ) label = \"2. Plume ID(s)\" id_plume = IntegerField ( label = label , validators = [ InputRequired (), NumberRange ( min = 1 , message = \"Plume ID must be a positive integer\" )], ) # REFERENCES plumes (id_plume) label = \"3. Plume Observation Date\" observation_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"4. Plume Origin CARB Estimated Latitude\" # I think lat/longs are failing because they were renamed ... lat_carb = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LATITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LATITUDE_VALIDATION )], ) label = \"5. Plume Origin CARB Estimated Longitude\" long_carb = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LONGITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LONGITUDE_VALIDATION )], ) label = \"6. CARB Message ID\" id_message = StringField ( label = label , validators = [ Optional ()], ) # Section 3 label = \"Q1. Facility Name\" facility_name = StringField ( label = label , validators = [ InputRequired ()], ) label = \"Q2. Facility SWIS ID\" id_arb_swis = StringField ( label = label , validators = [ Optional ()], ) label = \"Q3. Contact Name\" contact_name = StringField ( label = label , validators = [ InputRequired ()], ) # contact_phone = StringField(label=\"Contact Phone\", validators=[InputRequired()]) label = \"Q4. Contact Phone\" message = \"Invalid phone number. Phone number must be in format '(123) 456-7890' or '(123) 456-7890 x1234567'.\" contact_phone = StringField ( label = label , validators = [ InputRequired (), Regexp ( regex = r \"^\\(\\d {3} \\) \\d {3} -\\d {4} ( x\\d{1,7})?$\" , message = message ) ], ) label = \"Q5. Contact Email\" contact_email = EmailField ( label = label , validators = [ InputRequired (), Email ()]) # Section 4 label = \"Q6. Date of owner/operator's follow-up ground monitoring.\" inspection_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired (), ], format = HTML_LOCAL_TIME_FORMAT , ) label = \"Q7. Instrument used to locate the leak (e.g., Fisher Scientific TVA2020; RKI Multigas Analyzer Eagle 2; TDL).\" instrument = StringField ( label = label , validators = [ InputRequired ()]) label = \"Q8. Was a leak identified through prior knowledge or by follow-up monitoring after receipt of a CARB plume notice?\" emission_identified_flag_fk = SelectField ( label = label , choices = [], validators = [ InputRequired (), ], ) label = ( f \"Q9. If no leaks were found, please describe any events or activities that may have \" f \"contributed to the plume observed on the date provided in Section 2.\" ) additional_activities = TextAreaField ( label = label , validators = [ Optional ()], ) # Section 5 label = \"Q10: Maximum concentration of methane leak (in ppmv).\" initial_leak_concentration = DecimalField ( label = label , validators = [ InputRequired ()], ) label = \"Q11. Please provide a revised latitude if the leak location differs from CARB's estimate in Section 2.\" lat_revised = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LATITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LATITUDE_VALIDATION )], ) label = \"Q12. Please provide a revised longitude if the leak location differs from CARB's estimate in Section 2.\" long_revised = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LONGITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LONGITUDE_VALIDATION )], ) label = \"Q13: Please select from the drop-down menu which option best matches the description of the leak.\" emission_type_fk = SelectField ( label = label , choices = [], validators = [ InputRequired (), ], ) label = \"Q14. Please select from the drop-down menu which option best describes the location of the leak.\" emission_location = SelectField ( label = label , choices = [], validators = [ InputRequired (), ], ) label = ( f \"Q15. Please provide a more detailed description of the leak location, \" f \"including grid ID number or component name, if applicable.\" ) emission_location_notes = TextAreaField ( label = label , validators = [], ) label = \"Q16. Please select the most likely cause of the leak.\" emission_cause = SelectField ( label = label , choices = [], validators = [ InputRequired (), ], ) label = ( f \"Q17 (Optional). Please select an alternative cause (only if suspected). \" f \"This should not be the same as your Q16 response.\" ) emission_cause_secondary = SelectField ( label = label , choices = [], validators = [ Optional ()], ) label = ( f \"Q18 (Optional). Please select an alternative cause (only if suspected). \" f \"This should not be the same as your Q16 or Q17 responses.\" ) emission_cause_tertiary = SelectField ( label = label , choices = [], validators = [ Optional ()], ) label = ( f \"Q19. Please provide a more detailed description of the cause(s), \" f \"including the reason for and duration of any construction activity or downtime.\" ) emission_cause_notes = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = \"Q20. Describe any corrective actions taken.\" mitigation_actions = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = \"Q21. Repair date.\" mitigation_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q22. Re-monitored date.\" re_monitored_timestamp = DateTimeLocalField ( label = label , validators = [ Optional ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q23. Re-monitored methane concentration after repair (ppmv).\" re_monitored_concentration = DecimalField ( label = label , validators = [ InputRequired ()], ) label = ( f \"Q24. Was the leak location monitored in the most recent \" f \"prior quarterly/annual surface emissions or quarterly component leak monitoring event?\" ) included_in_last_lmr = SelectField ( label = label , choices = [], validators = [ InputRequired (), ], ) label = \"Q25. If 'No' to Q24, please explain why the area was excluded from monitoring.\" included_in_last_lmr_description = TextAreaField ( label = label , validators = [ InputRequired ()]) label = \"Q26. Is this grid/component planned for inclusion in the next quarterly/annual leak monitoring?\" planned_for_next_lmr = SelectField ( label = label , choices = [], validators = [ InputRequired (), ], ) label = \"Q27. If 'No' to Q26, please state why the area will not be monitored.\" planned_for_next_lmr_description = TextAreaField ( label = label , validators = [ InputRequired ()]) label = \"Q28. Date of most recent surface emissions monitoring event (prior to this notification).\" last_component_leak_monitoring_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q29. Date of most recent component leak monitoring event (prior to this notification).\" last_surface_monitoring_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q30. Additional notes or comments.\" additional_notes = TextAreaField ( label = label , validators = [], ) label = \"1. CARB internal notes\" carb_notes = TextAreaField ( label = label , validators = [], ) def __init__ ( self , * args : Any , ** kwargs : Any ): \"\"\" Initialize the LandfillFeedback form and set up contingent selectors. Args: *args: Positional arguments passed to FlaskForm. **kwargs: Keyword arguments passed to FlaskForm. Notes: - Calls update_contingent_selectors() to initialize dropdowns. \"\"\" super () . __init__ ( * args , ** kwargs ) self . emission_identified_flag_fk . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_identified_flag_fk\" )) self . emission_type_fk . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_type_fk\" )) self . emission_location . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_location\" )) self . emission_cause . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_cause\" )) self . emission_cause_secondary . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_cause_secondary\" )) self . emission_cause_tertiary . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_cause_tertiary\" )) self . included_in_last_lmr . choices = coerce_choices ( Globals . drop_downs . get ( \"included_in_last_lmr\" )) self . planned_for_next_lmr . choices = coerce_choices ( Globals . drop_downs . get ( \"planned_for_next_lmr\" )) def update_contingent_selectors ( self ) -> None : \"\"\" Update contingent dropdown selectors based on current form state. This method looks up selector/contingent relationships defined in `Globals.drop_downs_contingent` and dynamically modifies the `choices` for child fields when a selector field has a known dependency. This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of `self.emission_location`. It ensures valid dropdown options and clears invalid selections. Assumes: - `self.emission_location`, `self.emission_cause`, `self.emission_cause_secondary`, and `self.emission_cause_tertiary` are all `SelectField` instances. - `Globals.drop_downs_contingent` contains a nested dictionary of location-contingent dropdown options. Returns: None Notes: - Uses Globals.drop_downs_contingent to update choices. - Should be called whenever a parent dropdown value changes. \"\"\" # todo - update contingent dropdowns? logger . debug ( f \"Running update_contingent_selectors()\" ) emission_location = self . emission_location . data logger . debug ( f \"Selected emission_location: { emission_location !r} \" ) emission_cause_dict = Globals . drop_downs_contingent . get ( \"emission_cause_contingent_on_emission_location\" , {} ) choices_raw = emission_cause_dict . get ( emission_location , []) logger . debug ( f \"Available contingent causes: { choices_raw !r} \" ) # Define headers primary_header = [ ( PLEASE_SELECT , PLEASE_SELECT , { \"disabled\" : True }), ( \"Not applicable as no leak was detected\" , \"Not applicable as no leak was detected\" , {}), ] secondary_tertiary_header = primary_header + [ ( \"Not applicable as no additional leak cause suspected\" , \"Not applicable as no additional leak cause suspected\" , {}), ] # Build full choices primary_choices = build_choices ( primary_header , choices_raw ) secondary_tertiary_choices = build_choices ( secondary_tertiary_header , choices_raw ) # Update each field's choices self . emission_cause . choices = primary_choices self . emission_cause_secondary . choices = secondary_tertiary_choices self . emission_cause_tertiary . choices = secondary_tertiary_choices def validate ( self , extra_validators = None ) -> bool : \"\"\" Perform full-form validation, including dynamic and cross-field checks. Args: extra_validators (list | None): Additional validators to apply. Returns: bool: True if the form is valid, False otherwise. Notes: - Enforces conditional requirements based on user input. - Calls determine_contingent_fields() for dynamic validation. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with an alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () self . update_contingent_selectors () ################################################################################################### # Set selectors with values not in their choice's list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each field's individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(f\"in the validator before super\") obj_diagnostics ( self , message = \"in the validator before super\" ) _ = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### # todo - move field level validation to separate function if self . emission_identified_flag_fk . data == \"No leak was detected\" : valid_options = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data not in valid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data not in valid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) # Q8 and Q13 should be coupled to Operator-aware response elif self . emission_identified_flag_fk . data == \"Operator was aware of the leak prior to receiving the CARB plume notification\" : valid_options = [ PLEASE_SELECT , \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_identified_flag_fk . data != \"No leak was detected\" : invalid_options = [ \"Not applicable as no leak was detected\" , ] if self . emission_type_fk . data in invalid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data in invalid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data in invalid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data in invalid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data in invalid_options : self . emission_cause_tertiary . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) if self . inspection_timestamp . data and self . mitigation_timestamp . data : if self . mitigation_timestamp . data < self . inspection_timestamp . data : self . mitigation_timestamp . errors . append ( \"Date of mitigation cannot be prior to initial site inspection.\" ) # todo - add that 2nd and 3rd can't be repeats ignore_repeats = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if ( self . emission_cause_secondary . data not in ignore_repeats and self . emission_cause_secondary . data in [ self . emission_cause . data ]): self . emission_cause_secondary . errors . append ( f \"Q17 appears to be a repeat\" ) if ( self . emission_cause_tertiary . data not in ignore_repeats and self . emission_cause_tertiary . data in [ self . emission_cause . data , self . emission_cause_secondary . data ]): self . emission_cause_secondary . errors . append ( f \"Q18 appears to be a repeat\" ) # not sure if this test makes sense since they may have know about it prior to the plume (going to comment out) # if self.observation_timestamp.data and self.inspection_timestamp.data: # if self.inspection_timestamp.data < self.observation_timestamp.data: # self.inspection_timestamp.errors.append( # \"Date of inspection cannot be prior to date of initial plume observation.\") ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields with a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) return form_valid def determine_contingent_fields ( self ): \"\"\" Enforce dynamic field-level validation for contingent fields. Notes: - Adjusts validators for fields that depend on other field values. - Called during form validation to ensure correct requirements. \"\"\" # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_emission_identified = [ \"additional_activities\" , \"initial_leak_concentration\" , # \"lat_revised\", # \"long_revised\", \"emission_type_fk\" , \"emission_location\" , # \"emission_location_notes\", \"emission_cause\" , # \"emission_cause_secondary\", # \"emission_cause_tertiary\", \"emission_cause_notes\" , \"mitigation_actions\" , \"mitigation_timestamp\" , \"re_monitored_timestamp\" , \"re_monitored_concentration\" , \"included_in_last_lmr\" , \"included_in_last_lmr_description\" , \"planned_for_next_lmr\" , \"planned_for_next_lmr_description\" , \"last_surface_monitoring_timestamp\" , \"last_component_leak_monitoring_timestamp\" , \"additional_notes\" , ] # todo - update logic for new selectors emission_identified_test = self . emission_identified_flag_fk . data != \"No leak was detected\" # print(f\"{emission_identified_test=}\") change_validators_on_test ( self , emission_identified_test , required_if_emission_identified ) if emission_identified_test : lmr_included_test = self . included_in_last_lmr . data == \"No\" logger . debug ( f \" { lmr_included_test =} \" ) change_validators_on_test ( self , lmr_included_test , [ \"included_in_last_lmr_description\" ]) lmr_planned_test = self . planned_for_next_lmr . data == \"No\" logger . debug ( f \" { lmr_planned_test =} \" ) change_validators_on_test ( self , lmr_planned_test , [ \"planned_for_next_lmr_description\" ])","title":"LandfillFeedback"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback.__init__","text":"Initialize the LandfillFeedback form and set up contingent selectors. Parameters: *args ( Any , default: () ) \u2013 Positional arguments passed to FlaskForm. **kwargs ( Any , default: {} ) \u2013 Keyword arguments passed to FlaskForm. Notes Calls update_contingent_selectors() to initialize dropdowns. Source code in arb\\portal\\wtf_landfill.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 def __init__ ( self , * args : Any , ** kwargs : Any ): \"\"\" Initialize the LandfillFeedback form and set up contingent selectors. Args: *args: Positional arguments passed to FlaskForm. **kwargs: Keyword arguments passed to FlaskForm. Notes: - Calls update_contingent_selectors() to initialize dropdowns. \"\"\" super () . __init__ ( * args , ** kwargs ) self . emission_identified_flag_fk . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_identified_flag_fk\" )) self . emission_type_fk . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_type_fk\" )) self . emission_location . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_location\" )) self . emission_cause . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_cause\" )) self . emission_cause_secondary . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_cause_secondary\" )) self . emission_cause_tertiary . choices = coerce_choices ( Globals . drop_downs . get ( \"emission_cause_tertiary\" )) self . included_in_last_lmr . choices = coerce_choices ( Globals . drop_downs . get ( \"included_in_last_lmr\" )) self . planned_for_next_lmr . choices = coerce_choices ( Globals . drop_downs . get ( \"planned_for_next_lmr\" ))","title":"__init__"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback.determine_contingent_fields","text":"Enforce dynamic field-level validation for contingent fields. Notes Adjusts validators for fields that depend on other field values. Called during form validation to ensure correct requirements. Source code in arb\\portal\\wtf_landfill.py 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 def determine_contingent_fields ( self ): \"\"\" Enforce dynamic field-level validation for contingent fields. Notes: - Adjusts validators for fields that depend on other field values. - Called during form validation to ensure correct requirements. \"\"\" # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_emission_identified = [ \"additional_activities\" , \"initial_leak_concentration\" , # \"lat_revised\", # \"long_revised\", \"emission_type_fk\" , \"emission_location\" , # \"emission_location_notes\", \"emission_cause\" , # \"emission_cause_secondary\", # \"emission_cause_tertiary\", \"emission_cause_notes\" , \"mitigation_actions\" , \"mitigation_timestamp\" , \"re_monitored_timestamp\" , \"re_monitored_concentration\" , \"included_in_last_lmr\" , \"included_in_last_lmr_description\" , \"planned_for_next_lmr\" , \"planned_for_next_lmr_description\" , \"last_surface_monitoring_timestamp\" , \"last_component_leak_monitoring_timestamp\" , \"additional_notes\" , ] # todo - update logic for new selectors emission_identified_test = self . emission_identified_flag_fk . data != \"No leak was detected\" # print(f\"{emission_identified_test=}\") change_validators_on_test ( self , emission_identified_test , required_if_emission_identified ) if emission_identified_test : lmr_included_test = self . included_in_last_lmr . data == \"No\" logger . debug ( f \" { lmr_included_test =} \" ) change_validators_on_test ( self , lmr_included_test , [ \"included_in_last_lmr_description\" ]) lmr_planned_test = self . planned_for_next_lmr . data == \"No\" logger . debug ( f \" { lmr_planned_test =} \" ) change_validators_on_test ( self , lmr_planned_test , [ \"planned_for_next_lmr_description\" ])","title":"determine_contingent_fields"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback.update_contingent_selectors","text":"Update contingent dropdown selectors based on current form state. This method looks up selector/contingent relationships defined in Globals.drop_downs_contingent and dynamically modifies the choices for child fields when a selector field has a known dependency. This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of self.emission_location . It ensures valid dropdown options and clears invalid selections. Assumes self.emission_location , self.emission_cause , self.emission_cause_secondary , and self.emission_cause_tertiary are all SelectField instances. Globals.drop_downs_contingent contains a nested dictionary of location-contingent dropdown options. Returns: None \u2013 None Notes Uses Globals.drop_downs_contingent to update choices. Should be called whenever a parent dropdown value changes. Source code in arb\\portal\\wtf_landfill.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 def update_contingent_selectors ( self ) -> None : \"\"\" Update contingent dropdown selectors based on current form state. This method looks up selector/contingent relationships defined in `Globals.drop_downs_contingent` and dynamically modifies the `choices` for child fields when a selector field has a known dependency. This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of `self.emission_location`. It ensures valid dropdown options and clears invalid selections. Assumes: - `self.emission_location`, `self.emission_cause`, `self.emission_cause_secondary`, and `self.emission_cause_tertiary` are all `SelectField` instances. - `Globals.drop_downs_contingent` contains a nested dictionary of location-contingent dropdown options. Returns: None Notes: - Uses Globals.drop_downs_contingent to update choices. - Should be called whenever a parent dropdown value changes. \"\"\" # todo - update contingent dropdowns? logger . debug ( f \"Running update_contingent_selectors()\" ) emission_location = self . emission_location . data logger . debug ( f \"Selected emission_location: { emission_location !r} \" ) emission_cause_dict = Globals . drop_downs_contingent . get ( \"emission_cause_contingent_on_emission_location\" , {} ) choices_raw = emission_cause_dict . get ( emission_location , []) logger . debug ( f \"Available contingent causes: { choices_raw !r} \" ) # Define headers primary_header = [ ( PLEASE_SELECT , PLEASE_SELECT , { \"disabled\" : True }), ( \"Not applicable as no leak was detected\" , \"Not applicable as no leak was detected\" , {}), ] secondary_tertiary_header = primary_header + [ ( \"Not applicable as no additional leak cause suspected\" , \"Not applicable as no additional leak cause suspected\" , {}), ] # Build full choices primary_choices = build_choices ( primary_header , choices_raw ) secondary_tertiary_choices = build_choices ( secondary_tertiary_header , choices_raw ) # Update each field's choices self . emission_cause . choices = primary_choices self . emission_cause_secondary . choices = secondary_tertiary_choices self . emission_cause_tertiary . choices = secondary_tertiary_choices","title":"update_contingent_selectors"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback.validate","text":"Perform full-form validation, including dynamic and cross-field checks. Parameters: extra_validators ( list | None , default: None ) \u2013 Additional validators to apply. Returns: bool ( bool ) \u2013 True if the form is valid, False otherwise. Notes Enforces conditional requirements based on user input. Calls determine_contingent_fields() for dynamic validation. Source code in arb\\portal\\wtf_landfill.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 def validate ( self , extra_validators = None ) -> bool : \"\"\" Perform full-form validation, including dynamic and cross-field checks. Args: extra_validators (list | None): Additional validators to apply. Returns: bool: True if the form is valid, False otherwise. Notes: - Enforces conditional requirements based on user input. - Calls determine_contingent_fields() for dynamic validation. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with an alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () self . update_contingent_selectors () ################################################################################################### # Set selectors with values not in their choice's list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each field's individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(f\"in the validator before super\") obj_diagnostics ( self , message = \"in the validator before super\" ) _ = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### # todo - move field level validation to separate function if self . emission_identified_flag_fk . data == \"No leak was detected\" : valid_options = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data not in valid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data not in valid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) # Q8 and Q13 should be coupled to Operator-aware response elif self . emission_identified_flag_fk . data == \"Operator was aware of the leak prior to receiving the CARB plume notification\" : valid_options = [ PLEASE_SELECT , \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_identified_flag_fk . data != \"No leak was detected\" : invalid_options = [ \"Not applicable as no leak was detected\" , ] if self . emission_type_fk . data in invalid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data in invalid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data in invalid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data in invalid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data in invalid_options : self . emission_cause_tertiary . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) if self . inspection_timestamp . data and self . mitigation_timestamp . data : if self . mitigation_timestamp . data < self . inspection_timestamp . data : self . mitigation_timestamp . errors . append ( \"Date of mitigation cannot be prior to initial site inspection.\" ) # todo - add that 2nd and 3rd can't be repeats ignore_repeats = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if ( self . emission_cause_secondary . data not in ignore_repeats and self . emission_cause_secondary . data in [ self . emission_cause . data ]): self . emission_cause_secondary . errors . append ( f \"Q17 appears to be a repeat\" ) if ( self . emission_cause_tertiary . data not in ignore_repeats and self . emission_cause_tertiary . data in [ self . emission_cause . data , self . emission_cause_secondary . data ]): self . emission_cause_secondary . errors . append ( f \"Q18 appears to be a repeat\" ) # not sure if this test makes sense since they may have know about it prior to the plume (going to comment out) # if self.observation_timestamp.data and self.inspection_timestamp.data: # if self.inspection_timestamp.data < self.observation_timestamp.data: # self.inspection_timestamp.errors.append( # \"Date of inspection cannot be prior to date of initial plume observation.\") ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields with a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) return form_valid","title":"validate"},{"location":"reference/arb/portal/wtf_oil_and_gas/","text":"arb.portal.wtf_oil_and_gas Oil & Gas Feedback Form (WTForms) for the ARB Feedback Portal. Defines the OGFeedback class, a complex feedback form used for collecting structured data about methane emission incidents in the oil and gas sector. The form logic mirrors the official O&G spreadsheet and includes conditional field validation, dynamic dropdown dependencies, and timestamp-based consistency checks. Module_Attributes OGFeedback (type): WTForms form class for oil & gas feedback data. logger (logging.Logger): Logger instance for this module. Examples: form = OGFeedback() form.process(request.form) if form.validate_on_submit(): process_feedback_data(form.data) Notes Enforces the correct response flows based on regulatory logic (e.g., 95669.1(b)(1) exclusions). Fields such as id_incidence are read-only and display-only. Contingent dropdowns are updated via update_contingent_selectors() . Cross-dependencies (e.g., OGI required if no venting exclusion) are enforced dynamically. The logger emits a debug message when this file is loaded. OGFeedback Bases: FlaskForm WTForms class for collecting feedback on Oil & Gas methane emissions. This form models the structure of the O&G feedback spreadsheet and enforces regulatory logic outlined in California methane rules (e.g., 95669.1). Sections include metadata, inspection information, emissions details, mitigation actions, and contact data. Notes All form fields are defined as class attributes below. Examples: form = OGFeedback() form.process(request.form) if form.validate_on_submit(): process_feedback_data(form.data) Notes Sector-specific contingent dropdowns are handled via Globals. Validators are adjusted at runtime depending on the selected conditions. Source code in arb\\portal\\wtf_oil_and_gas.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 class OGFeedback ( FlaskForm ): \"\"\" WTForms class for collecting feedback on Oil & Gas methane emissions. This form models the structure of the O&G feedback spreadsheet and enforces regulatory logic outlined in California methane rules (e.g., 95669.1). Sections include metadata, inspection information, emissions details, mitigation actions, and contact data. Notes: - All form fields are defined as class attributes below. Examples: form = OGFeedback() form.process(request.form) if form.validate_on_submit(): process_feedback_data(form.data) Notes: - Sector-specific contingent dropdowns are handled via Globals. - Validators are adjusted at runtime depending on the selected conditions. \"\"\" # venting through inspection (not through the 95669.1(b)(1) exclusion) venting_responses = [ \"Venting-construction/maintenance\" , \"Venting-routine\" , ] # These are considered leaks that require mitigation unintentional_leak = [ \"Unintentional-leak\" , \"Unintentional-non-component\" , ] # Section 3 # This field is read-only and displayed for context only. It should not be edited or submitted. label = \"1. Incidence/Emission ID\" id_incidence = IntegerField ( label , validators = [ Optional ()], render_kw = { \"readonly\" : True } ) label = \"2. Plume ID(s)\" id_plume = IntegerField ( label = label , validators = [ InputRequired (), NumberRange ( min = 1 , message = \"Plume ID must be a positive integer\" )], ) # REFERENCES plumes (id_plume) label = \"3. Plume Observation Timestamp(s)\" observation_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"4. Plume CARB Estimated Latitude\" lat_carb = DecimalField ( label = label , places = GPS_RESOLUTION , validators = [ InputRequired (), NumberRange ( ** LATITUDE_VALIDATION )], ) label = \"5. Plume CARB Estimated Longitude\" long_carb = DecimalField ( label = label , places = GPS_RESOLUTION , validators = [ InputRequired (), NumberRange ( ** LONGITUDE_VALIDATION )], ) label = \"6. CARB Message ID\" id_message = StringField ( label = label , validators = [ Optional ()], ) # Section 4 label = \"Q1. Facility Name\" facility_name = StringField ( label = label , validators = [ InputRequired ()], ) label = \"Q2. Facility's Cal e-GGRT ARB ID (if known)\" id_arb_eggrt = StringField ( label = label , validators = [ Optional ()], ) label = \"Q3. Contact Name\" contact_name = StringField ( label = label , validators = [ InputRequired ()], ) # contact_phone = StringField(label=\"Contact Phone\", validators=[InputRequired()]) label = \"Q4. Contact Phone Number\" message = \"Invalid phone number. Phone number must be in format '(123) 456-7890' or '(123) 456-7890 x1234567'.\" contact_phone = StringField ( label = label , validators = [ InputRequired (), Regexp ( regex = r \"^\\(\\d {3} \\) \\d {3} -\\d {4} ( x\\d{1,7})?$\" , message = message ) ], ) label = \"Q5. Contact Email Address\" contact_email = EmailField ( label = label , validators = [ InputRequired (), Email ()], ) # Section 5 label = ( f \"Q6. Was the plume a result of activity-based venting that is being reported \" f \"per section 95669.1(b)(1) of the Oil and Gas Methane Regulation?\" ) venting_exclusion = SelectField ( label = label , choices = [], validators = [ InputRequired ()], ) label = ( f \"Q7. If you answered 'Yes' to Q6, please provide a brief summary of the source of the venting \" f \"defined by Regulation 95669.1(b)(1) and why the venting occurred.\" ) message = \"If provided, a description must be at least 30 characters.\" venting_description_1 = TextAreaField ( label = label , validators = [ InputRequired (), Length ( min = 30 , message = message )], ) # Section 6 label = \"Q8. Was an OGI inspection performed?\" ogi_performed = SelectField ( label = label , choices = [], validators = [ InputRequired ()], ) label = \"Q9. If you answered 'Yes' to Q8, what date and time was the OGI inspection performed?\" ogi_date = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"Q10. If you answered 'Yes' to Q8, what type of source was found using OGI?\" ogi_result = SelectField ( label = label , choices = [], validators = [ InputRequired ()], ) label = \"Q11. Was a Method 21 inspection performed?\" method21_performed = SelectField ( label = label , choices = [], validators = [ InputRequired ()], ) label = \"Q12. If you answered 'Yes' to Q11, what date and time was the Method 21 inspection performed?\" method21_date = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"Q13. If you answered 'Yes' to Q11, what type of source was found using Method 21?\" method21_result = SelectField ( label = label , choices = [], validators = [ InputRequired ()], ) label = \"Q14. If you answered 'Yes' to Q11, what was the initial leak concentration in ppmv (if applicable)?\" initial_leak_concentration = FloatField ( label = label , validators = [ InputRequired ()], ) label = ( f \"Q15. If you answered 'Venting' to Q10 or Q13, please provide a brief summary of the source \" f \"of the venting discovered during the ground inspection and why the venting occurred.\" ) venting_description_2 = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = ( f \"Q16. If you answered a 'Unintentional-leak' or 'Unintentional-non-component' to Q10 or Q13, \" f \"please provide a description of your initial mitigation plan.\" ) initial_mitigation_plan = TextAreaField ( label = label , validators = [ InputRequired ()], ) # Section 7 label = f \"Q17. What type of equipment is at the source of the emissions?\" equipment_at_source = SelectField ( label = label , choices = [], validators = [ InputRequired ()], ) label = \"Q18. If you answered 'Other' for Q17, please provide an additional description of the equipment.\" equipment_other_description = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = f \"Q19. If your source is a component, what type of component is at the source of the emissions?\" component_at_source = SelectField ( label = label , choices = [], validators = [], ) label = \"Q20. If you answered 'Other' for Q19, please provide an additional description of the component.\" component_other_description = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = f \"Q21. Repair/mitigation completion date & time (if applicable).\" repair_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = f \"Q22. Final repair concentration in ppmv (if applicable).\" final_repair_concentration = FloatField ( label = label , validators = [ InputRequired ()], ) label = f \"Q23. Repair/Mitigation actions taken (if applicable).\" repair_description = StringField ( label = label , validators = [ InputRequired ()], ) # Section 8 label = f \"Q24. Additional notes or comments.\" additional_notes = TextAreaField ( label = label , validators = [], ) label = \"1. CARB internal notes\" carb_notes = TextAreaField ( label = label , validators = [], ) def __init__ ( self , * args : Any , ** kwargs : Any ): \"\"\" Initialize the OGFeedback form and set up contingent selectors. Args: *args: Positional arguments passed to FlaskForm. **kwargs: Keyword arguments passed to FlaskForm. Notes: - Calls update_contingent_selectors() to initialize dropdowns. \"\"\" super () . __init__ ( * args , ** kwargs ) self . venting_exclusion . choices = coerce_choices ( Globals . drop_downs . get ( \"venting_exclusion\" )) self . ogi_performed . choices = coerce_choices ( Globals . drop_downs . get ( \"ogi_performed\" )) self . ogi_result . choices = coerce_choices ( Globals . drop_downs . get ( \"ogi_result\" )) self . method21_performed . choices = coerce_choices ( Globals . drop_downs . get ( \"method21_performed\" )) self . method21_result . choices = coerce_choices ( Globals . drop_downs . get ( \"method21_result\" )) self . equipment_at_source . choices = coerce_choices ( Globals . drop_downs . get ( \"equipment_at_source\" )) self . component_at_source . choices = coerce_choices ( Globals . drop_downs . get ( \"component_at_source\" )) def update_contingent_selectors ( self ) -> None : \"\"\" Update contingent dropdown choices based on parent field values. Notes: - Uses Globals.drop_downs_contingent to update choices. - Should be called whenever a parent dropdown value changes. \"\"\" pass def validate ( self , extra_validators = None ) -> bool : \"\"\" Perform full-form validation, including dynamic and cross-field checks. Args: extra_validators (list | None): Additional validators to apply. Returns: bool: True if the form is valid, False otherwise. Notes: - Enforces conditional requirements based on user input. - Calls determine_contingent_fields() for dynamic validation. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with an alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () ################################################################################################### # Set selectors with values not in their choice's list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each fields individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(f\"in the validator before super\") _ = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### if self . observation_timestamp . data and self . ogi_date . data : if self . observation_timestamp . data > self . ogi_date . data : self . ogi_date . errors . append ( \"Initial OGI timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . method21_date . data : if self . observation_timestamp . data > self . method21_date . data : self . method21_date . errors . append ( \"Initial Method 21 timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . repair_timestamp . data : if self . observation_timestamp . data > self . repair_timestamp . data : self . method21_date . errors . append ( \"Repair timestamp must be after the plume observation timestamp\" ) if self . venting_exclusion and self . ogi_result . data : if self . venting_exclusion . data == \"Yes\" : if self . ogi_result . data in [ \"Unintentional-leak\" ]: self . ogi_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with OGI.\" ) if self . venting_exclusion and self . method21_result . data : if self . venting_exclusion . data == \"Yes\" : if self . method21_result . data in [ \"Unintentional-leak\" ]: self . method21_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with Method 21.\" ) if self . ogi_result . data in self . unintentional_leak : if self . method21_performed . data != \"Yes\" : self . method21_performed . errors . append ( \"If a leak was detected via OGI, Method 21 must be performed.\" ) if self . ogi_performed . data == \"No\" : if self . ogi_date . data : self . ogi_date . errors . append ( \"Can't have an OGI inspection date if OGI was not performed\" ) # print(f\"{self.ogi_result.data=}\") if self . ogi_result . data != PLEASE_SELECT : if self . ogi_result . data != \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Can't have an OGI result if OGI was not performed\" ) if self . method21_performed . data == \"No\" : if self . method21_date . data : self . method21_date . errors . append ( \"Can't have an Method 21 inspection date if Method 21 was not performed\" ) if self . initial_leak_concentration . data : self . initial_leak_concentration . errors . append ( \"Can't have an Method 21 concentration if Method 21 was not performed\" ) # print(f\"{self.method21_result.data=}\") if self . method21_result . data != PLEASE_SELECT : if self . method21_result . data != \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Can't have an Method 21 result if Method 21 was not performed\" ) if self . venting_exclusion . data == \"No\" and self . ogi_performed . data == \"No\" and self . method21_performed . data == \"No\" : self . method21_performed . errors . append ( \"If you do not claim a venting exclusion, Method 21 or OGI must be performed.\" ) # todo (consider) - you could also remove the option for not applicable rather than the following two tests if self . ogi_performed . data == \"Yes\" : if self . ogi_result . data == \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Invalid response given your Q8 answer\" ) if self . method21_performed . data == \"Yes\" : if self . method21_result . data == \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Invalid response given your Q11 answer\" ) ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields with a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) logger . debug ( f \"after validate(): { self . errors =} \" ) return form_valid def determine_contingent_fields ( self ) -> None : \"\"\" Enforce dynamic field-level validation for contingent fields. Affects validation logic such as: - 95669.1(b)(1) exclusions where OGI inspection is not required. - Skipping downstream fields when \"No leak was detected\" is selected. - Making \"Other\" explanations required only if \"Other\" is selected. Notes: - Adjusts validators for fields that depend on other field values. - Called during form validation to ensure correct requirements. - Should be called before validation to sync rules with input state. - Venting-related exclusions may need careful ordering to preserve business logic. \"\"\" # logger.debug(f\"In determine_contingent_fields()\") # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_venting_exclusion = [ \"venting_description_1\" , ] optional_if_venting_exclusion = [ \"ogi_performed\" , \"ogi_date\" , \"ogi_result\" , \"method21_performed\" , \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , \"venting_description_2\" , \"initial_mitigation_plan\" , \"equipment_at_source\" , \"equipment_other_description\" , \"component_at_source\" , \"component_other_description\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , \"additional_notes\" , ] venting_exclusion_test = self . venting_exclusion . data == \"Yes\" # logger.debug(f\"\\n\\t{venting_exclusion_test=}, {self.venting_exclusion_test.data=}\") change_validators_on_test ( self , venting_exclusion_test , required_if_venting_exclusion , optional_if_venting_exclusion ) required_if_ogi_performed = [ \"ogi_date\" , \"ogi_result\" , ] ogi_test = self . ogi_performed . data == \"Yes\" change_validators_on_test ( self , ogi_test , required_if_ogi_performed ) required_if_method21_performed = [ \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , ] method21_test = self . method21_performed . data == \"Yes\" change_validators_on_test ( self , method21_test , required_if_method21_performed ) required_if_venting_on_inspection = [ \"venting_description_2\" , ] venting2_test = False if self . ogi_result . data in self . venting_responses or self . method21_result . data in self . venting_responses : venting2_test = True change_validators_on_test ( self , venting2_test , required_if_venting_on_inspection ) required_if_unintentional = [ \"initial_mitigation_plan\" , \"equipment_at_source\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , ] unintentional_test = False if self . ogi_result . data in self . unintentional_leak or self . method21_result . data in self . unintentional_leak : unintentional_test = True change_validators_on_test ( self , unintentional_test , required_if_unintentional ) required_if_equipment_other = [ \"equipment_other_description\" , ] equipment_other_test = self . equipment_at_source . data == \"Other\" change_validators_on_test ( self , equipment_other_test , required_if_equipment_other ) required_if_component_other = [ \"component_other_description\" , ] component_other_test = self . component_at_source . data == \"Other\" change_validators_on_test ( self , component_other_test , required_if_component_other ) __init__ ( * args , ** kwargs ) Initialize the OGFeedback form and set up contingent selectors. Parameters: *args ( Any , default: () ) \u2013 Positional arguments passed to FlaskForm. **kwargs ( Any , default: {} ) \u2013 Keyword arguments passed to FlaskForm. Notes Calls update_contingent_selectors() to initialize dropdowns. Source code in arb\\portal\\wtf_oil_and_gas.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def __init__ ( self , * args : Any , ** kwargs : Any ): \"\"\" Initialize the OGFeedback form and set up contingent selectors. Args: *args: Positional arguments passed to FlaskForm. **kwargs: Keyword arguments passed to FlaskForm. Notes: - Calls update_contingent_selectors() to initialize dropdowns. \"\"\" super () . __init__ ( * args , ** kwargs ) self . venting_exclusion . choices = coerce_choices ( Globals . drop_downs . get ( \"venting_exclusion\" )) self . ogi_performed . choices = coerce_choices ( Globals . drop_downs . get ( \"ogi_performed\" )) self . ogi_result . choices = coerce_choices ( Globals . drop_downs . get ( \"ogi_result\" )) self . method21_performed . choices = coerce_choices ( Globals . drop_downs . get ( \"method21_performed\" )) self . method21_result . choices = coerce_choices ( Globals . drop_downs . get ( \"method21_result\" )) self . equipment_at_source . choices = coerce_choices ( Globals . drop_downs . get ( \"equipment_at_source\" )) self . component_at_source . choices = coerce_choices ( Globals . drop_downs . get ( \"component_at_source\" )) determine_contingent_fields () Enforce dynamic field-level validation for contingent fields. Affects validation logic such as: - 95669.1(b)(1) exclusions where OGI inspection is not required. - Skipping downstream fields when \"No leak was detected\" is selected. - Making \"Other\" explanations required only if \"Other\" is selected. Notes Adjusts validators for fields that depend on other field values. Called during form validation to ensure correct requirements. Should be called before validation to sync rules with input state. Venting-related exclusions may need careful ordering to preserve business logic. Source code in arb\\portal\\wtf_oil_and_gas.py 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def determine_contingent_fields ( self ) -> None : \"\"\" Enforce dynamic field-level validation for contingent fields. Affects validation logic such as: - 95669.1(b)(1) exclusions where OGI inspection is not required. - Skipping downstream fields when \"No leak was detected\" is selected. - Making \"Other\" explanations required only if \"Other\" is selected. Notes: - Adjusts validators for fields that depend on other field values. - Called during form validation to ensure correct requirements. - Should be called before validation to sync rules with input state. - Venting-related exclusions may need careful ordering to preserve business logic. \"\"\" # logger.debug(f\"In determine_contingent_fields()\") # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_venting_exclusion = [ \"venting_description_1\" , ] optional_if_venting_exclusion = [ \"ogi_performed\" , \"ogi_date\" , \"ogi_result\" , \"method21_performed\" , \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , \"venting_description_2\" , \"initial_mitigation_plan\" , \"equipment_at_source\" , \"equipment_other_description\" , \"component_at_source\" , \"component_other_description\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , \"additional_notes\" , ] venting_exclusion_test = self . venting_exclusion . data == \"Yes\" # logger.debug(f\"\\n\\t{venting_exclusion_test=}, {self.venting_exclusion_test.data=}\") change_validators_on_test ( self , venting_exclusion_test , required_if_venting_exclusion , optional_if_venting_exclusion ) required_if_ogi_performed = [ \"ogi_date\" , \"ogi_result\" , ] ogi_test = self . ogi_performed . data == \"Yes\" change_validators_on_test ( self , ogi_test , required_if_ogi_performed ) required_if_method21_performed = [ \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , ] method21_test = self . method21_performed . data == \"Yes\" change_validators_on_test ( self , method21_test , required_if_method21_performed ) required_if_venting_on_inspection = [ \"venting_description_2\" , ] venting2_test = False if self . ogi_result . data in self . venting_responses or self . method21_result . data in self . venting_responses : venting2_test = True change_validators_on_test ( self , venting2_test , required_if_venting_on_inspection ) required_if_unintentional = [ \"initial_mitigation_plan\" , \"equipment_at_source\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , ] unintentional_test = False if self . ogi_result . data in self . unintentional_leak or self . method21_result . data in self . unintentional_leak : unintentional_test = True change_validators_on_test ( self , unintentional_test , required_if_unintentional ) required_if_equipment_other = [ \"equipment_other_description\" , ] equipment_other_test = self . equipment_at_source . data == \"Other\" change_validators_on_test ( self , equipment_other_test , required_if_equipment_other ) required_if_component_other = [ \"component_other_description\" , ] component_other_test = self . component_at_source . data == \"Other\" change_validators_on_test ( self , component_other_test , required_if_component_other ) update_contingent_selectors () Update contingent dropdown choices based on parent field values. Notes Uses Globals.drop_downs_contingent to update choices. Should be called whenever a parent dropdown value changes. Source code in arb\\portal\\wtf_oil_and_gas.py 314 315 316 317 318 319 320 321 def update_contingent_selectors ( self ) -> None : \"\"\" Update contingent dropdown choices based on parent field values. Notes: - Uses Globals.drop_downs_contingent to update choices. - Should be called whenever a parent dropdown value changes. \"\"\" validate ( extra_validators = None ) Perform full-form validation, including dynamic and cross-field checks. Parameters: extra_validators ( list | None , default: None ) \u2013 Additional validators to apply. Returns: bool ( bool ) \u2013 True if the form is valid, False otherwise. Notes Enforces conditional requirements based on user input. Calls determine_contingent_fields() for dynamic validation. Source code in arb\\portal\\wtf_oil_and_gas.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def validate ( self , extra_validators = None ) -> bool : \"\"\" Perform full-form validation, including dynamic and cross-field checks. Args: extra_validators (list | None): Additional validators to apply. Returns: bool: True if the form is valid, False otherwise. Notes: - Enforces conditional requirements based on user input. - Calls determine_contingent_fields() for dynamic validation. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with an alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () ################################################################################################### # Set selectors with values not in their choice's list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each fields individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(f\"in the validator before super\") _ = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### if self . observation_timestamp . data and self . ogi_date . data : if self . observation_timestamp . data > self . ogi_date . data : self . ogi_date . errors . append ( \"Initial OGI timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . method21_date . data : if self . observation_timestamp . data > self . method21_date . data : self . method21_date . errors . append ( \"Initial Method 21 timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . repair_timestamp . data : if self . observation_timestamp . data > self . repair_timestamp . data : self . method21_date . errors . append ( \"Repair timestamp must be after the plume observation timestamp\" ) if self . venting_exclusion and self . ogi_result . data : if self . venting_exclusion . data == \"Yes\" : if self . ogi_result . data in [ \"Unintentional-leak\" ]: self . ogi_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with OGI.\" ) if self . venting_exclusion and self . method21_result . data : if self . venting_exclusion . data == \"Yes\" : if self . method21_result . data in [ \"Unintentional-leak\" ]: self . method21_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with Method 21.\" ) if self . ogi_result . data in self . unintentional_leak : if self . method21_performed . data != \"Yes\" : self . method21_performed . errors . append ( \"If a leak was detected via OGI, Method 21 must be performed.\" ) if self . ogi_performed . data == \"No\" : if self . ogi_date . data : self . ogi_date . errors . append ( \"Can't have an OGI inspection date if OGI was not performed\" ) # print(f\"{self.ogi_result.data=}\") if self . ogi_result . data != PLEASE_SELECT : if self . ogi_result . data != \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Can't have an OGI result if OGI was not performed\" ) if self . method21_performed . data == \"No\" : if self . method21_date . data : self . method21_date . errors . append ( \"Can't have an Method 21 inspection date if Method 21 was not performed\" ) if self . initial_leak_concentration . data : self . initial_leak_concentration . errors . append ( \"Can't have an Method 21 concentration if Method 21 was not performed\" ) # print(f\"{self.method21_result.data=}\") if self . method21_result . data != PLEASE_SELECT : if self . method21_result . data != \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Can't have an Method 21 result if Method 21 was not performed\" ) if self . venting_exclusion . data == \"No\" and self . ogi_performed . data == \"No\" and self . method21_performed . data == \"No\" : self . method21_performed . errors . append ( \"If you do not claim a venting exclusion, Method 21 or OGI must be performed.\" ) # todo (consider) - you could also remove the option for not applicable rather than the following two tests if self . ogi_performed . data == \"Yes\" : if self . ogi_result . data == \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Invalid response given your Q8 answer\" ) if self . method21_performed . data == \"Yes\" : if self . method21_result . data == \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Invalid response given your Q11 answer\" ) ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields with a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) logger . debug ( f \"after validate(): { self . errors =} \" ) return form_valid","title":"arb.portal.wtf_oil_and_gas"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arbportalwtf_oil_and_gas","text":"Oil & Gas Feedback Form (WTForms) for the ARB Feedback Portal. Defines the OGFeedback class, a complex feedback form used for collecting structured data about methane emission incidents in the oil and gas sector. The form logic mirrors the official O&G spreadsheet and includes conditional field validation, dynamic dropdown dependencies, and timestamp-based consistency checks. Module_Attributes OGFeedback (type): WTForms form class for oil & gas feedback data. logger (logging.Logger): Logger instance for this module. Examples: form = OGFeedback() form.process(request.form) if form.validate_on_submit(): process_feedback_data(form.data) Notes Enforces the correct response flows based on regulatory logic (e.g., 95669.1(b)(1) exclusions). Fields such as id_incidence are read-only and display-only. Contingent dropdowns are updated via update_contingent_selectors() . Cross-dependencies (e.g., OGI required if no venting exclusion) are enforced dynamically. The logger emits a debug message when this file is loaded.","title":"arb.portal.wtf_oil_and_gas"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback","text":"Bases: FlaskForm WTForms class for collecting feedback on Oil & Gas methane emissions. This form models the structure of the O&G feedback spreadsheet and enforces regulatory logic outlined in California methane rules (e.g., 95669.1). Sections include metadata, inspection information, emissions details, mitigation actions, and contact data. Notes All form fields are defined as class attributes below. Examples: form = OGFeedback() form.process(request.form) if form.validate_on_submit(): process_feedback_data(form.data) Notes Sector-specific contingent dropdowns are handled via Globals. Validators are adjusted at runtime depending on the selected conditions. Source code in arb\\portal\\wtf_oil_and_gas.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 class OGFeedback ( FlaskForm ): \"\"\" WTForms class for collecting feedback on Oil & Gas methane emissions. This form models the structure of the O&G feedback spreadsheet and enforces regulatory logic outlined in California methane rules (e.g., 95669.1). Sections include metadata, inspection information, emissions details, mitigation actions, and contact data. Notes: - All form fields are defined as class attributes below. Examples: form = OGFeedback() form.process(request.form) if form.validate_on_submit(): process_feedback_data(form.data) Notes: - Sector-specific contingent dropdowns are handled via Globals. - Validators are adjusted at runtime depending on the selected conditions. \"\"\" # venting through inspection (not through the 95669.1(b)(1) exclusion) venting_responses = [ \"Venting-construction/maintenance\" , \"Venting-routine\" , ] # These are considered leaks that require mitigation unintentional_leak = [ \"Unintentional-leak\" , \"Unintentional-non-component\" , ] # Section 3 # This field is read-only and displayed for context only. It should not be edited or submitted. label = \"1. Incidence/Emission ID\" id_incidence = IntegerField ( label , validators = [ Optional ()], render_kw = { \"readonly\" : True } ) label = \"2. Plume ID(s)\" id_plume = IntegerField ( label = label , validators = [ InputRequired (), NumberRange ( min = 1 , message = \"Plume ID must be a positive integer\" )], ) # REFERENCES plumes (id_plume) label = \"3. Plume Observation Timestamp(s)\" observation_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"4. Plume CARB Estimated Latitude\" lat_carb = DecimalField ( label = label , places = GPS_RESOLUTION , validators = [ InputRequired (), NumberRange ( ** LATITUDE_VALIDATION )], ) label = \"5. Plume CARB Estimated Longitude\" long_carb = DecimalField ( label = label , places = GPS_RESOLUTION , validators = [ InputRequired (), NumberRange ( ** LONGITUDE_VALIDATION )], ) label = \"6. CARB Message ID\" id_message = StringField ( label = label , validators = [ Optional ()], ) # Section 4 label = \"Q1. Facility Name\" facility_name = StringField ( label = label , validators = [ InputRequired ()], ) label = \"Q2. Facility's Cal e-GGRT ARB ID (if known)\" id_arb_eggrt = StringField ( label = label , validators = [ Optional ()], ) label = \"Q3. Contact Name\" contact_name = StringField ( label = label , validators = [ InputRequired ()], ) # contact_phone = StringField(label=\"Contact Phone\", validators=[InputRequired()]) label = \"Q4. Contact Phone Number\" message = \"Invalid phone number. Phone number must be in format '(123) 456-7890' or '(123) 456-7890 x1234567'.\" contact_phone = StringField ( label = label , validators = [ InputRequired (), Regexp ( regex = r \"^\\(\\d {3} \\) \\d {3} -\\d {4} ( x\\d{1,7})?$\" , message = message ) ], ) label = \"Q5. Contact Email Address\" contact_email = EmailField ( label = label , validators = [ InputRequired (), Email ()], ) # Section 5 label = ( f \"Q6. Was the plume a result of activity-based venting that is being reported \" f \"per section 95669.1(b)(1) of the Oil and Gas Methane Regulation?\" ) venting_exclusion = SelectField ( label = label , choices = [], validators = [ InputRequired ()], ) label = ( f \"Q7. If you answered 'Yes' to Q6, please provide a brief summary of the source of the venting \" f \"defined by Regulation 95669.1(b)(1) and why the venting occurred.\" ) message = \"If provided, a description must be at least 30 characters.\" venting_description_1 = TextAreaField ( label = label , validators = [ InputRequired (), Length ( min = 30 , message = message )], ) # Section 6 label = \"Q8. Was an OGI inspection performed?\" ogi_performed = SelectField ( label = label , choices = [], validators = [ InputRequired ()], ) label = \"Q9. If you answered 'Yes' to Q8, what date and time was the OGI inspection performed?\" ogi_date = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"Q10. If you answered 'Yes' to Q8, what type of source was found using OGI?\" ogi_result = SelectField ( label = label , choices = [], validators = [ InputRequired ()], ) label = \"Q11. Was a Method 21 inspection performed?\" method21_performed = SelectField ( label = label , choices = [], validators = [ InputRequired ()], ) label = \"Q12. If you answered 'Yes' to Q11, what date and time was the Method 21 inspection performed?\" method21_date = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"Q13. If you answered 'Yes' to Q11, what type of source was found using Method 21?\" method21_result = SelectField ( label = label , choices = [], validators = [ InputRequired ()], ) label = \"Q14. If you answered 'Yes' to Q11, what was the initial leak concentration in ppmv (if applicable)?\" initial_leak_concentration = FloatField ( label = label , validators = [ InputRequired ()], ) label = ( f \"Q15. If you answered 'Venting' to Q10 or Q13, please provide a brief summary of the source \" f \"of the venting discovered during the ground inspection and why the venting occurred.\" ) venting_description_2 = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = ( f \"Q16. If you answered a 'Unintentional-leak' or 'Unintentional-non-component' to Q10 or Q13, \" f \"please provide a description of your initial mitigation plan.\" ) initial_mitigation_plan = TextAreaField ( label = label , validators = [ InputRequired ()], ) # Section 7 label = f \"Q17. What type of equipment is at the source of the emissions?\" equipment_at_source = SelectField ( label = label , choices = [], validators = [ InputRequired ()], ) label = \"Q18. If you answered 'Other' for Q17, please provide an additional description of the equipment.\" equipment_other_description = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = f \"Q19. If your source is a component, what type of component is at the source of the emissions?\" component_at_source = SelectField ( label = label , choices = [], validators = [], ) label = \"Q20. If you answered 'Other' for Q19, please provide an additional description of the component.\" component_other_description = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = f \"Q21. Repair/mitigation completion date & time (if applicable).\" repair_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = f \"Q22. Final repair concentration in ppmv (if applicable).\" final_repair_concentration = FloatField ( label = label , validators = [ InputRequired ()], ) label = f \"Q23. Repair/Mitigation actions taken (if applicable).\" repair_description = StringField ( label = label , validators = [ InputRequired ()], ) # Section 8 label = f \"Q24. Additional notes or comments.\" additional_notes = TextAreaField ( label = label , validators = [], ) label = \"1. CARB internal notes\" carb_notes = TextAreaField ( label = label , validators = [], ) def __init__ ( self , * args : Any , ** kwargs : Any ): \"\"\" Initialize the OGFeedback form and set up contingent selectors. Args: *args: Positional arguments passed to FlaskForm. **kwargs: Keyword arguments passed to FlaskForm. Notes: - Calls update_contingent_selectors() to initialize dropdowns. \"\"\" super () . __init__ ( * args , ** kwargs ) self . venting_exclusion . choices = coerce_choices ( Globals . drop_downs . get ( \"venting_exclusion\" )) self . ogi_performed . choices = coerce_choices ( Globals . drop_downs . get ( \"ogi_performed\" )) self . ogi_result . choices = coerce_choices ( Globals . drop_downs . get ( \"ogi_result\" )) self . method21_performed . choices = coerce_choices ( Globals . drop_downs . get ( \"method21_performed\" )) self . method21_result . choices = coerce_choices ( Globals . drop_downs . get ( \"method21_result\" )) self . equipment_at_source . choices = coerce_choices ( Globals . drop_downs . get ( \"equipment_at_source\" )) self . component_at_source . choices = coerce_choices ( Globals . drop_downs . get ( \"component_at_source\" )) def update_contingent_selectors ( self ) -> None : \"\"\" Update contingent dropdown choices based on parent field values. Notes: - Uses Globals.drop_downs_contingent to update choices. - Should be called whenever a parent dropdown value changes. \"\"\" pass def validate ( self , extra_validators = None ) -> bool : \"\"\" Perform full-form validation, including dynamic and cross-field checks. Args: extra_validators (list | None): Additional validators to apply. Returns: bool: True if the form is valid, False otherwise. Notes: - Enforces conditional requirements based on user input. - Calls determine_contingent_fields() for dynamic validation. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with an alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () ################################################################################################### # Set selectors with values not in their choice's list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each fields individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(f\"in the validator before super\") _ = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### if self . observation_timestamp . data and self . ogi_date . data : if self . observation_timestamp . data > self . ogi_date . data : self . ogi_date . errors . append ( \"Initial OGI timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . method21_date . data : if self . observation_timestamp . data > self . method21_date . data : self . method21_date . errors . append ( \"Initial Method 21 timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . repair_timestamp . data : if self . observation_timestamp . data > self . repair_timestamp . data : self . method21_date . errors . append ( \"Repair timestamp must be after the plume observation timestamp\" ) if self . venting_exclusion and self . ogi_result . data : if self . venting_exclusion . data == \"Yes\" : if self . ogi_result . data in [ \"Unintentional-leak\" ]: self . ogi_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with OGI.\" ) if self . venting_exclusion and self . method21_result . data : if self . venting_exclusion . data == \"Yes\" : if self . method21_result . data in [ \"Unintentional-leak\" ]: self . method21_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with Method 21.\" ) if self . ogi_result . data in self . unintentional_leak : if self . method21_performed . data != \"Yes\" : self . method21_performed . errors . append ( \"If a leak was detected via OGI, Method 21 must be performed.\" ) if self . ogi_performed . data == \"No\" : if self . ogi_date . data : self . ogi_date . errors . append ( \"Can't have an OGI inspection date if OGI was not performed\" ) # print(f\"{self.ogi_result.data=}\") if self . ogi_result . data != PLEASE_SELECT : if self . ogi_result . data != \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Can't have an OGI result if OGI was not performed\" ) if self . method21_performed . data == \"No\" : if self . method21_date . data : self . method21_date . errors . append ( \"Can't have an Method 21 inspection date if Method 21 was not performed\" ) if self . initial_leak_concentration . data : self . initial_leak_concentration . errors . append ( \"Can't have an Method 21 concentration if Method 21 was not performed\" ) # print(f\"{self.method21_result.data=}\") if self . method21_result . data != PLEASE_SELECT : if self . method21_result . data != \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Can't have an Method 21 result if Method 21 was not performed\" ) if self . venting_exclusion . data == \"No\" and self . ogi_performed . data == \"No\" and self . method21_performed . data == \"No\" : self . method21_performed . errors . append ( \"If you do not claim a venting exclusion, Method 21 or OGI must be performed.\" ) # todo (consider) - you could also remove the option for not applicable rather than the following two tests if self . ogi_performed . data == \"Yes\" : if self . ogi_result . data == \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Invalid response given your Q8 answer\" ) if self . method21_performed . data == \"Yes\" : if self . method21_result . data == \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Invalid response given your Q11 answer\" ) ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields with a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) logger . debug ( f \"after validate(): { self . errors =} \" ) return form_valid def determine_contingent_fields ( self ) -> None : \"\"\" Enforce dynamic field-level validation for contingent fields. Affects validation logic such as: - 95669.1(b)(1) exclusions where OGI inspection is not required. - Skipping downstream fields when \"No leak was detected\" is selected. - Making \"Other\" explanations required only if \"Other\" is selected. Notes: - Adjusts validators for fields that depend on other field values. - Called during form validation to ensure correct requirements. - Should be called before validation to sync rules with input state. - Venting-related exclusions may need careful ordering to preserve business logic. \"\"\" # logger.debug(f\"In determine_contingent_fields()\") # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_venting_exclusion = [ \"venting_description_1\" , ] optional_if_venting_exclusion = [ \"ogi_performed\" , \"ogi_date\" , \"ogi_result\" , \"method21_performed\" , \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , \"venting_description_2\" , \"initial_mitigation_plan\" , \"equipment_at_source\" , \"equipment_other_description\" , \"component_at_source\" , \"component_other_description\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , \"additional_notes\" , ] venting_exclusion_test = self . venting_exclusion . data == \"Yes\" # logger.debug(f\"\\n\\t{venting_exclusion_test=}, {self.venting_exclusion_test.data=}\") change_validators_on_test ( self , venting_exclusion_test , required_if_venting_exclusion , optional_if_venting_exclusion ) required_if_ogi_performed = [ \"ogi_date\" , \"ogi_result\" , ] ogi_test = self . ogi_performed . data == \"Yes\" change_validators_on_test ( self , ogi_test , required_if_ogi_performed ) required_if_method21_performed = [ \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , ] method21_test = self . method21_performed . data == \"Yes\" change_validators_on_test ( self , method21_test , required_if_method21_performed ) required_if_venting_on_inspection = [ \"venting_description_2\" , ] venting2_test = False if self . ogi_result . data in self . venting_responses or self . method21_result . data in self . venting_responses : venting2_test = True change_validators_on_test ( self , venting2_test , required_if_venting_on_inspection ) required_if_unintentional = [ \"initial_mitigation_plan\" , \"equipment_at_source\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , ] unintentional_test = False if self . ogi_result . data in self . unintentional_leak or self . method21_result . data in self . unintentional_leak : unintentional_test = True change_validators_on_test ( self , unintentional_test , required_if_unintentional ) required_if_equipment_other = [ \"equipment_other_description\" , ] equipment_other_test = self . equipment_at_source . data == \"Other\" change_validators_on_test ( self , equipment_other_test , required_if_equipment_other ) required_if_component_other = [ \"component_other_description\" , ] component_other_test = self . component_at_source . data == \"Other\" change_validators_on_test ( self , component_other_test , required_if_component_other )","title":"OGFeedback"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback.__init__","text":"Initialize the OGFeedback form and set up contingent selectors. Parameters: *args ( Any , default: () ) \u2013 Positional arguments passed to FlaskForm. **kwargs ( Any , default: {} ) \u2013 Keyword arguments passed to FlaskForm. Notes Calls update_contingent_selectors() to initialize dropdowns. Source code in arb\\portal\\wtf_oil_and_gas.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 def __init__ ( self , * args : Any , ** kwargs : Any ): \"\"\" Initialize the OGFeedback form and set up contingent selectors. Args: *args: Positional arguments passed to FlaskForm. **kwargs: Keyword arguments passed to FlaskForm. Notes: - Calls update_contingent_selectors() to initialize dropdowns. \"\"\" super () . __init__ ( * args , ** kwargs ) self . venting_exclusion . choices = coerce_choices ( Globals . drop_downs . get ( \"venting_exclusion\" )) self . ogi_performed . choices = coerce_choices ( Globals . drop_downs . get ( \"ogi_performed\" )) self . ogi_result . choices = coerce_choices ( Globals . drop_downs . get ( \"ogi_result\" )) self . method21_performed . choices = coerce_choices ( Globals . drop_downs . get ( \"method21_performed\" )) self . method21_result . choices = coerce_choices ( Globals . drop_downs . get ( \"method21_result\" )) self . equipment_at_source . choices = coerce_choices ( Globals . drop_downs . get ( \"equipment_at_source\" )) self . component_at_source . choices = coerce_choices ( Globals . drop_downs . get ( \"component_at_source\" ))","title":"__init__"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback.determine_contingent_fields","text":"Enforce dynamic field-level validation for contingent fields. Affects validation logic such as: - 95669.1(b)(1) exclusions where OGI inspection is not required. - Skipping downstream fields when \"No leak was detected\" is selected. - Making \"Other\" explanations required only if \"Other\" is selected. Notes Adjusts validators for fields that depend on other field values. Called during form validation to ensure correct requirements. Should be called before validation to sync rules with input state. Venting-related exclusions may need careful ordering to preserve business logic. Source code in arb\\portal\\wtf_oil_and_gas.py 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 def determine_contingent_fields ( self ) -> None : \"\"\" Enforce dynamic field-level validation for contingent fields. Affects validation logic such as: - 95669.1(b)(1) exclusions where OGI inspection is not required. - Skipping downstream fields when \"No leak was detected\" is selected. - Making \"Other\" explanations required only if \"Other\" is selected. Notes: - Adjusts validators for fields that depend on other field values. - Called during form validation to ensure correct requirements. - Should be called before validation to sync rules with input state. - Venting-related exclusions may need careful ordering to preserve business logic. \"\"\" # logger.debug(f\"In determine_contingent_fields()\") # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_venting_exclusion = [ \"venting_description_1\" , ] optional_if_venting_exclusion = [ \"ogi_performed\" , \"ogi_date\" , \"ogi_result\" , \"method21_performed\" , \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , \"venting_description_2\" , \"initial_mitigation_plan\" , \"equipment_at_source\" , \"equipment_other_description\" , \"component_at_source\" , \"component_other_description\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , \"additional_notes\" , ] venting_exclusion_test = self . venting_exclusion . data == \"Yes\" # logger.debug(f\"\\n\\t{venting_exclusion_test=}, {self.venting_exclusion_test.data=}\") change_validators_on_test ( self , venting_exclusion_test , required_if_venting_exclusion , optional_if_venting_exclusion ) required_if_ogi_performed = [ \"ogi_date\" , \"ogi_result\" , ] ogi_test = self . ogi_performed . data == \"Yes\" change_validators_on_test ( self , ogi_test , required_if_ogi_performed ) required_if_method21_performed = [ \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , ] method21_test = self . method21_performed . data == \"Yes\" change_validators_on_test ( self , method21_test , required_if_method21_performed ) required_if_venting_on_inspection = [ \"venting_description_2\" , ] venting2_test = False if self . ogi_result . data in self . venting_responses or self . method21_result . data in self . venting_responses : venting2_test = True change_validators_on_test ( self , venting2_test , required_if_venting_on_inspection ) required_if_unintentional = [ \"initial_mitigation_plan\" , \"equipment_at_source\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , ] unintentional_test = False if self . ogi_result . data in self . unintentional_leak or self . method21_result . data in self . unintentional_leak : unintentional_test = True change_validators_on_test ( self , unintentional_test , required_if_unintentional ) required_if_equipment_other = [ \"equipment_other_description\" , ] equipment_other_test = self . equipment_at_source . data == \"Other\" change_validators_on_test ( self , equipment_other_test , required_if_equipment_other ) required_if_component_other = [ \"component_other_description\" , ] component_other_test = self . component_at_source . data == \"Other\" change_validators_on_test ( self , component_other_test , required_if_component_other )","title":"determine_contingent_fields"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback.update_contingent_selectors","text":"Update contingent dropdown choices based on parent field values. Notes Uses Globals.drop_downs_contingent to update choices. Should be called whenever a parent dropdown value changes. Source code in arb\\portal\\wtf_oil_and_gas.py 314 315 316 317 318 319 320 321 def update_contingent_selectors ( self ) -> None : \"\"\" Update contingent dropdown choices based on parent field values. Notes: - Uses Globals.drop_downs_contingent to update choices. - Should be called whenever a parent dropdown value changes. \"\"\"","title":"update_contingent_selectors"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback.validate","text":"Perform full-form validation, including dynamic and cross-field checks. Parameters: extra_validators ( list | None , default: None ) \u2013 Additional validators to apply. Returns: bool ( bool ) \u2013 True if the form is valid, False otherwise. Notes Enforces conditional requirements based on user input. Calls determine_contingent_fields() for dynamic validation. Source code in arb\\portal\\wtf_oil_and_gas.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def validate ( self , extra_validators = None ) -> bool : \"\"\" Perform full-form validation, including dynamic and cross-field checks. Args: extra_validators (list | None): Additional validators to apply. Returns: bool: True if the form is valid, False otherwise. Notes: - Enforces conditional requirements based on user input. - Calls determine_contingent_fields() for dynamic validation. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with an alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () ################################################################################################### # Set selectors with values not in their choice's list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each fields individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(f\"in the validator before super\") _ = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### if self . observation_timestamp . data and self . ogi_date . data : if self . observation_timestamp . data > self . ogi_date . data : self . ogi_date . errors . append ( \"Initial OGI timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . method21_date . data : if self . observation_timestamp . data > self . method21_date . data : self . method21_date . errors . append ( \"Initial Method 21 timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . repair_timestamp . data : if self . observation_timestamp . data > self . repair_timestamp . data : self . method21_date . errors . append ( \"Repair timestamp must be after the plume observation timestamp\" ) if self . venting_exclusion and self . ogi_result . data : if self . venting_exclusion . data == \"Yes\" : if self . ogi_result . data in [ \"Unintentional-leak\" ]: self . ogi_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with OGI.\" ) if self . venting_exclusion and self . method21_result . data : if self . venting_exclusion . data == \"Yes\" : if self . method21_result . data in [ \"Unintentional-leak\" ]: self . method21_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with Method 21.\" ) if self . ogi_result . data in self . unintentional_leak : if self . method21_performed . data != \"Yes\" : self . method21_performed . errors . append ( \"If a leak was detected via OGI, Method 21 must be performed.\" ) if self . ogi_performed . data == \"No\" : if self . ogi_date . data : self . ogi_date . errors . append ( \"Can't have an OGI inspection date if OGI was not performed\" ) # print(f\"{self.ogi_result.data=}\") if self . ogi_result . data != PLEASE_SELECT : if self . ogi_result . data != \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Can't have an OGI result if OGI was not performed\" ) if self . method21_performed . data == \"No\" : if self . method21_date . data : self . method21_date . errors . append ( \"Can't have an Method 21 inspection date if Method 21 was not performed\" ) if self . initial_leak_concentration . data : self . initial_leak_concentration . errors . append ( \"Can't have an Method 21 concentration if Method 21 was not performed\" ) # print(f\"{self.method21_result.data=}\") if self . method21_result . data != PLEASE_SELECT : if self . method21_result . data != \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Can't have an Method 21 result if Method 21 was not performed\" ) if self . venting_exclusion . data == \"No\" and self . ogi_performed . data == \"No\" and self . method21_performed . data == \"No\" : self . method21_performed . errors . append ( \"If you do not claim a venting exclusion, Method 21 or OGI must be performed.\" ) # todo (consider) - you could also remove the option for not applicable rather than the following two tests if self . ogi_performed . data == \"Yes\" : if self . ogi_result . data == \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Invalid response given your Q8 answer\" ) if self . method21_performed . data == \"Yes\" : if self . method21_result . data == \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Invalid response given your Q11 answer\" ) ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields with a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) logger . debug ( f \"after validate(): { self . errors =} \" ) return form_valid","title":"validate"},{"location":"reference/arb/portal/wtf_upload/","text":"arb.portal.wtf_upload WTForms-based upload form for the ARB Feedback Portal. Defines a minimal form used to upload Excel files via the web interface. Typically used in the /upload route. Module_Attributes UploadForm (type): WTForms form class for file uploads. logger (logging.Logger): Logger instance for this module. Examples: form = UploadForm() if form.validate_on_submit(): # handle file upload pass Notes Leverages Flask-WTF integration with Bootstrap-compatible rendering. Additional validation for file size or filename may be added externally. The logger emits a debug message when this file is loaded. UploadForm Bases: FlaskForm WTForm for uploading Excel or JSON files via the ARB Feedback Portal. Attributes: file ( FileField ) \u2013 Upload field for selecting a .xls or .xlsx file. submit ( SubmitField ) \u2013 Form button to initiate upload. Examples: form = UploadForm() if form.validate_on_submit(): # handle file upload pass Notes Uses Flask-WTF and integrates with Bootstrap templates. File extension restrictions enforced via FileAllowed . Form is rendered in the Upload UI at /upload . Source code in arb\\portal\\wtf_upload.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class UploadForm ( FlaskForm ): \"\"\" WTForm for uploading Excel or JSON files via the ARB Feedback Portal. Attributes: file (FileField): Upload field for selecting a `.xls` or `.xlsx` file. submit (SubmitField): Form button to initiate upload. Examples: form = UploadForm() if form.validate_on_submit(): # handle file upload pass Notes: - Uses Flask-WTF and integrates with Bootstrap templates. - File extension restrictions enforced via `FileAllowed`. - Form is rendered in the Upload UI at `/upload`. \"\"\" file = FileField ( \"Choose Excel File\" , validators = [ DataRequired (), FileAllowed ([ 'xls' , 'xlsx' ], 'Excel files only!' )] ) submit = SubmitField ( \"Upload\" )","title":"arb.portal.wtf_upload"},{"location":"reference/arb/portal/wtf_upload/#arbportalwtf_upload","text":"WTForms-based upload form for the ARB Feedback Portal. Defines a minimal form used to upload Excel files via the web interface. Typically used in the /upload route. Module_Attributes UploadForm (type): WTForms form class for file uploads. logger (logging.Logger): Logger instance for this module. Examples: form = UploadForm() if form.validate_on_submit(): # handle file upload pass Notes Leverages Flask-WTF integration with Bootstrap-compatible rendering. Additional validation for file size or filename may be added externally. The logger emits a debug message when this file is loaded.","title":"arb.portal.wtf_upload"},{"location":"reference/arb/portal/wtf_upload/#arb.portal.wtf_upload.UploadForm","text":"Bases: FlaskForm WTForm for uploading Excel or JSON files via the ARB Feedback Portal. Attributes: file ( FileField ) \u2013 Upload field for selecting a .xls or .xlsx file. submit ( SubmitField ) \u2013 Form button to initiate upload. Examples: form = UploadForm() if form.validate_on_submit(): # handle file upload pass Notes Uses Flask-WTF and integrates with Bootstrap templates. File extension restrictions enforced via FileAllowed . Form is rendered in the Upload UI at /upload . Source code in arb\\portal\\wtf_upload.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 class UploadForm ( FlaskForm ): \"\"\" WTForm for uploading Excel or JSON files via the ARB Feedback Portal. Attributes: file (FileField): Upload field for selecting a `.xls` or `.xlsx` file. submit (SubmitField): Form button to initiate upload. Examples: form = UploadForm() if form.validate_on_submit(): # handle file upload pass Notes: - Uses Flask-WTF and integrates with Bootstrap templates. - File extension restrictions enforced via `FileAllowed`. - Form is rendered in the Upload UI at `/upload`. \"\"\" file = FileField ( \"Choose Excel File\" , validators = [ DataRequired (), FileAllowed ([ 'xls' , 'xlsx' ], 'Excel files only!' )] ) submit = SubmitField ( \"Upload\" )","title":"UploadForm"},{"location":"reference/arb/portal/config/accessors/","text":"arb.portal.config.accessors Config accessors for centralized and typed Flask config use. This module provides centralized access to Flask current_app.config settings using typed functions. Centralizing config usage improves maintainability, enables easier testing, and reduces repetition across modules. Attributes: get_processed_versions_dir ( function ) \u2013 Returns the processed versions directory. get_upload_folder ( function ) \u2013 Returns the upload folder path. get_payload_save_dir ( function ) \u2013 Returns the payload save directory. get_app_mode ( function ) \u2013 Returns the current app mode. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.config.accessors import get_upload_folder upload_path = get_upload_folder() Notes All accessors raise KeyError if the required config key is missing (except get_app_mode). get_app_mode defaults to 'dev' if not set. get_app_mode () Returns the mode the app is running in (e.g., 'dev', 'prod'). Returns: str ( str ) \u2013 Configured application mode. Defaults to 'dev' if not explicitly set. Examples: mode = get_app_mode() Returns 'dev' or the configured app mode Source code in arb\\portal\\config\\accessors.py 84 85 86 87 88 89 90 91 92 93 94 95 def get_app_mode () -> str : \"\"\" Returns the mode the app is running in (e.g., 'dev', 'prod'). Returns: str: Configured application mode. Defaults to 'dev' if not explicitly set. Examples: mode = get_app_mode() # Returns 'dev' or the configured app mode \"\"\" return current_app . config . get ( \"APP_MODE\" , \"dev\" ) get_database_uri () Returns the SQLAlchemy database URI from the Flask app configuration. Returns: str ( str ) \u2013 The configured SQLAlchemy database URI. Raises: KeyError \u2013 If 'SQLALCHEMY_DATABASE_URI' is not in the config. Examples: db_uri = get_database_uri() Returns the database connection string Source code in arb\\portal\\config\\accessors.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def get_database_uri () -> str : \"\"\" Returns the SQLAlchemy database URI from the Flask app configuration. Returns: str: The configured SQLAlchemy database URI. Raises: KeyError: If 'SQLALCHEMY_DATABASE_URI' is not in the config. Examples: db_uri = get_database_uri() # Returns the database connection string \"\"\" return current_app . config [ \"SQLALCHEMY_DATABASE_URI\" ] get_payload_save_dir () Returns the default path for saving intermediate JSON payloads. Returns: Path ( Path ) \u2013 Directory path where payloads are saved. Raises: KeyError \u2013 If 'PAYLOAD_SAVE_DIR' is not in the config. Examples: payload_dir = get_payload_save_dir() Returns Path object for payload save directory Source code in arb\\portal\\config\\accessors.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def get_payload_save_dir () -> Path : \"\"\" Returns the default path for saving intermediate JSON payloads. Returns: Path: Directory path where payloads are saved. Raises: KeyError: If 'PAYLOAD_SAVE_DIR' is not in the config. Examples: payload_dir = get_payload_save_dir() # Returns Path object for payload save directory \"\"\" return Path ( current_app . config [ \"PAYLOAD_SAVE_DIR\" ]) get_processed_versions_dir () Returns the root directory where processed Excel versions are stored. Returns: Path ( Path ) \u2013 Absolute path to the directory. Raises: KeyError \u2013 If 'PROCESSED_VERSIONS_DIR' is not in the config. Examples: processed_dir = get_processed_versions_dir() Returns Path object for processed versions directory Source code in arb\\portal\\config\\accessors.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def get_processed_versions_dir () -> Path : \"\"\" Returns the root directory where processed Excel versions are stored. Returns: Path: Absolute path to the directory. Raises: KeyError: If 'PROCESSED_VERSIONS_DIR' is not in the config. Examples: processed_dir = get_processed_versions_dir() # Returns Path object for processed versions directory \"\"\" return Path ( current_app . config [ \"PROCESSED_VERSIONS_DIR\" ]) get_upload_folder () Returns the folder where uploaded files are saved. Returns: Path ( Path ) \u2013 Absolute path to the upload directory. Raises: KeyError \u2013 If 'UPLOAD_FOLDER' is not in the config. Examples: upload_dir = get_upload_folder() Returns Path object for upload directory Source code in arb\\portal\\config\\accessors.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def get_upload_folder () -> Path : \"\"\" Returns the folder where uploaded files are saved. Returns: Path: Absolute path to the upload directory. Raises: KeyError: If 'UPLOAD_FOLDER' is not in the config. Examples: upload_dir = get_upload_folder() # Returns Path object for upload directory \"\"\" return Path ( current_app . config [ \"UPLOAD_FOLDER\" ])","title":"arb.portal.config.accessors"},{"location":"reference/arb/portal/config/accessors/#arbportalconfigaccessors","text":"Config accessors for centralized and typed Flask config use. This module provides centralized access to Flask current_app.config settings using typed functions. Centralizing config usage improves maintainability, enables easier testing, and reduces repetition across modules. Attributes: get_processed_versions_dir ( function ) \u2013 Returns the processed versions directory. get_upload_folder ( function ) \u2013 Returns the upload folder path. get_payload_save_dir ( function ) \u2013 Returns the payload save directory. get_app_mode ( function ) \u2013 Returns the current app mode. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.config.accessors import get_upload_folder upload_path = get_upload_folder() Notes All accessors raise KeyError if the required config key is missing (except get_app_mode). get_app_mode defaults to 'dev' if not set.","title":"arb.portal.config.accessors"},{"location":"reference/arb/portal/config/accessors/#arb.portal.config.accessors.get_app_mode","text":"Returns the mode the app is running in (e.g., 'dev', 'prod'). Returns: str ( str ) \u2013 Configured application mode. Defaults to 'dev' if not explicitly set. Examples: mode = get_app_mode()","title":"get_app_mode"},{"location":"reference/arb/portal/config/accessors/#arb.portal.config.accessors.get_app_mode--returns-dev-or-the-configured-app-mode","text":"Source code in arb\\portal\\config\\accessors.py 84 85 86 87 88 89 90 91 92 93 94 95 def get_app_mode () -> str : \"\"\" Returns the mode the app is running in (e.g., 'dev', 'prod'). Returns: str: Configured application mode. Defaults to 'dev' if not explicitly set. Examples: mode = get_app_mode() # Returns 'dev' or the configured app mode \"\"\" return current_app . config . get ( \"APP_MODE\" , \"dev\" )","title":"Returns 'dev' or the configured app mode"},{"location":"reference/arb/portal/config/accessors/#arb.portal.config.accessors.get_database_uri","text":"Returns the SQLAlchemy database URI from the Flask app configuration. Returns: str ( str ) \u2013 The configured SQLAlchemy database URI. Raises: KeyError \u2013 If 'SQLALCHEMY_DATABASE_URI' is not in the config. Examples: db_uri = get_database_uri()","title":"get_database_uri"},{"location":"reference/arb/portal/config/accessors/#arb.portal.config.accessors.get_database_uri--returns-the-database-connection-string","text":"Source code in arb\\portal\\config\\accessors.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def get_database_uri () -> str : \"\"\" Returns the SQLAlchemy database URI from the Flask app configuration. Returns: str: The configured SQLAlchemy database URI. Raises: KeyError: If 'SQLALCHEMY_DATABASE_URI' is not in the config. Examples: db_uri = get_database_uri() # Returns the database connection string \"\"\" return current_app . config [ \"SQLALCHEMY_DATABASE_URI\" ]","title":"Returns the database connection string"},{"location":"reference/arb/portal/config/accessors/#arb.portal.config.accessors.get_payload_save_dir","text":"Returns the default path for saving intermediate JSON payloads. Returns: Path ( Path ) \u2013 Directory path where payloads are saved. Raises: KeyError \u2013 If 'PAYLOAD_SAVE_DIR' is not in the config. Examples: payload_dir = get_payload_save_dir()","title":"get_payload_save_dir"},{"location":"reference/arb/portal/config/accessors/#arb.portal.config.accessors.get_payload_save_dir--returns-path-object-for-payload-save-directory","text":"Source code in arb\\portal\\config\\accessors.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def get_payload_save_dir () -> Path : \"\"\" Returns the default path for saving intermediate JSON payloads. Returns: Path: Directory path where payloads are saved. Raises: KeyError: If 'PAYLOAD_SAVE_DIR' is not in the config. Examples: payload_dir = get_payload_save_dir() # Returns Path object for payload save directory \"\"\" return Path ( current_app . config [ \"PAYLOAD_SAVE_DIR\" ])","title":"Returns Path object for payload save directory"},{"location":"reference/arb/portal/config/accessors/#arb.portal.config.accessors.get_processed_versions_dir","text":"Returns the root directory where processed Excel versions are stored. Returns: Path ( Path ) \u2013 Absolute path to the directory. Raises: KeyError \u2013 If 'PROCESSED_VERSIONS_DIR' is not in the config. Examples: processed_dir = get_processed_versions_dir()","title":"get_processed_versions_dir"},{"location":"reference/arb/portal/config/accessors/#arb.portal.config.accessors.get_processed_versions_dir--returns-path-object-for-processed-versions-directory","text":"Source code in arb\\portal\\config\\accessors.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def get_processed_versions_dir () -> Path : \"\"\" Returns the root directory where processed Excel versions are stored. Returns: Path: Absolute path to the directory. Raises: KeyError: If 'PROCESSED_VERSIONS_DIR' is not in the config. Examples: processed_dir = get_processed_versions_dir() # Returns Path object for processed versions directory \"\"\" return Path ( current_app . config [ \"PROCESSED_VERSIONS_DIR\" ])","title":"Returns Path object for processed versions directory"},{"location":"reference/arb/portal/config/accessors/#arb.portal.config.accessors.get_upload_folder","text":"Returns the folder where uploaded files are saved. Returns: Path ( Path ) \u2013 Absolute path to the upload directory. Raises: KeyError \u2013 If 'UPLOAD_FOLDER' is not in the config. Examples: upload_dir = get_upload_folder()","title":"get_upload_folder"},{"location":"reference/arb/portal/config/accessors/#arb.portal.config.accessors.get_upload_folder--returns-path-object-for-upload-directory","text":"Source code in arb\\portal\\config\\accessors.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def get_upload_folder () -> Path : \"\"\" Returns the folder where uploaded files are saved. Returns: Path: Absolute path to the upload directory. Raises: KeyError: If 'UPLOAD_FOLDER' is not in the config. Examples: upload_dir = get_upload_folder() # Returns Path object for upload directory \"\"\" return Path ( current_app . config [ \"UPLOAD_FOLDER\" ])","title":"Returns Path object for upload directory"},{"location":"reference/arb/portal/config/settings/","text":"arb.portal.config.settings Environment-specific configuration classes for the ARB Feedback Portal Flask application. Defines base and derived configuration classes used by the ARB portal. Each config class inherits from BaseConfig and may override environment-specific values. Attributes: BaseConfig ( class ) \u2013 Base configuration shared across all environments. DevelopmentConfig ( class ) \u2013 Configuration for local development. ProductionConfig ( class ) \u2013 Configuration for deployed production environments. TestingConfig ( class ) \u2013 Configuration for isolated testing environments. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.config.settings import DevelopmentConfig, ProductionConfig, TestingConfig app.config.from_object(DevelopmentConfig) Notes Static and environment-derived values belong here. Runtime-dependent settings (platform, CLI, etc.) should go in startup/runtime_info.py . BaseConfig Base configuration shared across all environments. Attributes: POSTGRES_DB_URI ( str ) \u2013 Default PostgresQL URI if DATABASE_URI is unset. SQLALCHEMY_ENGINE_OPTIONS ( dict ) \u2013 Connection settings for SQLAlchemy. SECRET_KEY ( str ) \u2013 Flask session key. SQLALCHEMY_DATABASE_URI ( str ) \u2013 Final URI used by the app. SQLALCHEMY_TRACK_MODIFICATIONS ( bool ) \u2013 SQLAlchemy event system flag. EXPLAIN_TEMPLATE_LOADING ( bool ) \u2013 Whether to trace template resolution errors. WTF_CSRF_ENABLED ( bool ) \u2013 Cross-site request forgery protection toggle. LOG_LEVEL ( str ) \u2013 Default logging level. TIMEZONE ( str ) \u2013 Target timezone for timestamp formatting. FAST_LOAD ( bool ) \u2013 Enables performance optimizations at startup. logger ( Logger ) \u2013 Logger instance for this module. Examples: app.config.from_object(BaseConfig) Notes All environment-specific configs inherit from this class. FAST_LOAD can be set via the FAST_LOAD environment variable. Source code in arb\\portal\\config\\settings.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class BaseConfig : \"\"\" Base configuration shared across all environments. Attributes: POSTGRES_DB_URI (str): Default PostgresQL URI if DATABASE_URI is unset. SQLALCHEMY_ENGINE_OPTIONS (dict): Connection settings for SQLAlchemy. SECRET_KEY (str): Flask session key. SQLALCHEMY_DATABASE_URI (str): Final URI used by the app. SQLALCHEMY_TRACK_MODIFICATIONS (bool): SQLAlchemy event system flag. EXPLAIN_TEMPLATE_LOADING (bool): Whether to trace template resolution errors. WTF_CSRF_ENABLED (bool): Cross-site request forgery protection toggle. LOG_LEVEL (str): Default logging level. TIMEZONE (str): Target timezone for timestamp formatting. FAST_LOAD (bool): Enables performance optimizations at startup. logger (logging.Logger): Logger instance for this module. Examples: app.config.from_object(BaseConfig) Notes: - All environment-specific configs inherit from this class. - FAST_LOAD can be set via the FAST_LOAD environment variable. \"\"\" # noinspection SpellCheckingInspection POSTGRES_DB_URI = ( 'postgresql+psycopg2://methane:methaneCH4@prj-bus-methane-aurora-postgresql-instance-1' '.cdae8kkz3fpi.us-west-2.rds.amazonaws.com/plumetracker' ) SQLALCHEMY_ENGINE_OPTIONS = { 'connect_args' : { # 'options': '-c search_path=satellite_tracker_demo1,public -c timezone=UTC' # practice schema 'options' : '-c search_path=satellite_tracker_new,public -c timezone=UTC' # dan's live schema } } SECRET_KEY = os . environ . get ( 'SECRET_KEY' ) or 'secret-key-goes-here' SQLALCHEMY_DATABASE_URI = os . environ . get ( 'DATABASE_URI' ) or POSTGRES_DB_URI SQLALCHEMY_TRACK_MODIFICATIONS = False # When enabled, Flask will log detailed information about templating files # consider setting to True if you're getting TemplateNotFound errors. EXPLAIN_TEMPLATE_LOADING = False # Recommended setting for most use cases. WTF_CSRF_ENABLED = True LOG_LEVEL = \"INFO\" TIMEZONE = \"America/Los_Angeles\" # --------------------------------------------------------------------- # Get/Set other relevant environmental variables here and commandline arguments. # for example: set FAST_LOAD=true # --------------------------------------------------------------------- FAST_LOAD = False # flask does not allow for custom arguments, so the next block is commented out # if \"--fast-load\" in sys.argv: # print(f\"--fast-load detected in CLI arguments\") # FAST_LOAD = True if os . getenv ( \"FAST_LOAD\" ) == \"true\" : logger . info ( f \"FAST_LOAD detected in CLI arguments\" ) FAST_LOAD = True logger . info ( f \" { FAST_LOAD = } \" ) DevelopmentConfig Bases: BaseConfig Configuration for local development. Attributes: DEBUG ( bool ) \u2013 Enables debug mode. FLASK_ENV ( str ) \u2013 Flask environment indicator. LOG_LEVEL ( str ) \u2013 Logging level (default: \"DEBUG\"). Examples: app.config.from_object(DevelopmentConfig) Notes Inherits from BaseConfig. Sets DEBUG to True and LOG_LEVEL to \"DEBUG\". Source code in arb\\portal\\config\\settings.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class DevelopmentConfig ( BaseConfig ): \"\"\" Configuration for local development. Attributes: DEBUG (bool): Enables debug mode. FLASK_ENV (str): Flask environment indicator. LOG_LEVEL (str): Logging level (default: \"DEBUG\"). Examples: app.config.from_object(DevelopmentConfig) Notes: - Inherits from BaseConfig. - Sets DEBUG to True and LOG_LEVEL to \"DEBUG\". \"\"\" DEBUG = True FLASK_ENV = \"development\" # EXPLAIN_TEMPLATE_LOADING = True LOG_LEVEL = \"DEBUG\" ProductionConfig Bases: BaseConfig Configuration for deployed production environments. Attributes: DEBUG ( bool ) \u2013 Disables debug features. FLASK_ENV ( str ) \u2013 Environment label for Flask runtime. WTF_CSRF_ENABLED ( bool ) \u2013 Enables CSRF protection. LOG_LEVEL ( str ) \u2013 Logging level (default: \"INFO\"). Examples: app.config.from_object(ProductionConfig) Notes Inherits from BaseConfig. Sets DEBUG to False and LOG_LEVEL to \"INFO\". Source code in arb\\portal\\config\\settings.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 class ProductionConfig ( BaseConfig ): \"\"\" Configuration for deployed production environments. Attributes: DEBUG (bool): Disables debug features. FLASK_ENV (str): Environment label for Flask runtime. WTF_CSRF_ENABLED (bool): Enables CSRF protection. LOG_LEVEL (str): Logging level (default: \"INFO\"). Examples: app.config.from_object(ProductionConfig) Notes: - Inherits from BaseConfig. - Sets DEBUG to False and LOG_LEVEL to \"INFO\". \"\"\" DEBUG = False FLASK_ENV = \"production\" WTF_CSRF_ENABLED = True LOG_LEVEL = \"INFO\" TestingConfig Bases: BaseConfig Configuration for isolated testing environments. Attributes: TESTING ( bool ) \u2013 Enables Flask test mode. DEBUG ( bool ) \u2013 Enables debug logging. FLASK_ENV ( str ) \u2013 Flask environment label. WTF_CSRF_ENABLED ( bool ) \u2013 Disables CSRF for test convenience. LOG_LEVEL ( str ) \u2013 Logging level (default: \"WARNING\"). Examples: app.config.from_object(TestingConfig) Notes Inherits from BaseConfig. Sets TESTING to True and LOG_LEVEL to \"WARNING\". Source code in arb\\portal\\config\\settings.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class TestingConfig ( BaseConfig ): \"\"\" Configuration for isolated testing environments. Attributes: TESTING (bool): Enables Flask test mode. DEBUG (bool): Enables debug logging. FLASK_ENV (str): Flask environment label. WTF_CSRF_ENABLED (bool): Disables CSRF for test convenience. LOG_LEVEL (str): Logging level (default: \"WARNING\"). Examples: app.config.from_object(TestingConfig) Notes: - Inherits from BaseConfig. - Sets TESTING to True and LOG_LEVEL to \"WARNING\". \"\"\" TESTING = True DEBUG = True FLASK_ENV = \"testing\" WTF_CSRF_ENABLED = False LOG_LEVEL = \"WARNING\"","title":"arb.portal.config.settings"},{"location":"reference/arb/portal/config/settings/#arbportalconfigsettings","text":"Environment-specific configuration classes for the ARB Feedback Portal Flask application. Defines base and derived configuration classes used by the ARB portal. Each config class inherits from BaseConfig and may override environment-specific values. Attributes: BaseConfig ( class ) \u2013 Base configuration shared across all environments. DevelopmentConfig ( class ) \u2013 Configuration for local development. ProductionConfig ( class ) \u2013 Configuration for deployed production environments. TestingConfig ( class ) \u2013 Configuration for isolated testing environments. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.config.settings import DevelopmentConfig, ProductionConfig, TestingConfig app.config.from_object(DevelopmentConfig) Notes Static and environment-derived values belong here. Runtime-dependent settings (platform, CLI, etc.) should go in startup/runtime_info.py .","title":"arb.portal.config.settings"},{"location":"reference/arb/portal/config/settings/#arb.portal.config.settings.BaseConfig","text":"Base configuration shared across all environments. Attributes: POSTGRES_DB_URI ( str ) \u2013 Default PostgresQL URI if DATABASE_URI is unset. SQLALCHEMY_ENGINE_OPTIONS ( dict ) \u2013 Connection settings for SQLAlchemy. SECRET_KEY ( str ) \u2013 Flask session key. SQLALCHEMY_DATABASE_URI ( str ) \u2013 Final URI used by the app. SQLALCHEMY_TRACK_MODIFICATIONS ( bool ) \u2013 SQLAlchemy event system flag. EXPLAIN_TEMPLATE_LOADING ( bool ) \u2013 Whether to trace template resolution errors. WTF_CSRF_ENABLED ( bool ) \u2013 Cross-site request forgery protection toggle. LOG_LEVEL ( str ) \u2013 Default logging level. TIMEZONE ( str ) \u2013 Target timezone for timestamp formatting. FAST_LOAD ( bool ) \u2013 Enables performance optimizations at startup. logger ( Logger ) \u2013 Logger instance for this module. Examples: app.config.from_object(BaseConfig) Notes All environment-specific configs inherit from this class. FAST_LOAD can be set via the FAST_LOAD environment variable. Source code in arb\\portal\\config\\settings.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class BaseConfig : \"\"\" Base configuration shared across all environments. Attributes: POSTGRES_DB_URI (str): Default PostgresQL URI if DATABASE_URI is unset. SQLALCHEMY_ENGINE_OPTIONS (dict): Connection settings for SQLAlchemy. SECRET_KEY (str): Flask session key. SQLALCHEMY_DATABASE_URI (str): Final URI used by the app. SQLALCHEMY_TRACK_MODIFICATIONS (bool): SQLAlchemy event system flag. EXPLAIN_TEMPLATE_LOADING (bool): Whether to trace template resolution errors. WTF_CSRF_ENABLED (bool): Cross-site request forgery protection toggle. LOG_LEVEL (str): Default logging level. TIMEZONE (str): Target timezone for timestamp formatting. FAST_LOAD (bool): Enables performance optimizations at startup. logger (logging.Logger): Logger instance for this module. Examples: app.config.from_object(BaseConfig) Notes: - All environment-specific configs inherit from this class. - FAST_LOAD can be set via the FAST_LOAD environment variable. \"\"\" # noinspection SpellCheckingInspection POSTGRES_DB_URI = ( 'postgresql+psycopg2://methane:methaneCH4@prj-bus-methane-aurora-postgresql-instance-1' '.cdae8kkz3fpi.us-west-2.rds.amazonaws.com/plumetracker' ) SQLALCHEMY_ENGINE_OPTIONS = { 'connect_args' : { # 'options': '-c search_path=satellite_tracker_demo1,public -c timezone=UTC' # practice schema 'options' : '-c search_path=satellite_tracker_new,public -c timezone=UTC' # dan's live schema } } SECRET_KEY = os . environ . get ( 'SECRET_KEY' ) or 'secret-key-goes-here' SQLALCHEMY_DATABASE_URI = os . environ . get ( 'DATABASE_URI' ) or POSTGRES_DB_URI SQLALCHEMY_TRACK_MODIFICATIONS = False # When enabled, Flask will log detailed information about templating files # consider setting to True if you're getting TemplateNotFound errors. EXPLAIN_TEMPLATE_LOADING = False # Recommended setting for most use cases. WTF_CSRF_ENABLED = True LOG_LEVEL = \"INFO\" TIMEZONE = \"America/Los_Angeles\" # --------------------------------------------------------------------- # Get/Set other relevant environmental variables here and commandline arguments. # for example: set FAST_LOAD=true # --------------------------------------------------------------------- FAST_LOAD = False # flask does not allow for custom arguments, so the next block is commented out # if \"--fast-load\" in sys.argv: # print(f\"--fast-load detected in CLI arguments\") # FAST_LOAD = True if os . getenv ( \"FAST_LOAD\" ) == \"true\" : logger . info ( f \"FAST_LOAD detected in CLI arguments\" ) FAST_LOAD = True logger . info ( f \" { FAST_LOAD = } \" )","title":"BaseConfig"},{"location":"reference/arb/portal/config/settings/#arb.portal.config.settings.DevelopmentConfig","text":"Bases: BaseConfig Configuration for local development. Attributes: DEBUG ( bool ) \u2013 Enables debug mode. FLASK_ENV ( str ) \u2013 Flask environment indicator. LOG_LEVEL ( str ) \u2013 Logging level (default: \"DEBUG\"). Examples: app.config.from_object(DevelopmentConfig) Notes Inherits from BaseConfig. Sets DEBUG to True and LOG_LEVEL to \"DEBUG\". Source code in arb\\portal\\config\\settings.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 class DevelopmentConfig ( BaseConfig ): \"\"\" Configuration for local development. Attributes: DEBUG (bool): Enables debug mode. FLASK_ENV (str): Flask environment indicator. LOG_LEVEL (str): Logging level (default: \"DEBUG\"). Examples: app.config.from_object(DevelopmentConfig) Notes: - Inherits from BaseConfig. - Sets DEBUG to True and LOG_LEVEL to \"DEBUG\". \"\"\" DEBUG = True FLASK_ENV = \"development\" # EXPLAIN_TEMPLATE_LOADING = True LOG_LEVEL = \"DEBUG\"","title":"DevelopmentConfig"},{"location":"reference/arb/portal/config/settings/#arb.portal.config.settings.ProductionConfig","text":"Bases: BaseConfig Configuration for deployed production environments. Attributes: DEBUG ( bool ) \u2013 Disables debug features. FLASK_ENV ( str ) \u2013 Environment label for Flask runtime. WTF_CSRF_ENABLED ( bool ) \u2013 Enables CSRF protection. LOG_LEVEL ( str ) \u2013 Logging level (default: \"INFO\"). Examples: app.config.from_object(ProductionConfig) Notes Inherits from BaseConfig. Sets DEBUG to False and LOG_LEVEL to \"INFO\". Source code in arb\\portal\\config\\settings.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 class ProductionConfig ( BaseConfig ): \"\"\" Configuration for deployed production environments. Attributes: DEBUG (bool): Disables debug features. FLASK_ENV (str): Environment label for Flask runtime. WTF_CSRF_ENABLED (bool): Enables CSRF protection. LOG_LEVEL (str): Logging level (default: \"INFO\"). Examples: app.config.from_object(ProductionConfig) Notes: - Inherits from BaseConfig. - Sets DEBUG to False and LOG_LEVEL to \"INFO\". \"\"\" DEBUG = False FLASK_ENV = \"production\" WTF_CSRF_ENABLED = True LOG_LEVEL = \"INFO\"","title":"ProductionConfig"},{"location":"reference/arb/portal/config/settings/#arb.portal.config.settings.TestingConfig","text":"Bases: BaseConfig Configuration for isolated testing environments. Attributes: TESTING ( bool ) \u2013 Enables Flask test mode. DEBUG ( bool ) \u2013 Enables debug logging. FLASK_ENV ( str ) \u2013 Flask environment label. WTF_CSRF_ENABLED ( bool ) \u2013 Disables CSRF for test convenience. LOG_LEVEL ( str ) \u2013 Logging level (default: \"WARNING\"). Examples: app.config.from_object(TestingConfig) Notes Inherits from BaseConfig. Sets TESTING to True and LOG_LEVEL to \"WARNING\". Source code in arb\\portal\\config\\settings.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 class TestingConfig ( BaseConfig ): \"\"\" Configuration for isolated testing environments. Attributes: TESTING (bool): Enables Flask test mode. DEBUG (bool): Enables debug logging. FLASK_ENV (str): Flask environment label. WTF_CSRF_ENABLED (bool): Disables CSRF for test convenience. LOG_LEVEL (str): Logging level (default: \"WARNING\"). Examples: app.config.from_object(TestingConfig) Notes: - Inherits from BaseConfig. - Sets TESTING to True and LOG_LEVEL to \"WARNING\". \"\"\" TESTING = True DEBUG = True FLASK_ENV = \"testing\" WTF_CSRF_ENABLED = False LOG_LEVEL = \"WARNING\"","title":"TestingConfig"},{"location":"reference/arb/portal/startup/db/","text":"arb.portal.startup.db Database initialization and reflection routines for the ARB Feedback Portal. These functions are intended to be called during Flask app startup (from create_app() ) to configure SQLAlchemy metadata, initialize models, and create missing tables. Attributes: logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.startup.db import reflect_database, db_initialize_and_create reflect_database() db_initialize_and_create() Notes SQLAlchemy models must be explicitly imported to register before table creation. Logging is enabled throughout to trace database state and startup flow. The logger emits a debug message when this file is loaded. db_create () Create all tables defined in SQLAlchemy metadata if they don\u2019t exist. Examples: db_create() Creates all missing tables in the database Notes Skips creation if FAST_LOAD=True is set in the app config. Logs warnings and info for tracing. Source code in arb\\portal\\startup\\db.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def db_create () -> None : \"\"\" Create all tables defined in SQLAlchemy metadata if they don\u2019t exist. Examples: db_create() # Creates all missing tables in the database Notes: - Skips creation if FAST_LOAD=True is set in the app config. - Logs warnings and info for tracing. \"\"\" if current_app . config . get ( \"FAST_LOAD\" , False ) is True : logger . warning ( f \"Skipping table creation for FAST_LOAD=True.\" ) return logger . info ( f \"Creating all missing tables.\" ) db . create_all () logger . debug ( f \"Database schema created.\" ) db_initialize () Import and register SQLAlchemy ORM models. Examples: db_initialize() Registers all models for table creation Notes Import must be executed (even if unused) to register models. Source code in arb\\portal\\startup\\db.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def db_initialize () -> None : \"\"\" Import and register SQLAlchemy ORM models. Examples: db_initialize() # Registers all models for table creation Notes: - Import must be executed (even if unused) to register models. \"\"\" logger . info ( f \"Initializing database models.\" ) # Add model registration below # noinspection PyUnresolvedReferences import arb.portal.sqla_models as models db_initialize_and_create () Register models and create missing tables in one call. Examples: db_initialize_and_create() Registers models and ensures all tables exist Notes Combines db_initialize() and db_create() for convenience. Logs info upon successful database initialization. Source code in arb\\portal\\startup\\db.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def db_initialize_and_create () -> None : \"\"\" Register models and create missing tables in one call. Examples: db_initialize_and_create() # Registers models and ensures all tables exist Notes: - Combines db_initialize() and db_create() for convenience. - Logs info upon successful database initialization. \"\"\" db_initialize () db_create () logger . info ( f \"Database initialized and tables ensured.\" ) reflect_database () Reflect the existing database into SQLAlchemy metadata. Examples: reflect_database() Reflects all tables into SQLAlchemy metadata Notes Enables access to existing tables even without defined ORM models. Logs info and debug messages for tracing. Source code in arb\\portal\\startup\\db.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def reflect_database () -> None : \"\"\" Reflect the existing database into SQLAlchemy metadata. Examples: reflect_database() # Reflects all tables into SQLAlchemy metadata Notes: - Enables access to existing tables even without defined ORM models. - Logs info and debug messages for tracing. \"\"\" logger . info ( f \"Reflecting database metadata.\" ) try : logger . info ( f \"Database engine URI: { db . engine . url } \" ) db . metadata . reflect ( bind = db . engine ) logger . debug ( f \"Reflection complete.\" ) logger . info ( f \"Reflected tables: { list ( db . metadata . tables . keys ()) } \" ) except Exception as e : logger . error ( f \"Error during database reflection: { e } \" )","title":"arb.portal.startup.db"},{"location":"reference/arb/portal/startup/db/#arbportalstartupdb","text":"Database initialization and reflection routines for the ARB Feedback Portal. These functions are intended to be called during Flask app startup (from create_app() ) to configure SQLAlchemy metadata, initialize models, and create missing tables. Attributes: logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.startup.db import reflect_database, db_initialize_and_create reflect_database() db_initialize_and_create() Notes SQLAlchemy models must be explicitly imported to register before table creation. Logging is enabled throughout to trace database state and startup flow. The logger emits a debug message when this file is loaded.","title":"arb.portal.startup.db"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.db_create","text":"Create all tables defined in SQLAlchemy metadata if they don\u2019t exist. Examples: db_create()","title":"db_create"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.db_create--creates-all-missing-tables-in-the-database","text":"Notes Skips creation if FAST_LOAD=True is set in the app config. Logs warnings and info for tracing. Source code in arb\\portal\\startup\\db.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def db_create () -> None : \"\"\" Create all tables defined in SQLAlchemy metadata if they don\u2019t exist. Examples: db_create() # Creates all missing tables in the database Notes: - Skips creation if FAST_LOAD=True is set in the app config. - Logs warnings and info for tracing. \"\"\" if current_app . config . get ( \"FAST_LOAD\" , False ) is True : logger . warning ( f \"Skipping table creation for FAST_LOAD=True.\" ) return logger . info ( f \"Creating all missing tables.\" ) db . create_all () logger . debug ( f \"Database schema created.\" )","title":"Creates all missing tables in the database"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.db_initialize","text":"Import and register SQLAlchemy ORM models. Examples: db_initialize()","title":"db_initialize"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.db_initialize--registers-all-models-for-table-creation","text":"Notes Import must be executed (even if unused) to register models. Source code in arb\\portal\\startup\\db.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def db_initialize () -> None : \"\"\" Import and register SQLAlchemy ORM models. Examples: db_initialize() # Registers all models for table creation Notes: - Import must be executed (even if unused) to register models. \"\"\" logger . info ( f \"Initializing database models.\" ) # Add model registration below # noinspection PyUnresolvedReferences import arb.portal.sqla_models as models","title":"Registers all models for table creation"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.db_initialize_and_create","text":"Register models and create missing tables in one call. Examples: db_initialize_and_create()","title":"db_initialize_and_create"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.db_initialize_and_create--registers-models-and-ensures-all-tables-exist","text":"Notes Combines db_initialize() and db_create() for convenience. Logs info upon successful database initialization. Source code in arb\\portal\\startup\\db.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def db_initialize_and_create () -> None : \"\"\" Register models and create missing tables in one call. Examples: db_initialize_and_create() # Registers models and ensures all tables exist Notes: - Combines db_initialize() and db_create() for convenience. - Logs info upon successful database initialization. \"\"\" db_initialize () db_create () logger . info ( f \"Database initialized and tables ensured.\" )","title":"Registers models and ensures all tables exist"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.reflect_database","text":"Reflect the existing database into SQLAlchemy metadata. Examples: reflect_database()","title":"reflect_database"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.reflect_database--reflects-all-tables-into-sqlalchemy-metadata","text":"Notes Enables access to existing tables even without defined ORM models. Logs info and debug messages for tracing. Source code in arb\\portal\\startup\\db.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def reflect_database () -> None : \"\"\" Reflect the existing database into SQLAlchemy metadata. Examples: reflect_database() # Reflects all tables into SQLAlchemy metadata Notes: - Enables access to existing tables even without defined ORM models. - Logs info and debug messages for tracing. \"\"\" logger . info ( f \"Reflecting database metadata.\" ) try : logger . info ( f \"Database engine URI: { db . engine . url } \" ) db . metadata . reflect ( bind = db . engine ) logger . debug ( f \"Reflection complete.\" ) logger . info ( f \"Reflected tables: { list ( db . metadata . tables . keys ()) } \" ) except Exception as e : logger . error ( f \"Error during database reflection: { e } \" )","title":"Reflects all tables into SQLAlchemy metadata"},{"location":"reference/arb/portal/startup/flask/","text":"arb.portal.startup.flask Flask-specific application setup utilities for the ARB Feedback Portal. This module configures Flask app behavior, including: - Jinja2 environment customization - Upload limits and paths - Flask logger settings - Custom template filters and globals Attributes: configure_flask_app ( function ) \u2013 Applies global configuration to a Flask app instance. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.startup.flask import configure_flask_app app = Flask( name ) configure_flask_app(app) Notes Should be invoked during application factory setup. The logger emits a debug message when this file is loaded. configure_flask_app ( app ) Apply global configuration to the Flask app instance. Parameters: app ( Flask ) \u2013 The Flask application to configure. Returns: None \u2013 None Examples: from arb.portal.startup.flask import configure_flask_app app = Flask( name ) configure_flask_app(app) Configures Jinja2 environment: Enables strict mode for undefined variables Trims and left-strips whitespace blocks Registers custom filters and timezone globals Upload settings: Sets UPLOAD_FOLDER to the shared upload path (as a string for Flask compatibility) Limits MAX_CONTENT_LENGTH to 16MB Logger: Applies LOG_LEVEL from app config Disables Werkzeug color log markup Notes Should be called before registering blueprints or running the app. Modifies app config and Jinja environment in place. UPLOAD_FOLDER is always set as a string (not a Path object) for maximum compatibility with Flask and extensions. Source code in arb\\portal\\startup\\flask.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def configure_flask_app ( app : Flask ) -> None : \"\"\" Apply global configuration to the Flask app instance. Args: app (Flask): The Flask application to configure. Returns: None Examples: from arb.portal.startup.flask import configure_flask_app app = Flask(__name__) configure_flask_app(app) Configures: - Jinja2 environment: * Enables strict mode for undefined variables * Trims and left-strips whitespace blocks * Registers custom filters and timezone globals - Upload settings: * Sets `UPLOAD_FOLDER` to the shared upload path (as a string for Flask compatibility) * Limits `MAX_CONTENT_LENGTH` to 16MB - Logger: * Applies `LOG_LEVEL` from app config * Disables Werkzeug color log markup Notes: - Should be called before registering blueprints or running the app. - Modifies app config and Jinja environment in place. - `UPLOAD_FOLDER` is always set as a string (not a Path object) for maximum compatibility with Flask and extensions. \"\"\" logger . debug ( f \"configure_flask_app() called\" ) app . jinja_env . globals [ \"app_name\" ] = \"CARB Feedback Portal\" # ------------------------------------------------------------------------- # Logging Configuration # ------------------------------------------------------------------------- logger . setLevel ( app . config . get ( \"LOG_LEVEL\" , \"INFO\" )) # Logging: Turn off color coding (avoids special terminal characters in the log file) werkzeug . serving . _log_add_style = False # ------------------------------------------------------------------------- # Upload Configuration # ------------------------------------------------------------------------- app . config [ 'UPLOAD_FOLDER' ] = str ( UPLOAD_PATH ) # Always store as string for Flask compatibility app . config [ 'MAX_CONTENT_LENGTH' ] = 16 * 1024 * 1024 # 16MB max upload # ------------------------------------------------------------------------- # Jinja Configuration # ------------------------------------------------------------------------- # Next two lines help auto-reload HTML changes and warn about missing blocks or undefined variables. app . config [ \"TEMPLATES_AUTO_RELOAD\" ] = True app . jinja_env . auto_reload = True app . jinja_env . undefined = StrictUndefined # Jinja: Trim whitespace before/after {{ }} text injection app . jinja_env . trim_blocks = True app . jinja_env . lstrip_blocks = True # Jinja: custom filters for debugging and string manipulation app . jinja_env . filters [ 'debug' ] = diag_recursive app . jinja_env . filters [ 'args_to_string' ] = args_to_string app . jinja_env . filters [ 'utc_iso_str_to_ca_str' ] = utc_iso_str_to_ca_str # Jinja: expose Python ZoneInfo class to templates for local time conversion app . jinja_env . globals [ \"california_tz\" ] = ZoneInfo ( \"America/Los_Angeles\" ) logger . debug ( f \"Flask Jinja2 globals and logging initialized.\" )","title":"arb.portal.startup.flask"},{"location":"reference/arb/portal/startup/flask/#arbportalstartupflask","text":"Flask-specific application setup utilities for the ARB Feedback Portal. This module configures Flask app behavior, including: - Jinja2 environment customization - Upload limits and paths - Flask logger settings - Custom template filters and globals Attributes: configure_flask_app ( function ) \u2013 Applies global configuration to a Flask app instance. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.startup.flask import configure_flask_app app = Flask( name ) configure_flask_app(app) Notes Should be invoked during application factory setup. The logger emits a debug message when this file is loaded.","title":"arb.portal.startup.flask"},{"location":"reference/arb/portal/startup/flask/#arb.portal.startup.flask.configure_flask_app","text":"Apply global configuration to the Flask app instance. Parameters: app ( Flask ) \u2013 The Flask application to configure. Returns: None \u2013 None Examples: from arb.portal.startup.flask import configure_flask_app app = Flask( name ) configure_flask_app(app) Configures Jinja2 environment: Enables strict mode for undefined variables Trims and left-strips whitespace blocks Registers custom filters and timezone globals Upload settings: Sets UPLOAD_FOLDER to the shared upload path (as a string for Flask compatibility) Limits MAX_CONTENT_LENGTH to 16MB Logger: Applies LOG_LEVEL from app config Disables Werkzeug color log markup Notes Should be called before registering blueprints or running the app. Modifies app config and Jinja environment in place. UPLOAD_FOLDER is always set as a string (not a Path object) for maximum compatibility with Flask and extensions. Source code in arb\\portal\\startup\\flask.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def configure_flask_app ( app : Flask ) -> None : \"\"\" Apply global configuration to the Flask app instance. Args: app (Flask): The Flask application to configure. Returns: None Examples: from arb.portal.startup.flask import configure_flask_app app = Flask(__name__) configure_flask_app(app) Configures: - Jinja2 environment: * Enables strict mode for undefined variables * Trims and left-strips whitespace blocks * Registers custom filters and timezone globals - Upload settings: * Sets `UPLOAD_FOLDER` to the shared upload path (as a string for Flask compatibility) * Limits `MAX_CONTENT_LENGTH` to 16MB - Logger: * Applies `LOG_LEVEL` from app config * Disables Werkzeug color log markup Notes: - Should be called before registering blueprints or running the app. - Modifies app config and Jinja environment in place. - `UPLOAD_FOLDER` is always set as a string (not a Path object) for maximum compatibility with Flask and extensions. \"\"\" logger . debug ( f \"configure_flask_app() called\" ) app . jinja_env . globals [ \"app_name\" ] = \"CARB Feedback Portal\" # ------------------------------------------------------------------------- # Logging Configuration # ------------------------------------------------------------------------- logger . setLevel ( app . config . get ( \"LOG_LEVEL\" , \"INFO\" )) # Logging: Turn off color coding (avoids special terminal characters in the log file) werkzeug . serving . _log_add_style = False # ------------------------------------------------------------------------- # Upload Configuration # ------------------------------------------------------------------------- app . config [ 'UPLOAD_FOLDER' ] = str ( UPLOAD_PATH ) # Always store as string for Flask compatibility app . config [ 'MAX_CONTENT_LENGTH' ] = 16 * 1024 * 1024 # 16MB max upload # ------------------------------------------------------------------------- # Jinja Configuration # ------------------------------------------------------------------------- # Next two lines help auto-reload HTML changes and warn about missing blocks or undefined variables. app . config [ \"TEMPLATES_AUTO_RELOAD\" ] = True app . jinja_env . auto_reload = True app . jinja_env . undefined = StrictUndefined # Jinja: Trim whitespace before/after {{ }} text injection app . jinja_env . trim_blocks = True app . jinja_env . lstrip_blocks = True # Jinja: custom filters for debugging and string manipulation app . jinja_env . filters [ 'debug' ] = diag_recursive app . jinja_env . filters [ 'args_to_string' ] = args_to_string app . jinja_env . filters [ 'utc_iso_str_to_ca_str' ] = utc_iso_str_to_ca_str # Jinja: expose Python ZoneInfo class to templates for local time conversion app . jinja_env . globals [ \"california_tz\" ] = ZoneInfo ( \"America/Los_Angeles\" ) logger . debug ( f \"Flask Jinja2 globals and logging initialized.\" )","title":"configure_flask_app"},{"location":"reference/arb/portal/startup/runtime_info/","text":"arb.portal.startup.runtime_info Provides runtime metadata and dynamic paths for the application. This module defines Project root and key directories (uploads, logs, static) Operating system detection (Windows, Linux, macOS) Platform-level info useful for conditional behavior Diagnostic tools for runtime environment inspection Attributes: PROJECT_ROOT ( Path ) \u2013 Path to the project root directory. UPLOAD_PATH ( Path ) \u2013 Path to the uploads directory. LOG_DIR ( Path ) \u2013 Path to the logs directory. LOG_FILE ( Path ) \u2013 Path to the main log file. STATIC_DIR ( Path ) \u2013 Path to the static assets directory. IS_WINDOWS ( bool ) \u2013 True if running on Windows. IS_LINUX ( bool ) \u2013 True if running on Linux. IS_MAC ( bool ) \u2013 True if running on macOS. print_runtime_diagnostics ( function ) \u2013 Prints/logs runtime diagnostics. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.startup.runtime_info import ( PROJECT_ROOT, UPLOAD_PATH, LOG_DIR, IS_WINDOWS, IS_LINUX, IS_MAC, print_runtime_diagnostics) print_runtime_diagnostics() Notes The project root directory is assumed to be named \"feedback_portal\". Directory resolution is based on the known app structure. The logger emits a debug message when this file is loaded. The project root directory is assumed to be named \"feedback_portal\". If the app is run from: feedback_portal/source/production/arb/wsgi.py then directory resolution is: Path( file ).resolve().parents[0] \u2192 .../arb Path( file ).resolve().parents[1] \u2192 .../production Path( file ).resolve().parents[2] \u2192 .../source Path( file ).resolve().parents[3] \u2192 .../feedback_portal print_runtime_diagnostics () Print and log detected runtime paths and platform flags for debugging. Examples: print_runtime_diagnostics() Logs platform, directory, and path info for debugging Notes Outputs platform name, OS flags, and resolved paths. Uses the logger for info-level diagnostics. Source code in arb\\portal\\startup\\runtime_info.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def print_runtime_diagnostics () -> None : \"\"\" Print and log detected runtime paths and platform flags for debugging. Examples: print_runtime_diagnostics() # Logs platform, directory, and path info for debugging Notes: - Outputs platform name, OS flags, and resolved paths. - Uses the logger for info-level diagnostics. \"\"\" logger . info ( f \" { 'PLATFORM' : <20 } = { PLATFORM } \" ) logger . info ( f \" { 'IS_WINDOWS' : <20 } = { IS_WINDOWS } \" ) logger . info ( f \" { 'IS_LINUX' : <20 } = { IS_LINUX } \" ) logger . info ( f \" { 'IS_MAC' : <20 } = { IS_MAC } \" ) logger . info ( f \" { 'PROJECT_ROOT' : <20 } = { PROJECT_ROOT } \" ) logger . info ( f \" { 'UPLOAD_PATH' : <20 } = { UPLOAD_PATH } \" ) logger . info ( f \" { 'LOG_DIR' : <20 } = { LOG_DIR } \" ) logger . info ( f \" { 'STATIC_DIR' : <20 } = { STATIC_DIR } \" )","title":"arb.portal.startup.runtime_info"},{"location":"reference/arb/portal/startup/runtime_info/#arbportalstartupruntime_info","text":"Provides runtime metadata and dynamic paths for the application. This module defines Project root and key directories (uploads, logs, static) Operating system detection (Windows, Linux, macOS) Platform-level info useful for conditional behavior Diagnostic tools for runtime environment inspection Attributes: PROJECT_ROOT ( Path ) \u2013 Path to the project root directory. UPLOAD_PATH ( Path ) \u2013 Path to the uploads directory. LOG_DIR ( Path ) \u2013 Path to the logs directory. LOG_FILE ( Path ) \u2013 Path to the main log file. STATIC_DIR ( Path ) \u2013 Path to the static assets directory. IS_WINDOWS ( bool ) \u2013 True if running on Windows. IS_LINUX ( bool ) \u2013 True if running on Linux. IS_MAC ( bool ) \u2013 True if running on macOS. print_runtime_diagnostics ( function ) \u2013 Prints/logs runtime diagnostics. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.startup.runtime_info import ( PROJECT_ROOT, UPLOAD_PATH, LOG_DIR, IS_WINDOWS, IS_LINUX, IS_MAC, print_runtime_diagnostics) print_runtime_diagnostics() Notes The project root directory is assumed to be named \"feedback_portal\". Directory resolution is based on the known app structure. The logger emits a debug message when this file is loaded. The project root directory is assumed to be named \"feedback_portal\". If the app is run from: feedback_portal/source/production/arb/wsgi.py then directory resolution is: Path( file ).resolve().parents[0] \u2192 .../arb Path( file ).resolve().parents[1] \u2192 .../production Path( file ).resolve().parents[2] \u2192 .../source Path( file ).resolve().parents[3] \u2192 .../feedback_portal","title":"arb.portal.startup.runtime_info"},{"location":"reference/arb/portal/startup/runtime_info/#arb.portal.startup.runtime_info.print_runtime_diagnostics","text":"Print and log detected runtime paths and platform flags for debugging. Examples: print_runtime_diagnostics()","title":"print_runtime_diagnostics"},{"location":"reference/arb/portal/startup/runtime_info/#arb.portal.startup.runtime_info.print_runtime_diagnostics--logs-platform-directory-and-path-info-for-debugging","text":"Notes Outputs platform name, OS flags, and resolved paths. Uses the logger for info-level diagnostics. Source code in arb\\portal\\startup\\runtime_info.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def print_runtime_diagnostics () -> None : \"\"\" Print and log detected runtime paths and platform flags for debugging. Examples: print_runtime_diagnostics() # Logs platform, directory, and path info for debugging Notes: - Outputs platform name, OS flags, and resolved paths. - Uses the logger for info-level diagnostics. \"\"\" logger . info ( f \" { 'PLATFORM' : <20 } = { PLATFORM } \" ) logger . info ( f \" { 'IS_WINDOWS' : <20 } = { IS_WINDOWS } \" ) logger . info ( f \" { 'IS_LINUX' : <20 } = { IS_LINUX } \" ) logger . info ( f \" { 'IS_MAC' : <20 } = { IS_MAC } \" ) logger . info ( f \" { 'PROJECT_ROOT' : <20 } = { PROJECT_ROOT } \" ) logger . info ( f \" { 'UPLOAD_PATH' : <20 } = { UPLOAD_PATH } \" ) logger . info ( f \" { 'LOG_DIR' : <20 } = { LOG_DIR } \" ) logger . info ( f \" { 'STATIC_DIR' : <20 } = { STATIC_DIR } \" )","title":"Logs platform, directory, and path info for debugging"},{"location":"reference/arb/portal/utils/db_ingest_util/","text":"arb.portal.utils.db_ingest_util Database ingestion helpers for inserting or updating rows based on structured dictionaries, particularly those derived from Excel templates. This module provides Generic row ingestion from any dict using SQLAlchemy reflection Excel-specific wrapper for sector-based data (xl_dict_to_database) Attributes: logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.db_ingest_util import xl_dict_to_database, dict_to_database id_, sector = xl_dict_to_database(db, base, xl_dict) Notes Used by upload and staging routes to process Excel/JSON payloads. The logger emits a debug message when this file is loaded. convert_excel_to_json_if_valid ( file_path ) Convert an uploaded Excel or JSON file into a standardized JSON format, and return the output path and detected sector. Parameters: file_path ( Path ) \u2013 Path to the uploaded file (Excel or JSON). Returns: tuple [ Path | None, str | None] \u2013 tuple[Path | None, str | None]: - JSON file path (parsed or original), - sector string (if detected). Source code in arb\\portal\\utils\\db_ingest_util.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def convert_excel_to_json_if_valid ( file_path : Path ) -> tuple [ Path | None , str | None ]: \"\"\" Convert an uploaded Excel or JSON file into a standardized JSON format, and return the output path and detected sector. Args: file_path (Path): Path to the uploaded file (Excel or JSON). Returns: tuple[Path | None, str | None]: - JSON file path (parsed or original), - sector string (if detected). \"\"\" json_path = convert_upload_to_json ( file_path ) if json_path : logger . debug ( f \"File converted or passed through to JSON: { json_path } \" ) sector = extract_sector_from_json ( json_path ) return json_path , sector else : logger . warning ( f \"Unable to convert uploaded file to JSON: { file_path } \" ) return None , None convert_file_to_json_old ( file_path ) Depreciated. use convert_excel_to_json_if_valid instead. Convert an uploaded Excel file to a JSON file, if possible. Parameters: file_path ( Path ) \u2013 Path to the saved Excel file. Returns: Path | None \u2013 Path | None: Path to the generated JSON file, or None if conversion failed. Source code in arb\\portal\\utils\\db_ingest_util.py 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def convert_file_to_json_old ( file_path : Path ) -> Path | None : \"\"\" Depreciated. use convert_excel_to_json_if_valid instead. Convert an uploaded Excel file to a JSON file, if possible. Args: file_path (Path): Path to the saved Excel file. Returns: Path | None: Path to the generated JSON file, or None if conversion failed. \"\"\" json_file_path = get_json_file_name_old ( file_path ) if not json_file_path : logger . warning ( f \"File { file_path } could not be converted to JSON.\" ) return None logger . debug ( f \"Converted Excel to JSON: { json_file_path } \" ) return json_file_path dict_to_database ( db , base , data_dict , table_name = 'incidences' , primary_key = 'id_incidence' , json_field = 'misc_json' , dry_run = False ) Insert or update a row in the specified table using a dictionary payload. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy DB instance. base ( AutomapBase ) \u2013 Reflected model metadata. data_dict ( dict ) \u2013 Dictionary payload to insert/update. table_name ( str , default: 'incidences' ) \u2013 Table name to target. primary_key ( str , default: 'id_incidence' ) \u2013 Primary key column. json_field ( str , default: 'misc_json' ) \u2013 Name of JSON field for form payload. dry_run ( bool , default: False ) \u2013 If True, simulate logic without writing to DB. Returns: int ( int ) \u2013 The id_incidence (or equivalent PK) inferred from payload or model. Raises: ValueError \u2013 If data_dict is empty. AttributeError \u2013 If PK cannot be resolved. Examples: id_ = dict_to_database(db, base, data_dict) Inserts or updates the row in the database Notes Uses get_ensured_row and update_model_with_payload for safe upsert. Commits the session unless dry_run is True. Source code in arb\\portal\\utils\\db_ingest_util.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def dict_to_database ( db : SQLAlchemy , base : AutomapBase , data_dict : dict , table_name : str = \"incidences\" , primary_key : str = \"id_incidence\" , json_field : str = \"misc_json\" , dry_run : bool = False ) -> int : \"\"\" Insert or update a row in the specified table using a dictionary payload. Args: db (SQLAlchemy): SQLAlchemy DB instance. base (AutomapBase): Reflected model metadata. data_dict (dict): Dictionary payload to insert/update. table_name (str): Table name to target. primary_key (str): Primary key column. json_field (str): Name of JSON field for form payload. dry_run (bool): If True, simulate logic without writing to DB. Returns: int: The id_incidence (or equivalent PK) inferred from payload or model. Raises: ValueError: If data_dict is empty. AttributeError: If PK cannot be resolved. Examples: id_ = dict_to_database(db, base, data_dict) # Inserts or updates the row in the database Notes: - Uses get_ensured_row and update_model_with_payload for safe upsert. - Commits the session unless dry_run is True. \"\"\" from arb.utils.wtf_forms_util import update_model_with_payload if not data_dict : msg = \"Attempt to add empty entry to database\" logger . warning ( msg ) raise ValueError ( msg ) id_ = data_dict . get ( primary_key ) model , id_ , is_new_row = get_ensured_row ( db = db , base = base , table_name = table_name , primary_key_name = primary_key , id_ = id_ ) if is_new_row : logger . debug ( f \"Backfilling { primary_key } = { id_ } into payload\" ) data_dict [ primary_key ] = id_ update_model_with_payload ( model , data_dict , json_field = json_field ) if not dry_run : db . session . add ( model ) db . session . commit () # Final safety: extract final PK from the model try : return getattr ( model , primary_key ) except AttributeError as e : logger . error ( f \"Model has no attribute ' { primary_key } ': { e } \" ) raise extract_sector_from_json ( json_path ) Extract the sector name from a JSON file generated from Excel. Parameters: json_path ( Path ) \u2013 Path to the JSON file. Returns: str | None \u2013 str | None: The sector name if found; otherwise, None. Source code in arb\\portal\\utils\\db_ingest_util.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 def extract_sector_from_json ( json_path : Path ) -> str | None : \"\"\" Extract the sector name from a JSON file generated from Excel. Args: json_path (Path): Path to the JSON file. Returns: str | None: The sector name if found; otherwise, None. \"\"\" try : json_data , _ = json_load_with_meta ( json_path ) return json_data . get ( \"metadata\" , {}) . get ( \"sector\" ) except Exception as e : logger . warning ( f \"Could not extract sector from { json_path } : { e } \" ) return None extract_tab_and_sector ( xl_dict , tab_name = 'Feedback Form' ) Extract form data from Excel-parsed JSON and include sector from metadata. Parameters: xl_dict ( dict ) \u2013 Parsed Excel document with 'metadata' and 'tab_contents'. tab_name ( str , default: 'Feedback Form' ) \u2013 Name of the worksheet tab to extract. Defaults to 'Feedback Form'. Returns: dict ( dict ) \u2013 Combined payload with tab_contents and sector included. Raises: ValueError \u2013 If the tab_name is missing or sector cannot be determined. Examples: form_data = extract_tab_and_sector(xl_dict) Returns a dict with form data and sector Notes Ensures consistency between staged and production uploads. Logs a warning if sector is missing in metadata. Source code in arb\\portal\\utils\\db_ingest_util.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def extract_tab_and_sector ( xl_dict : dict , tab_name : str = \"Feedback Form\" ) -> dict : \"\"\" Extract form data from Excel-parsed JSON and include sector from metadata. Args: xl_dict (dict): Parsed Excel document with 'metadata' and 'tab_contents'. tab_name (str): Name of the worksheet tab to extract. Defaults to 'Feedback Form'. Returns: dict: Combined payload with tab_contents and sector included. Raises: ValueError: If the tab_name is missing or sector cannot be determined. Examples: form_data = extract_tab_and_sector(xl_dict) # Returns a dict with form data and sector Notes: - Ensures consistency between staged and production uploads. - Logs a warning if sector is missing in metadata. \"\"\" if \"tab_contents\" not in xl_dict or tab_name not in xl_dict [ \"tab_contents\" ]: raise ValueError ( f \"Tab ' { tab_name } ' not found in xl_dict\" ) # Extract sector from metadata metadata = xl_dict . get ( \"metadata\" , {}) sector = metadata . get ( \"sector\" ) if not sector : logger . warning ( f \"No sector found in xl_dict metadata: { metadata } \" ) sector = \"Unknown\" # Fallback to prevent errors # Get form data and add sector form_data = xl_dict [ \"tab_contents\" ][ tab_name ] . copy () form_data [ \"sector\" ] = sector logger . debug ( f \"extract_tab_and_sector: extracted form data with sector ' { sector } ' from tab ' { tab_name } '\" ) return form_data json_file_to_db ( db , file_name , base , dry_run = False ) Parse and optionally insert a structured JSON file into DB. Parameters: db ( SQLAlchemy ) \u2013 DB engine. file_name ( Path ) \u2013 Path to .json file. base ( AutomapBase ) \u2013 Reflected schema. dry_run ( bool , default: False ) \u2013 If True, simulate insert only. Returns: tuple [ int , str ] \u2013 tuple[int, str]: (id_incidence, sector) Examples: id_, sector = json_file_to_db(db, \"file.json\", base) Loads JSON and inserts or stages the data Source code in arb\\portal\\utils\\db_ingest_util.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def json_file_to_db ( db : SQLAlchemy , file_name : str | Path , base : AutomapBase , dry_run : bool = False ) -> tuple [ int , str ]: \"\"\" Parse and optionally insert a structured JSON file into DB. Args: db (SQLAlchemy): DB engine. file_name (Path): Path to .json file. base (AutomapBase): Reflected schema. dry_run (bool): If True, simulate insert only. Returns: tuple[int, str]: (id_incidence, sector) Examples: id_, sector = json_file_to_db(db, \"file.json\", base) # Loads JSON and inserts or stages the data \"\"\" # todo - datetime - looks like this is where the json file gets loaded json_as_dict , metadata = json_load_with_meta ( file_name ) return xl_dict_to_database ( db , base , json_as_dict , dry_run = dry_run ) store_staged_payload ( id_ , sector , json_data ) Save a parsed but uncommitted JSON payload to a staging directory. Parameters: id_ ( int ) \u2013 Incidence ID. sector ( str ) \u2013 Sector name (used for file naming). json_data ( dict ) \u2013 Parsed JSON dictionary to save. Returns: Path ( Path ) \u2013 Path to the saved staging file. Source code in arb\\portal\\utils\\db_ingest_util.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 def store_staged_payload ( id_ : int , sector : str , json_data : dict ) -> Path : \"\"\" Save a parsed but uncommitted JSON payload to a staging directory. Args: id_ (int): Incidence ID. sector (str): Sector name (used for file naming). json_data (dict): Parsed JSON dictionary to save. Returns: Path: Path to the saved staging file. \"\"\" from arb.utils.io_wrappers import save_json_safely staging_dir = Path ( get_upload_folder ()) / \"staging\" staging_dir . mkdir ( parents = True , exist_ok = True ) file_name = f \"id_ { id_ } _ { sector . lower () } .json\" path = staging_dir / file_name save_json_safely ( json_data , path ) return path upload_and_stage_only ( db , upload_dir , request_file , base ) Save an uploaded Excel or JSON file, convert to JSON, and stage it for review. This function mimics upload_and_update_db() to ensure parity, but differs in that: - It does NOT update the database. - It returns the parsed JSON dict for review purposes. - It saves the current DB misc_json as 'base_misc_json' in the staged file's metadata. - It uses a timestamped filename for the staged file. - It ensures all values are JSON-serializable (datetime \u2192 ISO strings, etc.) before staging. Parameters: db ( SQLAlchemy ) \u2013 Active SQLAlchemy database instance. upload_dir ( str | Path ) \u2013 Target upload folder path. request_file ( FileStorage ) \u2013 Uploaded file from Flask request. base ( AutomapBase ) \u2013 Reflected metadata (currently unused but passed for consistency). Returns: Path \u2013 tuple[Path, int | None, str | None, dict, str]: Saved file path, extracted id_incidence, int | None \u2013 sector name, parsed JSON contents, and the staged filename (not full path). Examples: file_path, id_, sector, json_data, staged_filename = upload_and_stage_only(db, upload_dir, request_file, base) Handles upload, conversion, and staging Notes Staging will be blocked if id_incidence is missing or not a valid positive integer. Source code in arb\\portal\\utils\\db_ingest_util.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def upload_and_stage_only ( db : SQLAlchemy , upload_dir : str | Path , request_file : FileStorage , base : AutomapBase ) -> tuple [ Path , int | None , str | None , dict , str ]: \"\"\" Save an uploaded Excel or JSON file, convert to JSON, and stage it for review. This function mimics upload_and_update_db() to ensure parity, but differs in that: - It does NOT update the database. - It returns the parsed JSON dict for review purposes. - It saves the current DB misc_json as 'base_misc_json' in the staged file's metadata. - It uses a timestamped filename for the staged file. - It ensures all values are JSON-serializable (datetime \u2192 ISO strings, etc.) before staging. Args: db (SQLAlchemy): Active SQLAlchemy database instance. upload_dir (str | Path): Target upload folder path. request_file (FileStorage): Uploaded file from Flask request. base (AutomapBase): Reflected metadata (currently unused but passed for consistency). Returns: tuple[Path, int | None, str | None, dict, str]: Saved file path, extracted id_incidence, sector name, parsed JSON contents, and the staged filename (not full path). Examples: file_path, id_, sector, json_data, staged_filename = upload_and_stage_only(db, upload_dir, request_file, base) # Handles upload, conversion, and staging Notes: - Staging will be blocked if id_incidence is missing or not a valid positive integer. \"\"\" from arb.utils.json import json_save_with_meta from arb.utils.wtf_forms_util import prep_payload_for_json logger . debug ( f \"upload_and_stage_only() called with { request_file =} \" ) id_ = None sector = None json_data = {} file_path = upload_single_file ( upload_dir , request_file ) add_file_to_upload_table ( db , file_path , status = \"File Added\" , description = \"Staged only (no DB write)\" ) json_path , sector = convert_excel_to_json_if_valid ( file_path ) # --- DIAGNOSTIC: Generate import audit --- try : parse_result = parse_xl_file ( file_path ) audit = generate_import_audit ( file_path , parse_result , xl_schema_map , route = \"upload_staged\" ) audit_log_path = LOG_DIR / \"import_audit.log\" audit_log_path . parent . mkdir ( parents = True , exist_ok = True ) with open ( audit_log_path , \"a\" , encoding = \"utf-8\" ) as f : f . write ( audit + \" \\n\\n \" ) except Exception as e : logging . getLogger ( __name__ ) . warning ( f \"Failed to generate import audit: { e } \" ) # --- END DIAGNOSTIC --- if json_path : json_data , _ = json_load_with_meta ( json_path ) id_candidate = extract_id_from_json ( json_data ) if not ( isinstance ( id_candidate , int ) and id_candidate > 0 ): logger . warning ( f \"Staging blocked: id_incidence missing or not a valid positive integer in { file_path . name } \" ) return file_path , None , None , {}, \"\" id_ = extract_id_from_json ( json_data ) # \ud83c\udd95 Staging logic: write to upload_dir/staging/{id_}_ts_YYYYMMDD_HHMMSS.json if id_ : model , _ , _ = get_ensured_row ( db , base , table_name = \"incidences\" , primary_key_name = \"id_incidence\" , id_ = id_ ) base_misc_json = getattr ( model , \"misc_json\" , {}) or {} json_data = prep_payload_for_json ( json_data ) staging_dir = Path ( upload_dir ) / \"staging\" staging_dir . mkdir ( parents = True , exist_ok = True ) staged_filename = f \"id_ { id_ } _ts_ { datetime . datetime . now () . strftime ( '%Y%m %d _%H%M%S' ) } .json\" staged_path = staging_dir / staged_filename json_save_with_meta ( staged_path , json_data , metadata = { \"base_misc_json\" : base_misc_json }) logger . debug ( f \"Staged JSON saved to: { staged_path } \" ) add_file_to_upload_table ( db , staged_path , status = \"Staged JSON\" , description = \"Staged file with base_misc_json\" ) return file_path , id_ , sector , json_data , staged_filename else : logger . warning ( \"id_incidence could not be extracted. Staging file was not created.\" ) return file_path , None , None , {}, \"\" upload_and_update_db ( db , upload_dir , request_file , base ) Save uploaded Excel file, convert to JSON, and write parsed contents to the database. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. upload_dir ( str | Path ) \u2013 Directory where the uploaded file should be saved. request_file ( FileStorage ) \u2013 File uploaded via the Flask request. base ( AutomapBase ) \u2013 SQLAlchemy base object from automap reflection. Returns: tuple [ Path , int | None, str | None] \u2013 tuple[Path, int | None, str | None]: Tuple of: - file path of saved Excel file, - id_incidence of inserted row (if any), - sector extracted from the JSON file. Examples: file_name, id_, sector = upload_and_update_db(db, upload_dir, request_file, base) Handles upload, conversion, and DB insert Notes Performs a full ingest: logs the file, parses Excel \u2192 JSON, and inserts the data into the database. If the file cannot be parsed or inserted, None values are returned. Uploads will be blocked if id_incidence is missing or not a valid positive integer. Source code in arb\\portal\\utils\\db_ingest_util.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 def upload_and_update_db ( db : SQLAlchemy , upload_dir : str | Path , request_file : FileStorage , base : AutomapBase ) -> tuple [ Path , int | None , str | None ]: \"\"\" Save uploaded Excel file, convert to JSON, and write parsed contents to the database. Args: db (SQLAlchemy): SQLAlchemy database instance. upload_dir (str | Path): Directory where the uploaded file should be saved. request_file (FileStorage): File uploaded via the Flask request. base (AutomapBase): SQLAlchemy base object from automap reflection. Returns: tuple[Path, int | None, str | None]: Tuple of: - file path of saved Excel file, - id_incidence of inserted row (if any), - sector extracted from the JSON file. Examples: file_name, id_, sector = upload_and_update_db(db, upload_dir, request_file, base) # Handles upload, conversion, and DB insert Notes: - Performs a full ingest: logs the file, parses Excel \u2192 JSON, and inserts the data into the database. - If the file cannot be parsed or inserted, None values are returned. - Uploads will be blocked if id_incidence is missing or not a valid positive integer. \"\"\" logger . debug ( f \"upload_and_update_db() called with { request_file =} \" ) id_ = None sector = None file_path = upload_single_file ( upload_dir , request_file ) add_file_to_upload_table ( db , file_path , status = \"File Added\" , description = None ) json_path , sector = convert_excel_to_json_if_valid ( file_path ) # --- DIAGNOSTIC: Generate import audit --- try : parse_result = parse_xl_file ( file_path ) audit = generate_import_audit ( file_path , parse_result , xl_schema_map , route = \"upload_file\" ) audit_log_path = LOG_DIR / \"import_audit.log\" audit_log_path . parent . mkdir ( parents = True , exist_ok = True ) with open ( audit_log_path , \"a\" , encoding = \"utf-8\" ) as f : f . write ( audit + \" \\n\\n \" ) except Exception as e : logging . getLogger ( __name__ ) . warning ( f \"Failed to generate import audit: { e } \" ) # --- END DIAGNOSTIC --- if json_path : json_data , _ = json_load_with_meta ( json_path ) id_candidate = extract_id_from_json ( json_data ) if not ( isinstance ( id_candidate , int ) and id_candidate > 0 ): logger . warning ( f \"Upload blocked: id_incidence missing or not a valid positive integer in { file_path . name } \" ) return file_path , None , None id_ , _ = json_file_to_db ( db , json_path , base ) # \u2705 Perform full ingest return file_path , id_ , sector upload_and_update_db_old ( db , upload_dir , request_file , base ) Deprecated: used prior to staged update refactor (2025-06-11) Save uploaded file, parse contents, and insert or update DB rows. Parameters: db ( SQLAlchemy ) \u2013 Database instance. upload_dir ( str | Path ) \u2013 Directory where file will be saved. request_file ( FileStorage ) \u2013 Flask request.files[...] object. base ( AutomapBase ) \u2013 Automapped schema metadata. Returns: tuple [ Path , int | None, str | None] \u2013 tuple[Path, int | None, str | None]: Filename, id_incidence, sector. Examples: file_name, id_, sector = upload_and_update_db_old(db, upload_dir, request_file, base) Handles upload and DB insert (deprecated) Notes If the file is Excel and can be converted to JSON, saves a JSON version and returns the filename. Deprecated in favor of staged upload logic. Source code in arb\\portal\\utils\\db_ingest_util.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def upload_and_update_db_old ( db : SQLAlchemy , upload_dir : str | Path , request_file : FileStorage , base : AutomapBase ) -> tuple [ Path , int | None , str | None ]: \"\"\" Deprecated: used prior to staged update refactor (2025-06-11) Save uploaded file, parse contents, and insert or update DB rows. Args: db (SQLAlchemy): Database instance. upload_dir (str | Path): Directory where file will be saved. request_file (FileStorage): Flask `request.files[...]` object. base (AutomapBase): Automapped schema metadata. Returns: tuple[Path, int | None, str | None]: Filename, id_incidence, sector. Examples: file_name, id_, sector = upload_and_update_db_old(db, upload_dir, request_file, base) # Handles upload and DB insert (deprecated) Notes: - If the file is Excel and can be converted to JSON, saves a JSON version and returns the filename. - Deprecated in favor of staged upload logic. \"\"\" logger . debug ( f \"upload_and_update_db() called with { request_file =} \" ) id_ = None sector = None file_name = upload_single_file ( upload_dir , request_file ) add_file_to_upload_table ( db , file_name , status = \"File Added\" , description = None ) # if the file is xl and can be converted to JSON, # save a JSON version of the file and return the filename json_file_name = get_json_file_name_old ( file_name ) if json_file_name : id_ , sector = json_file_to_db ( db , json_file_name , base ) return file_name , id_ , sector xl_dict_to_database ( db , base , xl_dict , tab_name = 'Feedback Form' , dry_run = False ) Convert parsed Excel payload to DB insert/update or staging. Parameters: db ( SQLAlchemy ) \u2013 DB instance. base ( AutomapBase ) \u2013 Reflected schema base. xl_dict ( dict ) \u2013 JSON payload from Excel parser. tab_name ( str , default: 'Feedback Form' ) \u2013 Sheet name to extract. dry_run ( bool , default: False ) \u2013 If True, simulate insert only. Returns: tuple [ int , str ] \u2013 tuple[int, str]: (id_incidence, sector) Examples: id_, sector = xl_dict_to_database(db, base, xl_dict) Inserts or stages the Excel data Notes For future consistency, consider using extract_tab_and_sector for all ingestion. Source code in arb\\portal\\utils\\db_ingest_util.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def xl_dict_to_database ( db : SQLAlchemy , base : AutomapBase , xl_dict : dict , tab_name : str = \"Feedback Form\" , dry_run : bool = False ) -> tuple [ int , str ]: \"\"\" Convert parsed Excel payload to DB insert/update or staging. Args: db (SQLAlchemy): DB instance. base (AutomapBase): Reflected schema base. xl_dict (dict): JSON payload from Excel parser. tab_name (str): Sheet name to extract. dry_run (bool): If True, simulate insert only. Returns: tuple[int, str]: (id_incidence, sector) Examples: id_, sector = xl_dict_to_database(db, base, xl_dict) # Inserts or stages the Excel data Notes: - For future consistency, consider using extract_tab_and_sector for all ingestion. \"\"\" logger . debug ( f \"xl_dict_to_database() called with { xl_dict =} \" ) metadata = xl_dict [ \"metadata\" ] sector = metadata [ \"sector\" ] tab_data = xl_dict [ \"tab_contents\" ][ tab_name ] tab_data [ \"sector\" ] = sector id_ = dict_to_database ( db , base , tab_data , dry_run = dry_run ) return id_ , sector","title":"arb.portal.utils.db_ingest_util"},{"location":"reference/arb/portal/utils/db_ingest_util/#arbportalutilsdb_ingest_util","text":"Database ingestion helpers for inserting or updating rows based on structured dictionaries, particularly those derived from Excel templates. This module provides Generic row ingestion from any dict using SQLAlchemy reflection Excel-specific wrapper for sector-based data (xl_dict_to_database) Attributes: logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.db_ingest_util import xl_dict_to_database, dict_to_database id_, sector = xl_dict_to_database(db, base, xl_dict) Notes Used by upload and staging routes to process Excel/JSON payloads. The logger emits a debug message when this file is loaded.","title":"arb.portal.utils.db_ingest_util"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.convert_excel_to_json_if_valid","text":"Convert an uploaded Excel or JSON file into a standardized JSON format, and return the output path and detected sector. Parameters: file_path ( Path ) \u2013 Path to the uploaded file (Excel or JSON). Returns: tuple [ Path | None, str | None] \u2013 tuple[Path | None, str | None]: - JSON file path (parsed or original), - sector string (if detected). Source code in arb\\portal\\utils\\db_ingest_util.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def convert_excel_to_json_if_valid ( file_path : Path ) -> tuple [ Path | None , str | None ]: \"\"\" Convert an uploaded Excel or JSON file into a standardized JSON format, and return the output path and detected sector. Args: file_path (Path): Path to the uploaded file (Excel or JSON). Returns: tuple[Path | None, str | None]: - JSON file path (parsed or original), - sector string (if detected). \"\"\" json_path = convert_upload_to_json ( file_path ) if json_path : logger . debug ( f \"File converted or passed through to JSON: { json_path } \" ) sector = extract_sector_from_json ( json_path ) return json_path , sector else : logger . warning ( f \"Unable to convert uploaded file to JSON: { file_path } \" ) return None , None","title":"convert_excel_to_json_if_valid"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.convert_file_to_json_old","text":"Depreciated. use convert_excel_to_json_if_valid instead. Convert an uploaded Excel file to a JSON file, if possible. Parameters: file_path ( Path ) \u2013 Path to the saved Excel file. Returns: Path | None \u2013 Path | None: Path to the generated JSON file, or None if conversion failed. Source code in arb\\portal\\utils\\db_ingest_util.py 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 def convert_file_to_json_old ( file_path : Path ) -> Path | None : \"\"\" Depreciated. use convert_excel_to_json_if_valid instead. Convert an uploaded Excel file to a JSON file, if possible. Args: file_path (Path): Path to the saved Excel file. Returns: Path | None: Path to the generated JSON file, or None if conversion failed. \"\"\" json_file_path = get_json_file_name_old ( file_path ) if not json_file_path : logger . warning ( f \"File { file_path } could not be converted to JSON.\" ) return None logger . debug ( f \"Converted Excel to JSON: { json_file_path } \" ) return json_file_path","title":"convert_file_to_json_old"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.dict_to_database","text":"Insert or update a row in the specified table using a dictionary payload. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy DB instance. base ( AutomapBase ) \u2013 Reflected model metadata. data_dict ( dict ) \u2013 Dictionary payload to insert/update. table_name ( str , default: 'incidences' ) \u2013 Table name to target. primary_key ( str , default: 'id_incidence' ) \u2013 Primary key column. json_field ( str , default: 'misc_json' ) \u2013 Name of JSON field for form payload. dry_run ( bool , default: False ) \u2013 If True, simulate logic without writing to DB. Returns: int ( int ) \u2013 The id_incidence (or equivalent PK) inferred from payload or model. Raises: ValueError \u2013 If data_dict is empty. AttributeError \u2013 If PK cannot be resolved. Examples: id_ = dict_to_database(db, base, data_dict)","title":"dict_to_database"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.dict_to_database--inserts-or-updates-the-row-in-the-database","text":"Notes Uses get_ensured_row and update_model_with_payload for safe upsert. Commits the session unless dry_run is True. Source code in arb\\portal\\utils\\db_ingest_util.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def dict_to_database ( db : SQLAlchemy , base : AutomapBase , data_dict : dict , table_name : str = \"incidences\" , primary_key : str = \"id_incidence\" , json_field : str = \"misc_json\" , dry_run : bool = False ) -> int : \"\"\" Insert or update a row in the specified table using a dictionary payload. Args: db (SQLAlchemy): SQLAlchemy DB instance. base (AutomapBase): Reflected model metadata. data_dict (dict): Dictionary payload to insert/update. table_name (str): Table name to target. primary_key (str): Primary key column. json_field (str): Name of JSON field for form payload. dry_run (bool): If True, simulate logic without writing to DB. Returns: int: The id_incidence (or equivalent PK) inferred from payload or model. Raises: ValueError: If data_dict is empty. AttributeError: If PK cannot be resolved. Examples: id_ = dict_to_database(db, base, data_dict) # Inserts or updates the row in the database Notes: - Uses get_ensured_row and update_model_with_payload for safe upsert. - Commits the session unless dry_run is True. \"\"\" from arb.utils.wtf_forms_util import update_model_with_payload if not data_dict : msg = \"Attempt to add empty entry to database\" logger . warning ( msg ) raise ValueError ( msg ) id_ = data_dict . get ( primary_key ) model , id_ , is_new_row = get_ensured_row ( db = db , base = base , table_name = table_name , primary_key_name = primary_key , id_ = id_ ) if is_new_row : logger . debug ( f \"Backfilling { primary_key } = { id_ } into payload\" ) data_dict [ primary_key ] = id_ update_model_with_payload ( model , data_dict , json_field = json_field ) if not dry_run : db . session . add ( model ) db . session . commit () # Final safety: extract final PK from the model try : return getattr ( model , primary_key ) except AttributeError as e : logger . error ( f \"Model has no attribute ' { primary_key } ': { e } \" ) raise","title":"Inserts or updates the row in the database"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.extract_sector_from_json","text":"Extract the sector name from a JSON file generated from Excel. Parameters: json_path ( Path ) \u2013 Path to the JSON file. Returns: str | None \u2013 str | None: The sector name if found; otherwise, None. Source code in arb\\portal\\utils\\db_ingest_util.py 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 def extract_sector_from_json ( json_path : Path ) -> str | None : \"\"\" Extract the sector name from a JSON file generated from Excel. Args: json_path (Path): Path to the JSON file. Returns: str | None: The sector name if found; otherwise, None. \"\"\" try : json_data , _ = json_load_with_meta ( json_path ) return json_data . get ( \"metadata\" , {}) . get ( \"sector\" ) except Exception as e : logger . warning ( f \"Could not extract sector from { json_path } : { e } \" ) return None","title":"extract_sector_from_json"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.extract_tab_and_sector","text":"Extract form data from Excel-parsed JSON and include sector from metadata. Parameters: xl_dict ( dict ) \u2013 Parsed Excel document with 'metadata' and 'tab_contents'. tab_name ( str , default: 'Feedback Form' ) \u2013 Name of the worksheet tab to extract. Defaults to 'Feedback Form'. Returns: dict ( dict ) \u2013 Combined payload with tab_contents and sector included. Raises: ValueError \u2013 If the tab_name is missing or sector cannot be determined. Examples: form_data = extract_tab_and_sector(xl_dict)","title":"extract_tab_and_sector"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.extract_tab_and_sector--returns-a-dict-with-form-data-and-sector","text":"Notes Ensures consistency between staged and production uploads. Logs a warning if sector is missing in metadata. Source code in arb\\portal\\utils\\db_ingest_util.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def extract_tab_and_sector ( xl_dict : dict , tab_name : str = \"Feedback Form\" ) -> dict : \"\"\" Extract form data from Excel-parsed JSON and include sector from metadata. Args: xl_dict (dict): Parsed Excel document with 'metadata' and 'tab_contents'. tab_name (str): Name of the worksheet tab to extract. Defaults to 'Feedback Form'. Returns: dict: Combined payload with tab_contents and sector included. Raises: ValueError: If the tab_name is missing or sector cannot be determined. Examples: form_data = extract_tab_and_sector(xl_dict) # Returns a dict with form data and sector Notes: - Ensures consistency between staged and production uploads. - Logs a warning if sector is missing in metadata. \"\"\" if \"tab_contents\" not in xl_dict or tab_name not in xl_dict [ \"tab_contents\" ]: raise ValueError ( f \"Tab ' { tab_name } ' not found in xl_dict\" ) # Extract sector from metadata metadata = xl_dict . get ( \"metadata\" , {}) sector = metadata . get ( \"sector\" ) if not sector : logger . warning ( f \"No sector found in xl_dict metadata: { metadata } \" ) sector = \"Unknown\" # Fallback to prevent errors # Get form data and add sector form_data = xl_dict [ \"tab_contents\" ][ tab_name ] . copy () form_data [ \"sector\" ] = sector logger . debug ( f \"extract_tab_and_sector: extracted form data with sector ' { sector } ' from tab ' { tab_name } '\" ) return form_data","title":"Returns a dict with form data and sector"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.json_file_to_db","text":"Parse and optionally insert a structured JSON file into DB. Parameters: db ( SQLAlchemy ) \u2013 DB engine. file_name ( Path ) \u2013 Path to .json file. base ( AutomapBase ) \u2013 Reflected schema. dry_run ( bool , default: False ) \u2013 If True, simulate insert only. Returns: tuple [ int , str ] \u2013 tuple[int, str]: (id_incidence, sector) Examples: id_, sector = json_file_to_db(db, \"file.json\", base)","title":"json_file_to_db"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.json_file_to_db--loads-json-and-inserts-or-stages-the-data","text":"Source code in arb\\portal\\utils\\db_ingest_util.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def json_file_to_db ( db : SQLAlchemy , file_name : str | Path , base : AutomapBase , dry_run : bool = False ) -> tuple [ int , str ]: \"\"\" Parse and optionally insert a structured JSON file into DB. Args: db (SQLAlchemy): DB engine. file_name (Path): Path to .json file. base (AutomapBase): Reflected schema. dry_run (bool): If True, simulate insert only. Returns: tuple[int, str]: (id_incidence, sector) Examples: id_, sector = json_file_to_db(db, \"file.json\", base) # Loads JSON and inserts or stages the data \"\"\" # todo - datetime - looks like this is where the json file gets loaded json_as_dict , metadata = json_load_with_meta ( file_name ) return xl_dict_to_database ( db , base , json_as_dict , dry_run = dry_run )","title":"Loads JSON and inserts or stages the data"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.store_staged_payload","text":"Save a parsed but uncommitted JSON payload to a staging directory. Parameters: id_ ( int ) \u2013 Incidence ID. sector ( str ) \u2013 Sector name (used for file naming). json_data ( dict ) \u2013 Parsed JSON dictionary to save. Returns: Path ( Path ) \u2013 Path to the saved staging file. Source code in arb\\portal\\utils\\db_ingest_util.py 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 def store_staged_payload ( id_ : int , sector : str , json_data : dict ) -> Path : \"\"\" Save a parsed but uncommitted JSON payload to a staging directory. Args: id_ (int): Incidence ID. sector (str): Sector name (used for file naming). json_data (dict): Parsed JSON dictionary to save. Returns: Path: Path to the saved staging file. \"\"\" from arb.utils.io_wrappers import save_json_safely staging_dir = Path ( get_upload_folder ()) / \"staging\" staging_dir . mkdir ( parents = True , exist_ok = True ) file_name = f \"id_ { id_ } _ { sector . lower () } .json\" path = staging_dir / file_name save_json_safely ( json_data , path ) return path","title":"store_staged_payload"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.upload_and_stage_only","text":"Save an uploaded Excel or JSON file, convert to JSON, and stage it for review. This function mimics upload_and_update_db() to ensure parity, but differs in that: - It does NOT update the database. - It returns the parsed JSON dict for review purposes. - It saves the current DB misc_json as 'base_misc_json' in the staged file's metadata. - It uses a timestamped filename for the staged file. - It ensures all values are JSON-serializable (datetime \u2192 ISO strings, etc.) before staging. Parameters: db ( SQLAlchemy ) \u2013 Active SQLAlchemy database instance. upload_dir ( str | Path ) \u2013 Target upload folder path. request_file ( FileStorage ) \u2013 Uploaded file from Flask request. base ( AutomapBase ) \u2013 Reflected metadata (currently unused but passed for consistency). Returns: Path \u2013 tuple[Path, int | None, str | None, dict, str]: Saved file path, extracted id_incidence, int | None \u2013 sector name, parsed JSON contents, and the staged filename (not full path). Examples: file_path, id_, sector, json_data, staged_filename = upload_and_stage_only(db, upload_dir, request_file, base)","title":"upload_and_stage_only"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.upload_and_stage_only--handles-upload-conversion-and-staging","text":"Notes Staging will be blocked if id_incidence is missing or not a valid positive integer. Source code in arb\\portal\\utils\\db_ingest_util.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 def upload_and_stage_only ( db : SQLAlchemy , upload_dir : str | Path , request_file : FileStorage , base : AutomapBase ) -> tuple [ Path , int | None , str | None , dict , str ]: \"\"\" Save an uploaded Excel or JSON file, convert to JSON, and stage it for review. This function mimics upload_and_update_db() to ensure parity, but differs in that: - It does NOT update the database. - It returns the parsed JSON dict for review purposes. - It saves the current DB misc_json as 'base_misc_json' in the staged file's metadata. - It uses a timestamped filename for the staged file. - It ensures all values are JSON-serializable (datetime \u2192 ISO strings, etc.) before staging. Args: db (SQLAlchemy): Active SQLAlchemy database instance. upload_dir (str | Path): Target upload folder path. request_file (FileStorage): Uploaded file from Flask request. base (AutomapBase): Reflected metadata (currently unused but passed for consistency). Returns: tuple[Path, int | None, str | None, dict, str]: Saved file path, extracted id_incidence, sector name, parsed JSON contents, and the staged filename (not full path). Examples: file_path, id_, sector, json_data, staged_filename = upload_and_stage_only(db, upload_dir, request_file, base) # Handles upload, conversion, and staging Notes: - Staging will be blocked if id_incidence is missing or not a valid positive integer. \"\"\" from arb.utils.json import json_save_with_meta from arb.utils.wtf_forms_util import prep_payload_for_json logger . debug ( f \"upload_and_stage_only() called with { request_file =} \" ) id_ = None sector = None json_data = {} file_path = upload_single_file ( upload_dir , request_file ) add_file_to_upload_table ( db , file_path , status = \"File Added\" , description = \"Staged only (no DB write)\" ) json_path , sector = convert_excel_to_json_if_valid ( file_path ) # --- DIAGNOSTIC: Generate import audit --- try : parse_result = parse_xl_file ( file_path ) audit = generate_import_audit ( file_path , parse_result , xl_schema_map , route = \"upload_staged\" ) audit_log_path = LOG_DIR / \"import_audit.log\" audit_log_path . parent . mkdir ( parents = True , exist_ok = True ) with open ( audit_log_path , \"a\" , encoding = \"utf-8\" ) as f : f . write ( audit + \" \\n\\n \" ) except Exception as e : logging . getLogger ( __name__ ) . warning ( f \"Failed to generate import audit: { e } \" ) # --- END DIAGNOSTIC --- if json_path : json_data , _ = json_load_with_meta ( json_path ) id_candidate = extract_id_from_json ( json_data ) if not ( isinstance ( id_candidate , int ) and id_candidate > 0 ): logger . warning ( f \"Staging blocked: id_incidence missing or not a valid positive integer in { file_path . name } \" ) return file_path , None , None , {}, \"\" id_ = extract_id_from_json ( json_data ) # \ud83c\udd95 Staging logic: write to upload_dir/staging/{id_}_ts_YYYYMMDD_HHMMSS.json if id_ : model , _ , _ = get_ensured_row ( db , base , table_name = \"incidences\" , primary_key_name = \"id_incidence\" , id_ = id_ ) base_misc_json = getattr ( model , \"misc_json\" , {}) or {} json_data = prep_payload_for_json ( json_data ) staging_dir = Path ( upload_dir ) / \"staging\" staging_dir . mkdir ( parents = True , exist_ok = True ) staged_filename = f \"id_ { id_ } _ts_ { datetime . datetime . now () . strftime ( '%Y%m %d _%H%M%S' ) } .json\" staged_path = staging_dir / staged_filename json_save_with_meta ( staged_path , json_data , metadata = { \"base_misc_json\" : base_misc_json }) logger . debug ( f \"Staged JSON saved to: { staged_path } \" ) add_file_to_upload_table ( db , staged_path , status = \"Staged JSON\" , description = \"Staged file with base_misc_json\" ) return file_path , id_ , sector , json_data , staged_filename else : logger . warning ( \"id_incidence could not be extracted. Staging file was not created.\" ) return file_path , None , None , {}, \"\"","title":"Handles upload, conversion, and staging"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.upload_and_update_db","text":"Save uploaded Excel file, convert to JSON, and write parsed contents to the database. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. upload_dir ( str | Path ) \u2013 Directory where the uploaded file should be saved. request_file ( FileStorage ) \u2013 File uploaded via the Flask request. base ( AutomapBase ) \u2013 SQLAlchemy base object from automap reflection. Returns: tuple [ Path , int | None, str | None] \u2013 tuple[Path, int | None, str | None]: Tuple of: - file path of saved Excel file, - id_incidence of inserted row (if any), - sector extracted from the JSON file. Examples: file_name, id_, sector = upload_and_update_db(db, upload_dir, request_file, base)","title":"upload_and_update_db"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.upload_and_update_db--handles-upload-conversion-and-db-insert","text":"Notes Performs a full ingest: logs the file, parses Excel \u2192 JSON, and inserts the data into the database. If the file cannot be parsed or inserted, None values are returned. Uploads will be blocked if id_incidence is missing or not a valid positive integer. Source code in arb\\portal\\utils\\db_ingest_util.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 def upload_and_update_db ( db : SQLAlchemy , upload_dir : str | Path , request_file : FileStorage , base : AutomapBase ) -> tuple [ Path , int | None , str | None ]: \"\"\" Save uploaded Excel file, convert to JSON, and write parsed contents to the database. Args: db (SQLAlchemy): SQLAlchemy database instance. upload_dir (str | Path): Directory where the uploaded file should be saved. request_file (FileStorage): File uploaded via the Flask request. base (AutomapBase): SQLAlchemy base object from automap reflection. Returns: tuple[Path, int | None, str | None]: Tuple of: - file path of saved Excel file, - id_incidence of inserted row (if any), - sector extracted from the JSON file. Examples: file_name, id_, sector = upload_and_update_db(db, upload_dir, request_file, base) # Handles upload, conversion, and DB insert Notes: - Performs a full ingest: logs the file, parses Excel \u2192 JSON, and inserts the data into the database. - If the file cannot be parsed or inserted, None values are returned. - Uploads will be blocked if id_incidence is missing or not a valid positive integer. \"\"\" logger . debug ( f \"upload_and_update_db() called with { request_file =} \" ) id_ = None sector = None file_path = upload_single_file ( upload_dir , request_file ) add_file_to_upload_table ( db , file_path , status = \"File Added\" , description = None ) json_path , sector = convert_excel_to_json_if_valid ( file_path ) # --- DIAGNOSTIC: Generate import audit --- try : parse_result = parse_xl_file ( file_path ) audit = generate_import_audit ( file_path , parse_result , xl_schema_map , route = \"upload_file\" ) audit_log_path = LOG_DIR / \"import_audit.log\" audit_log_path . parent . mkdir ( parents = True , exist_ok = True ) with open ( audit_log_path , \"a\" , encoding = \"utf-8\" ) as f : f . write ( audit + \" \\n\\n \" ) except Exception as e : logging . getLogger ( __name__ ) . warning ( f \"Failed to generate import audit: { e } \" ) # --- END DIAGNOSTIC --- if json_path : json_data , _ = json_load_with_meta ( json_path ) id_candidate = extract_id_from_json ( json_data ) if not ( isinstance ( id_candidate , int ) and id_candidate > 0 ): logger . warning ( f \"Upload blocked: id_incidence missing or not a valid positive integer in { file_path . name } \" ) return file_path , None , None id_ , _ = json_file_to_db ( db , json_path , base ) # \u2705 Perform full ingest return file_path , id_ , sector","title":"Handles upload, conversion, and DB insert"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.upload_and_update_db_old","text":"Deprecated: used prior to staged update refactor (2025-06-11) Save uploaded file, parse contents, and insert or update DB rows. Parameters: db ( SQLAlchemy ) \u2013 Database instance. upload_dir ( str | Path ) \u2013 Directory where file will be saved. request_file ( FileStorage ) \u2013 Flask request.files[...] object. base ( AutomapBase ) \u2013 Automapped schema metadata. Returns: tuple [ Path , int | None, str | None] \u2013 tuple[Path, int | None, str | None]: Filename, id_incidence, sector. Examples: file_name, id_, sector = upload_and_update_db_old(db, upload_dir, request_file, base)","title":"upload_and_update_db_old"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.upload_and_update_db_old--handles-upload-and-db-insert-deprecated","text":"Notes If the file is Excel and can be converted to JSON, saves a JSON version and returns the filename. Deprecated in favor of staged upload logic. Source code in arb\\portal\\utils\\db_ingest_util.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def upload_and_update_db_old ( db : SQLAlchemy , upload_dir : str | Path , request_file : FileStorage , base : AutomapBase ) -> tuple [ Path , int | None , str | None ]: \"\"\" Deprecated: used prior to staged update refactor (2025-06-11) Save uploaded file, parse contents, and insert or update DB rows. Args: db (SQLAlchemy): Database instance. upload_dir (str | Path): Directory where file will be saved. request_file (FileStorage): Flask `request.files[...]` object. base (AutomapBase): Automapped schema metadata. Returns: tuple[Path, int | None, str | None]: Filename, id_incidence, sector. Examples: file_name, id_, sector = upload_and_update_db_old(db, upload_dir, request_file, base) # Handles upload and DB insert (deprecated) Notes: - If the file is Excel and can be converted to JSON, saves a JSON version and returns the filename. - Deprecated in favor of staged upload logic. \"\"\" logger . debug ( f \"upload_and_update_db() called with { request_file =} \" ) id_ = None sector = None file_name = upload_single_file ( upload_dir , request_file ) add_file_to_upload_table ( db , file_name , status = \"File Added\" , description = None ) # if the file is xl and can be converted to JSON, # save a JSON version of the file and return the filename json_file_name = get_json_file_name_old ( file_name ) if json_file_name : id_ , sector = json_file_to_db ( db , json_file_name , base ) return file_name , id_ , sector","title":"Handles upload and DB insert (deprecated)"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.xl_dict_to_database","text":"Convert parsed Excel payload to DB insert/update or staging. Parameters: db ( SQLAlchemy ) \u2013 DB instance. base ( AutomapBase ) \u2013 Reflected schema base. xl_dict ( dict ) \u2013 JSON payload from Excel parser. tab_name ( str , default: 'Feedback Form' ) \u2013 Sheet name to extract. dry_run ( bool , default: False ) \u2013 If True, simulate insert only. Returns: tuple [ int , str ] \u2013 tuple[int, str]: (id_incidence, sector) Examples: id_, sector = xl_dict_to_database(db, base, xl_dict)","title":"xl_dict_to_database"},{"location":"reference/arb/portal/utils/db_ingest_util/#arb.portal.utils.db_ingest_util.xl_dict_to_database--inserts-or-stages-the-excel-data","text":"Notes For future consistency, consider using extract_tab_and_sector for all ingestion. Source code in arb\\portal\\utils\\db_ingest_util.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def xl_dict_to_database ( db : SQLAlchemy , base : AutomapBase , xl_dict : dict , tab_name : str = \"Feedback Form\" , dry_run : bool = False ) -> tuple [ int , str ]: \"\"\" Convert parsed Excel payload to DB insert/update or staging. Args: db (SQLAlchemy): DB instance. base (AutomapBase): Reflected schema base. xl_dict (dict): JSON payload from Excel parser. tab_name (str): Sheet name to extract. dry_run (bool): If True, simulate insert only. Returns: tuple[int, str]: (id_incidence, sector) Examples: id_, sector = xl_dict_to_database(db, base, xl_dict) # Inserts or stages the Excel data Notes: - For future consistency, consider using extract_tab_and_sector for all ingestion. \"\"\" logger . debug ( f \"xl_dict_to_database() called with { xl_dict =} \" ) metadata = xl_dict [ \"metadata\" ] sector = metadata [ \"sector\" ] tab_data = xl_dict [ \"tab_contents\" ][ tab_name ] tab_data [ \"sector\" ] = sector id_ = dict_to_database ( db , base , tab_data , dry_run = dry_run ) return id_ , sector","title":"Inserts or stages the Excel data"},{"location":"reference/arb/portal/utils/db_introspection_util/","text":"arb.portal.utils.db_introspection_util Database utility functions for dynamic schema operations using SQLAlchemy reflection. This module allows runtime access to models and retrieval or creation of rows using flexible table and column identifiers. Attributes: get_ensured_row ( function ) \u2013 Retrieve or create a row in a table by primary key. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.db_introspection_util import get_ensured_row model, id_, is_new = get_ensured_row(db, base, table_name=\"incidences\", id_=123) Notes Used by ingestion and upload utilities for safe upsert operations. The logger emits a debug message when this file is loaded. get_ensured_row ( db , base , table_name = 'incidences' , primary_key_name = 'id_incidence' , id_ = None , add_to_session = False ) Retrieve or create a row in the specified table using a primary key. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. base ( AutomapBase ) \u2013 Reflected SQLAlchemy base metadata. table_name ( str , default: 'incidences' ) \u2013 Table name to operate on. Defaults to 'incidences'. primary_key_name ( str , default: 'id_incidence' ) \u2013 Name of the primary key column. Defaults to 'id_incidence'. id_ ( int | None , default: None ) \u2013 Primary key value. If None, a new row is created. add_to_session ( bool , default: False ) \u2013 If True, add new model instances to the session for tracking. Defaults to False for backward compatibility with upload_file. Set to True for staged uploads that need session tracking. Returns: tuple [ Any , bool ] \u2013 tuple[Any, bool]: (model, is_new_row) - model: SQLAlchemy ORM instance - is_new_row: Whether a new row was created (True/False) Raises: AttributeError \u2013 If the model class lacks the specified primary key. UnmappedClassError \u2013 If the table name is not mapped in metadata. Examples: model, id_, is_new = get_ensured_row(db, base, table_name=\"incidences\", id_=123) Retrieves or creates a row in the incidences table Notes MODIFIED FOR STAGED UPLOADS: Added add_to_session parameter to support staged upload functionality without breaking existing upload_file behavior. When add_to_session=True, new models are added to the session for proper tracking. When add_to_session=False (default), behavior remains unchanged for upload_file compatibility. Logs detailed diagnostics for debugging and session state. Source code in arb\\portal\\utils\\db_introspection_util.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def get_ensured_row ( db : SQLAlchemy , base : AutomapBase , table_name : str = \"incidences\" , primary_key_name : str = \"id_incidence\" , id_ : int | None = None , add_to_session : bool = False ) -> tuple [ Any , bool ]: \"\"\" Retrieve or create a row in the specified table using a primary key. Args: db (SQLAlchemy): SQLAlchemy database instance. base (AutomapBase): Reflected SQLAlchemy base metadata. table_name (str): Table name to operate on. Defaults to 'incidences'. primary_key_name (str): Name of the primary key column. Defaults to 'id_incidence'. id_ (int | None): Primary key value. If None, a new row is created. add_to_session (bool): If True, add new model instances to the session for tracking. Defaults to False for backward compatibility with upload_file. Set to True for staged uploads that need session tracking. Returns: tuple[Any, bool]: (model, is_new_row) - model: SQLAlchemy ORM instance - is_new_row: Whether a new row was created (True/False) Raises: AttributeError: If the model class lacks the specified primary key. UnmappedClassError: If the table name is not mapped in metadata. Examples: model, id_, is_new = get_ensured_row(db, base, table_name=\"incidences\", id_=123) # Retrieves or creates a row in the incidences table Notes: - MODIFIED FOR STAGED UPLOADS: Added add_to_session parameter to support staged upload functionality without breaking existing upload_file behavior. - When add_to_session=True, new models are added to the session for proper tracking. - When add_to_session=False (default), behavior remains unchanged for upload_file compatibility. - Logs detailed diagnostics for debugging and session state. \"\"\" # \ud83c\udd95 DIAGNOSTIC: Log function entry logger . info ( f \"[get_ensured_row] ENTRY: table_name= { table_name } , \" f \"primary_key_name= { primary_key_name } , id_= { id_ } , add_to_session= { add_to_session } \" ) is_new_row = False session = db . session table = get_class_from_table_name ( base , table_name ) # type: ignore # mapped ORM class if table is None : raise ValueError ( f \"Table ' { table_name } ' not found or not mapped in metadata.\" ) logger . info ( f \"[get_ensured_row] Table class: { table } , Session: { session is not None } \" ) if id_ is not None : logger . info ( f \"[get_ensured_row] Attempting to retrieve existing row with { primary_key_name } = { id_ } \" ) model = session . get ( table , id_ ) # type: ignore if model is None : is_new_row = True logger . info ( f \"[get_ensured_row] No existing row found; creating new { table_name } row with { primary_key_name } = { id_ } \" ) model = table ( ** { primary_key_name : id_ }) # type: ignore # \ud83c\udd95 CONDITIONAL: Add to session only if requested (for staged uploads) if add_to_session : session . add ( model ) logger . info ( f \"[get_ensured_row] Added new model to session for tracking\" ) logger . info ( f \"[get_ensured_row] Created model instance: { model } , \" f \"model. { primary_key_name } = { getattr ( model , primary_key_name , 'N/A' ) } \" ) else : logger . info ( f \"[get_ensured_row] Found existing row: { model } , \" f \"model. { primary_key_name } = { getattr ( model , primary_key_name , 'N/A' ) } \" ) else : is_new_row = True logger . info ( f \"[get_ensured_row] Creating new { table_name } row with auto-generated { primary_key_name } \" ) model = table ( ** { primary_key_name : None }) # type: ignore session . add ( model ) logger . info ( f \"[get_ensured_row] About to commit new row to database\" ) try : session . commit () logger . info ( f \"[get_ensured_row] \u2705 Successfully committed new row to database\" ) except Exception as e : logger . error ( f \"[get_ensured_row] \u274c Failed to commit new row: { e } \" ) logger . exception ( f \"[get_ensured_row] Full exception details:\" ) raise id_ = getattr ( model , primary_key_name ) logger . info ( f \"[get_ensured_row] New row created with { primary_key_name } = { id_ } \" ) # \ud83c\udd95 DIAGNOSTIC: Final state check logger . info ( f \"[get_ensured_row] RETURN: model= { model } , id_= { id_ } , is_new_row= { is_new_row } \" ) # Check if model is in session from sqlalchemy.orm import object_session model_session = object_session ( model ) logger . info ( f \"[get_ensured_row] Model session check: { model_session is not None } , \" f \"Model in session: { model in model_session if model_session else False } \" ) return model , id_ , is_new_row","title":"arb.portal.utils.db_introspection_util"},{"location":"reference/arb/portal/utils/db_introspection_util/#arbportalutilsdb_introspection_util","text":"Database utility functions for dynamic schema operations using SQLAlchemy reflection. This module allows runtime access to models and retrieval or creation of rows using flexible table and column identifiers. Attributes: get_ensured_row ( function ) \u2013 Retrieve or create a row in a table by primary key. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.db_introspection_util import get_ensured_row model, id_, is_new = get_ensured_row(db, base, table_name=\"incidences\", id_=123) Notes Used by ingestion and upload utilities for safe upsert operations. The logger emits a debug message when this file is loaded.","title":"arb.portal.utils.db_introspection_util"},{"location":"reference/arb/portal/utils/db_introspection_util/#arb.portal.utils.db_introspection_util.get_ensured_row","text":"Retrieve or create a row in the specified table using a primary key. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. base ( AutomapBase ) \u2013 Reflected SQLAlchemy base metadata. table_name ( str , default: 'incidences' ) \u2013 Table name to operate on. Defaults to 'incidences'. primary_key_name ( str , default: 'id_incidence' ) \u2013 Name of the primary key column. Defaults to 'id_incidence'. id_ ( int | None , default: None ) \u2013 Primary key value. If None, a new row is created. add_to_session ( bool , default: False ) \u2013 If True, add new model instances to the session for tracking. Defaults to False for backward compatibility with upload_file. Set to True for staged uploads that need session tracking. Returns: tuple [ Any , bool ] \u2013 tuple[Any, bool]: (model, is_new_row) - model: SQLAlchemy ORM instance - is_new_row: Whether a new row was created (True/False) Raises: AttributeError \u2013 If the model class lacks the specified primary key. UnmappedClassError \u2013 If the table name is not mapped in metadata. Examples: model, id_, is_new = get_ensured_row(db, base, table_name=\"incidences\", id_=123)","title":"get_ensured_row"},{"location":"reference/arb/portal/utils/db_introspection_util/#arb.portal.utils.db_introspection_util.get_ensured_row--retrieves-or-creates-a-row-in-the-incidences-table","text":"Notes MODIFIED FOR STAGED UPLOADS: Added add_to_session parameter to support staged upload functionality without breaking existing upload_file behavior. When add_to_session=True, new models are added to the session for proper tracking. When add_to_session=False (default), behavior remains unchanged for upload_file compatibility. Logs detailed diagnostics for debugging and session state. Source code in arb\\portal\\utils\\db_introspection_util.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def get_ensured_row ( db : SQLAlchemy , base : AutomapBase , table_name : str = \"incidences\" , primary_key_name : str = \"id_incidence\" , id_ : int | None = None , add_to_session : bool = False ) -> tuple [ Any , bool ]: \"\"\" Retrieve or create a row in the specified table using a primary key. Args: db (SQLAlchemy): SQLAlchemy database instance. base (AutomapBase): Reflected SQLAlchemy base metadata. table_name (str): Table name to operate on. Defaults to 'incidences'. primary_key_name (str): Name of the primary key column. Defaults to 'id_incidence'. id_ (int | None): Primary key value. If None, a new row is created. add_to_session (bool): If True, add new model instances to the session for tracking. Defaults to False for backward compatibility with upload_file. Set to True for staged uploads that need session tracking. Returns: tuple[Any, bool]: (model, is_new_row) - model: SQLAlchemy ORM instance - is_new_row: Whether a new row was created (True/False) Raises: AttributeError: If the model class lacks the specified primary key. UnmappedClassError: If the table name is not mapped in metadata. Examples: model, id_, is_new = get_ensured_row(db, base, table_name=\"incidences\", id_=123) # Retrieves or creates a row in the incidences table Notes: - MODIFIED FOR STAGED UPLOADS: Added add_to_session parameter to support staged upload functionality without breaking existing upload_file behavior. - When add_to_session=True, new models are added to the session for proper tracking. - When add_to_session=False (default), behavior remains unchanged for upload_file compatibility. - Logs detailed diagnostics for debugging and session state. \"\"\" # \ud83c\udd95 DIAGNOSTIC: Log function entry logger . info ( f \"[get_ensured_row] ENTRY: table_name= { table_name } , \" f \"primary_key_name= { primary_key_name } , id_= { id_ } , add_to_session= { add_to_session } \" ) is_new_row = False session = db . session table = get_class_from_table_name ( base , table_name ) # type: ignore # mapped ORM class if table is None : raise ValueError ( f \"Table ' { table_name } ' not found or not mapped in metadata.\" ) logger . info ( f \"[get_ensured_row] Table class: { table } , Session: { session is not None } \" ) if id_ is not None : logger . info ( f \"[get_ensured_row] Attempting to retrieve existing row with { primary_key_name } = { id_ } \" ) model = session . get ( table , id_ ) # type: ignore if model is None : is_new_row = True logger . info ( f \"[get_ensured_row] No existing row found; creating new { table_name } row with { primary_key_name } = { id_ } \" ) model = table ( ** { primary_key_name : id_ }) # type: ignore # \ud83c\udd95 CONDITIONAL: Add to session only if requested (for staged uploads) if add_to_session : session . add ( model ) logger . info ( f \"[get_ensured_row] Added new model to session for tracking\" ) logger . info ( f \"[get_ensured_row] Created model instance: { model } , \" f \"model. { primary_key_name } = { getattr ( model , primary_key_name , 'N/A' ) } \" ) else : logger . info ( f \"[get_ensured_row] Found existing row: { model } , \" f \"model. { primary_key_name } = { getattr ( model , primary_key_name , 'N/A' ) } \" ) else : is_new_row = True logger . info ( f \"[get_ensured_row] Creating new { table_name } row with auto-generated { primary_key_name } \" ) model = table ( ** { primary_key_name : None }) # type: ignore session . add ( model ) logger . info ( f \"[get_ensured_row] About to commit new row to database\" ) try : session . commit () logger . info ( f \"[get_ensured_row] \u2705 Successfully committed new row to database\" ) except Exception as e : logger . error ( f \"[get_ensured_row] \u274c Failed to commit new row: { e } \" ) logger . exception ( f \"[get_ensured_row] Full exception details:\" ) raise id_ = getattr ( model , primary_key_name ) logger . info ( f \"[get_ensured_row] New row created with { primary_key_name } = { id_ } \" ) # \ud83c\udd95 DIAGNOSTIC: Final state check logger . info ( f \"[get_ensured_row] RETURN: model= { model } , id_= { id_ } , is_new_row= { is_new_row } \" ) # Check if model is in session from sqlalchemy.orm import object_session model_session = object_session ( model ) logger . info ( f \"[get_ensured_row] Model session check: { model_session is not None } , \" f \"Model in session: { model in model_session if model_session else False } \" ) return model , id_ , is_new_row","title":"Retrieves or creates a row in the incidences table"},{"location":"reference/arb/portal/utils/file_upload_util/","text":"arb.portal.utils.file_upload_util file_upload_util.py Utility functions for managing uploaded files in the feedback portal. This module provides functionality to record uploaded files into the UploadedFile table for audit tracking and troubleshooting purposes. Attributes: add_file_to_upload_table ( function ) \u2013 Insert a record into the UploadedFile table. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.file_upload_util import add_file_to_upload_table add_file_to_upload_table(db, file_name=\"uploads/test.xlsx\", status=\"success\") Notes Used for audit trails and diagnostics of file uploads. The logger emits a debug message when this file is loaded. add_file_to_upload_table ( db , file_name , status = None , description = None ) Insert a record into the UploadedFile table for audit and diagnostics. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. file_name ( str | Path ) \u2013 File path or name to be recorded. status ( str | None , default: None ) \u2013 Optional upload status label. description ( str | None , default: None ) \u2013 Optional notes for the upload event. Returns: None \u2013 None Examples: add_file_to_upload_table(db, file_name=\"uploads/test.xlsx\", status=\"success\") Records the file upload event in the UploadedFile table Notes Commits the new record immediately to the database. Used for troubleshooting and audit trails of uploads. Source code in arb\\portal\\utils\\file_upload_util.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def add_file_to_upload_table ( db : SQLAlchemy , file_name : str | Path , status : Optional [ str ] = None , description : Optional [ str ] = None ) -> None : # noqa \"\"\" Insert a record into the `UploadedFile` table for audit and diagnostics. Args: db (SQLAlchemy): SQLAlchemy database instance. file_name (str | Path): File path or name to be recorded. status (str | None): Optional upload status label. description (str | None): Optional notes for the upload event. Returns: None Examples: add_file_to_upload_table(db, file_name=\"uploads/test.xlsx\", status=\"success\") # Records the file upload event in the UploadedFile table Notes: - Commits the new record immediately to the database. - Used for troubleshooting and audit trails of uploads. \"\"\" # todo (consider) to wrap commit in log? from arb.portal.sqla_models import UploadedFile if file_name is None : raise ValueError ( \"file_name cannot be None; must be a valid file path or name.\" ) logger . debug ( f \"Adding uploaded file to upload table: { file_name =} \" ) model_uploaded_file = UploadedFile ( path = str ( file_name ), status = status if status is not None else None , description = description if description is not None else None , ) # type: ignore db . session . add ( model_uploaded_file ) db . session . commit () logger . debug ( f \" { model_uploaded_file =} \" )","title":"arb.portal.utils.file_upload_util"},{"location":"reference/arb/portal/utils/file_upload_util/#arbportalutilsfile_upload_util","text":"file_upload_util.py Utility functions for managing uploaded files in the feedback portal. This module provides functionality to record uploaded files into the UploadedFile table for audit tracking and troubleshooting purposes. Attributes: add_file_to_upload_table ( function ) \u2013 Insert a record into the UploadedFile table. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.file_upload_util import add_file_to_upload_table add_file_to_upload_table(db, file_name=\"uploads/test.xlsx\", status=\"success\") Notes Used for audit trails and diagnostics of file uploads. The logger emits a debug message when this file is loaded.","title":"arb.portal.utils.file_upload_util"},{"location":"reference/arb/portal/utils/file_upload_util/#arb.portal.utils.file_upload_util.add_file_to_upload_table","text":"Insert a record into the UploadedFile table for audit and diagnostics. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. file_name ( str | Path ) \u2013 File path or name to be recorded. status ( str | None , default: None ) \u2013 Optional upload status label. description ( str | None , default: None ) \u2013 Optional notes for the upload event. Returns: None \u2013 None Examples: add_file_to_upload_table(db, file_name=\"uploads/test.xlsx\", status=\"success\")","title":"add_file_to_upload_table"},{"location":"reference/arb/portal/utils/file_upload_util/#arb.portal.utils.file_upload_util.add_file_to_upload_table--records-the-file-upload-event-in-the-uploadedfile-table","text":"Notes Commits the new record immediately to the database. Used for troubleshooting and audit trails of uploads. Source code in arb\\portal\\utils\\file_upload_util.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def add_file_to_upload_table ( db : SQLAlchemy , file_name : str | Path , status : Optional [ str ] = None , description : Optional [ str ] = None ) -> None : # noqa \"\"\" Insert a record into the `UploadedFile` table for audit and diagnostics. Args: db (SQLAlchemy): SQLAlchemy database instance. file_name (str | Path): File path or name to be recorded. status (str | None): Optional upload status label. description (str | None): Optional notes for the upload event. Returns: None Examples: add_file_to_upload_table(db, file_name=\"uploads/test.xlsx\", status=\"success\") # Records the file upload event in the UploadedFile table Notes: - Commits the new record immediately to the database. - Used for troubleshooting and audit trails of uploads. \"\"\" # todo (consider) to wrap commit in log? from arb.portal.sqla_models import UploadedFile if file_name is None : raise ValueError ( \"file_name cannot be None; must be a valid file path or name.\" ) logger . debug ( f \"Adding uploaded file to upload table: { file_name =} \" ) model_uploaded_file = UploadedFile ( path = str ( file_name ), status = status if status is not None else None , description = description if description is not None else None , ) # type: ignore db . session . add ( model_uploaded_file ) db . session . commit () logger . debug ( f \" { model_uploaded_file =} \" )","title":"Records the file upload event in the UploadedFile table"},{"location":"reference/arb/portal/utils/form_mapper/","text":"arb.portal.utils.form_mapper Filtering logic for querying the portal_updates table in the feedback portal. This module provides functions to parse and apply filters from request arguments, including ID ranges, substrings, and date filters, to SQLAlchemy queries. Attributes: apply_portal_update_filters ( function ) \u2013 Applies user-defined filters to a portal_updates query. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.form_mapper import apply_portal_update_filters filtered_query = apply_portal_update_filters(query, PortalUpdate, request.args) Notes Used by the feedback portal interface for advanced filtering of update logs. Supports flexible ID and date range parsing. apply_portal_update_filters ( query , portal_update_model , args ) Apply user-defined filters to a PortalUpdate SQLAlchemy query. Parameters: query ( Query ) \u2013 Query to be filtered. portal_update_model ( DeclarativeMeta | type [ Any ] ) \u2013 ORM model class for the portal_updates table. args ( dict ) \u2013 Typically from request.args , containing filter values. Returns: Query ( Query ) \u2013 Modified SQLAlchemy query with filters applied. Examples: filtered_query = apply_portal_update_filters(query, PortalUpdate, request.args) Applies filters for key, user, comments, ID ranges, and date ranges Notes Supported filters: substring matches (key, user, comments), ID exact/range, date range. ID formats: '123', '100-200', '-250', '300-', '123,150-200,250-'. Invalid or malformed filter parts are ignored. Date filters expect 'YYYY-MM-DD' format; invalid dates are ignored. Additional notes on supported ID formats (via filter_id_incidence): \"123\" \u2192 Matches ID 123 exactly \"100-200\" \u2192 Matches IDs from 100 to 200 inclusive \"-250\" \u2192 Matches all IDs \u2264 250 \"300-\" \u2192 Matches all IDs \u2265 300 \"123,150-200,250-\" \u2192 Mixed exacts and ranges \"abc, 100-xyz, 222\" \u2192 Invalid parts are ignored Source code in arb\\portal\\utils\\form_mapper.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def apply_portal_update_filters ( query : Query , portal_update_model : DeclarativeMeta | type [ Any ], args : dict ) -> Query : \"\"\" Apply user-defined filters to a `PortalUpdate` SQLAlchemy query. Args: query (Query): Query to be filtered. portal_update_model (DeclarativeMeta | type[Any]): ORM model class for the portal_updates table. args (dict): Typically from `request.args`, containing filter values. Returns: Query: Modified SQLAlchemy query with filters applied. Examples: filtered_query = apply_portal_update_filters(query, PortalUpdate, request.args) # Applies filters for key, user, comments, ID ranges, and date ranges Notes: - Supported filters: substring matches (key, user, comments), ID exact/range, date range. - ID formats: '123', '100-200', '-250', '300-', '123,150-200,250-'. - Invalid or malformed filter parts are ignored. - Date filters expect 'YYYY-MM-DD' format; invalid dates are ignored. Additional notes on supported ID formats (via filter_id_incidence): ------------------------------------------------ - \"123\" \u2192 Matches ID 123 exactly - \"100-200\" \u2192 Matches IDs from 100 to 200 inclusive - \"-250\" \u2192 Matches all IDs \u2264 250 - \"300-\" \u2192 Matches all IDs \u2265 300 - \"123,150-200,250-\" \u2192 Mixed exacts and ranges - \"abc, 100-xyz, 222\" \u2192 Invalid parts are ignored \"\"\" filter_key = args . get ( \"filter_key\" , \"\" ) . strip () filter_user = args . get ( \"filter_user\" , \"\" ) . strip () filter_comments = args . get ( \"filter_comments\" , \"\" ) . strip () filter_id_incidence = args . get ( \"filter_id_incidence\" , \"\" ) . strip () start_date_str = args . get ( \"start_date\" , \"\" ) . strip () end_date_str = args . get ( \"end_date\" , \"\" ) . strip () if filter_key : # noinspection PyUnresolvedReferences query = query . filter ( portal_update_model . key . ilike ( f \"% { filter_key } %\" )) # type: ignore if filter_user : # noinspection PyUnresolvedReferences query = query . filter ( portal_update_model . user . ilike ( f \"% { filter_user } %\" )) # type: ignore if filter_comments : # noinspection PyUnresolvedReferences query = query . filter ( portal_update_model . comments . ilike ( f \"% { filter_comments } %\" )) # type: ignore if filter_id_incidence : id_exact = set () id_range_clauses = [] for part in filter_id_incidence . split ( \",\" ): part = part . strip () if not part : continue if \"-\" in part : try : start , end = part . split ( \"-\" ) start = start . strip () end = end . strip () if start and end : start_val = int ( start ) end_val = int ( end ) if start_val <= end_val : # noinspection PyUnresolvedReferences id_range_clauses . append ( portal_update_model . id_incidence . between ( start_val , end_val )) # type: ignore elif start : start_val = int ( start ) # noinspection PyUnresolvedReferences id_range_clauses . append ( portal_update_model . id_incidence >= start_val ) # type: ignore elif end : end_val = int ( end ) # noinspection PyUnresolvedReferences id_range_clauses . append ( portal_update_model . id_incidence <= end_val ) # type: ignore except ValueError : continue # Ignore malformed part elif part . isdigit (): id_exact . add ( int ( part )) clause_list = [] if id_exact : # noinspection PyUnresolvedReferences clause_list . append ( portal_update_model . id_incidence . in_ ( sorted ( id_exact ))) # type: ignore clause_list . extend ( id_range_clauses ) if clause_list : query = query . filter ( or_ ( * clause_list )) try : if start_date_str : start_dt = datetime . strptime ( start_date_str , \"%Y-%m- %d \" ) # noinspection PyUnresolvedReferences query = query . filter ( portal_update_model . timestamp >= start_dt ) # type: ignore if end_date_str : end_dt = datetime . strptime ( end_date_str , \"%Y-%m- %d \" ) end_dt = end_dt . replace ( hour = 23 , minute = 59 , second = 59 ) # noinspection PyUnresolvedReferences query = query . filter ( portal_update_model . timestamp <= end_dt ) # type: ignore except ValueError : pass # Silently ignore invalid date inputs return query","title":"arb.portal.utils.form_mapper"},{"location":"reference/arb/portal/utils/form_mapper/#arbportalutilsform_mapper","text":"Filtering logic for querying the portal_updates table in the feedback portal. This module provides functions to parse and apply filters from request arguments, including ID ranges, substrings, and date filters, to SQLAlchemy queries. Attributes: apply_portal_update_filters ( function ) \u2013 Applies user-defined filters to a portal_updates query. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.form_mapper import apply_portal_update_filters filtered_query = apply_portal_update_filters(query, PortalUpdate, request.args) Notes Used by the feedback portal interface for advanced filtering of update logs. Supports flexible ID and date range parsing.","title":"arb.portal.utils.form_mapper"},{"location":"reference/arb/portal/utils/form_mapper/#arb.portal.utils.form_mapper.apply_portal_update_filters","text":"Apply user-defined filters to a PortalUpdate SQLAlchemy query. Parameters: query ( Query ) \u2013 Query to be filtered. portal_update_model ( DeclarativeMeta | type [ Any ] ) \u2013 ORM model class for the portal_updates table. args ( dict ) \u2013 Typically from request.args , containing filter values. Returns: Query ( Query ) \u2013 Modified SQLAlchemy query with filters applied. Examples: filtered_query = apply_portal_update_filters(query, PortalUpdate, request.args)","title":"apply_portal_update_filters"},{"location":"reference/arb/portal/utils/form_mapper/#arb.portal.utils.form_mapper.apply_portal_update_filters--applies-filters-for-key-user-comments-id-ranges-and-date-ranges","text":"Notes Supported filters: substring matches (key, user, comments), ID exact/range, date range. ID formats: '123', '100-200', '-250', '300-', '123,150-200,250-'. Invalid or malformed filter parts are ignored. Date filters expect 'YYYY-MM-DD' format; invalid dates are ignored.","title":"Applies filters for key, user, comments, ID ranges, and date ranges"},{"location":"reference/arb/portal/utils/form_mapper/#arb.portal.utils.form_mapper.apply_portal_update_filters--additional-notes-on-supported-id-formats-via-filter_id_incidence","text":"\"123\" \u2192 Matches ID 123 exactly \"100-200\" \u2192 Matches IDs from 100 to 200 inclusive \"-250\" \u2192 Matches all IDs \u2264 250 \"300-\" \u2192 Matches all IDs \u2265 300 \"123,150-200,250-\" \u2192 Mixed exacts and ranges \"abc, 100-xyz, 222\" \u2192 Invalid parts are ignored Source code in arb\\portal\\utils\\form_mapper.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def apply_portal_update_filters ( query : Query , portal_update_model : DeclarativeMeta | type [ Any ], args : dict ) -> Query : \"\"\" Apply user-defined filters to a `PortalUpdate` SQLAlchemy query. Args: query (Query): Query to be filtered. portal_update_model (DeclarativeMeta | type[Any]): ORM model class for the portal_updates table. args (dict): Typically from `request.args`, containing filter values. Returns: Query: Modified SQLAlchemy query with filters applied. Examples: filtered_query = apply_portal_update_filters(query, PortalUpdate, request.args) # Applies filters for key, user, comments, ID ranges, and date ranges Notes: - Supported filters: substring matches (key, user, comments), ID exact/range, date range. - ID formats: '123', '100-200', '-250', '300-', '123,150-200,250-'. - Invalid or malformed filter parts are ignored. - Date filters expect 'YYYY-MM-DD' format; invalid dates are ignored. Additional notes on supported ID formats (via filter_id_incidence): ------------------------------------------------ - \"123\" \u2192 Matches ID 123 exactly - \"100-200\" \u2192 Matches IDs from 100 to 200 inclusive - \"-250\" \u2192 Matches all IDs \u2264 250 - \"300-\" \u2192 Matches all IDs \u2265 300 - \"123,150-200,250-\" \u2192 Mixed exacts and ranges - \"abc, 100-xyz, 222\" \u2192 Invalid parts are ignored \"\"\" filter_key = args . get ( \"filter_key\" , \"\" ) . strip () filter_user = args . get ( \"filter_user\" , \"\" ) . strip () filter_comments = args . get ( \"filter_comments\" , \"\" ) . strip () filter_id_incidence = args . get ( \"filter_id_incidence\" , \"\" ) . strip () start_date_str = args . get ( \"start_date\" , \"\" ) . strip () end_date_str = args . get ( \"end_date\" , \"\" ) . strip () if filter_key : # noinspection PyUnresolvedReferences query = query . filter ( portal_update_model . key . ilike ( f \"% { filter_key } %\" )) # type: ignore if filter_user : # noinspection PyUnresolvedReferences query = query . filter ( portal_update_model . user . ilike ( f \"% { filter_user } %\" )) # type: ignore if filter_comments : # noinspection PyUnresolvedReferences query = query . filter ( portal_update_model . comments . ilike ( f \"% { filter_comments } %\" )) # type: ignore if filter_id_incidence : id_exact = set () id_range_clauses = [] for part in filter_id_incidence . split ( \",\" ): part = part . strip () if not part : continue if \"-\" in part : try : start , end = part . split ( \"-\" ) start = start . strip () end = end . strip () if start and end : start_val = int ( start ) end_val = int ( end ) if start_val <= end_val : # noinspection PyUnresolvedReferences id_range_clauses . append ( portal_update_model . id_incidence . between ( start_val , end_val )) # type: ignore elif start : start_val = int ( start ) # noinspection PyUnresolvedReferences id_range_clauses . append ( portal_update_model . id_incidence >= start_val ) # type: ignore elif end : end_val = int ( end ) # noinspection PyUnresolvedReferences id_range_clauses . append ( portal_update_model . id_incidence <= end_val ) # type: ignore except ValueError : continue # Ignore malformed part elif part . isdigit (): id_exact . add ( int ( part )) clause_list = [] if id_exact : # noinspection PyUnresolvedReferences clause_list . append ( portal_update_model . id_incidence . in_ ( sorted ( id_exact ))) # type: ignore clause_list . extend ( id_range_clauses ) if clause_list : query = query . filter ( or_ ( * clause_list )) try : if start_date_str : start_dt = datetime . strptime ( start_date_str , \"%Y-%m- %d \" ) # noinspection PyUnresolvedReferences query = query . filter ( portal_update_model . timestamp >= start_dt ) # type: ignore if end_date_str : end_dt = datetime . strptime ( end_date_str , \"%Y-%m- %d \" ) end_dt = end_dt . replace ( hour = 23 , minute = 59 , second = 59 ) # noinspection PyUnresolvedReferences query = query . filter ( portal_update_model . timestamp <= end_dt ) # type: ignore except ValueError : pass # Silently ignore invalid date inputs return query","title":"Additional notes on supported ID formats (via filter_id_incidence):"},{"location":"reference/arb/portal/utils/github_and_ai/","text":"arb.portal.utils.github_and_ai","title":"arb.portal.utils.github_and_ai"},{"location":"reference/arb/portal/utils/github_and_ai/#arbportalutilsgithub_and_ai","text":"","title":"arb.portal.utils.github_and_ai"},{"location":"reference/arb/portal/utils/import_audit/","text":"arb.portal.utils.import_audit import_audit.py This module generates a robust, human-readable audit trail for Excel file imports in the feedback portal. Purpose To provide a double-check of the import process by independently reading the Excel file and comparing its contents to the schema and the output of parse_xl_file. To help users and developers diagnose issues with field mapping, type conversion, whitespace, and dropdown validation. To provide a clear, aligned, per-field diagnostic log for compliance and debugging. How it works Takes the output of parse_xl_file(file_path) (parse_result) and the schema map. Independently opens the Excel file with openpyxl. For each field in the schema for the tab of interest (e.g., 'Feedback Form'), it: Reads the raw value and label from the spreadsheet. Compares them to the schema. Attempts type conversion and whitespace cleaning, logging any issues. Summarizes label and value matches. Limitations This audit does NOT use the portal's main logging infrastructure or business logic; it is a static, schema-driven check. It may not catch all business logic errors, cross-field dependencies, or dynamic validation rules enforced elsewhere in the portal. It is intended as a best-effort, field-level diagnostic and not a full validation of the import pipeline. Usage generate_import_audit(file_path, parse_result, schema_map, logs=None, route=\"\") Output Returns a string suitable for writing to a log file. field_diagnostics ( field , lookup , ws , logs ) Generate diagnostics for a single field, including label/value comparison, whitespace warnings, type conversion logs, and dropdown validation. Args: field: Field name (schema key). lookup: Schema dict for this field. ws: openpyxl worksheet for the tab. logs: List of log messages from the import process. Returns: List of lines for this field's diagnostics. Source code in arb\\portal\\utils\\import_audit.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def field_diagnostics ( field : str , lookup : Dict , ws : Any , logs : List [ str ]) -> List [ str ]: \"\"\" Generate diagnostics for a single field, including label/value comparison, whitespace warnings, type conversion logs, and dropdown validation. Args: field: Field name (schema key). lookup: Schema dict for this field. ws: openpyxl worksheet for the tab. logs: List of log messages from the import process. Returns: List of lines for this field's diagnostics. \"\"\" pad = lambda label : pad_label ( label , PAD_WIDTH ) field_lines = [] schema_label = lookup . get ( 'label' ) label_address = lookup . get ( 'label_address' ) value_address = lookup . get ( 'value_address' ) value_type = lookup . get ( 'value_type' ) is_drop_down = lookup . get ( 'is_drop_down' ) spreadsheet_label = None if label_address : try : cell = ws [ label_address ] if isinstance ( cell , tuple ): cell = cell [ 0 ] spreadsheet_label = cell . value except Exception : spreadsheet_label = None label_match = ( spreadsheet_label == schema_label ) if ( spreadsheet_label is not None and schema_label is not None ) else None spreadsheet_raw_value = None if value_address : try : cell = ws [ value_address ] if isinstance ( cell , tuple ): cell = cell [ 0 ] spreadsheet_raw_value = cell . value except Exception : spreadsheet_raw_value = None db_value , conversion_logs = try_type_conversion ( spreadsheet_raw_value , value_type ) value_match_line = None if db_value == spreadsheet_raw_value : value_match_line = f \" { pad ( 'Value Match' ) } : True\" elif ( ( isinstance ( spreadsheet_raw_value , ( int , float , str )) and isinstance ( db_value , ( int , float , str ))) and str ( spreadsheet_raw_value ) == str ( db_value ) ): value_match_line = f \" { pad ( 'Value Match' ) } : True (Type Conversion: { type ( spreadsheet_raw_value ) . __name__ } \u2192 { type ( db_value ) . __name__ } , Value Preserved)\" else : value_match_line = f \" { pad ( 'Value Match' ) } : False\" display_spreadsheet_raw_value = spreadsheet_raw_value . strip () if isinstance ( spreadsheet_raw_value , str ) else spreadsheet_raw_value display_db_value = db_value . strip () if isinstance ( db_value , str ) else db_value whitespace_warning = None spreadsheet_ws_stripped = isinstance ( spreadsheet_raw_value , str ) and spreadsheet_raw_value != display_spreadsheet_raw_value db_ws_stripped = isinstance ( db_value , str ) and db_value != display_db_value if spreadsheet_ws_stripped and db_ws_stripped : whitespace_warning = f \"WARNING: Whitespace was stripped from both spreadsheet and db value (before: { json . dumps ( spreadsheet_raw_value , default = json_serializer ) } , after: { json . dumps ( display_db_value , default = json_serializer ) } )\" elif spreadsheet_ws_stripped : whitespace_warning = f \"WARNING: Whitespace was stripped from spreadsheet value (before: { json . dumps ( spreadsheet_raw_value , default = json_serializer ) } , after: { json . dumps ( display_spreadsheet_raw_value , default = json_serializer ) } )\" elif db_ws_stripped : whitespace_warning = f \"WARNING: Whitespace was stripped from db value (before: { json . dumps ( db_value , default = json_serializer ) } , after: { json . dumps ( display_db_value , default = json_serializer ) } )\" field_lines . append ( f \" Field { pad_label ( '' , PAD_WIDTH - 5 ) } : { json . dumps ( field ) } \" ) field_lines . append ( f \" { pad ( 'Schema Label' ) } : { json . dumps ( schema_label , default = json_serializer ) } \" ) field_lines . append ( f \" { pad ( 'Spreadsheet Label' ) } : { json . dumps ( spreadsheet_label , default = json_serializer ) } \" ) field_lines . append ( f \" { pad ( 'Value' ) } : { json . dumps ( display_spreadsheet_raw_value , default = json_serializer ) } \" ) field_lines . append ( f \" { pad ( 'Data Type' ) } : { getattr ( value_type , '__name__' , str ( value_type )) if value_type else None } \" ) field_lines . append ( f \" { pad ( 'Value Match' ) } : { value_match_line . split ( ':' , 1 )[ 1 ] . strip () } \" ) if whitespace_warning : field_lines . append ( f \" { pad ( 'Warning' ) } : { whitespace_warning } \" ) if conversion_logs : for log in conversion_logs : if log . lower () . startswith ( 'type conversion failed' ): field_lines . append ( f \" { pad ( 'Invalid Input Ignored' ) } : INVALID INPUT IGNORED: { log } \" ) else : field_lines . append ( f \" { pad ( 'Log' ) } : { log } \" ) field_lines . append ( \"\" ) return field_lines format_header ( file_path , route , parse_result , import_id , import_time ) Generate the audit log header with file, schema, and sector info. Args: file_path: Path to the imported file. route: Route or context for the import (for logging). parse_result: Output of parse_xl_file (metadata, schemas, tab_contents). Returns: List of header lines for the audit log. Source code in arb\\portal\\utils\\import_audit.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def format_header ( file_path : Path , route : str , parse_result : Dict , import_id : str , import_time : str ) -> List [ str ]: \"\"\" Generate the audit log header with file, schema, and sector info. Args: file_path: Path to the imported file. route: Route or context for the import (for logging). parse_result: Output of parse_xl_file (metadata, schemas, tab_contents). Returns: List of header lines for the audit log. \"\"\" lines = [ f \"=== BEGIN AUDIT: { file_path . name } ===\" , f \"Import ID { pad_label ( '' , PAD_WIDTH - 8 ) } : { import_id } \" , f \"Import Started { pad_label ( '' , PAD_WIDTH - 13 ) } : { import_time } \" , ] if route : lines . append ( f \"Route { pad_label ( '' , PAD_WIDTH - 5 ) } : { route } \" ) lines . append ( \"\" ) lines . append ( \"--- FILE METADATA ---\" ) meta = parse_result . get ( 'metadata' , {}) schema_tab = parse_result . get ( 'schemas' , {}) sector = meta . get ( 'sector' ) or meta . get ( 'Sector' ) schema_name = None if schema_tab : schema_name = next ( iter ( schema_tab . values ()), None ) lines . append ( f \"Sector { pad_label ( '' , PAD_WIDTH - 6 ) } : { sector } \" ) lines . append ( f \"Schema { pad_label ( '' , PAD_WIDTH - 6 ) } : { schema_name } \" ) return lines generate_import_audit ( file_path , parse_result , schema_map , logs = None , route = '' ) Generate a human-readable audit trail for an Excel import. Parameters: file_path ( Path ) \u2013 Path to the imported file. parse_result ( Dict ) \u2013 Output of parse_xl_file (metadata, schemas, tab_contents). schema_map ( Dict ) \u2013 Loaded schema map (from xl_parse.py). logs ( Optional [ List [ str ]] , default: None ) \u2013 List of log messages from the import process. route ( str , default: '' ) \u2013 Route or context for the import (for logging). Returns: str \u2013 The audit log as a string. Source code in arb\\portal\\utils\\import_audit.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def generate_import_audit ( file_path : Path , parse_result : Dict , schema_map : Dict , logs : Optional [ List [ str ]] = None , route : str = \"\" ) -> str : \"\"\" Generate a human-readable audit trail for an Excel import. Args: file_path: Path to the imported file. parse_result: Output of parse_xl_file (metadata, schemas, tab_contents). schema_map: Loaded schema map (from xl_parse.py). logs: List of log messages from the import process. route: Route or context for the import (for logging). Returns: The audit log as a string. \"\"\" if logs is None : logs = [] import_time = datetime . datetime . now () . strftime ( \"%Y-%m- %d %H:%M:%S\" ) import_id = f \" { file_path . stem } _ { import_time . replace ( '-' , '' ) . replace ( ':' , '' ) . replace ( ' ' , '_' ) } \" lines = format_header ( file_path , route , parse_result , import_id , import_time ) tab_name = 'Feedback Form' tab_schemas = parse_result . get ( 'schemas' , {}) if tab_name not in tab_schemas : lines . append ( f \"Tab: { tab_name } not found in spreadsheet schemas.\" ) lines . append ( f \"=== END AUDIT: { file_path . name } ===\" ) return ' \\n ' . join ( lines ) schema_version = tab_schemas [ tab_name ] if schema_version not in schema_map : lines . append ( f \"Schema version ' { schema_version } ' not found in schema_map.\" ) lines . append ( f \"=== END AUDIT: { file_path . name } ===\" ) return ' \\n ' . join ( lines ) schema_dict = schema_map [ schema_version ][ 'schema' ] wb = openpyxl . load_workbook ( file_path , data_only = True ) if tab_name not in wb . sheetnames : lines . append ( f \"Tab: { tab_name } not found in Excel file.\" ) lines . append ( f \"=== END AUDIT: { file_path . name } ===\" ) return ' \\n ' . join ( lines ) ws = wb [ tab_name ] lines . append ( \"\" ) lines . append ( \"--- FIELD DIAGNOSTICS ---\" ) label_match_count = 0 label_mismatch_count = 0 value_match_count = 0 value_mismatch_count = 0 warning_count = 0 invalid_input_count = 0 notes = set () for field , lookup in schema_dict . items (): field_lines = field_diagnostics ( field , lookup , ws , logs ) for line in field_lines : if line . strip () . startswith ( 'Label Match' ): if line . strip () . endswith ( 'True' ): label_match_count += 1 elif line . strip () . endswith ( 'False' ): label_mismatch_count += 1 if line . strip () . startswith ( 'Value Match' ): if line . strip () . endswith ( 'True' ): value_match_count += 1 elif line . strip () . endswith ( 'False' ): value_mismatch_count += 1 if 'WARNING:' in line : warning_count += 1 notes . add ( 'Some spreadsheet or database values had extra whitespace that was removed during import.' ) if 'INVALID INPUT IGNORED:' in line : invalid_input_count += 1 notes . add ( 'Some fields contained invalid data and were ignored. Please check your input for correct types and formats.' ) lines . extend ( field_lines ) lines . extend ( summary_section ( label_match_count , label_mismatch_count , value_match_count , value_mismatch_count , warning_count , invalid_input_count )) lines . extend ( machine_readable_summary ( import_id , label_match_count + label_mismatch_count , warning_count , invalid_input_count , import_time )) if notes : lines . append ( '--- NOTES / SUGGESTIONS ---' ) for note in notes : lines . append ( f '- { note } ' ) lines . append ( '' ) lines . append ( f \"=== END AUDIT: { file_path . name } ===\" ) return ' \\n ' . join ( lines ) normalize_label ( label ) Normalize a label for audit display (strip, replace line breaks). Source code in arb\\portal\\utils\\import_audit.py 53 54 55 56 57 def normalize_label ( label : Any ) -> str : \"\"\"Normalize a label for audit display (strip, replace line breaks).\"\"\" if isinstance ( label , str ): return label . strip () . replace ( ' \\n ' , ' ' ) . replace ( ' \\r ' , ' ' ) return label pad_label ( label , width = PAD_WIDTH ) Pad a label to a fixed width for aligned output. Source code in arb\\portal\\utils\\import_audit.py 48 49 50 def pad_label ( label : str , width : int = PAD_WIDTH ) -> str : \"\"\"Pad a label to a fixed width for aligned output.\"\"\" return label . ljust ( width ) summary_section ( label_match_count , label_mismatch_count , value_match_count , value_mismatch_count , warning_count , invalid_input_count ) Generate the summary section for the audit log. Args: label_match_count: Number of fields with label match True. label_mismatch_count: Number of fields with label match False. value_match_count: Number of fields with value match True. value_mismatch_count: Number of fields with value match False. Returns: List of lines for the summary section. Source code in arb\\portal\\utils\\import_audit.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def summary_section ( label_match_count : int , label_mismatch_count : int , value_match_count : int , value_mismatch_count : int , warning_count : int , invalid_input_count : int ) -> List [ str ]: \"\"\" Generate the summary section for the audit log. Args: label_match_count: Number of fields with label match True. label_mismatch_count: Number of fields with label match False. value_match_count: Number of fields with value match True. value_mismatch_count: Number of fields with value match False. Returns: List of lines for the summary section. \"\"\" lines = [ \"\" , \"--- SUMMARY ---\" , f \" { pad_label ( 'Fields Checked' ) } : { label_match_count + label_mismatch_count } \" , f \" { pad_label ( 'Fields With Warnings' ) } : { warning_count } \" , f \" { pad_label ( 'Fields With Invalid Input' ) } : { invalid_input_count } \" , f \" { pad_label ( 'Label Match' ) } : { label_match_count } pass, { label_mismatch_count } fail\" , f \" { pad_label ( 'Value Match' ) } : { value_match_count } pass, { value_mismatch_count } fail\" , \"\" ] return lines try_type_conversion ( raw_value , value_type ) Attempt to convert raw_value to value_type. Return (converted_value, logs). Logs are human-readable explanations of what happened. Parameters: raw_value ( Any ) \u2013 The value to convert. value_type ( Any ) \u2013 The type to convert to. Returns: tuple [ Any , list [ str ]] \u2013 tuple[Any, list[str]]: The converted value and a list of log messages. Source code in arb\\portal\\utils\\import_audit.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def try_type_conversion ( raw_value : Any , value_type : Any ) -> tuple [ Any , list [ str ]]: \"\"\" Attempt to convert raw_value to value_type. Return (converted_value, logs). Logs are human-readable explanations of what happened. Args: raw_value (Any): The value to convert. value_type (Any): The type to convert to. Returns: tuple[Any, list[str]]: The converted value and a list of log messages. \"\"\" logs = [] db_value = raw_value if raw_value is not None : if value_type and not isinstance ( raw_value , value_type ): if raw_value == \"\" : db_value = None logs . append ( f \"Type conversion failed: empty string for expected type { getattr ( value_type , '__name__' , str ( value_type )) } . Value set to None.\" ) else : try : if value_type == datetime . datetime : from arb.utils.date_and_time import excel_str_to_naive_datetime , is_datetime_naive excel_val = raw_value if not isinstance ( excel_val , str ): excel_val = str ( excel_val ) local_datetime = excel_str_to_naive_datetime ( excel_val ) if local_datetime and not is_datetime_naive ( local_datetime ): logs . append ( f \"Type conversion failed: parsed datetime is not naive for value ' { raw_value } '. Value set to None.\" ) db_value = None elif local_datetime is None : logs . append ( f \"Type conversion failed: could not parse ' { raw_value } ' as datetime. Value set to None.\" ) db_value = None else : db_value = local_datetime logs . append ( f \"Type conversion successful: ' { raw_value } ' \u2192 { db_value } (datetime)\" ) else : converted = value_type ( raw_value ) db_value = converted logs . append ( f \"Type conversion successful: ' { raw_value } ' \u2192 { db_value } ( { getattr ( value_type , '__name__' , str ( value_type )) } )\" ) except ( ValueError , TypeError ) as e : logs . append ( f \"Type conversion failed: could not convert ' { raw_value } ' to { getattr ( value_type , '__name__' , str ( value_type )) } : { e } . Value set to None.\" ) db_value = None return db_value , logs","title":"arb.portal.utils.import_audit"},{"location":"reference/arb/portal/utils/import_audit/#arbportalutilsimport_audit","text":"import_audit.py This module generates a robust, human-readable audit trail for Excel file imports in the feedback portal. Purpose To provide a double-check of the import process by independently reading the Excel file and comparing its contents to the schema and the output of parse_xl_file. To help users and developers diagnose issues with field mapping, type conversion, whitespace, and dropdown validation. To provide a clear, aligned, per-field diagnostic log for compliance and debugging. How it works Takes the output of parse_xl_file(file_path) (parse_result) and the schema map. Independently opens the Excel file with openpyxl. For each field in the schema for the tab of interest (e.g., 'Feedback Form'), it: Reads the raw value and label from the spreadsheet. Compares them to the schema. Attempts type conversion and whitespace cleaning, logging any issues. Summarizes label and value matches. Limitations This audit does NOT use the portal's main logging infrastructure or business logic; it is a static, schema-driven check. It may not catch all business logic errors, cross-field dependencies, or dynamic validation rules enforced elsewhere in the portal. It is intended as a best-effort, field-level diagnostic and not a full validation of the import pipeline. Usage generate_import_audit(file_path, parse_result, schema_map, logs=None, route=\"\") Output Returns a string suitable for writing to a log file.","title":"arb.portal.utils.import_audit"},{"location":"reference/arb/portal/utils/import_audit/#arb.portal.utils.import_audit.field_diagnostics","text":"Generate diagnostics for a single field, including label/value comparison, whitespace warnings, type conversion logs, and dropdown validation. Args: field: Field name (schema key). lookup: Schema dict for this field. ws: openpyxl worksheet for the tab. logs: List of log messages from the import process. Returns: List of lines for this field's diagnostics. Source code in arb\\portal\\utils\\import_audit.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def field_diagnostics ( field : str , lookup : Dict , ws : Any , logs : List [ str ]) -> List [ str ]: \"\"\" Generate diagnostics for a single field, including label/value comparison, whitespace warnings, type conversion logs, and dropdown validation. Args: field: Field name (schema key). lookup: Schema dict for this field. ws: openpyxl worksheet for the tab. logs: List of log messages from the import process. Returns: List of lines for this field's diagnostics. \"\"\" pad = lambda label : pad_label ( label , PAD_WIDTH ) field_lines = [] schema_label = lookup . get ( 'label' ) label_address = lookup . get ( 'label_address' ) value_address = lookup . get ( 'value_address' ) value_type = lookup . get ( 'value_type' ) is_drop_down = lookup . get ( 'is_drop_down' ) spreadsheet_label = None if label_address : try : cell = ws [ label_address ] if isinstance ( cell , tuple ): cell = cell [ 0 ] spreadsheet_label = cell . value except Exception : spreadsheet_label = None label_match = ( spreadsheet_label == schema_label ) if ( spreadsheet_label is not None and schema_label is not None ) else None spreadsheet_raw_value = None if value_address : try : cell = ws [ value_address ] if isinstance ( cell , tuple ): cell = cell [ 0 ] spreadsheet_raw_value = cell . value except Exception : spreadsheet_raw_value = None db_value , conversion_logs = try_type_conversion ( spreadsheet_raw_value , value_type ) value_match_line = None if db_value == spreadsheet_raw_value : value_match_line = f \" { pad ( 'Value Match' ) } : True\" elif ( ( isinstance ( spreadsheet_raw_value , ( int , float , str )) and isinstance ( db_value , ( int , float , str ))) and str ( spreadsheet_raw_value ) == str ( db_value ) ): value_match_line = f \" { pad ( 'Value Match' ) } : True (Type Conversion: { type ( spreadsheet_raw_value ) . __name__ } \u2192 { type ( db_value ) . __name__ } , Value Preserved)\" else : value_match_line = f \" { pad ( 'Value Match' ) } : False\" display_spreadsheet_raw_value = spreadsheet_raw_value . strip () if isinstance ( spreadsheet_raw_value , str ) else spreadsheet_raw_value display_db_value = db_value . strip () if isinstance ( db_value , str ) else db_value whitespace_warning = None spreadsheet_ws_stripped = isinstance ( spreadsheet_raw_value , str ) and spreadsheet_raw_value != display_spreadsheet_raw_value db_ws_stripped = isinstance ( db_value , str ) and db_value != display_db_value if spreadsheet_ws_stripped and db_ws_stripped : whitespace_warning = f \"WARNING: Whitespace was stripped from both spreadsheet and db value (before: { json . dumps ( spreadsheet_raw_value , default = json_serializer ) } , after: { json . dumps ( display_db_value , default = json_serializer ) } )\" elif spreadsheet_ws_stripped : whitespace_warning = f \"WARNING: Whitespace was stripped from spreadsheet value (before: { json . dumps ( spreadsheet_raw_value , default = json_serializer ) } , after: { json . dumps ( display_spreadsheet_raw_value , default = json_serializer ) } )\" elif db_ws_stripped : whitespace_warning = f \"WARNING: Whitespace was stripped from db value (before: { json . dumps ( db_value , default = json_serializer ) } , after: { json . dumps ( display_db_value , default = json_serializer ) } )\" field_lines . append ( f \" Field { pad_label ( '' , PAD_WIDTH - 5 ) } : { json . dumps ( field ) } \" ) field_lines . append ( f \" { pad ( 'Schema Label' ) } : { json . dumps ( schema_label , default = json_serializer ) } \" ) field_lines . append ( f \" { pad ( 'Spreadsheet Label' ) } : { json . dumps ( spreadsheet_label , default = json_serializer ) } \" ) field_lines . append ( f \" { pad ( 'Value' ) } : { json . dumps ( display_spreadsheet_raw_value , default = json_serializer ) } \" ) field_lines . append ( f \" { pad ( 'Data Type' ) } : { getattr ( value_type , '__name__' , str ( value_type )) if value_type else None } \" ) field_lines . append ( f \" { pad ( 'Value Match' ) } : { value_match_line . split ( ':' , 1 )[ 1 ] . strip () } \" ) if whitespace_warning : field_lines . append ( f \" { pad ( 'Warning' ) } : { whitespace_warning } \" ) if conversion_logs : for log in conversion_logs : if log . lower () . startswith ( 'type conversion failed' ): field_lines . append ( f \" { pad ( 'Invalid Input Ignored' ) } : INVALID INPUT IGNORED: { log } \" ) else : field_lines . append ( f \" { pad ( 'Log' ) } : { log } \" ) field_lines . append ( \"\" ) return field_lines","title":"field_diagnostics"},{"location":"reference/arb/portal/utils/import_audit/#arb.portal.utils.import_audit.format_header","text":"Generate the audit log header with file, schema, and sector info. Args: file_path: Path to the imported file. route: Route or context for the import (for logging). parse_result: Output of parse_xl_file (metadata, schemas, tab_contents). Returns: List of header lines for the audit log. Source code in arb\\portal\\utils\\import_audit.py 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def format_header ( file_path : Path , route : str , parse_result : Dict , import_id : str , import_time : str ) -> List [ str ]: \"\"\" Generate the audit log header with file, schema, and sector info. Args: file_path: Path to the imported file. route: Route or context for the import (for logging). parse_result: Output of parse_xl_file (metadata, schemas, tab_contents). Returns: List of header lines for the audit log. \"\"\" lines = [ f \"=== BEGIN AUDIT: { file_path . name } ===\" , f \"Import ID { pad_label ( '' , PAD_WIDTH - 8 ) } : { import_id } \" , f \"Import Started { pad_label ( '' , PAD_WIDTH - 13 ) } : { import_time } \" , ] if route : lines . append ( f \"Route { pad_label ( '' , PAD_WIDTH - 5 ) } : { route } \" ) lines . append ( \"\" ) lines . append ( \"--- FILE METADATA ---\" ) meta = parse_result . get ( 'metadata' , {}) schema_tab = parse_result . get ( 'schemas' , {}) sector = meta . get ( 'sector' ) or meta . get ( 'Sector' ) schema_name = None if schema_tab : schema_name = next ( iter ( schema_tab . values ()), None ) lines . append ( f \"Sector { pad_label ( '' , PAD_WIDTH - 6 ) } : { sector } \" ) lines . append ( f \"Schema { pad_label ( '' , PAD_WIDTH - 6 ) } : { schema_name } \" ) return lines","title":"format_header"},{"location":"reference/arb/portal/utils/import_audit/#arb.portal.utils.import_audit.generate_import_audit","text":"Generate a human-readable audit trail for an Excel import. Parameters: file_path ( Path ) \u2013 Path to the imported file. parse_result ( Dict ) \u2013 Output of parse_xl_file (metadata, schemas, tab_contents). schema_map ( Dict ) \u2013 Loaded schema map (from xl_parse.py). logs ( Optional [ List [ str ]] , default: None ) \u2013 List of log messages from the import process. route ( str , default: '' ) \u2013 Route or context for the import (for logging). Returns: str \u2013 The audit log as a string. Source code in arb\\portal\\utils\\import_audit.py 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 def generate_import_audit ( file_path : Path , parse_result : Dict , schema_map : Dict , logs : Optional [ List [ str ]] = None , route : str = \"\" ) -> str : \"\"\" Generate a human-readable audit trail for an Excel import. Args: file_path: Path to the imported file. parse_result: Output of parse_xl_file (metadata, schemas, tab_contents). schema_map: Loaded schema map (from xl_parse.py). logs: List of log messages from the import process. route: Route or context for the import (for logging). Returns: The audit log as a string. \"\"\" if logs is None : logs = [] import_time = datetime . datetime . now () . strftime ( \"%Y-%m- %d %H:%M:%S\" ) import_id = f \" { file_path . stem } _ { import_time . replace ( '-' , '' ) . replace ( ':' , '' ) . replace ( ' ' , '_' ) } \" lines = format_header ( file_path , route , parse_result , import_id , import_time ) tab_name = 'Feedback Form' tab_schemas = parse_result . get ( 'schemas' , {}) if tab_name not in tab_schemas : lines . append ( f \"Tab: { tab_name } not found in spreadsheet schemas.\" ) lines . append ( f \"=== END AUDIT: { file_path . name } ===\" ) return ' \\n ' . join ( lines ) schema_version = tab_schemas [ tab_name ] if schema_version not in schema_map : lines . append ( f \"Schema version ' { schema_version } ' not found in schema_map.\" ) lines . append ( f \"=== END AUDIT: { file_path . name } ===\" ) return ' \\n ' . join ( lines ) schema_dict = schema_map [ schema_version ][ 'schema' ] wb = openpyxl . load_workbook ( file_path , data_only = True ) if tab_name not in wb . sheetnames : lines . append ( f \"Tab: { tab_name } not found in Excel file.\" ) lines . append ( f \"=== END AUDIT: { file_path . name } ===\" ) return ' \\n ' . join ( lines ) ws = wb [ tab_name ] lines . append ( \"\" ) lines . append ( \"--- FIELD DIAGNOSTICS ---\" ) label_match_count = 0 label_mismatch_count = 0 value_match_count = 0 value_mismatch_count = 0 warning_count = 0 invalid_input_count = 0 notes = set () for field , lookup in schema_dict . items (): field_lines = field_diagnostics ( field , lookup , ws , logs ) for line in field_lines : if line . strip () . startswith ( 'Label Match' ): if line . strip () . endswith ( 'True' ): label_match_count += 1 elif line . strip () . endswith ( 'False' ): label_mismatch_count += 1 if line . strip () . startswith ( 'Value Match' ): if line . strip () . endswith ( 'True' ): value_match_count += 1 elif line . strip () . endswith ( 'False' ): value_mismatch_count += 1 if 'WARNING:' in line : warning_count += 1 notes . add ( 'Some spreadsheet or database values had extra whitespace that was removed during import.' ) if 'INVALID INPUT IGNORED:' in line : invalid_input_count += 1 notes . add ( 'Some fields contained invalid data and were ignored. Please check your input for correct types and formats.' ) lines . extend ( field_lines ) lines . extend ( summary_section ( label_match_count , label_mismatch_count , value_match_count , value_mismatch_count , warning_count , invalid_input_count )) lines . extend ( machine_readable_summary ( import_id , label_match_count + label_mismatch_count , warning_count , invalid_input_count , import_time )) if notes : lines . append ( '--- NOTES / SUGGESTIONS ---' ) for note in notes : lines . append ( f '- { note } ' ) lines . append ( '' ) lines . append ( f \"=== END AUDIT: { file_path . name } ===\" ) return ' \\n ' . join ( lines )","title":"generate_import_audit"},{"location":"reference/arb/portal/utils/import_audit/#arb.portal.utils.import_audit.normalize_label","text":"Normalize a label for audit display (strip, replace line breaks). Source code in arb\\portal\\utils\\import_audit.py 53 54 55 56 57 def normalize_label ( label : Any ) -> str : \"\"\"Normalize a label for audit display (strip, replace line breaks).\"\"\" if isinstance ( label , str ): return label . strip () . replace ( ' \\n ' , ' ' ) . replace ( ' \\r ' , ' ' ) return label","title":"normalize_label"},{"location":"reference/arb/portal/utils/import_audit/#arb.portal.utils.import_audit.pad_label","text":"Pad a label to a fixed width for aligned output. Source code in arb\\portal\\utils\\import_audit.py 48 49 50 def pad_label ( label : str , width : int = PAD_WIDTH ) -> str : \"\"\"Pad a label to a fixed width for aligned output.\"\"\" return label . ljust ( width )","title":"pad_label"},{"location":"reference/arb/portal/utils/import_audit/#arb.portal.utils.import_audit.summary_section","text":"Generate the summary section for the audit log. Args: label_match_count: Number of fields with label match True. label_mismatch_count: Number of fields with label match False. value_match_count: Number of fields with value match True. value_mismatch_count: Number of fields with value match False. Returns: List of lines for the summary section. Source code in arb\\portal\\utils\\import_audit.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def summary_section ( label_match_count : int , label_mismatch_count : int , value_match_count : int , value_mismatch_count : int , warning_count : int , invalid_input_count : int ) -> List [ str ]: \"\"\" Generate the summary section for the audit log. Args: label_match_count: Number of fields with label match True. label_mismatch_count: Number of fields with label match False. value_match_count: Number of fields with value match True. value_mismatch_count: Number of fields with value match False. Returns: List of lines for the summary section. \"\"\" lines = [ \"\" , \"--- SUMMARY ---\" , f \" { pad_label ( 'Fields Checked' ) } : { label_match_count + label_mismatch_count } \" , f \" { pad_label ( 'Fields With Warnings' ) } : { warning_count } \" , f \" { pad_label ( 'Fields With Invalid Input' ) } : { invalid_input_count } \" , f \" { pad_label ( 'Label Match' ) } : { label_match_count } pass, { label_mismatch_count } fail\" , f \" { pad_label ( 'Value Match' ) } : { value_match_count } pass, { value_mismatch_count } fail\" , \"\" ] return lines","title":"summary_section"},{"location":"reference/arb/portal/utils/import_audit/#arb.portal.utils.import_audit.try_type_conversion","text":"Attempt to convert raw_value to value_type. Return (converted_value, logs). Logs are human-readable explanations of what happened. Parameters: raw_value ( Any ) \u2013 The value to convert. value_type ( Any ) \u2013 The type to convert to. Returns: tuple [ Any , list [ str ]] \u2013 tuple[Any, list[str]]: The converted value and a list of log messages. Source code in arb\\portal\\utils\\import_audit.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def try_type_conversion ( raw_value : Any , value_type : Any ) -> tuple [ Any , list [ str ]]: \"\"\" Attempt to convert raw_value to value_type. Return (converted_value, logs). Logs are human-readable explanations of what happened. Args: raw_value (Any): The value to convert. value_type (Any): The type to convert to. Returns: tuple[Any, list[str]]: The converted value and a list of log messages. \"\"\" logs = [] db_value = raw_value if raw_value is not None : if value_type and not isinstance ( raw_value , value_type ): if raw_value == \"\" : db_value = None logs . append ( f \"Type conversion failed: empty string for expected type { getattr ( value_type , '__name__' , str ( value_type )) } . Value set to None.\" ) else : try : if value_type == datetime . datetime : from arb.utils.date_and_time import excel_str_to_naive_datetime , is_datetime_naive excel_val = raw_value if not isinstance ( excel_val , str ): excel_val = str ( excel_val ) local_datetime = excel_str_to_naive_datetime ( excel_val ) if local_datetime and not is_datetime_naive ( local_datetime ): logs . append ( f \"Type conversion failed: parsed datetime is not naive for value ' { raw_value } '. Value set to None.\" ) db_value = None elif local_datetime is None : logs . append ( f \"Type conversion failed: could not parse ' { raw_value } ' as datetime. Value set to None.\" ) db_value = None else : db_value = local_datetime logs . append ( f \"Type conversion successful: ' { raw_value } ' \u2192 { db_value } (datetime)\" ) else : converted = value_type ( raw_value ) db_value = converted logs . append ( f \"Type conversion successful: ' { raw_value } ' \u2192 { db_value } ( { getattr ( value_type , '__name__' , str ( value_type )) } )\" ) except ( ValueError , TypeError ) as e : logs . append ( f \"Type conversion failed: could not convert ' { raw_value } ' to { getattr ( value_type , '__name__' , str ( value_type )) } : { e } . Value set to None.\" ) db_value = None return db_value , logs","title":"try_type_conversion"},{"location":"reference/arb/portal/utils/route_util/","text":"arb.portal.utils.route_util Utilities for preparing rendering context and template output for feedback form pages. This module supports both 'create' and 'update' operations for feedback forms, integrating SQLAlchemy model rows with WTForms-based forms, enforcing dropdown resets, and applying conditional rendering logic based on sector type and CRUD mode. Attributes: incidence_prep ( function ) \u2013 Prepares and renders feedback form pages. render_readonly_sector_view ( function ) \u2013 Renders read-only sector views. generate_upload_diagnostics ( function ) \u2013 Generates diagnostics for upload failures. generate_staging_diagnostics ( function ) \u2013 Generates diagnostics for staging failures. format_diagnostic_message ( function ) \u2013 Formats diagnostic messages for display. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.route_util import incidence_prep html = incidence_prep(model_row, 'create', 'Oil & Gas', 'Please Select') Notes Used by feedback portal routes for form rendering and diagnostics. Integrates with WTForms and SQLAlchemy models. format_diagnostic_message ( error_details , custom_message = 'Upload processing failed.' ) Format diagnostic information into a user-friendly message. Parameters: error_details ( List [ str ] ) \u2013 List of diagnostic messages from generate_upload_diagnostics. custom_message ( str , default: 'Upload processing failed.' ) \u2013 Custom message to prepend to diagnostics. Returns: str ( str ) \u2013 Formatted diagnostic message for display. Examples: msg = format_diagnostic_message([\"\u2705 File uploaded\", \"\u274c Save failed\"]) Returns a formatted string for display to the user Notes Summarizes successes and failures in the upload/staging process. Returns the custom message if no details are provided. Source code in arb\\portal\\utils\\route_util.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 def format_diagnostic_message ( error_details : List [ str ], custom_message : str = \"Upload processing failed.\" ) -> str : \"\"\" Format diagnostic information into a user-friendly message. Args: error_details (List[str]): List of diagnostic messages from generate_upload_diagnostics. custom_message (str): Custom message to prepend to diagnostics. Returns: str: Formatted diagnostic message for display. Examples: msg = format_diagnostic_message([\"\u2705 File uploaded\", \"\u274c Save failed\"]) # Returns a formatted string for display to the user Notes: - Summarizes successes and failures in the upload/staging process. - Returns the custom message if no details are provided. \"\"\" if not error_details : return custom_message # Find the last success and first failure successes = [ msg for msg in error_details if msg . startswith ( \"\u2705\" )] failures = [ msg for msg in error_details if msg . startswith ( \"\u274c\" )] if not failures : return f \" { custom_message } All steps completed successfully.\" # Build the message message_parts = [ custom_message ] message_parts . append ( \"\" ) message_parts . append ( \"Diagnostic Information:\" ) # Show all diagnostic steps for detail in error_details : message_parts . append ( f \" { detail } \" ) # Add summary if successes : message_parts . append ( \"\" ) message_parts . append ( f \"\u2705 { len ( successes ) } step(s) completed successfully\" ) message_parts . append ( f \"\u274c Failed at: { failures [ 0 ] . replace ( '\u274c ' , '' ) } \" ) return \" \\n \" . join ( message_parts ) generate_staging_diagnostics ( request_file , file_path = None , staged_filename = None , id_ = None , sector = None ) Generate diagnostic information specifically for staging failures. This function provides detailed diagnostics for the staging workflow, including staging file creation and metadata capture. Parameters: request_file ( FileStorage ) \u2013 The uploaded file object from Flask request. file_path ( Optional [ Path ] , default: None ) \u2013 Optional path to the saved file. staged_filename ( Optional [ str ] , default: None ) \u2013 Optional name of the staged file. id_ ( Optional [ int ] , default: None ) \u2013 Optional extracted ID. sector ( Optional [ str ] , default: None ) \u2013 Optional detected sector. Returns: List [ str ] \u2013 List[str]: List of diagnostic messages with \u2705/\u274c indicators. Examples: diagnostics = generate_staging_diagnostics(request_file, file_path, staged_filename, id_, sector) Returns a list of diagnostic messages for the staging process Notes Builds on upload diagnostics and adds staging-specific checks. Verifies staging file creation and metadata capture. Source code in arb\\portal\\utils\\route_util.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 def generate_staging_diagnostics ( request_file : FileStorage , file_path : Optional [ Path ] = None , staged_filename : Optional [ str ] = None , id_ : Optional [ int ] = None , sector : Optional [ str ] = None ) -> List [ str ]: \"\"\" Generate diagnostic information specifically for staging failures. This function provides detailed diagnostics for the staging workflow, including staging file creation and metadata capture. Args: request_file: The uploaded file object from Flask request. file_path (Optional[Path]): Optional path to the saved file. staged_filename (Optional[str]): Optional name of the staged file. id_ (Optional[int]): Optional extracted ID. sector (Optional[str]): Optional detected sector. Returns: List[str]: List of diagnostic messages with \u2705/\u274c indicators. Examples: diagnostics = generate_staging_diagnostics(request_file, file_path, staged_filename, id_, sector) # Returns a list of diagnostic messages for the staging process Notes: - Builds on upload diagnostics and adds staging-specific checks. - Verifies staging file creation and metadata capture. \"\"\" error_details = [] # Start with basic upload diagnostics basic_diagnostics = generate_upload_diagnostics ( request_file , file_path , include_id_extraction = True ) error_details . extend ( basic_diagnostics ) # Add staging-specific checks if staged_filename : error_details . append ( f \"\u2705 Staging file created: { staged_filename } \" ) # Check if staging file exists on disk try : from arb.portal.config.accessors import get_upload_folder staging_dir = Path ( get_upload_folder ()) / \"staging\" staged_path = staging_dir / staged_filename if staged_path . exists (): error_details . append ( \"\u2705 Staging file saved to disk successfully\" ) else : error_details . append ( \"\u274c Staging file not found on disk\" ) except Exception as e : error_details . append ( f \"\u274c Could not verify staging file: { str ( e ) } \" ) else : error_details . append ( \"\u274c Staging file creation failed\" ) # Check metadata capture if id_ and sector : error_details . append ( f \"\u2705 Metadata captured: ID= { id_ } , Sector= { sector } \" ) else : error_details . append ( \"\u274c Metadata capture incomplete\" ) return error_details generate_upload_diagnostics ( request_file , file_path = None , include_id_extraction = False ) Generate diagnostic information for upload failures. This function analyzes what succeeded and what failed in the upload process, providing detailed information to help users understand and fix issues. Parameters: request_file ( FileStorage ) \u2013 The uploaded file object from Flask request. file_path ( Optional [ Path ] , default: None ) \u2013 Optional path to the saved file. include_id_extraction ( bool , default: False ) \u2013 Whether to include ID extraction diagnostics (for staged uploads). Returns: List [ str ] \u2013 List[str]: List of diagnostic messages with \u2705/\u274c indicators. Examples: diagnostics = generate_upload_diagnostics(request_file, file_path) Returns a list of diagnostic messages for the upload process Notes Checks file upload, disk save, JSON conversion, and optional ID extraction. Returns early if any step fails. Source code in arb\\portal\\utils\\route_util.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def generate_upload_diagnostics ( request_file : FileStorage , file_path : Optional [ Path ] = None , include_id_extraction : bool = False ) -> List [ str ]: \"\"\" Generate diagnostic information for upload failures. This function analyzes what succeeded and what failed in the upload process, providing detailed information to help users understand and fix issues. Args: request_file: The uploaded file object from Flask request. file_path (Optional[Path]): Optional path to the saved file. include_id_extraction (bool): Whether to include ID extraction diagnostics (for staged uploads). Returns: List[str]: List of diagnostic messages with \u2705/\u274c indicators. Examples: diagnostics = generate_upload_diagnostics(request_file, file_path) # Returns a list of diagnostic messages for the upload process Notes: - Checks file upload, disk save, JSON conversion, and optional ID extraction. - Returns early if any step fails. \"\"\" error_details = [] # Check if file was uploaded successfully if request_file and request_file . filename : error_details . append ( \"\u2705 File uploaded successfully\" ) else : error_details . append ( \"\u274c No file selected or file upload failed\" ) return error_details # Check if file was saved to disk if file_path and file_path . exists (): error_details . append ( f \"\u2705 File saved to disk: { file_path . name } \" ) else : error_details . append ( \"\u274c File could not be saved to disk\" ) return error_details # Check if file can be converted to JSON try : from arb.portal.utils.db_ingest_util import convert_excel_to_json_if_valid json_path , sector = convert_excel_to_json_if_valid ( file_path ) if json_path : error_details . append ( f \"\u2705 File converted to JSON successfully\" ) error_details . append ( f \"\u2705 Sector detected: { sector } \" ) # Check ID extraction if requested if include_id_extraction : try : from arb.portal.utils.db_ingest_util import extract_id_from_json from arb.utils.json import json_load_with_meta json_data , _ = json_load_with_meta ( json_path ) id_ = extract_id_from_json ( json_data ) if id_ : error_details . append ( f \"\u2705 ID extracted successfully: { id_ } \" ) else : error_details . append ( \"\u274c Could not extract ID from JSON data\" ) except Exception as e : error_details . append ( f \"\u274c ID extraction failed: { str ( e ) } \" ) else : error_details . append ( \"\u274c File format not recognized - could not convert to JSON\" ) except Exception as e : error_details . append ( f \"\u274c JSON conversion failed: { str ( e ) } \" ) return error_details incidence_prep ( model_row , crud_type , sector_type , default_dropdown ) Generate the context and render the HTML template for a feedback record. Populates WTForms fields from the model and applies validation logic depending on the request method (GET/POST). Integrates conditional dropdown resets, CSRF-less validation, and feedback record persistence. Parameters: model_row ( AutomapBase ) \u2013 SQLAlchemy AutomapBase. crud_type ( str ) \u2013 'create' or 'update'. sector_type ( str ) \u2013 'Oil & Gas' or 'Landfill'. default_dropdown ( str ) \u2013 Value used to fill in blank selects. Returns: str | Response \u2013 str | Response: Rendered HTML from the appropriate feedback template or a Flask Response. Raises: ValueError \u2013 If the sector type is invalid. Examples: html = incidence_prep(model_row, 'update', 'Oil & Gas', 'Please Select') Renders the Oil & Gas feedback form for updating a record Notes Handles both GET and POST requests for feedback forms. Integrates with WTForms and SQLAlchemy models. Shows a success popup if validation passes on submit. Source code in arb\\portal\\utils\\route_util.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def incidence_prep ( model_row : AutomapBase , crud_type : str , sector_type : str , default_dropdown : str ) -> str | Response : \"\"\" Generate the context and render the HTML template for a feedback record. Populates WTForms fields from the model and applies validation logic depending on the request method (GET/POST). Integrates conditional dropdown resets, CSRF-less validation, and feedback record persistence. Args: model_row (AutomapBase): SQLAlchemy AutomapBase. crud_type (str): 'create' or 'update'. sector_type (str): 'Oil & Gas' or 'Landfill'. default_dropdown (str): Value used to fill in blank selects. Returns: str | Response: Rendered HTML from the appropriate feedback template or a Flask Response. Raises: ValueError: If the sector type is invalid. Examples: html = incidence_prep(model_row, 'update', 'Oil & Gas', 'Please Select') # Renders the Oil & Gas feedback form for updating a record Notes: - Handles both GET and POST requests for feedback forms. - Integrates with WTForms and SQLAlchemy models. - Shows a success popup if validation passes on submit. \"\"\" # The imports below can't be moved to the top of the file because they require Globals to be initialized # prior to first use (Globals.load_drop_downs(app, db)). from arb.portal.wtf_landfill import LandfillFeedback from arb.portal.wtf_oil_and_gas import OGFeedback logger . debug ( f \"incidence_prep() called with { crud_type =} , { sector_type =} \" ) sa_model_diagnostics ( model_row ) if default_dropdown is None : default_dropdown = PLEASE_SELECT if sector_type == \"Oil & Gas\" : logger . debug ( f \"( { sector_type =} ) will use an Oil & Gas Feedback Form\" ) wtf_form = OGFeedback () template_file = 'feedback_oil_and_gas.html' elif sector_type == \"Landfill\" : logger . debug ( f \"( { sector_type =} ) will use a Landfill Feedback Form\" ) wtf_form = LandfillFeedback () template_file = 'feedback_landfill.html' else : # Handle unsupported sectors with a read-only view logger . info ( f \"( { sector_type =} ) is not supported for interactive editing - showing read-only view\" ) return render_readonly_sector_view ( model_row , sector_type , crud_type ) if request . method == 'GET' : # Populate wtform from model data model_to_wtform ( model_row , wtf_form ) # todo - maybe put update contingencies here? # obj_diagnostics(wtf_form, message=\"wtf_form in incidence_prep() after model_to_wtform\") # For GET requests for row creation, don't validate and error_count_dict will be all zeros # For GET requests for row update, validate (except for the csrf token that is only present for a POST) if crud_type == 'update' : validate_no_csrf ( wtf_form , extra_validators = None ) # todo - trying to make sure invalid drop-downs become \"Please Select\" # may want to look into using validate_no_csrf or initialize_drop_downs (or combo) # Set all select elements that are a default value (None) to \"Please Select\" value initialize_drop_downs ( wtf_form , default = default_dropdown ) # logger.debug(f\"\\n\\t{wtf_form.data=}\") if request . method == 'POST' : # Validate and count errors wtf_form . validate () _ = wtf_count_errors ( wtf_form , log_errors = True ) # Diagnostics of the model before updating with wtform values # Likely can comment out model_before and add_commit_and_log_model # if you want less diagnostics and redundant commits model_before = sa_model_to_dict ( model_row ) wtform_to_model ( model_row , wtf_form , ignore_fields = [ \"id_incidence\" ]) add_commit_and_log_model ( db , model_row , comment = 'call to wtform_to_model()' , model_before = model_before ) # Determine the course of action for successful database update based on which button was submitted button = request . form . get ( 'submit_button' ) # todo - change the button name to save? if button == 'validate_and_submit' : logger . debug ( f \"validate_and_submit was pressed\" ) # Check if there are any validation errors error_count_dict = wtf_count_errors ( wtf_form , log_errors = True ) total_errors = sum ( error_count_dict . values ()) logger . debug ( f \"Error count dict: { error_count_dict } , total_errors: { total_errors } \" ) if total_errors == 0 : # No validation errors - show success popup logger . debug ( \"No validation errors found - showing success popup\" ) flash ( \"\u2705 All changes have been saved successfully! No validation warnings or errors found.\" , \"success\" ) return render_template ( template_file , wtf_form = wtf_form , crud_type = crud_type , error_count_dict = error_count_dict , id_incidence = getattr ( model_row , \"id_incidence\" , None ), show_success_popup = True ) else : logger . debug ( f \"Validation errors found: { total_errors } - not showing success popup\" ) error_count_dict = wtf_count_errors ( wtf_form , log_errors = True ) logger . debug ( f \"incidence_prep() about to render get template\" ) return render_template ( template_file , wtf_form = wtf_form , crud_type = crud_type , error_count_dict = error_count_dict , id_incidence = getattr ( model_row , \"id_incidence\" , None ), show_success_popup = False , # Default to False for regular form display ) render_readonly_sector_view ( model_row , sector_type , crud_type ) Render a read-only view for sectors that don't have interactive forms. Parameters: model_row ( AutomapBase ) \u2013 Database model row containing incidence data. sector_type ( str ) \u2013 The unsupported sector type. crud_type ( str ) \u2013 Type of CRUD operation ('create', 'update', 'delete'). Returns: str ( str ) \u2013 Rendered HTML with formatted read-only data. Examples: html = render_readonly_sector_view(model_row, 'Unknown', 'update') Renders a read-only view for an unsupported sector Notes Used for sectors that do not have interactive feedback forms. Displays all misc_json fields in alphabetical order. Source code in arb\\portal\\utils\\route_util.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def render_readonly_sector_view ( model_row : AutomapBase , sector_type : str , crud_type : str ) -> str : \"\"\" Render a read-only view for sectors that don't have interactive forms. Args: model_row (AutomapBase): Database model row containing incidence data. sector_type (str): The unsupported sector type. crud_type (str): Type of CRUD operation ('create', 'update', 'delete'). Returns: str: Rendered HTML with formatted read-only data. Examples: html = render_readonly_sector_view(model_row, 'Unknown', 'update') # Renders a read-only view for an unsupported sector Notes: - Used for sectors that do not have interactive feedback forms. - Displays all misc_json fields in alphabetical order. \"\"\" logger . debug ( f \"render_readonly_sector_view() called for sector_type= { sector_type } \" ) # Extract data from the model id_incidence = getattr ( model_row , \"id_incidence\" , None ) misc_json = getattr ( model_row , \"misc_json\" , {}) or {} # Create alphabetically sorted list of all fields all_fields = [] # Add all misc_json fields, sorted alphabetically if misc_json : for key , value in sorted ( misc_json . items ()): all_fields . append (( key , value )) return render_template ( 'readonly_sector_view.html' , sector_type = sector_type , id_incidence = id_incidence , crud_type = crud_type , all_fields = all_fields , misc_json = misc_json )","title":"arb.portal.utils.route_util"},{"location":"reference/arb/portal/utils/route_util/#arbportalutilsroute_util","text":"Utilities for preparing rendering context and template output for feedback form pages. This module supports both 'create' and 'update' operations for feedback forms, integrating SQLAlchemy model rows with WTForms-based forms, enforcing dropdown resets, and applying conditional rendering logic based on sector type and CRUD mode. Attributes: incidence_prep ( function ) \u2013 Prepares and renders feedback form pages. render_readonly_sector_view ( function ) \u2013 Renders read-only sector views. generate_upload_diagnostics ( function ) \u2013 Generates diagnostics for upload failures. generate_staging_diagnostics ( function ) \u2013 Generates diagnostics for staging failures. format_diagnostic_message ( function ) \u2013 Formats diagnostic messages for display. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.route_util import incidence_prep html = incidence_prep(model_row, 'create', 'Oil & Gas', 'Please Select') Notes Used by feedback portal routes for form rendering and diagnostics. Integrates with WTForms and SQLAlchemy models.","title":"arb.portal.utils.route_util"},{"location":"reference/arb/portal/utils/route_util/#arb.portal.utils.route_util.format_diagnostic_message","text":"Format diagnostic information into a user-friendly message. Parameters: error_details ( List [ str ] ) \u2013 List of diagnostic messages from generate_upload_diagnostics. custom_message ( str , default: 'Upload processing failed.' ) \u2013 Custom message to prepend to diagnostics. Returns: str ( str ) \u2013 Formatted diagnostic message for display. Examples: msg = format_diagnostic_message([\"\u2705 File uploaded\", \"\u274c Save failed\"])","title":"format_diagnostic_message"},{"location":"reference/arb/portal/utils/route_util/#arb.portal.utils.route_util.format_diagnostic_message--returns-a-formatted-string-for-display-to-the-user","text":"Notes Summarizes successes and failures in the upload/staging process. Returns the custom message if no details are provided. Source code in arb\\portal\\utils\\route_util.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 def format_diagnostic_message ( error_details : List [ str ], custom_message : str = \"Upload processing failed.\" ) -> str : \"\"\" Format diagnostic information into a user-friendly message. Args: error_details (List[str]): List of diagnostic messages from generate_upload_diagnostics. custom_message (str): Custom message to prepend to diagnostics. Returns: str: Formatted diagnostic message for display. Examples: msg = format_diagnostic_message([\"\u2705 File uploaded\", \"\u274c Save failed\"]) # Returns a formatted string for display to the user Notes: - Summarizes successes and failures in the upload/staging process. - Returns the custom message if no details are provided. \"\"\" if not error_details : return custom_message # Find the last success and first failure successes = [ msg for msg in error_details if msg . startswith ( \"\u2705\" )] failures = [ msg for msg in error_details if msg . startswith ( \"\u274c\" )] if not failures : return f \" { custom_message } All steps completed successfully.\" # Build the message message_parts = [ custom_message ] message_parts . append ( \"\" ) message_parts . append ( \"Diagnostic Information:\" ) # Show all diagnostic steps for detail in error_details : message_parts . append ( f \" { detail } \" ) # Add summary if successes : message_parts . append ( \"\" ) message_parts . append ( f \"\u2705 { len ( successes ) } step(s) completed successfully\" ) message_parts . append ( f \"\u274c Failed at: { failures [ 0 ] . replace ( '\u274c ' , '' ) } \" ) return \" \\n \" . join ( message_parts )","title":"Returns a formatted string for display to the user"},{"location":"reference/arb/portal/utils/route_util/#arb.portal.utils.route_util.generate_staging_diagnostics","text":"Generate diagnostic information specifically for staging failures. This function provides detailed diagnostics for the staging workflow, including staging file creation and metadata capture. Parameters: request_file ( FileStorage ) \u2013 The uploaded file object from Flask request. file_path ( Optional [ Path ] , default: None ) \u2013 Optional path to the saved file. staged_filename ( Optional [ str ] , default: None ) \u2013 Optional name of the staged file. id_ ( Optional [ int ] , default: None ) \u2013 Optional extracted ID. sector ( Optional [ str ] , default: None ) \u2013 Optional detected sector. Returns: List [ str ] \u2013 List[str]: List of diagnostic messages with \u2705/\u274c indicators. Examples: diagnostics = generate_staging_diagnostics(request_file, file_path, staged_filename, id_, sector)","title":"generate_staging_diagnostics"},{"location":"reference/arb/portal/utils/route_util/#arb.portal.utils.route_util.generate_staging_diagnostics--returns-a-list-of-diagnostic-messages-for-the-staging-process","text":"Notes Builds on upload diagnostics and adds staging-specific checks. Verifies staging file creation and metadata capture. Source code in arb\\portal\\utils\\route_util.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 def generate_staging_diagnostics ( request_file : FileStorage , file_path : Optional [ Path ] = None , staged_filename : Optional [ str ] = None , id_ : Optional [ int ] = None , sector : Optional [ str ] = None ) -> List [ str ]: \"\"\" Generate diagnostic information specifically for staging failures. This function provides detailed diagnostics for the staging workflow, including staging file creation and metadata capture. Args: request_file: The uploaded file object from Flask request. file_path (Optional[Path]): Optional path to the saved file. staged_filename (Optional[str]): Optional name of the staged file. id_ (Optional[int]): Optional extracted ID. sector (Optional[str]): Optional detected sector. Returns: List[str]: List of diagnostic messages with \u2705/\u274c indicators. Examples: diagnostics = generate_staging_diagnostics(request_file, file_path, staged_filename, id_, sector) # Returns a list of diagnostic messages for the staging process Notes: - Builds on upload diagnostics and adds staging-specific checks. - Verifies staging file creation and metadata capture. \"\"\" error_details = [] # Start with basic upload diagnostics basic_diagnostics = generate_upload_diagnostics ( request_file , file_path , include_id_extraction = True ) error_details . extend ( basic_diagnostics ) # Add staging-specific checks if staged_filename : error_details . append ( f \"\u2705 Staging file created: { staged_filename } \" ) # Check if staging file exists on disk try : from arb.portal.config.accessors import get_upload_folder staging_dir = Path ( get_upload_folder ()) / \"staging\" staged_path = staging_dir / staged_filename if staged_path . exists (): error_details . append ( \"\u2705 Staging file saved to disk successfully\" ) else : error_details . append ( \"\u274c Staging file not found on disk\" ) except Exception as e : error_details . append ( f \"\u274c Could not verify staging file: { str ( e ) } \" ) else : error_details . append ( \"\u274c Staging file creation failed\" ) # Check metadata capture if id_ and sector : error_details . append ( f \"\u2705 Metadata captured: ID= { id_ } , Sector= { sector } \" ) else : error_details . append ( \"\u274c Metadata capture incomplete\" ) return error_details","title":"Returns a list of diagnostic messages for the staging process"},{"location":"reference/arb/portal/utils/route_util/#arb.portal.utils.route_util.generate_upload_diagnostics","text":"Generate diagnostic information for upload failures. This function analyzes what succeeded and what failed in the upload process, providing detailed information to help users understand and fix issues. Parameters: request_file ( FileStorage ) \u2013 The uploaded file object from Flask request. file_path ( Optional [ Path ] , default: None ) \u2013 Optional path to the saved file. include_id_extraction ( bool , default: False ) \u2013 Whether to include ID extraction diagnostics (for staged uploads). Returns: List [ str ] \u2013 List[str]: List of diagnostic messages with \u2705/\u274c indicators. Examples: diagnostics = generate_upload_diagnostics(request_file, file_path)","title":"generate_upload_diagnostics"},{"location":"reference/arb/portal/utils/route_util/#arb.portal.utils.route_util.generate_upload_diagnostics--returns-a-list-of-diagnostic-messages-for-the-upload-process","text":"Notes Checks file upload, disk save, JSON conversion, and optional ID extraction. Returns early if any step fails. Source code in arb\\portal\\utils\\route_util.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 def generate_upload_diagnostics ( request_file : FileStorage , file_path : Optional [ Path ] = None , include_id_extraction : bool = False ) -> List [ str ]: \"\"\" Generate diagnostic information for upload failures. This function analyzes what succeeded and what failed in the upload process, providing detailed information to help users understand and fix issues. Args: request_file: The uploaded file object from Flask request. file_path (Optional[Path]): Optional path to the saved file. include_id_extraction (bool): Whether to include ID extraction diagnostics (for staged uploads). Returns: List[str]: List of diagnostic messages with \u2705/\u274c indicators. Examples: diagnostics = generate_upload_diagnostics(request_file, file_path) # Returns a list of diagnostic messages for the upload process Notes: - Checks file upload, disk save, JSON conversion, and optional ID extraction. - Returns early if any step fails. \"\"\" error_details = [] # Check if file was uploaded successfully if request_file and request_file . filename : error_details . append ( \"\u2705 File uploaded successfully\" ) else : error_details . append ( \"\u274c No file selected or file upload failed\" ) return error_details # Check if file was saved to disk if file_path and file_path . exists (): error_details . append ( f \"\u2705 File saved to disk: { file_path . name } \" ) else : error_details . append ( \"\u274c File could not be saved to disk\" ) return error_details # Check if file can be converted to JSON try : from arb.portal.utils.db_ingest_util import convert_excel_to_json_if_valid json_path , sector = convert_excel_to_json_if_valid ( file_path ) if json_path : error_details . append ( f \"\u2705 File converted to JSON successfully\" ) error_details . append ( f \"\u2705 Sector detected: { sector } \" ) # Check ID extraction if requested if include_id_extraction : try : from arb.portal.utils.db_ingest_util import extract_id_from_json from arb.utils.json import json_load_with_meta json_data , _ = json_load_with_meta ( json_path ) id_ = extract_id_from_json ( json_data ) if id_ : error_details . append ( f \"\u2705 ID extracted successfully: { id_ } \" ) else : error_details . append ( \"\u274c Could not extract ID from JSON data\" ) except Exception as e : error_details . append ( f \"\u274c ID extraction failed: { str ( e ) } \" ) else : error_details . append ( \"\u274c File format not recognized - could not convert to JSON\" ) except Exception as e : error_details . append ( f \"\u274c JSON conversion failed: { str ( e ) } \" ) return error_details","title":"Returns a list of diagnostic messages for the upload process"},{"location":"reference/arb/portal/utils/route_util/#arb.portal.utils.route_util.incidence_prep","text":"Generate the context and render the HTML template for a feedback record. Populates WTForms fields from the model and applies validation logic depending on the request method (GET/POST). Integrates conditional dropdown resets, CSRF-less validation, and feedback record persistence. Parameters: model_row ( AutomapBase ) \u2013 SQLAlchemy AutomapBase. crud_type ( str ) \u2013 'create' or 'update'. sector_type ( str ) \u2013 'Oil & Gas' or 'Landfill'. default_dropdown ( str ) \u2013 Value used to fill in blank selects. Returns: str | Response \u2013 str | Response: Rendered HTML from the appropriate feedback template or a Flask Response. Raises: ValueError \u2013 If the sector type is invalid. Examples: html = incidence_prep(model_row, 'update', 'Oil & Gas', 'Please Select')","title":"incidence_prep"},{"location":"reference/arb/portal/utils/route_util/#arb.portal.utils.route_util.incidence_prep--renders-the-oil-gas-feedback-form-for-updating-a-record","text":"Notes Handles both GET and POST requests for feedback forms. Integrates with WTForms and SQLAlchemy models. Shows a success popup if validation passes on submit. Source code in arb\\portal\\utils\\route_util.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def incidence_prep ( model_row : AutomapBase , crud_type : str , sector_type : str , default_dropdown : str ) -> str | Response : \"\"\" Generate the context and render the HTML template for a feedback record. Populates WTForms fields from the model and applies validation logic depending on the request method (GET/POST). Integrates conditional dropdown resets, CSRF-less validation, and feedback record persistence. Args: model_row (AutomapBase): SQLAlchemy AutomapBase. crud_type (str): 'create' or 'update'. sector_type (str): 'Oil & Gas' or 'Landfill'. default_dropdown (str): Value used to fill in blank selects. Returns: str | Response: Rendered HTML from the appropriate feedback template or a Flask Response. Raises: ValueError: If the sector type is invalid. Examples: html = incidence_prep(model_row, 'update', 'Oil & Gas', 'Please Select') # Renders the Oil & Gas feedback form for updating a record Notes: - Handles both GET and POST requests for feedback forms. - Integrates with WTForms and SQLAlchemy models. - Shows a success popup if validation passes on submit. \"\"\" # The imports below can't be moved to the top of the file because they require Globals to be initialized # prior to first use (Globals.load_drop_downs(app, db)). from arb.portal.wtf_landfill import LandfillFeedback from arb.portal.wtf_oil_and_gas import OGFeedback logger . debug ( f \"incidence_prep() called with { crud_type =} , { sector_type =} \" ) sa_model_diagnostics ( model_row ) if default_dropdown is None : default_dropdown = PLEASE_SELECT if sector_type == \"Oil & Gas\" : logger . debug ( f \"( { sector_type =} ) will use an Oil & Gas Feedback Form\" ) wtf_form = OGFeedback () template_file = 'feedback_oil_and_gas.html' elif sector_type == \"Landfill\" : logger . debug ( f \"( { sector_type =} ) will use a Landfill Feedback Form\" ) wtf_form = LandfillFeedback () template_file = 'feedback_landfill.html' else : # Handle unsupported sectors with a read-only view logger . info ( f \"( { sector_type =} ) is not supported for interactive editing - showing read-only view\" ) return render_readonly_sector_view ( model_row , sector_type , crud_type ) if request . method == 'GET' : # Populate wtform from model data model_to_wtform ( model_row , wtf_form ) # todo - maybe put update contingencies here? # obj_diagnostics(wtf_form, message=\"wtf_form in incidence_prep() after model_to_wtform\") # For GET requests for row creation, don't validate and error_count_dict will be all zeros # For GET requests for row update, validate (except for the csrf token that is only present for a POST) if crud_type == 'update' : validate_no_csrf ( wtf_form , extra_validators = None ) # todo - trying to make sure invalid drop-downs become \"Please Select\" # may want to look into using validate_no_csrf or initialize_drop_downs (or combo) # Set all select elements that are a default value (None) to \"Please Select\" value initialize_drop_downs ( wtf_form , default = default_dropdown ) # logger.debug(f\"\\n\\t{wtf_form.data=}\") if request . method == 'POST' : # Validate and count errors wtf_form . validate () _ = wtf_count_errors ( wtf_form , log_errors = True ) # Diagnostics of the model before updating with wtform values # Likely can comment out model_before and add_commit_and_log_model # if you want less diagnostics and redundant commits model_before = sa_model_to_dict ( model_row ) wtform_to_model ( model_row , wtf_form , ignore_fields = [ \"id_incidence\" ]) add_commit_and_log_model ( db , model_row , comment = 'call to wtform_to_model()' , model_before = model_before ) # Determine the course of action for successful database update based on which button was submitted button = request . form . get ( 'submit_button' ) # todo - change the button name to save? if button == 'validate_and_submit' : logger . debug ( f \"validate_and_submit was pressed\" ) # Check if there are any validation errors error_count_dict = wtf_count_errors ( wtf_form , log_errors = True ) total_errors = sum ( error_count_dict . values ()) logger . debug ( f \"Error count dict: { error_count_dict } , total_errors: { total_errors } \" ) if total_errors == 0 : # No validation errors - show success popup logger . debug ( \"No validation errors found - showing success popup\" ) flash ( \"\u2705 All changes have been saved successfully! No validation warnings or errors found.\" , \"success\" ) return render_template ( template_file , wtf_form = wtf_form , crud_type = crud_type , error_count_dict = error_count_dict , id_incidence = getattr ( model_row , \"id_incidence\" , None ), show_success_popup = True ) else : logger . debug ( f \"Validation errors found: { total_errors } - not showing success popup\" ) error_count_dict = wtf_count_errors ( wtf_form , log_errors = True ) logger . debug ( f \"incidence_prep() about to render get template\" ) return render_template ( template_file , wtf_form = wtf_form , crud_type = crud_type , error_count_dict = error_count_dict , id_incidence = getattr ( model_row , \"id_incidence\" , None ), show_success_popup = False , # Default to False for regular form display )","title":"Renders the Oil &amp; Gas feedback form for updating a record"},{"location":"reference/arb/portal/utils/route_util/#arb.portal.utils.route_util.render_readonly_sector_view","text":"Render a read-only view for sectors that don't have interactive forms. Parameters: model_row ( AutomapBase ) \u2013 Database model row containing incidence data. sector_type ( str ) \u2013 The unsupported sector type. crud_type ( str ) \u2013 Type of CRUD operation ('create', 'update', 'delete'). Returns: str ( str ) \u2013 Rendered HTML with formatted read-only data. Examples: html = render_readonly_sector_view(model_row, 'Unknown', 'update')","title":"render_readonly_sector_view"},{"location":"reference/arb/portal/utils/route_util/#arb.portal.utils.route_util.render_readonly_sector_view--renders-a-read-only-view-for-an-unsupported-sector","text":"Notes Used for sectors that do not have interactive feedback forms. Displays all misc_json fields in alphabetical order. Source code in arb\\portal\\utils\\route_util.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 def render_readonly_sector_view ( model_row : AutomapBase , sector_type : str , crud_type : str ) -> str : \"\"\" Render a read-only view for sectors that don't have interactive forms. Args: model_row (AutomapBase): Database model row containing incidence data. sector_type (str): The unsupported sector type. crud_type (str): Type of CRUD operation ('create', 'update', 'delete'). Returns: str: Rendered HTML with formatted read-only data. Examples: html = render_readonly_sector_view(model_row, 'Unknown', 'update') # Renders a read-only view for an unsupported sector Notes: - Used for sectors that do not have interactive feedback forms. - Displays all misc_json fields in alphabetical order. \"\"\" logger . debug ( f \"render_readonly_sector_view() called for sector_type= { sector_type } \" ) # Extract data from the model id_incidence = getattr ( model_row , \"id_incidence\" , None ) misc_json = getattr ( model_row , \"misc_json\" , {}) or {} # Create alphabetically sorted list of all fields all_fields = [] # Add all misc_json fields, sorted alphabetically if misc_json : for key , value in sorted ( misc_json . items ()): all_fields . append (( key , value )) return render_template ( 'readonly_sector_view.html' , sector_type = sector_type , id_incidence = id_incidence , crud_type = crud_type , all_fields = all_fields , misc_json = misc_json )","title":"Renders a read-only view for an unsupported sector"},{"location":"reference/arb/portal/utils/sector_util/","text":"arb.portal.utils.sector_util Sector utilities for resolving and classifying feedback form sectors. This module provides functions to extract sector payloads from Excel data, resolve sector and sector_type for incidences, and map sector names to broad classifications (e.g., 'Oil & Gas', 'Landfill'). Attributes: extract_sector_payload ( function ) \u2013 Combines worksheet tab and metadata into a payload. get_sector_info ( function ) \u2013 Resolves sector and sector_type for an incidence ID. resolve_sector ( function ) \u2013 Determines the correct sector from FK and JSON sources. get_sector_type ( function ) \u2013 Maps a sector name to its broad classification. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.sector_util import get_sector_info sector, sector_type = get_sector_info(db, base, 123) Notes Used by feedback portal ingestion and display logic. Integrates with SQLAlchemy and Excel ingestion utilities. extract_sector_payload ( xl_dict , metadata_key = 'metadata' , tab_name = 'Feedback Form' ) Combines worksheet tab contents with sector metadata into a single payload for database insertion. Parameters: xl_dict ( dict ) \u2013 Dictionary from an Excel-parsed source, containing 'metadata' and 'tab_contents'. metadata_key ( str , default: 'metadata' ) \u2013 Key in xl_dict containing the metadata dictionary (default: \"metadata\"). tab_name ( str , default: 'Feedback Form' ) \u2013 Name of the worksheet tab to extract from tab_contents (default: \"Feedback Form\"). Returns: dict ( dict ) \u2013 Combined payload with tab_contents and sector included. Raises: ValueError \u2013 If the metadata key, sector, or tab_name is missing. Examples: payload = extract_sector_payload(xl_dict) Returns a dict with tab data and sector for database insertion Notes Used during Excel ingestion to prepare data for database upload. Source code in arb\\portal\\utils\\sector_util.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def extract_sector_payload ( xl_dict : dict , metadata_key : str = \"metadata\" , tab_name : str = \"Feedback Form\" ) -> dict : \"\"\" Combines worksheet tab contents with sector metadata into a single payload for database insertion. Args: xl_dict (dict): Dictionary from an Excel-parsed source, containing 'metadata' and 'tab_contents'. metadata_key (str): Key in xl_dict containing the metadata dictionary (default: \"metadata\"). tab_name (str): Name of the worksheet tab to extract from tab_contents (default: \"Feedback Form\"). Returns: dict: Combined payload with tab_contents and sector included. Raises: ValueError: If the metadata key, sector, or tab_name is missing. Examples: payload = extract_sector_payload(xl_dict) # Returns a dict with tab data and sector for database insertion Notes: - Used during Excel ingestion to prepare data for database upload. \"\"\" if metadata_key not in xl_dict : raise ValueError ( f \"Expected key ' { metadata_key } ' in xl_dict but it was missing.\" ) metadata = xl_dict [ metadata_key ] sector = metadata . get ( \"sector\" ) if not sector : raise ValueError ( f \"Missing or empty 'sector' key in xl_dict[' { metadata_key } '].\" ) tab_contents = xl_dict . get ( \"tab_contents\" , {}) if tab_name not in tab_contents : raise ValueError ( f \"Tab ' { tab_name } ' not found in xl_dict['tab_contents'].\" ) tab_data = tab_contents [ tab_name ] . copy () tab_data [ \"sector\" ] = sector return tab_data get_sector_info ( db , base , id_ ) Resolve the sector and sector_type for a given incidence ID. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. base ( AutomapBase ) \u2013 SQLAlchemy Automapped declarative base. id_ ( int ) \u2013 ID of the row in the incidences table. Returns: tuple [ str , str ] \u2013 tuple[str, str]: (sector, sector_type) Examples: sector, sector_type = get_sector_info(db, base, 123) Returns the sector and its broad classification for the given ID Notes Uses both foreign key and misc_json to resolve the sector. Returns the sector and its type for display or further processing. Source code in arb\\portal\\utils\\sector_util.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def get_sector_info ( db : SQLAlchemy , base : AutomapBase , id_ : int ) -> tuple [ str , str ]: \"\"\" Resolve the sector and sector_type for a given incidence ID. Args: db (SQLAlchemy): SQLAlchemy database instance. base (AutomapBase): SQLAlchemy Automapped declarative base. id_ (int): ID of the row in the `incidences` table. Returns: tuple[str, str]: (sector, sector_type) Examples: sector, sector_type = get_sector_info(db, base, 123) # Returns the sector and its broad classification for the given ID Notes: - Uses both foreign key and misc_json to resolve the sector. - Returns the sector and its type for display or further processing. \"\"\" logger . debug ( f \"get_sector_info() called to determine sector & sector type for { id_ =} \" ) primary_table_name = \"incidences\" json_column = \"misc_json\" # Find the sector from the foreign table if incidence was created by plume tracker. sector_by_foreign_key = get_foreign_value ( db , base , primary_table_name = primary_table_name , foreign_table_name = \"sources\" , primary_table_fk_name = \"source_id\" , foreign_table_column_name = \"sector\" , primary_table_pk_value = id_ , ) # Get the row and misc_json field from the incidence table row , misc_json = get_table_row_and_column ( db , base , table_name = primary_table_name , column_name = json_column , id_ = id_ , ) if misc_json is None : misc_json = {} sector = resolve_sector ( sector_by_foreign_key , row , misc_json ) sector_type = get_sector_type ( sector ) logger . debug ( f \"get_sector_info() returning { sector =} { sector_type =} \" ) return sector , sector_type get_sector_type ( sector ) Map a sector name to its broad classification. Parameters: sector ( str ) \u2013 Input sector label. Returns: str ( str ) \u2013 One of \"Oil & Gas\", \"Landfill\", or the original sector name for unsupported sectors. Notes For unsupported sectors, returns the original sector name so the calling code can handle it appropriately (e.g., show read-only view). This prevents ValueError exceptions and allows graceful handling of new sectors. Examples: sector_type = get_sector_type('Oil & Gas') Returns 'Oil & Gas' sector_type = get_sector_type('Unknown') Returns 'Unknown' for unsupported sectors Source code in arb\\portal\\utils\\sector_util.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def get_sector_type ( sector : str ) -> str : \"\"\" Map a sector name to its broad classification. Args: sector (str): Input sector label. Returns: str: One of \"Oil & Gas\", \"Landfill\", or the original sector name for unsupported sectors. Notes: - For unsupported sectors, returns the original sector name so the calling code can handle it appropriately (e.g., show read-only view). - This prevents ValueError exceptions and allows graceful handling of new sectors. Examples: sector_type = get_sector_type('Oil & Gas') # Returns 'Oil & Gas' sector_type = get_sector_type('Unknown') # Returns 'Unknown' for unsupported sectors \"\"\" if sector in OIL_AND_GAS_SECTORS : return \"Oil & Gas\" elif sector in LANDFILL_SECTORS : return \"Landfill\" else : # Return the original sector name for unsupported sectors # This allows the calling code to handle it gracefully return sector resolve_sector ( sector_by_foreign_key , row , misc_json ) Determine the appropriate sector from FK and JSON sources. Parameters: sector_by_foreign_key ( str | None ) \u2013 Sector from sources table. row ( Any ) \u2013 Row from incidences table (SQLAlchemy result). misc_json ( dict ) \u2013 Parsed misc_json content. Returns: str ( str ) \u2013 Sector string. Notes Prefers the sector from misc_json if both are present and conflict. Logs warnings if sector cannot be determined. Does not raise on mismatch, but logs an error. Examples: sector = resolve_sector('Oil', row, {'sector': 'Oil'}) Returns 'Oil' as the resolved sector Source code in arb\\portal\\utils\\sector_util.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def resolve_sector ( sector_by_foreign_key : str | None , row : Any , misc_json : dict ) -> str : \"\"\" Determine the appropriate sector from FK and JSON sources. Args: sector_by_foreign_key (str | None): Sector from `sources` table. row (Any): Row from `incidences` table (SQLAlchemy result). misc_json (dict): Parsed `misc_json` content. Returns: str: Sector string. Notes: - Prefers the sector from misc_json if both are present and conflict. - Logs warnings if sector cannot be determined. - Does not raise on mismatch, but logs an error. Examples: sector = resolve_sector('Oil', row, {'sector': 'Oil'}) # Returns 'Oil' as the resolved sector \"\"\" logger . debug ( f \"resolve_sector() called with { sector_by_foreign_key =} , { row =} , { misc_json =} \" ) sector_by_json = misc_json . get ( \"sector\" ) if sector_by_foreign_key is None : logger . warning ( f \"sector column value in sources table is None.\" ) if sector_by_json is None : logger . warning ( f \"'sector' not in misc_json\" ) if sector_by_foreign_key is None and sector_by_json is None : logger . error ( f \"Can't determine incidence sector\" ) raise ValueError ( \"Can't determine incidence sector\" ) if sector_by_foreign_key is not None and sector_by_json is not None : if sector_by_foreign_key != sector_by_json : logger . error ( f \"Sector mismatch: { sector_by_foreign_key =} , { sector_by_json =} \" ) # raise ValueError(\"Can't determine incidence sector\") # sector_by_json given priority over foreign key sector = sector_by_json or sector_by_foreign_key logger . debug ( f \"resolve_sector() returning { sector =} \" ) return sector","title":"arb.portal.utils.sector_util"},{"location":"reference/arb/portal/utils/sector_util/#arbportalutilssector_util","text":"Sector utilities for resolving and classifying feedback form sectors. This module provides functions to extract sector payloads from Excel data, resolve sector and sector_type for incidences, and map sector names to broad classifications (e.g., 'Oil & Gas', 'Landfill'). Attributes: extract_sector_payload ( function ) \u2013 Combines worksheet tab and metadata into a payload. get_sector_info ( function ) \u2013 Resolves sector and sector_type for an incidence ID. resolve_sector ( function ) \u2013 Determines the correct sector from FK and JSON sources. get_sector_type ( function ) \u2013 Maps a sector name to its broad classification. logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.sector_util import get_sector_info sector, sector_type = get_sector_info(db, base, 123) Notes Used by feedback portal ingestion and display logic. Integrates with SQLAlchemy and Excel ingestion utilities.","title":"arb.portal.utils.sector_util"},{"location":"reference/arb/portal/utils/sector_util/#arb.portal.utils.sector_util.extract_sector_payload","text":"Combines worksheet tab contents with sector metadata into a single payload for database insertion. Parameters: xl_dict ( dict ) \u2013 Dictionary from an Excel-parsed source, containing 'metadata' and 'tab_contents'. metadata_key ( str , default: 'metadata' ) \u2013 Key in xl_dict containing the metadata dictionary (default: \"metadata\"). tab_name ( str , default: 'Feedback Form' ) \u2013 Name of the worksheet tab to extract from tab_contents (default: \"Feedback Form\"). Returns: dict ( dict ) \u2013 Combined payload with tab_contents and sector included. Raises: ValueError \u2013 If the metadata key, sector, or tab_name is missing. Examples: payload = extract_sector_payload(xl_dict)","title":"extract_sector_payload"},{"location":"reference/arb/portal/utils/sector_util/#arb.portal.utils.sector_util.extract_sector_payload--returns-a-dict-with-tab-data-and-sector-for-database-insertion","text":"Notes Used during Excel ingestion to prepare data for database upload. Source code in arb\\portal\\utils\\sector_util.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def extract_sector_payload ( xl_dict : dict , metadata_key : str = \"metadata\" , tab_name : str = \"Feedback Form\" ) -> dict : \"\"\" Combines worksheet tab contents with sector metadata into a single payload for database insertion. Args: xl_dict (dict): Dictionary from an Excel-parsed source, containing 'metadata' and 'tab_contents'. metadata_key (str): Key in xl_dict containing the metadata dictionary (default: \"metadata\"). tab_name (str): Name of the worksheet tab to extract from tab_contents (default: \"Feedback Form\"). Returns: dict: Combined payload with tab_contents and sector included. Raises: ValueError: If the metadata key, sector, or tab_name is missing. Examples: payload = extract_sector_payload(xl_dict) # Returns a dict with tab data and sector for database insertion Notes: - Used during Excel ingestion to prepare data for database upload. \"\"\" if metadata_key not in xl_dict : raise ValueError ( f \"Expected key ' { metadata_key } ' in xl_dict but it was missing.\" ) metadata = xl_dict [ metadata_key ] sector = metadata . get ( \"sector\" ) if not sector : raise ValueError ( f \"Missing or empty 'sector' key in xl_dict[' { metadata_key } '].\" ) tab_contents = xl_dict . get ( \"tab_contents\" , {}) if tab_name not in tab_contents : raise ValueError ( f \"Tab ' { tab_name } ' not found in xl_dict['tab_contents'].\" ) tab_data = tab_contents [ tab_name ] . copy () tab_data [ \"sector\" ] = sector return tab_data","title":"Returns a dict with tab data and sector for database insertion"},{"location":"reference/arb/portal/utils/sector_util/#arb.portal.utils.sector_util.get_sector_info","text":"Resolve the sector and sector_type for a given incidence ID. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. base ( AutomapBase ) \u2013 SQLAlchemy Automapped declarative base. id_ ( int ) \u2013 ID of the row in the incidences table. Returns: tuple [ str , str ] \u2013 tuple[str, str]: (sector, sector_type) Examples: sector, sector_type = get_sector_info(db, base, 123)","title":"get_sector_info"},{"location":"reference/arb/portal/utils/sector_util/#arb.portal.utils.sector_util.get_sector_info--returns-the-sector-and-its-broad-classification-for-the-given-id","text":"Notes Uses both foreign key and misc_json to resolve the sector. Returns the sector and its type for display or further processing. Source code in arb\\portal\\utils\\sector_util.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def get_sector_info ( db : SQLAlchemy , base : AutomapBase , id_ : int ) -> tuple [ str , str ]: \"\"\" Resolve the sector and sector_type for a given incidence ID. Args: db (SQLAlchemy): SQLAlchemy database instance. base (AutomapBase): SQLAlchemy Automapped declarative base. id_ (int): ID of the row in the `incidences` table. Returns: tuple[str, str]: (sector, sector_type) Examples: sector, sector_type = get_sector_info(db, base, 123) # Returns the sector and its broad classification for the given ID Notes: - Uses both foreign key and misc_json to resolve the sector. - Returns the sector and its type for display or further processing. \"\"\" logger . debug ( f \"get_sector_info() called to determine sector & sector type for { id_ =} \" ) primary_table_name = \"incidences\" json_column = \"misc_json\" # Find the sector from the foreign table if incidence was created by plume tracker. sector_by_foreign_key = get_foreign_value ( db , base , primary_table_name = primary_table_name , foreign_table_name = \"sources\" , primary_table_fk_name = \"source_id\" , foreign_table_column_name = \"sector\" , primary_table_pk_value = id_ , ) # Get the row and misc_json field from the incidence table row , misc_json = get_table_row_and_column ( db , base , table_name = primary_table_name , column_name = json_column , id_ = id_ , ) if misc_json is None : misc_json = {} sector = resolve_sector ( sector_by_foreign_key , row , misc_json ) sector_type = get_sector_type ( sector ) logger . debug ( f \"get_sector_info() returning { sector =} { sector_type =} \" ) return sector , sector_type","title":"Returns the sector and its broad classification for the given ID"},{"location":"reference/arb/portal/utils/sector_util/#arb.portal.utils.sector_util.get_sector_type","text":"Map a sector name to its broad classification. Parameters: sector ( str ) \u2013 Input sector label. Returns: str ( str ) \u2013 One of \"Oil & Gas\", \"Landfill\", or the original sector name for unsupported sectors. Notes For unsupported sectors, returns the original sector name so the calling code can handle it appropriately (e.g., show read-only view). This prevents ValueError exceptions and allows graceful handling of new sectors. Examples: sector_type = get_sector_type('Oil & Gas')","title":"get_sector_type"},{"location":"reference/arb/portal/utils/sector_util/#arb.portal.utils.sector_util.get_sector_type--returns-oil-gas","text":"sector_type = get_sector_type('Unknown')","title":"Returns 'Oil &amp; Gas'"},{"location":"reference/arb/portal/utils/sector_util/#arb.portal.utils.sector_util.get_sector_type--returns-unknown-for-unsupported-sectors","text":"Source code in arb\\portal\\utils\\sector_util.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 def get_sector_type ( sector : str ) -> str : \"\"\" Map a sector name to its broad classification. Args: sector (str): Input sector label. Returns: str: One of \"Oil & Gas\", \"Landfill\", or the original sector name for unsupported sectors. Notes: - For unsupported sectors, returns the original sector name so the calling code can handle it appropriately (e.g., show read-only view). - This prevents ValueError exceptions and allows graceful handling of new sectors. Examples: sector_type = get_sector_type('Oil & Gas') # Returns 'Oil & Gas' sector_type = get_sector_type('Unknown') # Returns 'Unknown' for unsupported sectors \"\"\" if sector in OIL_AND_GAS_SECTORS : return \"Oil & Gas\" elif sector in LANDFILL_SECTORS : return \"Landfill\" else : # Return the original sector name for unsupported sectors # This allows the calling code to handle it gracefully return sector","title":"Returns 'Unknown' for unsupported sectors"},{"location":"reference/arb/portal/utils/sector_util/#arb.portal.utils.sector_util.resolve_sector","text":"Determine the appropriate sector from FK and JSON sources. Parameters: sector_by_foreign_key ( str | None ) \u2013 Sector from sources table. row ( Any ) \u2013 Row from incidences table (SQLAlchemy result). misc_json ( dict ) \u2013 Parsed misc_json content. Returns: str ( str ) \u2013 Sector string. Notes Prefers the sector from misc_json if both are present and conflict. Logs warnings if sector cannot be determined. Does not raise on mismatch, but logs an error. Examples: sector = resolve_sector('Oil', row, {'sector': 'Oil'})","title":"resolve_sector"},{"location":"reference/arb/portal/utils/sector_util/#arb.portal.utils.sector_util.resolve_sector--returns-oil-as-the-resolved-sector","text":"Source code in arb\\portal\\utils\\sector_util.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def resolve_sector ( sector_by_foreign_key : str | None , row : Any , misc_json : dict ) -> str : \"\"\" Determine the appropriate sector from FK and JSON sources. Args: sector_by_foreign_key (str | None): Sector from `sources` table. row (Any): Row from `incidences` table (SQLAlchemy result). misc_json (dict): Parsed `misc_json` content. Returns: str: Sector string. Notes: - Prefers the sector from misc_json if both are present and conflict. - Logs warnings if sector cannot be determined. - Does not raise on mismatch, but logs an error. Examples: sector = resolve_sector('Oil', row, {'sector': 'Oil'}) # Returns 'Oil' as the resolved sector \"\"\" logger . debug ( f \"resolve_sector() called with { sector_by_foreign_key =} , { row =} , { misc_json =} \" ) sector_by_json = misc_json . get ( \"sector\" ) if sector_by_foreign_key is None : logger . warning ( f \"sector column value in sources table is None.\" ) if sector_by_json is None : logger . warning ( f \"'sector' not in misc_json\" ) if sector_by_foreign_key is None and sector_by_json is None : logger . error ( f \"Can't determine incidence sector\" ) raise ValueError ( \"Can't determine incidence sector\" ) if sector_by_foreign_key is not None and sector_by_json is not None : if sector_by_foreign_key != sector_by_json : logger . error ( f \"Sector mismatch: { sector_by_foreign_key =} , { sector_by_json =} \" ) # raise ValueError(\"Can't determine incidence sector\") # sector_by_json given priority over foreign key sector = sector_by_json or sector_by_foreign_key logger . debug ( f \"resolve_sector() returning { sector =} \" ) return sector","title":"Returns 'Oil' as the resolved sector"},{"location":"reference/arb/portal/utils/test_cleanup_util/","text":"arb.portal.utils.test_cleanup_util Test data cleanup utilities for removing testing rows from the database. This module provides functions to safely delete testing data from the portal_updates and incidences tables based on id_incidence ranges. Attributes: logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.test_cleanup_util import delete_testing_rows deleted_count = delete_testing_rows(db, base, 2000, 2999) Notes Designed for cleaning up testing data with id_incidence in ranges 2000-2999 or 4000+ Provides safety checks and logging for audit trails Should be used carefully in production environments delete_testing_range_2000_2999 ( db , base , dry_run = False ) Convenience function to delete testing rows with id_incidence in range 2000-2999. Parameters: db ( SQLAlchemy ) \u2013 Database instance. base ( AutomapBase ) \u2013 Reflected schema base. dry_run ( bool , default: False ) \u2013 If True, simulate deletion without actually deleting. Defaults to False. Returns: dict [ str , int ] \u2013 dict[str, int]: Dictionary with counts of deleted rows from each table. Examples: Delete testing rows 2000-2999 result = delete_testing_range_2000_2999(db, base) Returns: {'portal_updates': 5, 'incidences': 5} Notes Calls delete_testing_rows with range 2000-2999 Convenient for cleaning up first testing range Source code in arb\\portal\\utils\\test_cleanup_util.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def delete_testing_range_2000_2999 ( db : SQLAlchemy , base : AutomapBase , dry_run : bool = False ) -> dict [ str , int ]: \"\"\" Convenience function to delete testing rows with id_incidence in range 2000-2999. Args: db (SQLAlchemy): Database instance. base (AutomapBase): Reflected schema base. dry_run (bool): If True, simulate deletion without actually deleting. Defaults to False. Returns: dict[str, int]: Dictionary with counts of deleted rows from each table. Examples: # Delete testing rows 2000-2999 result = delete_testing_range_2000_2999(db, base) # Returns: {'portal_updates': 5, 'incidences': 5} Notes: - Calls delete_testing_rows with range 2000-2999 - Convenient for cleaning up first testing range \"\"\" return delete_testing_rows ( db , base , 2000 , 2999 , dry_run = dry_run ) delete_testing_range_4000_plus ( db , base , max_id = None , dry_run = False ) Convenience function to delete testing rows with id_incidence >= 4000. Parameters: db ( SQLAlchemy ) \u2013 Database instance. base ( AutomapBase ) \u2013 Reflected schema base. max_id ( Optional [ int ] , default: None ) \u2013 Maximum id_incidence value (inclusive). If None, deletes all >= 4000. dry_run ( bool , default: False ) \u2013 If True, simulate deletion without actually deleting. Defaults to False. Returns: dict [ str , int ] \u2013 dict[str, int]: Dictionary with counts of deleted rows from each table. Examples: Delete all testing rows >= 4000 result = delete_testing_range_4000_plus(db, base) Returns: {'portal_updates': 10, 'incidences': 10} Delete testing rows 4000-4999 result = delete_testing_range_4000_plus(db, base, max_id=4999) Returns: {'portal_updates': 5, 'incidences': 5} Notes Calls delete_testing_rows with range 4000 to max_id (or effectively unlimited if max_id is None) Convenient for cleaning up second testing range Source code in arb\\portal\\utils\\test_cleanup_util.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def delete_testing_range_4000_plus ( db : SQLAlchemy , base : AutomapBase , max_id : Optional [ int ] = None , dry_run : bool = False ) -> dict [ str , int ]: \"\"\" Convenience function to delete testing rows with id_incidence >= 4000. Args: db (SQLAlchemy): Database instance. base (AutomapBase): Reflected schema base. max_id (Optional[int]): Maximum id_incidence value (inclusive). If None, deletes all >= 4000. dry_run (bool): If True, simulate deletion without actually deleting. Defaults to False. Returns: dict[str, int]: Dictionary with counts of deleted rows from each table. Examples: # Delete all testing rows >= 4000 result = delete_testing_range_4000_plus(db, base) # Returns: {'portal_updates': 10, 'incidences': 10} # Delete testing rows 4000-4999 result = delete_testing_range_4000_plus(db, base, max_id=4999) # Returns: {'portal_updates': 5, 'incidences': 5} Notes: - Calls delete_testing_rows with range 4000 to max_id (or effectively unlimited if max_id is None) - Convenient for cleaning up second testing range \"\"\" if max_id is None : # Use a very large number to effectively delete all >= 4000 max_id = 999999999 return delete_testing_rows ( db , base , 4000 , max_id , dry_run = dry_run ) delete_testing_rows ( db , base , min_id , max_id , dry_run = False ) Delete rows from portal_updates and incidences tables where id_incidence is in the specified range. Parameters: db ( SQLAlchemy ) \u2013 Database instance. base ( AutomapBase ) \u2013 Reflected schema base. min_id ( int ) \u2013 Minimum id_incidence value (inclusive). max_id ( int ) \u2013 Maximum id_incidence value (inclusive). dry_run ( bool , default: False ) \u2013 If True, simulate deletion without actually deleting. Defaults to False. Returns: dict [ str , int ] \u2013 dict[str, int]: Dictionary with counts of deleted rows from each table. Raises: ValueError \u2013 If min_id > max_id or if range is invalid. Exception \u2013 If database operations fail. Examples: Delete testing rows with id_incidence 2000-2999 result = delete_testing_rows(db, base, 2000, 2999) Returns: {'portal_updates': 5, 'incidences': 5} Dry run to see what would be deleted result = delete_testing_rows(db, base, 4000, 4999, dry_run=True) Returns: {'portal_updates': 3, 'incidences': 3} (without actually deleting) Notes Deletes from portal_updates table first, then incidences table Logs detailed information about what is being deleted Provides safety through dry_run option Commits changes unless dry_run is True Source code in arb\\portal\\utils\\test_cleanup_util.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def delete_testing_rows ( db : SQLAlchemy , base : AutomapBase , min_id : int , max_id : int , dry_run : bool = False ) -> dict [ str , int ]: \"\"\" Delete rows from portal_updates and incidences tables where id_incidence is in the specified range. Args: db (SQLAlchemy): Database instance. base (AutomapBase): Reflected schema base. min_id (int): Minimum id_incidence value (inclusive). max_id (int): Maximum id_incidence value (inclusive). dry_run (bool): If True, simulate deletion without actually deleting. Defaults to False. Returns: dict[str, int]: Dictionary with counts of deleted rows from each table. Raises: ValueError: If min_id > max_id or if range is invalid. Exception: If database operations fail. Examples: # Delete testing rows with id_incidence 2000-2999 result = delete_testing_rows(db, base, 2000, 2999) # Returns: {'portal_updates': 5, 'incidences': 5} # Dry run to see what would be deleted result = delete_testing_rows(db, base, 4000, 4999, dry_run=True) # Returns: {'portal_updates': 3, 'incidences': 3} (without actually deleting) Notes: - Deletes from portal_updates table first, then incidences table - Logs detailed information about what is being deleted - Provides safety through dry_run option - Commits changes unless dry_run is True \"\"\" if min_id > max_id : raise ValueError ( f \"Invalid range: min_id ( { min_id } ) cannot be greater than max_id ( { max_id } )\" ) if min_id < 0 or max_id < 0 : raise ValueError ( \"id_incidence values must be non-negative\" ) logger . info ( f \"Starting deletion of testing rows with id_incidence range { min_id } - { max_id } (dry_run= { dry_run } )\" ) result = { 'portal_updates' : 0 , 'incidences' : 0 } try : # Get table models portal_updates_model = getattr ( base . classes , 'portal_updates' , None ) incidences_model = getattr ( base . classes , 'incidences' , None ) if not portal_updates_model : raise AttributeError ( \"portal_updates table not found in database schema\" ) if not incidences_model : raise AttributeError ( \"incidences table not found in database schema\" ) # Delete from portal_updates table first (foreign key dependency) portal_updates_query = db . session . query ( portal_updates_model ) . filter ( portal_updates_model . id_incidence >= min_id , portal_updates_model . id_incidence <= max_id ) portal_updates_count = portal_updates_query . count () logger . info ( f \"Found { portal_updates_count } rows in portal_updates table to delete\" ) if not dry_run and portal_updates_count > 0 : portal_updates_query . delete ( synchronize_session = False ) result [ 'portal_updates' ] = portal_updates_count logger . info ( f \"Deleted { portal_updates_count } rows from portal_updates table\" ) elif dry_run : result [ 'portal_updates' ] = portal_updates_count logger . info ( f \"Would delete { portal_updates_count } rows from portal_updates table (dry run)\" ) # Delete from incidences table incidences_query = db . session . query ( incidences_model ) . filter ( incidences_model . id_incidence >= min_id , incidences_model . id_incidence <= max_id ) incidences_count = incidences_query . count () logger . info ( f \"Found { incidences_count } rows in incidences table to delete\" ) if not dry_run and incidences_count > 0 : incidences_query . delete ( synchronize_session = False ) result [ 'incidences' ] = incidences_count logger . info ( f \"Deleted { incidences_count } rows from incidences table\" ) elif dry_run : result [ 'incidences' ] = incidences_count logger . info ( f \"Would delete { incidences_count } rows from incidences table (dry run)\" ) # Commit changes unless dry run if not dry_run : db . session . commit () logger . info ( f \"Successfully committed deletion of { sum ( result . values ()) } total rows\" ) else : logger . info ( f \"Dry run completed - would delete { sum ( result . values ()) } total rows\" ) return result except Exception as e : logger . error ( f \"Error during deletion of testing rows: { e } \" ) if not dry_run : db . session . rollback () logger . info ( \"Database session rolled back due to error\" ) raise list_testing_rows ( db , base , min_id = 2000 , max_id = None ) List testing rows from portal_updates and incidences tables without deleting them. Parameters: db ( SQLAlchemy ) \u2013 Database instance. base ( AutomapBase ) \u2013 Reflected schema base. min_id ( int , default: 2000 ) \u2013 Minimum id_incidence value (inclusive). Defaults to 2000. max_id ( Optional [ int ] , default: None ) \u2013 Maximum id_incidence value (inclusive). If None, lists all >= min_id. Returns: dict [ str , list ] \u2013 dict[str, list]: Dictionary with lists of row data from each table. Examples: List all testing rows >= 2000 result = list_testing_rows(db, base) Returns: {'portal_updates': [...], 'incidences': [...]} List testing rows 4000-4999 result = list_testing_rows(db, base, 4000, 4999) Returns: {'portal_updates': [...], 'incidences': [...]} Notes Useful for previewing what would be deleted Returns actual row data for inspection Does not modify the database Source code in arb\\portal\\utils\\test_cleanup_util.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def list_testing_rows ( db : SQLAlchemy , base : AutomapBase , min_id : int = 2000 , max_id : Optional [ int ] = None ) -> dict [ str , list ]: \"\"\" List testing rows from portal_updates and incidences tables without deleting them. Args: db (SQLAlchemy): Database instance. base (AutomapBase): Reflected schema base. min_id (int): Minimum id_incidence value (inclusive). Defaults to 2000. max_id (Optional[int]): Maximum id_incidence value (inclusive). If None, lists all >= min_id. Returns: dict[str, list]: Dictionary with lists of row data from each table. Examples: # List all testing rows >= 2000 result = list_testing_rows(db, base) # Returns: {'portal_updates': [...], 'incidences': [...]} # List testing rows 4000-4999 result = list_testing_rows(db, base, 4000, 4999) # Returns: {'portal_updates': [...], 'incidences': [...]} Notes: - Useful for previewing what would be deleted - Returns actual row data for inspection - Does not modify the database \"\"\" if max_id is None : max_id = 999999999 if min_id > max_id : raise ValueError ( f \"Invalid range: min_id ( { min_id } ) cannot be greater than max_id ( { max_id } )\" ) logger . info ( f \"Listing testing rows with id_incidence range { min_id } - { max_id } \" ) result = { 'portal_updates' : [], 'incidences' : []} try : # Get table models portal_updates_model = getattr ( base . classes , 'portal_updates' , None ) incidences_model = getattr ( base . classes , 'incidences' , None ) if not portal_updates_model : raise AttributeError ( \"portal_updates table not found in database schema\" ) if not incidences_model : raise AttributeError ( \"incidences table not found in database schema\" ) # Query portal_updates table portal_updates_rows = db . session . query ( portal_updates_model ) . filter ( portal_updates_model . id_incidence >= min_id , portal_updates_model . id_incidence <= max_id ) . all () result [ 'portal_updates' ] = [ { 'id_incidence' : row . id_incidence , 'sector' : getattr ( row , 'sector' , None ), 'created_at' : getattr ( row , 'created_at' , None ), 'updated_at' : getattr ( row , 'updated_at' , None ) } for row in portal_updates_rows ] # Query incidences table incidences_rows = db . session . query ( incidences_model ) . filter ( incidences_model . id_incidence >= min_id , incidences_model . id_incidence <= max_id ) . all () result [ 'incidences' ] = [ { 'id_incidence' : row . id_incidence , 'sector' : getattr ( row , 'sector' , None ), 'created_at' : getattr ( row , 'created_at' , None ), 'updated_at' : getattr ( row , 'updated_at' , None ) } for row in incidences_rows ] logger . info ( f \"Found { len ( result [ 'portal_updates' ]) } rows in portal_updates and { len ( result [ 'incidences' ]) } rows in incidences\" ) return result except Exception as e : logger . error ( f \"Error during listing of testing rows: { e } \" ) raise","title":"arb.portal.utils.test_cleanup_util"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arbportalutilstest_cleanup_util","text":"Test data cleanup utilities for removing testing rows from the database. This module provides functions to safely delete testing data from the portal_updates and incidences tables based on id_incidence ranges. Attributes: logger ( Logger ) \u2013 Logger instance for this module. Examples: from arb.portal.utils.test_cleanup_util import delete_testing_rows deleted_count = delete_testing_rows(db, base, 2000, 2999) Notes Designed for cleaning up testing data with id_incidence in ranges 2000-2999 or 4000+ Provides safety checks and logging for audit trails Should be used carefully in production environments","title":"arb.portal.utils.test_cleanup_util"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_range_2000_2999","text":"Convenience function to delete testing rows with id_incidence in range 2000-2999. Parameters: db ( SQLAlchemy ) \u2013 Database instance. base ( AutomapBase ) \u2013 Reflected schema base. dry_run ( bool , default: False ) \u2013 If True, simulate deletion without actually deleting. Defaults to False. Returns: dict [ str , int ] \u2013 dict[str, int]: Dictionary with counts of deleted rows from each table. Examples:","title":"delete_testing_range_2000_2999"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_range_2000_2999--delete-testing-rows-2000-2999","text":"result = delete_testing_range_2000_2999(db, base)","title":"Delete testing rows 2000-2999"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_range_2000_2999--returns-portal_updates-5-incidences-5","text":"Notes Calls delete_testing_rows with range 2000-2999 Convenient for cleaning up first testing range Source code in arb\\portal\\utils\\test_cleanup_util.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def delete_testing_range_2000_2999 ( db : SQLAlchemy , base : AutomapBase , dry_run : bool = False ) -> dict [ str , int ]: \"\"\" Convenience function to delete testing rows with id_incidence in range 2000-2999. Args: db (SQLAlchemy): Database instance. base (AutomapBase): Reflected schema base. dry_run (bool): If True, simulate deletion without actually deleting. Defaults to False. Returns: dict[str, int]: Dictionary with counts of deleted rows from each table. Examples: # Delete testing rows 2000-2999 result = delete_testing_range_2000_2999(db, base) # Returns: {'portal_updates': 5, 'incidences': 5} Notes: - Calls delete_testing_rows with range 2000-2999 - Convenient for cleaning up first testing range \"\"\" return delete_testing_rows ( db , base , 2000 , 2999 , dry_run = dry_run )","title":"Returns: {'portal_updates': 5, 'incidences': 5}"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_range_4000_plus","text":"Convenience function to delete testing rows with id_incidence >= 4000. Parameters: db ( SQLAlchemy ) \u2013 Database instance. base ( AutomapBase ) \u2013 Reflected schema base. max_id ( Optional [ int ] , default: None ) \u2013 Maximum id_incidence value (inclusive). If None, deletes all >= 4000. dry_run ( bool , default: False ) \u2013 If True, simulate deletion without actually deleting. Defaults to False. Returns: dict [ str , int ] \u2013 dict[str, int]: Dictionary with counts of deleted rows from each table. Examples:","title":"delete_testing_range_4000_plus"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_range_4000_plus--delete-all-testing-rows-4000","text":"result = delete_testing_range_4000_plus(db, base)","title":"Delete all testing rows &gt;= 4000"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_range_4000_plus--returns-portal_updates-10-incidences-10","text":"","title":"Returns: {'portal_updates': 10, 'incidences': 10}"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_range_4000_plus--delete-testing-rows-4000-4999","text":"result = delete_testing_range_4000_plus(db, base, max_id=4999)","title":"Delete testing rows 4000-4999"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_range_4000_plus--returns-portal_updates-5-incidences-5","text":"Notes Calls delete_testing_rows with range 4000 to max_id (or effectively unlimited if max_id is None) Convenient for cleaning up second testing range Source code in arb\\portal\\utils\\test_cleanup_util.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def delete_testing_range_4000_plus ( db : SQLAlchemy , base : AutomapBase , max_id : Optional [ int ] = None , dry_run : bool = False ) -> dict [ str , int ]: \"\"\" Convenience function to delete testing rows with id_incidence >= 4000. Args: db (SQLAlchemy): Database instance. base (AutomapBase): Reflected schema base. max_id (Optional[int]): Maximum id_incidence value (inclusive). If None, deletes all >= 4000. dry_run (bool): If True, simulate deletion without actually deleting. Defaults to False. Returns: dict[str, int]: Dictionary with counts of deleted rows from each table. Examples: # Delete all testing rows >= 4000 result = delete_testing_range_4000_plus(db, base) # Returns: {'portal_updates': 10, 'incidences': 10} # Delete testing rows 4000-4999 result = delete_testing_range_4000_plus(db, base, max_id=4999) # Returns: {'portal_updates': 5, 'incidences': 5} Notes: - Calls delete_testing_rows with range 4000 to max_id (or effectively unlimited if max_id is None) - Convenient for cleaning up second testing range \"\"\" if max_id is None : # Use a very large number to effectively delete all >= 4000 max_id = 999999999 return delete_testing_rows ( db , base , 4000 , max_id , dry_run = dry_run )","title":"Returns: {'portal_updates': 5, 'incidences': 5}"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_rows","text":"Delete rows from portal_updates and incidences tables where id_incidence is in the specified range. Parameters: db ( SQLAlchemy ) \u2013 Database instance. base ( AutomapBase ) \u2013 Reflected schema base. min_id ( int ) \u2013 Minimum id_incidence value (inclusive). max_id ( int ) \u2013 Maximum id_incidence value (inclusive). dry_run ( bool , default: False ) \u2013 If True, simulate deletion without actually deleting. Defaults to False. Returns: dict [ str , int ] \u2013 dict[str, int]: Dictionary with counts of deleted rows from each table. Raises: ValueError \u2013 If min_id > max_id or if range is invalid. Exception \u2013 If database operations fail. Examples:","title":"delete_testing_rows"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_rows--delete-testing-rows-with-id_incidence-2000-2999","text":"result = delete_testing_rows(db, base, 2000, 2999)","title":"Delete testing rows with id_incidence 2000-2999"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_rows--returns-portal_updates-5-incidences-5","text":"","title":"Returns: {'portal_updates': 5, 'incidences': 5}"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_rows--dry-run-to-see-what-would-be-deleted","text":"result = delete_testing_rows(db, base, 4000, 4999, dry_run=True)","title":"Dry run to see what would be deleted"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.delete_testing_rows--returns-portal_updates-3-incidences-3-without-actually-deleting","text":"Notes Deletes from portal_updates table first, then incidences table Logs detailed information about what is being deleted Provides safety through dry_run option Commits changes unless dry_run is True Source code in arb\\portal\\utils\\test_cleanup_util.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def delete_testing_rows ( db : SQLAlchemy , base : AutomapBase , min_id : int , max_id : int , dry_run : bool = False ) -> dict [ str , int ]: \"\"\" Delete rows from portal_updates and incidences tables where id_incidence is in the specified range. Args: db (SQLAlchemy): Database instance. base (AutomapBase): Reflected schema base. min_id (int): Minimum id_incidence value (inclusive). max_id (int): Maximum id_incidence value (inclusive). dry_run (bool): If True, simulate deletion without actually deleting. Defaults to False. Returns: dict[str, int]: Dictionary with counts of deleted rows from each table. Raises: ValueError: If min_id > max_id or if range is invalid. Exception: If database operations fail. Examples: # Delete testing rows with id_incidence 2000-2999 result = delete_testing_rows(db, base, 2000, 2999) # Returns: {'portal_updates': 5, 'incidences': 5} # Dry run to see what would be deleted result = delete_testing_rows(db, base, 4000, 4999, dry_run=True) # Returns: {'portal_updates': 3, 'incidences': 3} (without actually deleting) Notes: - Deletes from portal_updates table first, then incidences table - Logs detailed information about what is being deleted - Provides safety through dry_run option - Commits changes unless dry_run is True \"\"\" if min_id > max_id : raise ValueError ( f \"Invalid range: min_id ( { min_id } ) cannot be greater than max_id ( { max_id } )\" ) if min_id < 0 or max_id < 0 : raise ValueError ( \"id_incidence values must be non-negative\" ) logger . info ( f \"Starting deletion of testing rows with id_incidence range { min_id } - { max_id } (dry_run= { dry_run } )\" ) result = { 'portal_updates' : 0 , 'incidences' : 0 } try : # Get table models portal_updates_model = getattr ( base . classes , 'portal_updates' , None ) incidences_model = getattr ( base . classes , 'incidences' , None ) if not portal_updates_model : raise AttributeError ( \"portal_updates table not found in database schema\" ) if not incidences_model : raise AttributeError ( \"incidences table not found in database schema\" ) # Delete from portal_updates table first (foreign key dependency) portal_updates_query = db . session . query ( portal_updates_model ) . filter ( portal_updates_model . id_incidence >= min_id , portal_updates_model . id_incidence <= max_id ) portal_updates_count = portal_updates_query . count () logger . info ( f \"Found { portal_updates_count } rows in portal_updates table to delete\" ) if not dry_run and portal_updates_count > 0 : portal_updates_query . delete ( synchronize_session = False ) result [ 'portal_updates' ] = portal_updates_count logger . info ( f \"Deleted { portal_updates_count } rows from portal_updates table\" ) elif dry_run : result [ 'portal_updates' ] = portal_updates_count logger . info ( f \"Would delete { portal_updates_count } rows from portal_updates table (dry run)\" ) # Delete from incidences table incidences_query = db . session . query ( incidences_model ) . filter ( incidences_model . id_incidence >= min_id , incidences_model . id_incidence <= max_id ) incidences_count = incidences_query . count () logger . info ( f \"Found { incidences_count } rows in incidences table to delete\" ) if not dry_run and incidences_count > 0 : incidences_query . delete ( synchronize_session = False ) result [ 'incidences' ] = incidences_count logger . info ( f \"Deleted { incidences_count } rows from incidences table\" ) elif dry_run : result [ 'incidences' ] = incidences_count logger . info ( f \"Would delete { incidences_count } rows from incidences table (dry run)\" ) # Commit changes unless dry run if not dry_run : db . session . commit () logger . info ( f \"Successfully committed deletion of { sum ( result . values ()) } total rows\" ) else : logger . info ( f \"Dry run completed - would delete { sum ( result . values ()) } total rows\" ) return result except Exception as e : logger . error ( f \"Error during deletion of testing rows: { e } \" ) if not dry_run : db . session . rollback () logger . info ( \"Database session rolled back due to error\" ) raise","title":"Returns: {'portal_updates': 3, 'incidences': 3} (without actually deleting)"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.list_testing_rows","text":"List testing rows from portal_updates and incidences tables without deleting them. Parameters: db ( SQLAlchemy ) \u2013 Database instance. base ( AutomapBase ) \u2013 Reflected schema base. min_id ( int , default: 2000 ) \u2013 Minimum id_incidence value (inclusive). Defaults to 2000. max_id ( Optional [ int ] , default: None ) \u2013 Maximum id_incidence value (inclusive). If None, lists all >= min_id. Returns: dict [ str , list ] \u2013 dict[str, list]: Dictionary with lists of row data from each table. Examples:","title":"list_testing_rows"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.list_testing_rows--list-all-testing-rows-2000","text":"result = list_testing_rows(db, base)","title":"List all testing rows &gt;= 2000"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.list_testing_rows--returns-portal_updates-incidences","text":"","title":"Returns: {'portal_updates': [...], 'incidences': [...]}"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.list_testing_rows--list-testing-rows-4000-4999","text":"result = list_testing_rows(db, base, 4000, 4999)","title":"List testing rows 4000-4999"},{"location":"reference/arb/portal/utils/test_cleanup_util/#arb.portal.utils.test_cleanup_util.list_testing_rows--returns-portal_updates-incidences_1","text":"Notes Useful for previewing what would be deleted Returns actual row data for inspection Does not modify the database Source code in arb\\portal\\utils\\test_cleanup_util.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def list_testing_rows ( db : SQLAlchemy , base : AutomapBase , min_id : int = 2000 , max_id : Optional [ int ] = None ) -> dict [ str , list ]: \"\"\" List testing rows from portal_updates and incidences tables without deleting them. Args: db (SQLAlchemy): Database instance. base (AutomapBase): Reflected schema base. min_id (int): Minimum id_incidence value (inclusive). Defaults to 2000. max_id (Optional[int]): Maximum id_incidence value (inclusive). If None, lists all >= min_id. Returns: dict[str, list]: Dictionary with lists of row data from each table. Examples: # List all testing rows >= 2000 result = list_testing_rows(db, base) # Returns: {'portal_updates': [...], 'incidences': [...]} # List testing rows 4000-4999 result = list_testing_rows(db, base, 4000, 4999) # Returns: {'portal_updates': [...], 'incidences': [...]} Notes: - Useful for previewing what would be deleted - Returns actual row data for inspection - Does not modify the database \"\"\" if max_id is None : max_id = 999999999 if min_id > max_id : raise ValueError ( f \"Invalid range: min_id ( { min_id } ) cannot be greater than max_id ( { max_id } )\" ) logger . info ( f \"Listing testing rows with id_incidence range { min_id } - { max_id } \" ) result = { 'portal_updates' : [], 'incidences' : []} try : # Get table models portal_updates_model = getattr ( base . classes , 'portal_updates' , None ) incidences_model = getattr ( base . classes , 'incidences' , None ) if not portal_updates_model : raise AttributeError ( \"portal_updates table not found in database schema\" ) if not incidences_model : raise AttributeError ( \"incidences table not found in database schema\" ) # Query portal_updates table portal_updates_rows = db . session . query ( portal_updates_model ) . filter ( portal_updates_model . id_incidence >= min_id , portal_updates_model . id_incidence <= max_id ) . all () result [ 'portal_updates' ] = [ { 'id_incidence' : row . id_incidence , 'sector' : getattr ( row , 'sector' , None ), 'created_at' : getattr ( row , 'created_at' , None ), 'updated_at' : getattr ( row , 'updated_at' , None ) } for row in portal_updates_rows ] # Query incidences table incidences_rows = db . session . query ( incidences_model ) . filter ( incidences_model . id_incidence >= min_id , incidences_model . id_incidence <= max_id ) . all () result [ 'incidences' ] = [ { 'id_incidence' : row . id_incidence , 'sector' : getattr ( row , 'sector' , None ), 'created_at' : getattr ( row , 'created_at' , None ), 'updated_at' : getattr ( row , 'updated_at' , None ) } for row in incidences_rows ] logger . info ( f \"Found { len ( result [ 'portal_updates' ]) } rows in portal_updates and { len ( result [ 'incidences' ]) } rows in incidences\" ) return result except Exception as e : logger . error ( f \"Error during listing of testing rows: { e } \" ) raise","title":"Returns: {'portal_updates': [...], 'incidences': [...]}"},{"location":"reference/arb/utils/constants/","text":"arb.utils.constants Shared constants for ARB Feedback Portal utility modules. These constants are intended to remain immutable and serve as application-wide placeholders or configuration defaults for UI and utility logic. Notes: - Constants should not be modified at runtime. - Used across multiple modules for consistency. PLEASE_SELECT: Placeholder value for dropdown selectors to indicate a required user selection. PLEASE_SELECT = 'Please Select' module-attribute str: Placeholder value used in dropdown selectors to indicate a required user selection. Examples: Input : Used as the default value in a dropdown menu. Output: Dropdown displays 'Please Select' as the initial option. Notes Should be compared using equality (==), not identity (is). Do not modify this value at runtime.","title":"arb.utils.constants"},{"location":"reference/arb/utils/constants/#arbutilsconstants","text":"Shared constants for ARB Feedback Portal utility modules. These constants are intended to remain immutable and serve as application-wide placeholders or configuration defaults for UI and utility logic. Notes: - Constants should not be modified at runtime. - Used across multiple modules for consistency. PLEASE_SELECT: Placeholder value for dropdown selectors to indicate a required user selection.","title":"arb.utils.constants"},{"location":"reference/arb/utils/constants/#arb.utils.constants.PLEASE_SELECT","text":"str: Placeholder value used in dropdown selectors to indicate a required user selection. Examples: Input : Used as the default value in a dropdown menu. Output: Dropdown displays 'Please Select' as the initial option. Notes Should be compared using equality (==), not identity (is). Do not modify this value at runtime.","title":"PLEASE_SELECT"},{"location":"reference/arb/utils/database/","text":"arb.utils.database Miscellaneous database utilities. Includes helpers for dropping tables, executing SQL scripts, auto-reflecting base metadata, and bulk cleansing of JSON fields across database rows. Intended for use in migrations, diagnostics, and administrative scripts. Requires SQLAlchemy (for model and metadata operations) A db object and an automapped or declarative base from the Flask app Functions: Name Description - db_drop_all Drop all database tables - execute_sql_script Run external SQL script files - get_reflected_base Return a SQLAlchemy automap base - cleanse_misc_json Strip \"Please Select\" values from misc_json fields cleanse_misc_json ( db , base , table_name , json_column_name = 'misc_json' , remove_value = 'Please Select' , dry_run = False ) Remove key/value pairs in a JSON column where value == remove_value . Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance. Must not be None. base ( AutomapBase ) \u2013 Declarative or automap base. Must not be None. table_name ( str ) \u2013 Table name to target (e.g., 'incidences'). If None or empty, raises ValueError. json_column_name ( str , default: 'misc_json' ) \u2013 Column name to scan (default: \"misc_json\"). If None or empty, raises ValueError. remove_value ( str , default: 'Please Select' ) \u2013 Value to match for deletion (default: \"Please Select\"). If None, removes all keys with value None. dry_run ( bool , default: False ) \u2013 If True, logs changes but rolls back. Notes If table_name or json_column_name are invalid, a ValueError is raised. If remove_value is None, all keys with value None are removed. If the JSON column is not a dict, the row is skipped. If dry_run is True, changes are rolled back after logging. Raises: ValueError \u2013 If table or column cannot be found or mapped. RuntimeError \u2013 On failure to commit or query. Examples: Input : db, base, table_name=\"incidences\", json_column_name=\"misc_json\", remove_value=\"Please Select\", dry_run=True Output: Logs how many rows would be modified, rolls back changes Input : db, base, table_name=\"incidences\", json_column_name=\"misc_json\", remove_value=None, dry_run=False Output: Removes all keys with value None, commits changes Input : db, base, table_name=None Output: ValueError Input : db, base, table_name=\"incidences\", json_column_name=None Output: ValueError Source code in arb\\utils\\database.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def cleanse_misc_json ( db : SQLAlchemy , base : AutomapBase , # base is a mapped base, not to be passed directly to query table_name : str , json_column_name : str = \"misc_json\" , remove_value : str = \"Please Select\" , dry_run : bool = False ) -> None : \"\"\" Remove key/value pairs in a JSON column where value == `remove_value`. Args: db (SQLAlchemy): SQLAlchemy instance. Must not be None. base (AutomapBase): Declarative or automap base. Must not be None. table_name (str): Table name to target (e.g., 'incidences'). If None or empty, raises ValueError. json_column_name (str): Column name to scan (default: \"misc_json\"). If None or empty, raises ValueError. remove_value (str): Value to match for deletion (default: \"Please Select\"). If None, removes all keys with value None. dry_run (bool): If True, logs changes but rolls back. Notes: - If `table_name` or `json_column_name` are invalid, a ValueError is raised. - If `remove_value` is None, all keys with value None are removed. - If the JSON column is not a dict, the row is skipped. - If `dry_run` is True, changes are rolled back after logging. Raises: ValueError: If table or column cannot be found or mapped. RuntimeError: On failure to commit or query. Examples: Input : db, base, table_name=\"incidences\", json_column_name=\"misc_json\", remove_value=\"Please Select\", dry_run=True Output: Logs how many rows would be modified, rolls back changes Input : db, base, table_name=\"incidences\", json_column_name=\"misc_json\", remove_value=None, dry_run=False Output: Removes all keys with value None, commits changes Input : db, base, table_name=None Output: ValueError Input : db, base, table_name=\"incidences\", json_column_name=None Output: ValueError \"\"\" from arb.utils.sql_alchemy import get_class_from_table_name model_cls = get_class_from_table_name ( base , table_name ) if model_cls is None : raise ValueError ( f \"Table ' { table_name } ' not found or not mapped.\" ) if not hasattr ( model_cls , json_column_name ): raise ValueError ( f \"Column ' { json_column_name } ' not found on model for table ' { table_name } '.\" ) try : rows = db . session . query ( model_cls ) . all () # type: ignore count_total = len ( rows ) count_modified = 0 for row in rows : json_data = getattr ( row , json_column_name ) or {} if not isinstance ( json_data , dict ): continue filtered = { k : v for k , v in json_data . items () if v != remove_value } if filtered != json_data : setattr ( row , json_column_name , filtered ) from sqlalchemy.orm.attributes import flag_modified flag_modified ( row , json_column_name ) count_modified += 1 if dry_run : logger . info ( f \"[Dry Run] { count_modified } of { count_total } rows would be modified.\" ) db . session . rollback () else : db . session . commit () logger . info ( f \"[Committed] { count_modified } of { count_total } rows modified.\" ) except Exception as e : db . session . rollback () raise RuntimeError ( f \"Error during cleansing: { e } \" ) execute_sql_script ( script_path = None , connection = None ) Execute an SQL script using a provided or default SQLite connection. Parameters: script_path ( str | Path | None , default: None ) \u2013 Path to the .sql script. If None, defaults to '../sql_scripts/script_01.sql'. If an empty string is provided, a FileNotFoundError will be raised. connection ( Connection | None , default: None ) \u2013 SQLite connection (defaults to sqlite3.connect('app.db') if None). Notes If script_path is None, a default path is used. If it is an empty string or invalid, FileNotFoundError will occur. If connection is None, a new connection to 'app.db' is created. Raises: FileNotFoundError \u2013 If the script file does not exist or is an empty string. DatabaseError \u2013 If there is an error executing the script. Examples: Input : script_path=None, connection=None Output: Executes default script on 'app.db' Input : script_path=\"/tmp/test.sql\", connection=None Output: Executes /tmp/test.sql on 'app.db' Input : script_path=\"\", connection=None Output: FileNotFoundError Source code in arb\\utils\\database.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def execute_sql_script ( script_path : str | Path | None = None , connection : sqlite3 . Connection | None = None ) -> None : \"\"\" Execute an SQL script using a provided or default SQLite connection. Args: script_path (str | Path | None): Path to the `.sql` script. If None, defaults to '../sql_scripts/script_01.sql'. If an empty string is provided, a FileNotFoundError will be raised. connection (sqlite3.Connection | None): SQLite connection (defaults to `sqlite3.connect('app.db')` if None). Notes: - If `script_path` is None, a default path is used. If it is an empty string or invalid, FileNotFoundError will occur. - If `connection` is None, a new connection to 'app.db' is created. Raises: FileNotFoundError: If the script file does not exist or is an empty string. sqlite3.DatabaseError: If there is an error executing the script. Examples: Input : script_path=None, connection=None Output: Executes default script on 'app.db' Input : script_path=\"/tmp/test.sql\", connection=None Output: Executes /tmp/test.sql on 'app.db' Input : script_path=\"\", connection=None Output: FileNotFoundError \"\"\" logger . debug ( f \"execute_sql_script() called with { script_path =} , { connection =} \" ) if script_path is None : script_path = '../sql_scripts/script_01.sql' if connection is None : connection = sqlite3 . connect ( 'app.db' ) with open ( script_path ) as f : connection . executescript ( f . read ()) connection . commit () connection . close () get_reflected_base ( db ) Return a SQLAlchemy automap base using the existing metadata (no re-reflection). Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance with metadata. Must not be None. Returns: AutomapBase ( AutomapBase ) \u2013 Reflected base class. Raises: AttributeError \u2013 If db is None or does not have valid metadata. Examples: Input : db (valid SQLAlchemy instance) Output: AutomapBase instance Input : db=None Output: AttributeError Source code in arb\\utils\\database.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def get_reflected_base ( db : SQLAlchemy ) -> AutomapBase : \"\"\" Return a SQLAlchemy automap base using the existing metadata (no re-reflection). Args: db (SQLAlchemy): SQLAlchemy instance with metadata. Must not be None. Returns: AutomapBase: Reflected base class. Raises: AttributeError: If `db` is None or does not have valid metadata. Examples: Input : db (valid SQLAlchemy instance) Output: AutomapBase instance Input : db=None Output: AttributeError \"\"\" base = automap_base ( metadata = db . metadata ) # reuse metadata! base . prepare ( db . engine , reflect = False ) # no extra reflection return base","title":"arb.utils.database"},{"location":"reference/arb/utils/database/#arbutilsdatabase","text":"Miscellaneous database utilities. Includes helpers for dropping tables, executing SQL scripts, auto-reflecting base metadata, and bulk cleansing of JSON fields across database rows. Intended for use in migrations, diagnostics, and administrative scripts. Requires SQLAlchemy (for model and metadata operations) A db object and an automapped or declarative base from the Flask app Functions: Name Description - db_drop_all Drop all database tables - execute_sql_script Run external SQL script files - get_reflected_base Return a SQLAlchemy automap base - cleanse_misc_json Strip \"Please Select\" values from misc_json fields","title":"arb.utils.database"},{"location":"reference/arb/utils/database/#arb.utils.database.cleanse_misc_json","text":"Remove key/value pairs in a JSON column where value == remove_value . Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance. Must not be None. base ( AutomapBase ) \u2013 Declarative or automap base. Must not be None. table_name ( str ) \u2013 Table name to target (e.g., 'incidences'). If None or empty, raises ValueError. json_column_name ( str , default: 'misc_json' ) \u2013 Column name to scan (default: \"misc_json\"). If None or empty, raises ValueError. remove_value ( str , default: 'Please Select' ) \u2013 Value to match for deletion (default: \"Please Select\"). If None, removes all keys with value None. dry_run ( bool , default: False ) \u2013 If True, logs changes but rolls back. Notes If table_name or json_column_name are invalid, a ValueError is raised. If remove_value is None, all keys with value None are removed. If the JSON column is not a dict, the row is skipped. If dry_run is True, changes are rolled back after logging. Raises: ValueError \u2013 If table or column cannot be found or mapped. RuntimeError \u2013 On failure to commit or query. Examples: Input : db, base, table_name=\"incidences\", json_column_name=\"misc_json\", remove_value=\"Please Select\", dry_run=True Output: Logs how many rows would be modified, rolls back changes Input : db, base, table_name=\"incidences\", json_column_name=\"misc_json\", remove_value=None, dry_run=False Output: Removes all keys with value None, commits changes Input : db, base, table_name=None Output: ValueError Input : db, base, table_name=\"incidences\", json_column_name=None Output: ValueError Source code in arb\\utils\\database.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def cleanse_misc_json ( db : SQLAlchemy , base : AutomapBase , # base is a mapped base, not to be passed directly to query table_name : str , json_column_name : str = \"misc_json\" , remove_value : str = \"Please Select\" , dry_run : bool = False ) -> None : \"\"\" Remove key/value pairs in a JSON column where value == `remove_value`. Args: db (SQLAlchemy): SQLAlchemy instance. Must not be None. base (AutomapBase): Declarative or automap base. Must not be None. table_name (str): Table name to target (e.g., 'incidences'). If None or empty, raises ValueError. json_column_name (str): Column name to scan (default: \"misc_json\"). If None or empty, raises ValueError. remove_value (str): Value to match for deletion (default: \"Please Select\"). If None, removes all keys with value None. dry_run (bool): If True, logs changes but rolls back. Notes: - If `table_name` or `json_column_name` are invalid, a ValueError is raised. - If `remove_value` is None, all keys with value None are removed. - If the JSON column is not a dict, the row is skipped. - If `dry_run` is True, changes are rolled back after logging. Raises: ValueError: If table or column cannot be found or mapped. RuntimeError: On failure to commit or query. Examples: Input : db, base, table_name=\"incidences\", json_column_name=\"misc_json\", remove_value=\"Please Select\", dry_run=True Output: Logs how many rows would be modified, rolls back changes Input : db, base, table_name=\"incidences\", json_column_name=\"misc_json\", remove_value=None, dry_run=False Output: Removes all keys with value None, commits changes Input : db, base, table_name=None Output: ValueError Input : db, base, table_name=\"incidences\", json_column_name=None Output: ValueError \"\"\" from arb.utils.sql_alchemy import get_class_from_table_name model_cls = get_class_from_table_name ( base , table_name ) if model_cls is None : raise ValueError ( f \"Table ' { table_name } ' not found or not mapped.\" ) if not hasattr ( model_cls , json_column_name ): raise ValueError ( f \"Column ' { json_column_name } ' not found on model for table ' { table_name } '.\" ) try : rows = db . session . query ( model_cls ) . all () # type: ignore count_total = len ( rows ) count_modified = 0 for row in rows : json_data = getattr ( row , json_column_name ) or {} if not isinstance ( json_data , dict ): continue filtered = { k : v for k , v in json_data . items () if v != remove_value } if filtered != json_data : setattr ( row , json_column_name , filtered ) from sqlalchemy.orm.attributes import flag_modified flag_modified ( row , json_column_name ) count_modified += 1 if dry_run : logger . info ( f \"[Dry Run] { count_modified } of { count_total } rows would be modified.\" ) db . session . rollback () else : db . session . commit () logger . info ( f \"[Committed] { count_modified } of { count_total } rows modified.\" ) except Exception as e : db . session . rollback () raise RuntimeError ( f \"Error during cleansing: { e } \" )","title":"cleanse_misc_json"},{"location":"reference/arb/utils/database/#arb.utils.database.execute_sql_script","text":"Execute an SQL script using a provided or default SQLite connection. Parameters: script_path ( str | Path | None , default: None ) \u2013 Path to the .sql script. If None, defaults to '../sql_scripts/script_01.sql'. If an empty string is provided, a FileNotFoundError will be raised. connection ( Connection | None , default: None ) \u2013 SQLite connection (defaults to sqlite3.connect('app.db') if None). Notes If script_path is None, a default path is used. If it is an empty string or invalid, FileNotFoundError will occur. If connection is None, a new connection to 'app.db' is created. Raises: FileNotFoundError \u2013 If the script file does not exist or is an empty string. DatabaseError \u2013 If there is an error executing the script. Examples: Input : script_path=None, connection=None Output: Executes default script on 'app.db' Input : script_path=\"/tmp/test.sql\", connection=None Output: Executes /tmp/test.sql on 'app.db' Input : script_path=\"\", connection=None Output: FileNotFoundError Source code in arb\\utils\\database.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def execute_sql_script ( script_path : str | Path | None = None , connection : sqlite3 . Connection | None = None ) -> None : \"\"\" Execute an SQL script using a provided or default SQLite connection. Args: script_path (str | Path | None): Path to the `.sql` script. If None, defaults to '../sql_scripts/script_01.sql'. If an empty string is provided, a FileNotFoundError will be raised. connection (sqlite3.Connection | None): SQLite connection (defaults to `sqlite3.connect('app.db')` if None). Notes: - If `script_path` is None, a default path is used. If it is an empty string or invalid, FileNotFoundError will occur. - If `connection` is None, a new connection to 'app.db' is created. Raises: FileNotFoundError: If the script file does not exist or is an empty string. sqlite3.DatabaseError: If there is an error executing the script. Examples: Input : script_path=None, connection=None Output: Executes default script on 'app.db' Input : script_path=\"/tmp/test.sql\", connection=None Output: Executes /tmp/test.sql on 'app.db' Input : script_path=\"\", connection=None Output: FileNotFoundError \"\"\" logger . debug ( f \"execute_sql_script() called with { script_path =} , { connection =} \" ) if script_path is None : script_path = '../sql_scripts/script_01.sql' if connection is None : connection = sqlite3 . connect ( 'app.db' ) with open ( script_path ) as f : connection . executescript ( f . read ()) connection . commit () connection . close ()","title":"execute_sql_script"},{"location":"reference/arb/utils/database/#arb.utils.database.get_reflected_base","text":"Return a SQLAlchemy automap base using the existing metadata (no re-reflection). Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance with metadata. Must not be None. Returns: AutomapBase ( AutomapBase ) \u2013 Reflected base class. Raises: AttributeError \u2013 If db is None or does not have valid metadata. Examples: Input : db (valid SQLAlchemy instance) Output: AutomapBase instance Input : db=None Output: AttributeError Source code in arb\\utils\\database.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def get_reflected_base ( db : SQLAlchemy ) -> AutomapBase : \"\"\" Return a SQLAlchemy automap base using the existing metadata (no re-reflection). Args: db (SQLAlchemy): SQLAlchemy instance with metadata. Must not be None. Returns: AutomapBase: Reflected base class. Raises: AttributeError: If `db` is None or does not have valid metadata. Examples: Input : db (valid SQLAlchemy instance) Output: AutomapBase instance Input : db=None Output: AttributeError \"\"\" base = automap_base ( metadata = db . metadata ) # reuse metadata! base . prepare ( db . engine , reflect = False ) # no extra reflection return base","title":"get_reflected_base"},{"location":"reference/arb/utils/date_and_time/","text":"arb.utils.date_and_time Datetime parsing and timezone utilities for ISO 8601, UTC/Pacific conversion, and recursive datetime transformation in nested structures. This module implements the datetime data contract for the ARB Feedback Portal, ensuring consistent parsing, formatting, and timezone handling across all data ingestion and export workflows. Features: - ISO 8601 validation and parsing (via dateutil ) - Conversion between UTC and naive Pacific time (Los Angeles) - HTML and Excel datetime contract-compliant conversions - Recursive datetime transformations within nested dicts/lists/sets/tuples Timezone policy: - UTC_TZ and PACIFIC_TZ are globally defined using zoneinfo.ZoneInfo - Naive timestamps are only assumed to be UTC if explicitly configured via arguments Notes: - All functions are designed to be robust to edge cases and log warnings where assumptions are made. - This module is central to the datetime data contract for the ARB Feedback Portal. bulk_ca_naive_datetime_to_utc_datetime ( data ) Recursively convert all naive California local datetimes in a nested structure to UTC-aware datetimes. Parameters: data ( object ) \u2013 Structure containing datetimes (dict, list, set, tuple, etc.). If None, returns None. Returns: object ( object ) \u2013 Structure with all datetimes converted to UTC-aware datetimes. Non-datetime, non-container objects are returned unchanged. Notes If data is None, returns None. Source code in arb\\utils\\date_and_time.py 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 def bulk_ca_naive_datetime_to_utc_datetime ( data : object ) -> object : \"\"\" Recursively convert all naive California local datetimes in a nested structure to UTC-aware datetimes. Args: data (object): Structure containing datetimes (dict, list, set, tuple, etc.). If None, returns None. Returns: object: Structure with all datetimes converted to UTC-aware datetimes. Non-datetime, non-container objects are returned unchanged. Notes: - If `data` is None, returns None. \"\"\" if isinstance ( data , datetime ): return ca_naive_datetime_to_utc_datetime ( data ) elif isinstance ( data , Mapping ): return { bulk_ca_naive_datetime_to_utc_datetime ( k ): bulk_ca_naive_datetime_to_utc_datetime ( v ) for k , v in data . items ()} elif isinstance ( data , list ): return [ bulk_ca_naive_datetime_to_utc_datetime ( i ) for i in data ] elif isinstance ( data , tuple ): return tuple ( bulk_ca_naive_datetime_to_utc_datetime ( i ) for i in data ) elif isinstance ( data , set ): return { bulk_ca_naive_datetime_to_utc_datetime ( i ) for i in data } return data bulk_utc_datetime_to_ca_naive_datetime ( data , assume_naive_is_utc = False , utc_strict = True ) Recursively convert all UTC-aware datetimes in a nested structure to naive California local datetimes. Parameters: data ( object ) \u2013 Structure containing datetimes (dict, list, set, tuple, etc.). If None, returns None. assume_naive_is_utc ( bool , default: False ) \u2013 Whether to treat naive datetimes as UTC and log a warning. utc_strict ( bool , default: True ) \u2013 Whether to enforce UTC input. Returns: object ( object ) \u2013 Structure with all datetimes converted to naive California local. Non-datetime, non-container objects are returned unchanged. Notes If data is None, returns None. Source code in arb\\utils\\date_and_time.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 def bulk_utc_datetime_to_ca_naive_datetime ( data : object , assume_naive_is_utc : bool = False , utc_strict : bool = True ) -> object : \"\"\" Recursively convert all UTC-aware datetimes in a nested structure to naive California local datetimes. Args: data (object): Structure containing datetimes (dict, list, set, tuple, etc.). If None, returns None. assume_naive_is_utc (bool): Whether to treat naive datetimes as UTC and log a warning. utc_strict (bool): Whether to enforce UTC input. Returns: object: Structure with all datetimes converted to naive California local. Non-datetime, non-container objects are returned unchanged. Notes: - If `data` is None, returns None. \"\"\" if isinstance ( data , datetime ): return utc_datetime_to_ca_naive_datetime ( data , assume_naive_is_utc , utc_strict ) elif isinstance ( data , Mapping ): return { bulk_utc_datetime_to_ca_naive_datetime ( k , assume_naive_is_utc , utc_strict ): bulk_utc_datetime_to_ca_naive_datetime ( v , assume_naive_is_utc , utc_strict ) for k , v in data . items () } elif isinstance ( data , list ): return [ bulk_utc_datetime_to_ca_naive_datetime ( i , assume_naive_is_utc , utc_strict ) for i in data ] elif isinstance ( data , tuple ): return tuple ( bulk_utc_datetime_to_ca_naive_datetime ( i , assume_naive_is_utc , utc_strict ) for i in data ) elif isinstance ( data , set ): return { bulk_utc_datetime_to_ca_naive_datetime ( i , assume_naive_is_utc , utc_strict ) for i in data } return data ca_naive_datetime_to_utc_datetime ( ca_naive_dt ) Convert a naive California local datetime to a UTC-aware datetime. Parameters: ca_naive_dt ( datetime ) \u2013 Naive datetime (assumed Pacific Time). Must not be timezone-aware or None. Returns: datetime ( datetime ) \u2013 UTC-aware datetime. Raises: ValueError \u2013 If input is not naive (i.e., already has tzinfo) or is None. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0) Output: datetime.datetime(2025, 7, 4, 19, 0, tzinfo=ZoneInfo(\"UTC\")) Input : None Output: TypeError Notes If ca_naive_dt is None, a TypeError will be raised. Source code in arb\\utils\\date_and_time.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def ca_naive_datetime_to_utc_datetime ( ca_naive_dt : datetime ) -> datetime : \"\"\" Convert a naive California local datetime to a UTC-aware datetime. Args: ca_naive_dt (datetime): Naive datetime (assumed Pacific Time). Must not be timezone-aware or None. Returns: datetime: UTC-aware datetime. Raises: ValueError: If input is not naive (i.e., already has tzinfo) or is None. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0) Output: datetime.datetime(2025, 7, 4, 19, 0, tzinfo=ZoneInfo(\"UTC\")) Input : None Output: TypeError Notes: - If `ca_naive_dt` is None, a TypeError will be raised. \"\"\" if ca_naive_dt . tzinfo is not None : raise ValueError ( f \"Expected naive datetime, got { ca_naive_dt !r} \" ) return ca_naive_dt . replace ( tzinfo = PACIFIC_TZ ) . astimezone ( UTC_TZ ) excel_naive_datetime_to_utc_datetime ( excel_dt ) Convert a naive Excel datetime (assumed California local) to a UTC-aware datetime. Parameters: excel_dt ( datetime ) \u2013 Naive datetime from Excel. Must not be timezone-aware or None. Returns: datetime ( datetime ) \u2013 UTC-aware datetime. Raises: ValueError \u2013 If input is already timezone-aware or is None. Notes If excel_dt is None, a TypeError will be raised. Source code in arb\\utils\\date_and_time.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 def excel_naive_datetime_to_utc_datetime ( excel_dt : datetime ) -> datetime : \"\"\" Convert a naive Excel datetime (assumed California local) to a UTC-aware datetime. Args: excel_dt (datetime): Naive datetime from Excel. Must not be timezone-aware or None. Returns: datetime: UTC-aware datetime. Raises: ValueError: If input is already timezone-aware or is None. Notes: - If `excel_dt` is None, a TypeError will be raised. \"\"\" if excel_dt . tzinfo is not None : raise ValueError ( \"Excel datetime should be naive (no timezone)\" ) return ca_naive_datetime_to_utc_datetime ( excel_dt ) excel_str_to_naive_datetime ( excel_str ) Parse a string from an Excel cell to a naive datetime object (no timezone). Parameters: excel_str ( str ) \u2013 Excel cell value to parse. If None, not a string, or empty, returns None. Returns: datetime | None \u2013 datetime | None: Parsed naive datetime, or None if unparseable or input is not a string. Examples: Input : \"2025-07-04 12:00:00\" Output: datetime.datetime(2025, 7, 4, 12, 0) Input : None Output: None Input : \"\" Output: None Notes If excel_str is None, not a string, or empty, returns None. If parsing fails, returns None. Source code in arb\\utils\\date_and_time.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def excel_str_to_naive_datetime ( excel_str : str ) -> datetime | None : \"\"\" Parse a string from an Excel cell to a naive datetime object (no timezone). Args: excel_str (str): Excel cell value to parse. If None, not a string, or empty, returns None. Returns: datetime | None: Parsed naive datetime, or None if unparseable or input is not a string. Examples: Input : \"2025-07-04 12:00:00\" Output: datetime.datetime(2025, 7, 4, 12, 0) Input : None Output: None Input : \"\" Output: None Notes: - If `excel_str` is None, not a string, or empty, returns None. - If parsing fails, returns None. \"\"\" if not isinstance ( excel_str , str ) or not excel_str : return None try : return parser . parse ( excel_str ) except ( ValueError , TypeError ): return None html_naive_str_to_utc_datetime ( html_str ) Convert an HTML form datetime-local string (naive, California local) to a UTC-aware datetime. Parameters: html_str ( str ) \u2013 HTML datetime-local input value (e.g., \"2025-01-15T14:30\"). If None or empty, returns None. Returns: datetime | None \u2013 datetime | None: UTC-aware datetime, or None if input is empty or None. Examples: Input : \"2025-07-04T12:00\" Output: datetime.datetime(2025, 7, 4, 19, 0, tzinfo=ZoneInfo(\"UTC\")) Input : None Output: None Input : \"\" Output: None Notes If html_str is None or empty, returns None. If parsing fails, may raise ValueError. Source code in arb\\utils\\date_and_time.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def html_naive_str_to_utc_datetime ( html_str : str ) -> datetime | None : \"\"\" Convert an HTML form datetime-local string (naive, California local) to a UTC-aware datetime. Args: html_str (str): HTML datetime-local input value (e.g., \"2025-01-15T14:30\"). If None or empty, returns None. Returns: datetime | None: UTC-aware datetime, or None if input is empty or None. Examples: Input : \"2025-07-04T12:00\" Output: datetime.datetime(2025, 7, 4, 19, 0, tzinfo=ZoneInfo(\"UTC\")) Input : None Output: None Input : \"\" Output: None Notes: - If `html_str` is None or empty, returns None. - If parsing fails, may raise ValueError. \"\"\" if not html_str : return None ca_naive_dt = datetime . fromisoformat ( html_str ) return ca_naive_datetime_to_utc_datetime ( ca_naive_dt ) is_datetime_naive ( dt ) Return True if a datetime is naive (lacks timezone info). Parameters: dt ( datetime ) \u2013 Datetime to check. If None, returns False. Returns: bool ( bool ) \u2013 True if naive, False otherwise. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0) Output: True Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: False Input : None Output: False Notes If dt is None, returns False. Source code in arb\\utils\\date_and_time.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def is_datetime_naive ( dt : datetime ) -> bool : \"\"\" Return True if a datetime is naive (lacks timezone info). Args: dt (datetime): Datetime to check. If None, returns False. Returns: bool: True if naive, False otherwise. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0) Output: True Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: False Input : None Output: False Notes: - If `dt` is None, returns False. \"\"\" if dt is None : # None is not a datetime, so treat as not naive (or could raise error) return False if dt . tzinfo is None : # No timezone info attached: definitely naive return True if dt . tzinfo . utcoffset ( dt ) is None : # tzinfo is set, but utcoffset returns None: still considered naive return True # Otherwise, datetime is timezone-aware return False is_datetime_utc ( dt ) Return True if the datetime is timezone-aware and in UTC. Parameters: dt ( datetime ) \u2013 Datetime to check. If None or naive, returns False. Returns: bool ( bool ) \u2013 True if dt is timezone-aware and in UTC, False otherwise. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: True Input : datetime.datetime(2025, 7, 4, 12, 0) Output: False Input : None Output: False Notes If dt is None or naive, returns False. Source code in arb\\utils\\date_and_time.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def is_datetime_utc ( dt : datetime ) -> bool : \"\"\" Return True if the datetime is timezone-aware and in UTC. Args: dt (datetime): Datetime to check. If None or naive, returns False. Returns: bool: True if dt is timezone-aware and in UTC, False otherwise. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: True Input : datetime.datetime(2025, 7, 4, 12, 0) Output: False Input : None Output: False Notes: - If `dt` is None or naive, returns False. \"\"\" if dt is None : return False if dt . tzinfo is None : return False # UTC offset must be zero for UTC, regardless of tzinfo implementation return dt . tzinfo . utcoffset ( dt ) == timedelta ( 0 ) iso_str_to_utc_datetime ( iso_str , error_on_missing_tz = True ) Convert an ISO 8601 string to a UTC-aware datetime object. Parameters: iso_str ( str ) \u2013 ISO 8601-formatted datetime string. If None or empty, raises ValueError. error_on_missing_tz ( bool , default: True ) \u2013 If True, raise if no timezone info is present. If False, assumes UTC and logs a warning. Returns: datetime ( datetime ) \u2013 UTC-aware datetime object. Raises: ValueError \u2013 If the string is invalid, empty, None, or lacks timezone info (when required). Examples: Input : \"2025-07-04T12:00:00+00:00\" Output: datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Input : \"2025-07-04T12:00:00\" (with error_on_missing_tz=False) Output: datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Notes If iso_str is None or empty, a ValueError is raised. If error_on_missing_tz is False and the string is naive, UTC is assumed and a warning is logged. Source code in arb\\utils\\date_and_time.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def iso_str_to_utc_datetime ( iso_str : str , error_on_missing_tz : bool = True ) -> datetime : \"\"\" Convert an ISO 8601 string to a UTC-aware datetime object. Args: iso_str (str): ISO 8601-formatted datetime string. If None or empty, raises ValueError. error_on_missing_tz (bool): If True, raise if no timezone info is present. If False, assumes UTC and logs a warning. Returns: datetime: UTC-aware datetime object. Raises: ValueError: If the string is invalid, empty, None, or lacks timezone info (when required). Examples: Input : \"2025-07-04T12:00:00+00:00\" Output: datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Input : \"2025-07-04T12:00:00\" (with error_on_missing_tz=False) Output: datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Notes: - If `iso_str` is None or empty, a ValueError is raised. - If `error_on_missing_tz` is False and the string is naive, UTC is assumed and a warning is logged. \"\"\" try : dt = parser . isoparse ( iso_str ) except ( ValueError , TypeError ) as e : raise ValueError ( f \"Invalid ISO 8601 datetime string: ' { iso_str } '\" ) from e if dt . tzinfo is None : if error_on_missing_tz : raise ValueError ( \"Missing timezone info in ISO 8601 string.\" ) logger . warning ( f \"Assuming UTC for naive ISO string: { iso_str } \" ) dt = dt . replace ( tzinfo = UTC_TZ ) return dt . astimezone ( UTC_TZ ) utc_datetime_to_ca_naive_datetime ( utc_dt , assume_naive_is_utc = False , utc_strict = True ) Convert a UTC-aware (or optionally naive) datetime to naive California local time. Parameters: utc_dt ( datetime ) \u2013 The datetime to convert. If None, raises ValueError. If naive, must set assume_naive_is_utc=True. assume_naive_is_utc ( bool , default: False ) \u2013 If True, treat naive input as UTC and log a warning. utc_strict ( bool , default: True ) \u2013 If True, require input to be UTC; raises ValueError if not. Returns: datetime ( datetime ) \u2013 Naive California local datetime. Raises: ValueError \u2013 If input is None, is naive and assume_naive_is_utc is False, or if UTC check fails. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: datetime.datetime(2025, 7, 4, 5, 0) Input : datetime.datetime(2025, 7, 4, 12, 0) (assume_naive_is_utc=True) Output: datetime.datetime(2025, 7, 4, 5, 0) Input : None Output: ValueError Notes If utc_dt is None, a ValueError is raised. If assume_naive_is_utc is True, naive input is treated as UTC and a warning is logged. Source code in arb\\utils\\date_and_time.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def utc_datetime_to_ca_naive_datetime ( utc_dt : datetime , assume_naive_is_utc : bool = False , utc_strict : bool = True ) -> datetime : \"\"\" Convert a UTC-aware (or optionally naive) datetime to naive California local time. Args: utc_dt (datetime): The datetime to convert. If None, raises ValueError. If naive, must set assume_naive_is_utc=True. assume_naive_is_utc (bool): If True, treat naive input as UTC and log a warning. utc_strict (bool): If True, require input to be UTC; raises ValueError if not. Returns: datetime: Naive California local datetime. Raises: ValueError: If input is None, is naive and assume_naive_is_utc is False, or if UTC check fails. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: datetime.datetime(2025, 7, 4, 5, 0) Input : datetime.datetime(2025, 7, 4, 12, 0) (assume_naive_is_utc=True) Output: datetime.datetime(2025, 7, 4, 5, 0) Input : None Output: ValueError Notes: - If `utc_dt` is None, a ValueError is raised. - If `assume_naive_is_utc` is True, naive input is treated as UTC and a warning is logged. \"\"\" dt = utc_dt if dt . tzinfo is None : if assume_naive_is_utc : logger . warning ( f \"Assuming UTC for naive datetime.\" ) dt = dt . replace ( tzinfo = UTC_TZ ) else : raise ValueError ( \"Naive datetime provided without assume_naive_is_utc=True\" ) elif utc_strict and dt . tzinfo != UTC_TZ : raise ValueError ( \"datetime must be UTC when utc_strict=True\" ) return dt . astimezone ( PACIFIC_TZ ) . replace ( tzinfo = None ) utc_datetime_to_html_naive_str ( utc_dt ) Convert a UTC-aware datetime to a naive California local string for HTML form display. Parameters: utc_dt ( datetime ) \u2013 UTC-aware datetime. Must not be naive, non-UTC, or None. Returns: str ( str ) \u2013 California local datetime string (e.g., \"2025-01-15T14:30\"). Raises: ValueError \u2013 If input is naive, not UTC, or None. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: \"2025-07-04T05:00\" Input : None Output: ValueError Notes If utc_dt is None, a ValueError is raised. Source code in arb\\utils\\date_and_time.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def utc_datetime_to_html_naive_str ( utc_dt : datetime ) -> str : \"\"\" Convert a UTC-aware datetime to a naive California local string for HTML form display. Args: utc_dt (datetime): UTC-aware datetime. Must not be naive, non-UTC, or None. Returns: str: California local datetime string (e.g., \"2025-01-15T14:30\"). Raises: ValueError: If input is naive, not UTC, or None. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: \"2025-07-04T05:00\" Input : None Output: ValueError Notes: - If `utc_dt` is None, a ValueError is raised. \"\"\" if utc_dt . tzinfo is None : raise ValueError ( \"Datetime must be timezone-aware\" ) if utc_dt . tzinfo != UTC_TZ : raise ValueError ( \"Datetime must be UTC\" ) return utc_datetime_to_ca_naive_datetime ( utc_dt , utc_strict = True ) . strftime ( \"%Y-%m- %d T%H:%M\" ) utc_datetime_to_iso_str ( utc_dt ) Convert a timezone-aware datetime to a UTC ISO 8601 string. Parameters: utc_dt ( datetime ) \u2013 Timezone-aware datetime. Must not be naive or None. Returns: str ( str ) \u2013 UTC ISO 8601 string. Raises: ValueError \u2013 If input is naive or None. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: \"2025-07-04T12:00:00+00:00\" Input : None Output: ValueError Notes If utc_dt is None, a ValueError is raised. Source code in arb\\utils\\date_and_time.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def utc_datetime_to_iso_str ( utc_dt : datetime ) -> str : \"\"\" Convert a timezone-aware datetime to a UTC ISO 8601 string. Args: utc_dt (datetime): Timezone-aware datetime. Must not be naive or None. Returns: str: UTC ISO 8601 string. Raises: ValueError: If input is naive or None. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: \"2025-07-04T12:00:00+00:00\" Input : None Output: ValueError Notes: - If `utc_dt` is None, a ValueError is raised. \"\"\" if utc_dt . tzinfo is None : raise ValueError ( \"Datetime must be timezone-aware\" ) return utc_dt . astimezone ( UTC_TZ ) . isoformat () utc_iso_str_to_ca_str ( iso_str ) Convert a UTC ISO string to a California local time string for display. Parameters: iso_str ( str ) \u2013 UTC ISO 8601 string. If None or empty, returns empty string. If invalid, returns input as-is. Returns: str ( str ) \u2013 California local time string for display (e.g., \"2025-01-15T14:30\"), or original string if parsing fails. Notes If iso_str is None or empty, returns empty string. If parsing fails, returns the original string as-is. Source code in arb\\utils\\date_and_time.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def utc_iso_str_to_ca_str ( iso_str : str ) -> str : \"\"\" Convert a UTC ISO string to a California local time string for display. Args: iso_str (str): UTC ISO 8601 string. If None or empty, returns empty string. If invalid, returns input as-is. Returns: str: California local time string for display (e.g., \"2025-01-15T14:30\"), or original string if parsing fails. Notes: - If `iso_str` is None or empty, returns empty string. - If parsing fails, returns the original string as-is. \"\"\" if not iso_str : return \"\" try : utc_dt = iso_str_to_utc_datetime ( iso_str ) return utc_datetime_to_html_naive_str ( utc_dt ) except Exception : return iso_str # fallback: show as-is if parsing fails","title":"arb.utils.date_and_time"},{"location":"reference/arb/utils/date_and_time/#arbutilsdate_and_time","text":"Datetime parsing and timezone utilities for ISO 8601, UTC/Pacific conversion, and recursive datetime transformation in nested structures. This module implements the datetime data contract for the ARB Feedback Portal, ensuring consistent parsing, formatting, and timezone handling across all data ingestion and export workflows. Features: - ISO 8601 validation and parsing (via dateutil ) - Conversion between UTC and naive Pacific time (Los Angeles) - HTML and Excel datetime contract-compliant conversions - Recursive datetime transformations within nested dicts/lists/sets/tuples Timezone policy: - UTC_TZ and PACIFIC_TZ are globally defined using zoneinfo.ZoneInfo - Naive timestamps are only assumed to be UTC if explicitly configured via arguments Notes: - All functions are designed to be robust to edge cases and log warnings where assumptions are made. - This module is central to the datetime data contract for the ARB Feedback Portal.","title":"arb.utils.date_and_time"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.bulk_ca_naive_datetime_to_utc_datetime","text":"Recursively convert all naive California local datetimes in a nested structure to UTC-aware datetimes. Parameters: data ( object ) \u2013 Structure containing datetimes (dict, list, set, tuple, etc.). If None, returns None. Returns: object ( object ) \u2013 Structure with all datetimes converted to UTC-aware datetimes. Non-datetime, non-container objects are returned unchanged. Notes If data is None, returns None. Source code in arb\\utils\\date_and_time.py 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 def bulk_ca_naive_datetime_to_utc_datetime ( data : object ) -> object : \"\"\" Recursively convert all naive California local datetimes in a nested structure to UTC-aware datetimes. Args: data (object): Structure containing datetimes (dict, list, set, tuple, etc.). If None, returns None. Returns: object: Structure with all datetimes converted to UTC-aware datetimes. Non-datetime, non-container objects are returned unchanged. Notes: - If `data` is None, returns None. \"\"\" if isinstance ( data , datetime ): return ca_naive_datetime_to_utc_datetime ( data ) elif isinstance ( data , Mapping ): return { bulk_ca_naive_datetime_to_utc_datetime ( k ): bulk_ca_naive_datetime_to_utc_datetime ( v ) for k , v in data . items ()} elif isinstance ( data , list ): return [ bulk_ca_naive_datetime_to_utc_datetime ( i ) for i in data ] elif isinstance ( data , tuple ): return tuple ( bulk_ca_naive_datetime_to_utc_datetime ( i ) for i in data ) elif isinstance ( data , set ): return { bulk_ca_naive_datetime_to_utc_datetime ( i ) for i in data } return data","title":"bulk_ca_naive_datetime_to_utc_datetime"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.bulk_utc_datetime_to_ca_naive_datetime","text":"Recursively convert all UTC-aware datetimes in a nested structure to naive California local datetimes. Parameters: data ( object ) \u2013 Structure containing datetimes (dict, list, set, tuple, etc.). If None, returns None. assume_naive_is_utc ( bool , default: False ) \u2013 Whether to treat naive datetimes as UTC and log a warning. utc_strict ( bool , default: True ) \u2013 Whether to enforce UTC input. Returns: object ( object ) \u2013 Structure with all datetimes converted to naive California local. Non-datetime, non-container objects are returned unchanged. Notes If data is None, returns None. Source code in arb\\utils\\date_and_time.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 def bulk_utc_datetime_to_ca_naive_datetime ( data : object , assume_naive_is_utc : bool = False , utc_strict : bool = True ) -> object : \"\"\" Recursively convert all UTC-aware datetimes in a nested structure to naive California local datetimes. Args: data (object): Structure containing datetimes (dict, list, set, tuple, etc.). If None, returns None. assume_naive_is_utc (bool): Whether to treat naive datetimes as UTC and log a warning. utc_strict (bool): Whether to enforce UTC input. Returns: object: Structure with all datetimes converted to naive California local. Non-datetime, non-container objects are returned unchanged. Notes: - If `data` is None, returns None. \"\"\" if isinstance ( data , datetime ): return utc_datetime_to_ca_naive_datetime ( data , assume_naive_is_utc , utc_strict ) elif isinstance ( data , Mapping ): return { bulk_utc_datetime_to_ca_naive_datetime ( k , assume_naive_is_utc , utc_strict ): bulk_utc_datetime_to_ca_naive_datetime ( v , assume_naive_is_utc , utc_strict ) for k , v in data . items () } elif isinstance ( data , list ): return [ bulk_utc_datetime_to_ca_naive_datetime ( i , assume_naive_is_utc , utc_strict ) for i in data ] elif isinstance ( data , tuple ): return tuple ( bulk_utc_datetime_to_ca_naive_datetime ( i , assume_naive_is_utc , utc_strict ) for i in data ) elif isinstance ( data , set ): return { bulk_utc_datetime_to_ca_naive_datetime ( i , assume_naive_is_utc , utc_strict ) for i in data } return data","title":"bulk_utc_datetime_to_ca_naive_datetime"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.ca_naive_datetime_to_utc_datetime","text":"Convert a naive California local datetime to a UTC-aware datetime. Parameters: ca_naive_dt ( datetime ) \u2013 Naive datetime (assumed Pacific Time). Must not be timezone-aware or None. Returns: datetime ( datetime ) \u2013 UTC-aware datetime. Raises: ValueError \u2013 If input is not naive (i.e., already has tzinfo) or is None. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0) Output: datetime.datetime(2025, 7, 4, 19, 0, tzinfo=ZoneInfo(\"UTC\")) Input : None Output: TypeError Notes If ca_naive_dt is None, a TypeError will be raised. Source code in arb\\utils\\date_and_time.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def ca_naive_datetime_to_utc_datetime ( ca_naive_dt : datetime ) -> datetime : \"\"\" Convert a naive California local datetime to a UTC-aware datetime. Args: ca_naive_dt (datetime): Naive datetime (assumed Pacific Time). Must not be timezone-aware or None. Returns: datetime: UTC-aware datetime. Raises: ValueError: If input is not naive (i.e., already has tzinfo) or is None. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0) Output: datetime.datetime(2025, 7, 4, 19, 0, tzinfo=ZoneInfo(\"UTC\")) Input : None Output: TypeError Notes: - If `ca_naive_dt` is None, a TypeError will be raised. \"\"\" if ca_naive_dt . tzinfo is not None : raise ValueError ( f \"Expected naive datetime, got { ca_naive_dt !r} \" ) return ca_naive_dt . replace ( tzinfo = PACIFIC_TZ ) . astimezone ( UTC_TZ )","title":"ca_naive_datetime_to_utc_datetime"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.excel_naive_datetime_to_utc_datetime","text":"Convert a naive Excel datetime (assumed California local) to a UTC-aware datetime. Parameters: excel_dt ( datetime ) \u2013 Naive datetime from Excel. Must not be timezone-aware or None. Returns: datetime ( datetime ) \u2013 UTC-aware datetime. Raises: ValueError \u2013 If input is already timezone-aware or is None. Notes If excel_dt is None, a TypeError will be raised. Source code in arb\\utils\\date_and_time.py 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 def excel_naive_datetime_to_utc_datetime ( excel_dt : datetime ) -> datetime : \"\"\" Convert a naive Excel datetime (assumed California local) to a UTC-aware datetime. Args: excel_dt (datetime): Naive datetime from Excel. Must not be timezone-aware or None. Returns: datetime: UTC-aware datetime. Raises: ValueError: If input is already timezone-aware or is None. Notes: - If `excel_dt` is None, a TypeError will be raised. \"\"\" if excel_dt . tzinfo is not None : raise ValueError ( \"Excel datetime should be naive (no timezone)\" ) return ca_naive_datetime_to_utc_datetime ( excel_dt )","title":"excel_naive_datetime_to_utc_datetime"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.excel_str_to_naive_datetime","text":"Parse a string from an Excel cell to a naive datetime object (no timezone). Parameters: excel_str ( str ) \u2013 Excel cell value to parse. If None, not a string, or empty, returns None. Returns: datetime | None \u2013 datetime | None: Parsed naive datetime, or None if unparseable or input is not a string. Examples: Input : \"2025-07-04 12:00:00\" Output: datetime.datetime(2025, 7, 4, 12, 0) Input : None Output: None Input : \"\" Output: None Notes If excel_str is None, not a string, or empty, returns None. If parsing fails, returns None. Source code in arb\\utils\\date_and_time.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def excel_str_to_naive_datetime ( excel_str : str ) -> datetime | None : \"\"\" Parse a string from an Excel cell to a naive datetime object (no timezone). Args: excel_str (str): Excel cell value to parse. If None, not a string, or empty, returns None. Returns: datetime | None: Parsed naive datetime, or None if unparseable or input is not a string. Examples: Input : \"2025-07-04 12:00:00\" Output: datetime.datetime(2025, 7, 4, 12, 0) Input : None Output: None Input : \"\" Output: None Notes: - If `excel_str` is None, not a string, or empty, returns None. - If parsing fails, returns None. \"\"\" if not isinstance ( excel_str , str ) or not excel_str : return None try : return parser . parse ( excel_str ) except ( ValueError , TypeError ): return None","title":"excel_str_to_naive_datetime"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.html_naive_str_to_utc_datetime","text":"Convert an HTML form datetime-local string (naive, California local) to a UTC-aware datetime. Parameters: html_str ( str ) \u2013 HTML datetime-local input value (e.g., \"2025-01-15T14:30\"). If None or empty, returns None. Returns: datetime | None \u2013 datetime | None: UTC-aware datetime, or None if input is empty or None. Examples: Input : \"2025-07-04T12:00\" Output: datetime.datetime(2025, 7, 4, 19, 0, tzinfo=ZoneInfo(\"UTC\")) Input : None Output: None Input : \"\" Output: None Notes If html_str is None or empty, returns None. If parsing fails, may raise ValueError. Source code in arb\\utils\\date_and_time.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def html_naive_str_to_utc_datetime ( html_str : str ) -> datetime | None : \"\"\" Convert an HTML form datetime-local string (naive, California local) to a UTC-aware datetime. Args: html_str (str): HTML datetime-local input value (e.g., \"2025-01-15T14:30\"). If None or empty, returns None. Returns: datetime | None: UTC-aware datetime, or None if input is empty or None. Examples: Input : \"2025-07-04T12:00\" Output: datetime.datetime(2025, 7, 4, 19, 0, tzinfo=ZoneInfo(\"UTC\")) Input : None Output: None Input : \"\" Output: None Notes: - If `html_str` is None or empty, returns None. - If parsing fails, may raise ValueError. \"\"\" if not html_str : return None ca_naive_dt = datetime . fromisoformat ( html_str ) return ca_naive_datetime_to_utc_datetime ( ca_naive_dt )","title":"html_naive_str_to_utc_datetime"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.is_datetime_naive","text":"Return True if a datetime is naive (lacks timezone info). Parameters: dt ( datetime ) \u2013 Datetime to check. If None, returns False. Returns: bool ( bool ) \u2013 True if naive, False otherwise. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0) Output: True Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: False Input : None Output: False Notes If dt is None, returns False. Source code in arb\\utils\\date_and_time.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def is_datetime_naive ( dt : datetime ) -> bool : \"\"\" Return True if a datetime is naive (lacks timezone info). Args: dt (datetime): Datetime to check. If None, returns False. Returns: bool: True if naive, False otherwise. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0) Output: True Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: False Input : None Output: False Notes: - If `dt` is None, returns False. \"\"\" if dt is None : # None is not a datetime, so treat as not naive (or could raise error) return False if dt . tzinfo is None : # No timezone info attached: definitely naive return True if dt . tzinfo . utcoffset ( dt ) is None : # tzinfo is set, but utcoffset returns None: still considered naive return True # Otherwise, datetime is timezone-aware return False","title":"is_datetime_naive"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.is_datetime_utc","text":"Return True if the datetime is timezone-aware and in UTC. Parameters: dt ( datetime ) \u2013 Datetime to check. If None or naive, returns False. Returns: bool ( bool ) \u2013 True if dt is timezone-aware and in UTC, False otherwise. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: True Input : datetime.datetime(2025, 7, 4, 12, 0) Output: False Input : None Output: False Notes If dt is None or naive, returns False. Source code in arb\\utils\\date_and_time.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def is_datetime_utc ( dt : datetime ) -> bool : \"\"\" Return True if the datetime is timezone-aware and in UTC. Args: dt (datetime): Datetime to check. If None or naive, returns False. Returns: bool: True if dt is timezone-aware and in UTC, False otherwise. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: True Input : datetime.datetime(2025, 7, 4, 12, 0) Output: False Input : None Output: False Notes: - If `dt` is None or naive, returns False. \"\"\" if dt is None : return False if dt . tzinfo is None : return False # UTC offset must be zero for UTC, regardless of tzinfo implementation return dt . tzinfo . utcoffset ( dt ) == timedelta ( 0 )","title":"is_datetime_utc"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.iso_str_to_utc_datetime","text":"Convert an ISO 8601 string to a UTC-aware datetime object. Parameters: iso_str ( str ) \u2013 ISO 8601-formatted datetime string. If None or empty, raises ValueError. error_on_missing_tz ( bool , default: True ) \u2013 If True, raise if no timezone info is present. If False, assumes UTC and logs a warning. Returns: datetime ( datetime ) \u2013 UTC-aware datetime object. Raises: ValueError \u2013 If the string is invalid, empty, None, or lacks timezone info (when required). Examples: Input : \"2025-07-04T12:00:00+00:00\" Output: datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Input : \"2025-07-04T12:00:00\" (with error_on_missing_tz=False) Output: datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Notes If iso_str is None or empty, a ValueError is raised. If error_on_missing_tz is False and the string is naive, UTC is assumed and a warning is logged. Source code in arb\\utils\\date_and_time.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def iso_str_to_utc_datetime ( iso_str : str , error_on_missing_tz : bool = True ) -> datetime : \"\"\" Convert an ISO 8601 string to a UTC-aware datetime object. Args: iso_str (str): ISO 8601-formatted datetime string. If None or empty, raises ValueError. error_on_missing_tz (bool): If True, raise if no timezone info is present. If False, assumes UTC and logs a warning. Returns: datetime: UTC-aware datetime object. Raises: ValueError: If the string is invalid, empty, None, or lacks timezone info (when required). Examples: Input : \"2025-07-04T12:00:00+00:00\" Output: datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Input : \"2025-07-04T12:00:00\" (with error_on_missing_tz=False) Output: datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Notes: - If `iso_str` is None or empty, a ValueError is raised. - If `error_on_missing_tz` is False and the string is naive, UTC is assumed and a warning is logged. \"\"\" try : dt = parser . isoparse ( iso_str ) except ( ValueError , TypeError ) as e : raise ValueError ( f \"Invalid ISO 8601 datetime string: ' { iso_str } '\" ) from e if dt . tzinfo is None : if error_on_missing_tz : raise ValueError ( \"Missing timezone info in ISO 8601 string.\" ) logger . warning ( f \"Assuming UTC for naive ISO string: { iso_str } \" ) dt = dt . replace ( tzinfo = UTC_TZ ) return dt . astimezone ( UTC_TZ )","title":"iso_str_to_utc_datetime"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.utc_datetime_to_ca_naive_datetime","text":"Convert a UTC-aware (or optionally naive) datetime to naive California local time. Parameters: utc_dt ( datetime ) \u2013 The datetime to convert. If None, raises ValueError. If naive, must set assume_naive_is_utc=True. assume_naive_is_utc ( bool , default: False ) \u2013 If True, treat naive input as UTC and log a warning. utc_strict ( bool , default: True ) \u2013 If True, require input to be UTC; raises ValueError if not. Returns: datetime ( datetime ) \u2013 Naive California local datetime. Raises: ValueError \u2013 If input is None, is naive and assume_naive_is_utc is False, or if UTC check fails. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: datetime.datetime(2025, 7, 4, 5, 0) Input : datetime.datetime(2025, 7, 4, 12, 0) (assume_naive_is_utc=True) Output: datetime.datetime(2025, 7, 4, 5, 0) Input : None Output: ValueError Notes If utc_dt is None, a ValueError is raised. If assume_naive_is_utc is True, naive input is treated as UTC and a warning is logged. Source code in arb\\utils\\date_and_time.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def utc_datetime_to_ca_naive_datetime ( utc_dt : datetime , assume_naive_is_utc : bool = False , utc_strict : bool = True ) -> datetime : \"\"\" Convert a UTC-aware (or optionally naive) datetime to naive California local time. Args: utc_dt (datetime): The datetime to convert. If None, raises ValueError. If naive, must set assume_naive_is_utc=True. assume_naive_is_utc (bool): If True, treat naive input as UTC and log a warning. utc_strict (bool): If True, require input to be UTC; raises ValueError if not. Returns: datetime: Naive California local datetime. Raises: ValueError: If input is None, is naive and assume_naive_is_utc is False, or if UTC check fails. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: datetime.datetime(2025, 7, 4, 5, 0) Input : datetime.datetime(2025, 7, 4, 12, 0) (assume_naive_is_utc=True) Output: datetime.datetime(2025, 7, 4, 5, 0) Input : None Output: ValueError Notes: - If `utc_dt` is None, a ValueError is raised. - If `assume_naive_is_utc` is True, naive input is treated as UTC and a warning is logged. \"\"\" dt = utc_dt if dt . tzinfo is None : if assume_naive_is_utc : logger . warning ( f \"Assuming UTC for naive datetime.\" ) dt = dt . replace ( tzinfo = UTC_TZ ) else : raise ValueError ( \"Naive datetime provided without assume_naive_is_utc=True\" ) elif utc_strict and dt . tzinfo != UTC_TZ : raise ValueError ( \"datetime must be UTC when utc_strict=True\" ) return dt . astimezone ( PACIFIC_TZ ) . replace ( tzinfo = None )","title":"utc_datetime_to_ca_naive_datetime"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.utc_datetime_to_html_naive_str","text":"Convert a UTC-aware datetime to a naive California local string for HTML form display. Parameters: utc_dt ( datetime ) \u2013 UTC-aware datetime. Must not be naive, non-UTC, or None. Returns: str ( str ) \u2013 California local datetime string (e.g., \"2025-01-15T14:30\"). Raises: ValueError \u2013 If input is naive, not UTC, or None. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: \"2025-07-04T05:00\" Input : None Output: ValueError Notes If utc_dt is None, a ValueError is raised. Source code in arb\\utils\\date_and_time.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def utc_datetime_to_html_naive_str ( utc_dt : datetime ) -> str : \"\"\" Convert a UTC-aware datetime to a naive California local string for HTML form display. Args: utc_dt (datetime): UTC-aware datetime. Must not be naive, non-UTC, or None. Returns: str: California local datetime string (e.g., \"2025-01-15T14:30\"). Raises: ValueError: If input is naive, not UTC, or None. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: \"2025-07-04T05:00\" Input : None Output: ValueError Notes: - If `utc_dt` is None, a ValueError is raised. \"\"\" if utc_dt . tzinfo is None : raise ValueError ( \"Datetime must be timezone-aware\" ) if utc_dt . tzinfo != UTC_TZ : raise ValueError ( \"Datetime must be UTC\" ) return utc_datetime_to_ca_naive_datetime ( utc_dt , utc_strict = True ) . strftime ( \"%Y-%m- %d T%H:%M\" )","title":"utc_datetime_to_html_naive_str"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.utc_datetime_to_iso_str","text":"Convert a timezone-aware datetime to a UTC ISO 8601 string. Parameters: utc_dt ( datetime ) \u2013 Timezone-aware datetime. Must not be naive or None. Returns: str ( str ) \u2013 UTC ISO 8601 string. Raises: ValueError \u2013 If input is naive or None. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: \"2025-07-04T12:00:00+00:00\" Input : None Output: ValueError Notes If utc_dt is None, a ValueError is raised. Source code in arb\\utils\\date_and_time.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def utc_datetime_to_iso_str ( utc_dt : datetime ) -> str : \"\"\" Convert a timezone-aware datetime to a UTC ISO 8601 string. Args: utc_dt (datetime): Timezone-aware datetime. Must not be naive or None. Returns: str: UTC ISO 8601 string. Raises: ValueError: If input is naive or None. Examples: Input : datetime.datetime(2025, 7, 4, 12, 0, tzinfo=ZoneInfo(\"UTC\")) Output: \"2025-07-04T12:00:00+00:00\" Input : None Output: ValueError Notes: - If `utc_dt` is None, a ValueError is raised. \"\"\" if utc_dt . tzinfo is None : raise ValueError ( \"Datetime must be timezone-aware\" ) return utc_dt . astimezone ( UTC_TZ ) . isoformat ()","title":"utc_datetime_to_iso_str"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.utc_iso_str_to_ca_str","text":"Convert a UTC ISO string to a California local time string for display. Parameters: iso_str ( str ) \u2013 UTC ISO 8601 string. If None or empty, returns empty string. If invalid, returns input as-is. Returns: str ( str ) \u2013 California local time string for display (e.g., \"2025-01-15T14:30\"), or original string if parsing fails. Notes If iso_str is None or empty, returns empty string. If parsing fails, returns the original string as-is. Source code in arb\\utils\\date_and_time.py 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def utc_iso_str_to_ca_str ( iso_str : str ) -> str : \"\"\" Convert a UTC ISO string to a California local time string for display. Args: iso_str (str): UTC ISO 8601 string. If None or empty, returns empty string. If invalid, returns input as-is. Returns: str: California local time string for display (e.g., \"2025-01-15T14:30\"), or original string if parsing fails. Notes: - If `iso_str` is None or empty, returns empty string. - If parsing fails, returns the original string as-is. \"\"\" if not iso_str : return \"\" try : utc_dt = iso_str_to_utc_datetime ( iso_str ) return utc_datetime_to_html_naive_str ( utc_dt ) except Exception : return iso_str # fallback: show as-is if parsing fails","title":"utc_iso_str_to_ca_str"},{"location":"reference/arb/utils/diagnostics/","text":"arb.utils.diagnostics Diagnostic utilities for inspecting and logging Python objects. Provides: - Object introspection for development/debugging - Attribute/value logging (including hidden and callable members) - Dictionary comparisons and formatting - Recursive HTML-safe rendering of complex data structures - Integration with Flask for developer-oriented diagnostics Intended primarily for use in debug environments, template rendering, or ad-hoc inspection of application state during development. compare_dicts ( dict1 , dict2 , dict1_name = None , dict2_name = None ) Compare two dictionaries and log differences in keys and values. Parameters: dict1 ( dict ) \u2013 First dictionary. If None, treated as empty dict. dict2 ( dict ) \u2013 Second dictionary. If None, treated as empty dict. dict1_name ( str | None , default: None ) \u2013 Optional label for first dictionary in logs. dict2_name ( str | None , default: None ) \u2013 Optional label for second dictionary in logs. Returns: bool ( bool ) \u2013 True if dictionaries are equivalent; False otherwise. Examples: Input : dict1 = {\"a\": 1, \"b\": 2, \"c\": 3} dict2 = {\"a\": 1, \"b\": 4, \"d\": 5} Output: False Input : None, {\"a\": 1} Output: False Input : {\"a\": 1}, None Output: False Input : None, None Output: True Notes If either dict is None, it is treated as an empty dict. Source code in arb\\utils\\diagnostics.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def compare_dicts ( dict1 : dict , dict2 : dict , dict1_name : str | None = None , dict2_name : str | None = None ) -> bool : \"\"\" Compare two dictionaries and log differences in keys and values. Args: dict1 (dict): First dictionary. If None, treated as empty dict. dict2 (dict): Second dictionary. If None, treated as empty dict. dict1_name (str | None): Optional label for first dictionary in logs. dict2_name (str | None): Optional label for second dictionary in logs. Returns: bool: True if dictionaries are equivalent; False otherwise. Examples: Input : dict1 = {\"a\": 1, \"b\": 2, \"c\": 3} dict2 = {\"a\": 1, \"b\": 4, \"d\": 5} Output: False Input : None, {\"a\": 1} Output: False Input : {\"a\": 1}, None Output: False Input : None, None Output: True Notes: - If either dict is None, it is treated as an empty dict. \"\"\" if dict1 is None : dict1 = {} if dict2 is None : dict2 = {} dict1_name = dict1_name or \"dict_1\" dict2_name = dict2_name or \"dict_2\" logger . debug ( f \"compare_dicts called to compare { dict1_name } with { dict2_name } \" ) keys_in_dict1_not_in_dict2 = set ( dict1 ) - set ( dict2 ) keys_in_dict2_not_in_dict1 = set ( dict2 ) - set ( dict1 ) differing_values = { key : ( dict1 [ key ], dict2 [ key ]) for key in dict1 . keys () & dict2 . keys () if dict1 [ key ] != dict2 [ key ] } if keys_in_dict1_not_in_dict2 or keys_in_dict2_not_in_dict1 or differing_values : logger . debug ( f \"Key differences:\" ) if keys_in_dict1_not_in_dict2 : logger . debug ( f \"- In { dict1_name } but not in { dict2_name } : { sorted ( keys_in_dict1_not_in_dict2 ) } \" ) if keys_in_dict2_not_in_dict1 : logger . debug ( f \"- In { dict2_name } but not in { dict1_name } : { sorted ( keys_in_dict2_not_in_dict1 ) } \" ) if differing_values : logger . debug ( f \"Value differences:\" ) for key , ( v1 , v2 ) in dict ( sorted ( differing_values . items ())) . items (): logger . debug ( f \"- Key: ' { key } ', { dict1_name } : { v1 } , { dict2_name } : { v2 } \" ) return False return True diag_recursive ( x , depth = 0 , index = 0 ) Recursively log the structure and values of a nested iterable. Parameters: x ( object ) \u2013 Input object to inspect. If None, logs and returns. depth ( int , default: 0 ) \u2013 Current recursion depth. index ( int , default: 0 ) \u2013 Index at the current level (if applicable). Returns: None \u2013 None Examples: Input : [[1, 2], [3, 4]] Output: Logs nested structure and values Input : None Output: Logs and returns Notes Strings are treated as non-iterables. If x is None, logs and returns. Source code in arb\\utils\\diagnostics.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def diag_recursive ( x : object , depth : int = 0 , index : int = 0 ) -> None : \"\"\" Recursively log the structure and values of a nested iterable. Args: x (object): Input object to inspect. If None, logs and returns. depth (int): Current recursion depth. index (int): Index at the current level (if applicable). Returns: None Examples: Input : [[1, 2], [3, 4]] Output: Logs nested structure and values Input : None Output: Logs and returns Notes: - Strings are treated as non-iterables. - If `x` is None, logs and returns. \"\"\" indent = ' ' * 3 * depth if depth == 0 : logger . debug ( f \"diag_recursive diagnostics called \\n { '-' * 120 } \" ) logger . debug ( f \"Type: { type ( x ) } , Value: { x } \" ) else : logger . debug ( f \" { indent } Depth: { depth } , Index: { index } , Type: { type ( x ) } , Value: { x } \" ) if not isinstance ( x , str ): try : from collections.abc import Iterable if isinstance ( x , Iterable ): for i , y in enumerate ( x ): diag_recursive ( y , depth + 1 , index = i ) except TypeError : pass dict_to_str ( x , depth = 0 ) Convert a dictionary to a pretty-printed multiline string. Parameters: x ( dict ) \u2013 Dictionary to convert. If None, returns an empty string. depth ( int , default: 0 ) \u2013 Current indentation depth for nested dictionaries. Returns: str ( str ) \u2013 String representation of dictionary with indentation. Examples: Input : {\"a\": 1, \"b\": {\"c\": 2}} Output: a: 1 b: c: 2 Input : None Output: \"\" Notes If x is None, returns an empty string. Source code in arb\\utils\\diagnostics.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def dict_to_str ( x : dict , depth : int = 0 ) -> str : \"\"\" Convert a dictionary to a pretty-printed multiline string. Args: x (dict): Dictionary to convert. If None, returns an empty string. depth (int): Current indentation depth for nested dictionaries. Returns: str: String representation of dictionary with indentation. Examples: Input : {\"a\": 1, \"b\": {\"c\": 2}} Output: a: 1 b: c: 2 Input : None Output: \"\" Notes: - If `x` is None, returns an empty string. \"\"\" if x is None : return \"\" msg = \"\" indent = ' ' * 3 * depth for k , v in x . items (): msg += f \" { indent }{ k } : \\n \" if isinstance ( v , dict ): msg += dict_to_str ( v , depth = depth + 1 ) else : msg += f \" { indent }{ v } \\n \" return msg get_changed_fields ( new_dict , old_dict ) Extract fields from new_dict that differ from old_dict. Parameters: new_dict ( dict ) \u2013 New/updated values (e.g., from a form). If None, treated as empty dict. old_dict ( dict ) \u2013 Old/reference values (e.g., from model JSON). If None, treated as empty dict. Returns: dict ( dict ) \u2013 Keys with values that have changed. Notes Only keys present in new_dict are considered. This prevents unrelated fields from being overwritten when merging partial form data into a larger stored structure. If either dict is None, it is treated as an empty dict. Source code in arb\\utils\\diagnostics.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def get_changed_fields ( new_dict : dict , old_dict : dict ) -> dict : \"\"\" Extract fields from new_dict that differ from old_dict. Args: new_dict (dict): New/updated values (e.g., from a form). If None, treated as empty dict. old_dict (dict): Old/reference values (e.g., from model JSON). If None, treated as empty dict. Returns: dict: Keys with values that have changed. Notes: - Only keys present in new_dict are considered. This prevents unrelated fields from being overwritten when merging partial form data into a larger stored structure. - If either dict is None, it is treated as an empty dict. \"\"\" if new_dict is None : new_dict = {} if old_dict is None : old_dict = {} changes = {} for key in new_dict : if new_dict [ key ] != old_dict . get ( key ): changes [ key ] = new_dict [ key ] return changes list_differences ( iterable_01 , iterable_02 , iterable_01_name = 'List 1' , iterable_02_name = 'List 2' , print_warning = False ) Identify differences between two iterable objects (list or dict). Parameters: iterable_01 ( list | dict ) \u2013 First iterable object to compare. If None, treated as empty. iterable_02 ( list | dict ) \u2013 Second iterable object to compare. If None, treated as empty. iterable_01_name ( str , default: 'List 1' ) \u2013 Label for the first iterable in log messages. iterable_02_name ( str , default: 'List 2' ) \u2013 Label for the second iterable in log messages. print_warning ( bool , default: False ) \u2013 If True, log warnings for non-overlapping items. Returns: tuple [ list , list ] \u2013 tuple[list, list]: - Items in iterable_01 but not in iterable_02 - Items in iterable_02 but not in iterable_01 Examples: Input : [\"a\", \"b\"], [\"b\", \"c\"] Output: ([\"a\"], [\"c\"]) Input : None, [\"b\", \"c\"] Output: ([], [\"b\", \"c\"]) Input : [\"a\", \"b\"], None Output: ([\"a\", \"b\"], []) Notes If either iterable is None, it is treated as an empty list/dict. Source code in arb\\utils\\diagnostics.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def list_differences ( iterable_01 : list | dict , iterable_02 : list | dict , iterable_01_name : str = \"List 1\" , iterable_02_name : str = \"List 2\" , print_warning : bool = False ) -> tuple [ list , list ]: \"\"\" Identify differences between two iterable objects (list or dict). Args: iterable_01 (list | dict): First iterable object to compare. If None, treated as empty. iterable_02 (list | dict): Second iterable object to compare. If None, treated as empty. iterable_01_name (str): Label for the first iterable in log messages. iterable_02_name (str): Label for the second iterable in log messages. print_warning (bool): If True, log warnings for non-overlapping items. Returns: tuple[list, list]: - Items in `iterable_01` but not in `iterable_02` - Items in `iterable_02` but not in `iterable_01` Examples: Input : [\"a\", \"b\"], [\"b\", \"c\"] Output: ([\"a\"], [\"c\"]) Input : None, [\"b\", \"c\"] Output: ([], [\"b\", \"c\"]) Input : [\"a\", \"b\"], None Output: ([\"a\", \"b\"], []) Notes: - If either iterable is None, it is treated as an empty list/dict. \"\"\" if iterable_01 is None : iterable_01 = [] if iterable_02 is None : iterable_02 = [] in_iterable_1_only = [ x for x in iterable_01 if x not in iterable_02 ] in_iterable_2_only = [ x for x in iterable_02 if x not in iterable_01 ] if print_warning : if in_iterable_1_only : logger . warning ( f \"Warning: { iterable_01_name } has { len ( in_iterable_1_only ) } item(s) not in { iterable_02_name } : \\t { in_iterable_1_only } \" ) if in_iterable_2_only : logger . warning ( f \"Warning: { iterable_02_name } has { len ( in_iterable_2_only ) } item(s) not in { iterable_01_name } : \\t { in_iterable_2_only } \" ) return in_iterable_1_only , in_iterable_2_only obj_diagnostics ( obj , include_hidden = False , include_functions = False , message = None ) Log detailed diagnostics about an object's attributes and values. Parameters: obj ( Any ) \u2013 The object to inspect. If None, logs 'None' and returns. include_hidden ( bool , default: False ) \u2013 Whether to include private attributes (starting with _ ). include_functions ( bool , default: False ) \u2013 Whether to include methods or callable attributes. message ( str | None , default: None ) \u2013 Optional prefix message to label the diagnostic output. Returns: None \u2013 None Examples: Input : obj={'a': 1, 'b': 2}, include_hidden=False, include_functions=False Output: Logs all public attributes and their values Input : obj=None Output: Logs 'None' and returns Notes If obj is None, logs 'None' and returns. Source code in arb\\utils\\diagnostics.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def obj_diagnostics ( obj : Any , include_hidden : bool = False , include_functions : bool = False , message : str | None = None ) -> None : \"\"\" Log detailed diagnostics about an object's attributes and values. Args: obj (Any): The object to inspect. If None, logs 'None' and returns. include_hidden (bool): Whether to include private attributes (starting with `_`). include_functions (bool): Whether to include methods or callable attributes. message (str | None): Optional prefix message to label the diagnostic output. Returns: None Examples: Input : obj={'a': 1, 'b': 2}, include_hidden=False, include_functions=False Output: Logs all public attributes and their values Input : obj=None Output: Logs 'None' and returns Notes: - If `obj` is None, logs 'None' and returns. \"\"\" logger . debug ( f \"Diagnostics for: { obj } \" ) if message : logger . debug ( f \" { message =} \" ) for attr_name in dir ( obj ): attr_value = getattr ( obj , attr_name ) if not attr_name . startswith ( '_' ) or include_hidden : if callable ( attr_value ): if include_functions : logger . debug ( f \" { attr_name } (): is function\" ) else : logger . debug ( f \" { attr_name } { type ( attr_value ) } : \\t { attr_value } \" ) obj_to_html ( obj ) Convert any Python object to a formatted HTML string for Jinja rendering. Parameters: obj ( object ) \u2013 A Python object suitable for pprint . If None, returns an empty block. Returns: str ( str ) \u2013 HTML string wrapped in tags that are safe for use with |safe in templates. e.g.,{{ result|safe }} Examples: Input : {\"a\": 1, \"b\": {\"c\": 2}} Output: {'a': 1, 'b': {'c': 2}} Input : None Output: Notes The HTML content must be marked |safe in the template to avoid escaping. If obj is None, returns an empty block. Source code in arb\\utils\\diagnostics.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def obj_to_html ( obj : object ) -> str : \"\"\" Convert any Python object to a formatted HTML string for Jinja rendering. Args: obj (object): A Python object suitable for `pprint`. If None, returns an empty <pre> block. Returns: str: HTML string wrapped in <pre> tags that are safe for use with `|safe` in templates. e.g.,{{ result|safe }} Examples: Input : {\"a\": 1, \"b\": {\"c\": 2}} Output: <pre>{'a': 1, 'b': {'c': 2}}</pre> Input : None Output: <pre></pre> Notes: - The HTML content must be marked `|safe` in the template to avoid escaping. - If `obj` is None, returns an empty <pre> block. \"\"\" pp = pprint . PrettyPrinter ( indent = 4 , width = 200 ) formatted_data = pp . pformat ( obj ) soup = BeautifulSoup ( \"<pre></pre>\" , \"html.parser\" ) if soup . pre is not None : soup . pre . string = formatted_data return soup . prettify () run_diagnostics () Run example-based tests for diagnostic functions in this module. Logs example output for each function. Source code in arb\\utils\\diagnostics.py 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 def run_diagnostics () -> None : \"\"\" Run example-based tests for diagnostic functions in this module. Logs example output for each function. \"\"\" logger . debug ( f \"Running diagnostics on arb.utils.diagnostics module\" ) # obj_diagnostics class TestClass : def __init__ ( self ): self . x = 42 self . y = \"hello\" def greet ( self ): return \"hi\" obj = TestClass () obj_diagnostics ( obj , include_hidden = False , include_functions = True ) # list_differences a = [ \"x\" , \"y\" , \"z\" ] b = [ \"x\" , \"z\" , \"w\" ] list_differences ( a , b , \"List A\" , \"List B\" , print_warning = True ) # diag_recursive diag_recursive ([[ 1 , 2 ], [ 3 , [ 4 , 5 ]]]) # dict_to_str nested_dict = { \"a\" : 1 , \"b\" : { \"c\" : 2 , \"d\" : { \"e\" : 3 }}} logger . debug ( f \"dict_to_str output: \\t \" + dict_to_str ( nested_dict )) # obj_to_html html_result = obj_to_html ( nested_dict ) logger . debug ( f \"HTML representation of object (truncated): \\t \" + html_result [: 300 ]) # compare_dicts d1 = { \"x\" : 1 , \"y\" : 2 , \"z\" : 3 } d2 = { \"x\" : 1 , \"y\" : 99 , \"w\" : 0 } compare_dicts ( d1 , d2 , \"First Dict\" , \"Second Dict\" )","title":"arb.utils.diagnostics"},{"location":"reference/arb/utils/diagnostics/#arbutilsdiagnostics","text":"Diagnostic utilities for inspecting and logging Python objects. Provides: - Object introspection for development/debugging - Attribute/value logging (including hidden and callable members) - Dictionary comparisons and formatting - Recursive HTML-safe rendering of complex data structures - Integration with Flask for developer-oriented diagnostics Intended primarily for use in debug environments, template rendering, or ad-hoc inspection of application state during development.","title":"arb.utils.diagnostics"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.compare_dicts","text":"Compare two dictionaries and log differences in keys and values. Parameters: dict1 ( dict ) \u2013 First dictionary. If None, treated as empty dict. dict2 ( dict ) \u2013 Second dictionary. If None, treated as empty dict. dict1_name ( str | None , default: None ) \u2013 Optional label for first dictionary in logs. dict2_name ( str | None , default: None ) \u2013 Optional label for second dictionary in logs. Returns: bool ( bool ) \u2013 True if dictionaries are equivalent; False otherwise. Examples: Input : dict1 = {\"a\": 1, \"b\": 2, \"c\": 3} dict2 = {\"a\": 1, \"b\": 4, \"d\": 5} Output: False Input : None, {\"a\": 1} Output: False Input : {\"a\": 1}, None Output: False Input : None, None Output: True Notes If either dict is None, it is treated as an empty dict. Source code in arb\\utils\\diagnostics.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 def compare_dicts ( dict1 : dict , dict2 : dict , dict1_name : str | None = None , dict2_name : str | None = None ) -> bool : \"\"\" Compare two dictionaries and log differences in keys and values. Args: dict1 (dict): First dictionary. If None, treated as empty dict. dict2 (dict): Second dictionary. If None, treated as empty dict. dict1_name (str | None): Optional label for first dictionary in logs. dict2_name (str | None): Optional label for second dictionary in logs. Returns: bool: True if dictionaries are equivalent; False otherwise. Examples: Input : dict1 = {\"a\": 1, \"b\": 2, \"c\": 3} dict2 = {\"a\": 1, \"b\": 4, \"d\": 5} Output: False Input : None, {\"a\": 1} Output: False Input : {\"a\": 1}, None Output: False Input : None, None Output: True Notes: - If either dict is None, it is treated as an empty dict. \"\"\" if dict1 is None : dict1 = {} if dict2 is None : dict2 = {} dict1_name = dict1_name or \"dict_1\" dict2_name = dict2_name or \"dict_2\" logger . debug ( f \"compare_dicts called to compare { dict1_name } with { dict2_name } \" ) keys_in_dict1_not_in_dict2 = set ( dict1 ) - set ( dict2 ) keys_in_dict2_not_in_dict1 = set ( dict2 ) - set ( dict1 ) differing_values = { key : ( dict1 [ key ], dict2 [ key ]) for key in dict1 . keys () & dict2 . keys () if dict1 [ key ] != dict2 [ key ] } if keys_in_dict1_not_in_dict2 or keys_in_dict2_not_in_dict1 or differing_values : logger . debug ( f \"Key differences:\" ) if keys_in_dict1_not_in_dict2 : logger . debug ( f \"- In { dict1_name } but not in { dict2_name } : { sorted ( keys_in_dict1_not_in_dict2 ) } \" ) if keys_in_dict2_not_in_dict1 : logger . debug ( f \"- In { dict2_name } but not in { dict1_name } : { sorted ( keys_in_dict2_not_in_dict1 ) } \" ) if differing_values : logger . debug ( f \"Value differences:\" ) for key , ( v1 , v2 ) in dict ( sorted ( differing_values . items ())) . items (): logger . debug ( f \"- Key: ' { key } ', { dict1_name } : { v1 } , { dict2_name } : { v2 } \" ) return False return True","title":"compare_dicts"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.diag_recursive","text":"Recursively log the structure and values of a nested iterable. Parameters: x ( object ) \u2013 Input object to inspect. If None, logs and returns. depth ( int , default: 0 ) \u2013 Current recursion depth. index ( int , default: 0 ) \u2013 Index at the current level (if applicable). Returns: None \u2013 None Examples: Input : [[1, 2], [3, 4]] Output: Logs nested structure and values Input : None Output: Logs and returns Notes Strings are treated as non-iterables. If x is None, logs and returns. Source code in arb\\utils\\diagnostics.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def diag_recursive ( x : object , depth : int = 0 , index : int = 0 ) -> None : \"\"\" Recursively log the structure and values of a nested iterable. Args: x (object): Input object to inspect. If None, logs and returns. depth (int): Current recursion depth. index (int): Index at the current level (if applicable). Returns: None Examples: Input : [[1, 2], [3, 4]] Output: Logs nested structure and values Input : None Output: Logs and returns Notes: - Strings are treated as non-iterables. - If `x` is None, logs and returns. \"\"\" indent = ' ' * 3 * depth if depth == 0 : logger . debug ( f \"diag_recursive diagnostics called \\n { '-' * 120 } \" ) logger . debug ( f \"Type: { type ( x ) } , Value: { x } \" ) else : logger . debug ( f \" { indent } Depth: { depth } , Index: { index } , Type: { type ( x ) } , Value: { x } \" ) if not isinstance ( x , str ): try : from collections.abc import Iterable if isinstance ( x , Iterable ): for i , y in enumerate ( x ): diag_recursive ( y , depth + 1 , index = i ) except TypeError : pass","title":"diag_recursive"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.dict_to_str","text":"Convert a dictionary to a pretty-printed multiline string. Parameters: x ( dict ) \u2013 Dictionary to convert. If None, returns an empty string. depth ( int , default: 0 ) \u2013 Current indentation depth for nested dictionaries. Returns: str ( str ) \u2013 String representation of dictionary with indentation. Examples: Input : {\"a\": 1, \"b\": {\"c\": 2}} Output: a: 1 b: c: 2 Input : None Output: \"\" Notes If x is None, returns an empty string. Source code in arb\\utils\\diagnostics.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 def dict_to_str ( x : dict , depth : int = 0 ) -> str : \"\"\" Convert a dictionary to a pretty-printed multiline string. Args: x (dict): Dictionary to convert. If None, returns an empty string. depth (int): Current indentation depth for nested dictionaries. Returns: str: String representation of dictionary with indentation. Examples: Input : {\"a\": 1, \"b\": {\"c\": 2}} Output: a: 1 b: c: 2 Input : None Output: \"\" Notes: - If `x` is None, returns an empty string. \"\"\" if x is None : return \"\" msg = \"\" indent = ' ' * 3 * depth for k , v in x . items (): msg += f \" { indent }{ k } : \\n \" if isinstance ( v , dict ): msg += dict_to_str ( v , depth = depth + 1 ) else : msg += f \" { indent }{ v } \\n \" return msg","title":"dict_to_str"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.get_changed_fields","text":"Extract fields from new_dict that differ from old_dict. Parameters: new_dict ( dict ) \u2013 New/updated values (e.g., from a form). If None, treated as empty dict. old_dict ( dict ) \u2013 Old/reference values (e.g., from model JSON). If None, treated as empty dict. Returns: dict ( dict ) \u2013 Keys with values that have changed. Notes Only keys present in new_dict are considered. This prevents unrelated fields from being overwritten when merging partial form data into a larger stored structure. If either dict is None, it is treated as an empty dict. Source code in arb\\utils\\diagnostics.py 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 def get_changed_fields ( new_dict : dict , old_dict : dict ) -> dict : \"\"\" Extract fields from new_dict that differ from old_dict. Args: new_dict (dict): New/updated values (e.g., from a form). If None, treated as empty dict. old_dict (dict): Old/reference values (e.g., from model JSON). If None, treated as empty dict. Returns: dict: Keys with values that have changed. Notes: - Only keys present in new_dict are considered. This prevents unrelated fields from being overwritten when merging partial form data into a larger stored structure. - If either dict is None, it is treated as an empty dict. \"\"\" if new_dict is None : new_dict = {} if old_dict is None : old_dict = {} changes = {} for key in new_dict : if new_dict [ key ] != old_dict . get ( key ): changes [ key ] = new_dict [ key ] return changes","title":"get_changed_fields"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.list_differences","text":"Identify differences between two iterable objects (list or dict). Parameters: iterable_01 ( list | dict ) \u2013 First iterable object to compare. If None, treated as empty. iterable_02 ( list | dict ) \u2013 Second iterable object to compare. If None, treated as empty. iterable_01_name ( str , default: 'List 1' ) \u2013 Label for the first iterable in log messages. iterable_02_name ( str , default: 'List 2' ) \u2013 Label for the second iterable in log messages. print_warning ( bool , default: False ) \u2013 If True, log warnings for non-overlapping items. Returns: tuple [ list , list ] \u2013 tuple[list, list]: - Items in iterable_01 but not in iterable_02 - Items in iterable_02 but not in iterable_01 Examples: Input : [\"a\", \"b\"], [\"b\", \"c\"] Output: ([\"a\"], [\"c\"]) Input : None, [\"b\", \"c\"] Output: ([], [\"b\", \"c\"]) Input : [\"a\", \"b\"], None Output: ([\"a\", \"b\"], []) Notes If either iterable is None, it is treated as an empty list/dict. Source code in arb\\utils\\diagnostics.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def list_differences ( iterable_01 : list | dict , iterable_02 : list | dict , iterable_01_name : str = \"List 1\" , iterable_02_name : str = \"List 2\" , print_warning : bool = False ) -> tuple [ list , list ]: \"\"\" Identify differences between two iterable objects (list or dict). Args: iterable_01 (list | dict): First iterable object to compare. If None, treated as empty. iterable_02 (list | dict): Second iterable object to compare. If None, treated as empty. iterable_01_name (str): Label for the first iterable in log messages. iterable_02_name (str): Label for the second iterable in log messages. print_warning (bool): If True, log warnings for non-overlapping items. Returns: tuple[list, list]: - Items in `iterable_01` but not in `iterable_02` - Items in `iterable_02` but not in `iterable_01` Examples: Input : [\"a\", \"b\"], [\"b\", \"c\"] Output: ([\"a\"], [\"c\"]) Input : None, [\"b\", \"c\"] Output: ([], [\"b\", \"c\"]) Input : [\"a\", \"b\"], None Output: ([\"a\", \"b\"], []) Notes: - If either iterable is None, it is treated as an empty list/dict. \"\"\" if iterable_01 is None : iterable_01 = [] if iterable_02 is None : iterable_02 = [] in_iterable_1_only = [ x for x in iterable_01 if x not in iterable_02 ] in_iterable_2_only = [ x for x in iterable_02 if x not in iterable_01 ] if print_warning : if in_iterable_1_only : logger . warning ( f \"Warning: { iterable_01_name } has { len ( in_iterable_1_only ) } item(s) not in { iterable_02_name } : \\t { in_iterable_1_only } \" ) if in_iterable_2_only : logger . warning ( f \"Warning: { iterable_02_name } has { len ( in_iterable_2_only ) } item(s) not in { iterable_01_name } : \\t { in_iterable_2_only } \" ) return in_iterable_1_only , in_iterable_2_only","title":"list_differences"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.obj_diagnostics","text":"Log detailed diagnostics about an object's attributes and values. Parameters: obj ( Any ) \u2013 The object to inspect. If None, logs 'None' and returns. include_hidden ( bool , default: False ) \u2013 Whether to include private attributes (starting with _ ). include_functions ( bool , default: False ) \u2013 Whether to include methods or callable attributes. message ( str | None , default: None ) \u2013 Optional prefix message to label the diagnostic output. Returns: None \u2013 None Examples: Input : obj={'a': 1, 'b': 2}, include_hidden=False, include_functions=False Output: Logs all public attributes and their values Input : obj=None Output: Logs 'None' and returns Notes If obj is None, logs 'None' and returns. Source code in arb\\utils\\diagnostics.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def obj_diagnostics ( obj : Any , include_hidden : bool = False , include_functions : bool = False , message : str | None = None ) -> None : \"\"\" Log detailed diagnostics about an object's attributes and values. Args: obj (Any): The object to inspect. If None, logs 'None' and returns. include_hidden (bool): Whether to include private attributes (starting with `_`). include_functions (bool): Whether to include methods or callable attributes. message (str | None): Optional prefix message to label the diagnostic output. Returns: None Examples: Input : obj={'a': 1, 'b': 2}, include_hidden=False, include_functions=False Output: Logs all public attributes and their values Input : obj=None Output: Logs 'None' and returns Notes: - If `obj` is None, logs 'None' and returns. \"\"\" logger . debug ( f \"Diagnostics for: { obj } \" ) if message : logger . debug ( f \" { message =} \" ) for attr_name in dir ( obj ): attr_value = getattr ( obj , attr_name ) if not attr_name . startswith ( '_' ) or include_hidden : if callable ( attr_value ): if include_functions : logger . debug ( f \" { attr_name } (): is function\" ) else : logger . debug ( f \" { attr_name } { type ( attr_value ) } : \\t { attr_value } \" )","title":"obj_diagnostics"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.obj_to_html","text":"Convert any Python object to a formatted HTML string for Jinja rendering. Parameters: obj ( object ) \u2013 A Python object suitable for pprint . If None, returns an empty block. Returns: str ( str ) \u2013 HTML string wrapped in tags that are safe for use with |safe in templates. e.g.,{{ result|safe }} Examples: Input : {\"a\": 1, \"b\": {\"c\": 2}} Output: {'a': 1, 'b': {'c': 2}} Input : None Output: Notes The HTML content must be marked |safe in the template to avoid escaping. If obj is None, returns an empty block. Source code in arb\\utils\\diagnostics.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def obj_to_html ( obj : object ) -> str : \"\"\" Convert any Python object to a formatted HTML string for Jinja rendering. Args: obj (object): A Python object suitable for `pprint`. If None, returns an empty <pre> block. Returns: str: HTML string wrapped in <pre> tags that are safe for use with `|safe` in templates. e.g.,{{ result|safe }} Examples: Input : {\"a\": 1, \"b\": {\"c\": 2}} Output: <pre>{'a': 1, 'b': {'c': 2}}</pre> Input : None Output: <pre></pre> Notes: - The HTML content must be marked `|safe` in the template to avoid escaping. - If `obj` is None, returns an empty <pre> block. \"\"\" pp = pprint . PrettyPrinter ( indent = 4 , width = 200 ) formatted_data = pp . pformat ( obj ) soup = BeautifulSoup ( \"<pre></pre>\" , \"html.parser\" ) if soup . pre is not None : soup . pre . string = formatted_data return soup . prettify ()","title":"obj_to_html"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.run_diagnostics","text":"Run example-based tests for diagnostic functions in this module. Logs example output for each function. Source code in arb\\utils\\diagnostics.py 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 def run_diagnostics () -> None : \"\"\" Run example-based tests for diagnostic functions in this module. Logs example output for each function. \"\"\" logger . debug ( f \"Running diagnostics on arb.utils.diagnostics module\" ) # obj_diagnostics class TestClass : def __init__ ( self ): self . x = 42 self . y = \"hello\" def greet ( self ): return \"hi\" obj = TestClass () obj_diagnostics ( obj , include_hidden = False , include_functions = True ) # list_differences a = [ \"x\" , \"y\" , \"z\" ] b = [ \"x\" , \"z\" , \"w\" ] list_differences ( a , b , \"List A\" , \"List B\" , print_warning = True ) # diag_recursive diag_recursive ([[ 1 , 2 ], [ 3 , [ 4 , 5 ]]]) # dict_to_str nested_dict = { \"a\" : 1 , \"b\" : { \"c\" : 2 , \"d\" : { \"e\" : 3 }}} logger . debug ( f \"dict_to_str output: \\t \" + dict_to_str ( nested_dict )) # obj_to_html html_result = obj_to_html ( nested_dict ) logger . debug ( f \"HTML representation of object (truncated): \\t \" + html_result [: 300 ]) # compare_dicts d1 = { \"x\" : 1 , \"y\" : 2 , \"z\" : 3 } d2 = { \"x\" : 1 , \"y\" : 99 , \"w\" : 0 } compare_dicts ( d1 , d2 , \"First Dict\" , \"Second Dict\" )","title":"run_diagnostics"},{"location":"reference/arb/utils/file_io/","text":"arb.utils.file_io File and path utility functions for the ARB Feedback Portal. This module provides helpers for directory creation, secure file name generation, project root resolution, and efficient file reading. These utilities are designed to support robust file handling and diagnostics across ARB portal workflows. Features: - Ensures parent and target directories exist - Generates secure, timestamped file names using UTC - Dynamically resolves the project root based on directory structure - Efficiently reads the last N lines of large files Notes: - Uses werkzeug.utils.secure_filename to sanitize input filenames - Timestamps are formatted in UTC using the DATETIME_WITH_SECONDS pattern Intended use: - Shared helpers for ARB portal and related utilities - Promotes DRY principles and robust file handling Version: 1.0.0 ProjectRootNotFoundError Bases: ValueError Raised when no matching project root can be determined from the provided file path and candidate folder sequences. Source code in arb\\utils\\file_io.py 138 139 140 141 142 143 class ProjectRootNotFoundError ( ValueError ): \"\"\" Raised when no matching project root can be determined from the provided file path and candidate folder sequences. \"\"\" pass ensure_dir_exists ( dir_path ) Ensure that the specified directory exists, creating it if necessary. Parameters: dir_path ( str | Path ) \u2013 Path to the directory. If None or empty, no action is taken. Raises: ValueError \u2013 If the path exists but is not a directory. Returns: None \u2013 None Examples: Input : \"logs/output\" Output: Creates the directory and parents if needed Input : None Output: No action Input : \"\" Output: No action Notes If dir_path is None or empty, no action is taken. Source code in arb\\utils\\file_io.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def ensure_dir_exists ( dir_path : str | Path ) -> None : \"\"\" Ensure that the specified directory exists, creating it if necessary. Args: dir_path (str | Path): Path to the directory. If None or empty, no action is taken. Raises: ValueError: If the path exists but is not a directory. Returns: None Examples: Input : \"logs/output\" Output: Creates the directory and parents if needed Input : None Output: No action Input : \"\" Output: No action Notes: - If `dir_path` is None or empty, no action is taken. \"\"\" logger . debug ( f \"ensure_dir_exists() called for: { dir_path =} \" ) if not dir_path : return dir_path = Path ( dir_path ) if dir_path . exists () and not dir_path . is_dir (): raise ValueError ( f \"The path { dir_path } exists and is not a directory.\" ) dir_path . mkdir ( parents = True , exist_ok = True ) ensure_parent_dirs ( file_name ) Ensure that the parent directories for a given file path exist. Parameters: file_name ( str | Path ) \u2013 The full path to a file. Parent folders will be created if needed. If None or empty, no action is taken. Returns: None \u2013 None Examples: Input : \"/tmp/some/deep/file.txt\" Output: Ensures intermediate directories exist Input : \"local_file.txt\" Output: No error if directory already exists or is current Input : None Output: No action Input : \"\" Output: No action Notes If file_name is None or empty, no action is taken. Source code in arb\\utils\\file_io.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def ensure_parent_dirs ( file_name : str | Path ) -> None : \"\"\" Ensure that the parent directories for a given file path exist. Args: file_name (str | Path): The full path to a file. Parent folders will be created if needed. If None or empty, no action is taken. Returns: None Examples: Input : \"/tmp/some/deep/file.txt\" Output: Ensures intermediate directories exist Input : \"local_file.txt\" Output: No error if directory already exists or is current Input : None Output: No action Input : \"\" Output: No action Notes: - If `file_name` is None or empty, no action is taken. \"\"\" logger . debug ( f \"ensure_parent_dirs() called for: { file_name =} \" ) if not file_name : return file_path = Path ( file_name ) file_path . parent . mkdir ( parents = True , exist_ok = True ) get_project_root_dir ( file , match_parts ) Traverse up the directory tree from a file path to locate the root of a known structure. Parameters: file ( str | Path ) \u2013 The starting file path, typically __file__ . If None or empty, raises ValueError. match_parts ( list [ str ] ) \u2013 Folder names expected in the path, ordered from root to leaf. If None or empty, raises ValueError. Returns: Path ( Path ) \u2013 Path to the top of the matched folder chain. Raises: ValueError \u2013 If no matching structure is found in the parent hierarchy, or if arguments are None or empty. Examples: Input : \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\", [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"] Output: Path(\"/Users/tony/dev/feedback_portal\") Input : None, [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"] Output: ValueError Input : \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\", [] Output: ValueError Notes If file or match_parts is None or empty, raises ValueError. Source code in arb\\utils\\file_io.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def get_project_root_dir ( file : str | Path , match_parts : list [ str ]) -> Path : \"\"\" Traverse up the directory tree from a file path to locate the root of a known structure. Args: file (str | Path): The starting file path, typically `__file__`. If None or empty, raises ValueError. match_parts (list[str]): Folder names expected in the path, ordered from root to leaf. If None or empty, raises ValueError. Returns: Path: Path to the top of the matched folder chain. Raises: ValueError: If no matching structure is found in the parent hierarchy, or if arguments are None or empty. Examples: Input : \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\", [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"] Output: Path(\"/Users/tony/dev/feedback_portal\") Input : None, [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"] Output: ValueError Input : \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\", [] Output: ValueError Notes: - If `file` or `match_parts` is None or empty, raises ValueError. \"\"\" path = Path ( file ) . resolve () match_len = len ( match_parts ) current = path while current != current . parent : if list ( current . parts [ - match_len :]) == match_parts : return Path ( * current . parts [: len ( current . parts ) - match_len + 1 ]) current = current . parent raise ValueError ( f \"Could not locate project root using match_parts= { match_parts } from path= { path } \" ) get_secure_timestamped_file_name ( directory , file_name ) Generate a sanitized file name in the given directory, appending a UTC timestamp. Parameters: directory ( str | Path ) \u2013 Target directory where the file will be saved. If None or empty, uses the home directory. file_name ( str ) \u2013 Proposed name for the file, possibly unsafe. If None or empty, raises ValueError. Returns: Path ( Path ) \u2013 The full secure, timestamped file path. Examples: Input : \"/tmp\", \"user report.xlsx\" Output: Path(\"/home/user/tmp/user_report_ts_2025-05-05T12-30-00Z.xlsx\") Input : None, \"user report.xlsx\" Output: Path(\"/home/user/user_report_ts_2025-05-05T12-30-00Z.xlsx\") Input : \"/tmp\", None Output: ValueError Input : \"/tmp\", \"\" Output: ValueError Raises: ValueError \u2013 If file_name is None or empty. Notes Uses werkzeug.utils.secure_filename to sanitize input filenames. If directory is None or empty, uses the home directory. Source code in arb\\utils\\file_io.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def get_secure_timestamped_file_name ( directory : str | Path , file_name : str ) -> Path : \"\"\" Generate a sanitized file name in the given directory, appending a UTC timestamp. Args: directory (str | Path): Target directory where the file will be saved. If None or empty, uses the home directory. file_name (str): Proposed name for the file, possibly unsafe. If None or empty, raises ValueError. Returns: Path: The full secure, timestamped file path. Examples: Input : \"/tmp\", \"user report.xlsx\" Output: Path(\"/home/user/tmp/user_report_ts_2025-05-05T12-30-00Z.xlsx\") Input : None, \"user report.xlsx\" Output: Path(\"/home/user/user_report_ts_2025-05-05T12-30-00Z.xlsx\") Input : \"/tmp\", None Output: ValueError Input : \"/tmp\", \"\" Output: ValueError Raises: ValueError: If `file_name` is None or empty. Notes: - Uses `werkzeug.utils.secure_filename` to sanitize input filenames. - If `directory` is None or empty, uses the home directory. \"\"\" if not file_name : raise ValueError ( \"file_name must not be None or empty\" ) file_name_clean = secure_filename ( file_name ) full_path = Path . home () / directory / file_name_clean timestamp = datetime . now ( ZoneInfo ( \"UTC\" )) . strftime ( DATETIME_WITH_SECONDS ) new_name = f \" { full_path . stem } _ts_ { timestamp }{ full_path . suffix } \" return full_path . with_name ( new_name ) read_file_reverse ( path , n = 1000 , encoding = 'utf-8' ) Efficiently read the last n lines of a text file in reverse order, returning the result in normal top-down order (oldest to newest). This function is optimized for large files by avoiding full memory loads. It uses streaming reads from the end of the file, making it suitable for real-time diagnostics, log viewers, or tail-style interfaces. Parameters: path ( str | Path ) \u2013 Path to the log or text file. If None or empty, raises ValueError. n ( int , default: 1000 ) \u2013 Number of lines to read from the end of the file (default is 1000). encoding ( str , default: 'utf-8' ) \u2013 Text encoding used to decode the file (default is \"utf-8\"). Returns: list [ str ] \u2013 list[str]: A list of up to n lines from the end of the file, returned in chronological (not reverse) order. Raises: FileNotFoundError \u2013 If the file does not exist. OSError \u2013 If the file cannot be read due to permission or I/O issues. ValueError \u2013 If path is None or empty. Notes This method uses the file_read_backwards library, which performs disk-efficient reverse reads by buffering from the end. Handles variable-length lines and multi-byte encodings gracefully. Does not assume file fits in memory \u2014 ideal for large logs. Examples: Input : \"/var/log/syslog\", n=100 Output: Returns the last 100 lines in chronological order Input : None, n=100 Output: ValueError Input : \"\", n=100 Output: ValueError Source code in arb\\utils\\file_io.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def read_file_reverse ( path : str | Path , n : int = 1000 , encoding : str = \"utf-8\" ) -> list [ str ]: \"\"\" Efficiently read the last `n` lines of a text file in reverse order, returning the result in normal top-down order (oldest to newest). This function is optimized for large files by avoiding full memory loads. It uses streaming reads from the end of the file, making it suitable for real-time diagnostics, log viewers, or tail-style interfaces. Args: path (str | Path): Path to the log or text file. If None or empty, raises ValueError. n (int): Number of lines to read from the end of the file (default is 1000). encoding (str): Text encoding used to decode the file (default is \"utf-8\"). Returns: list[str]: A list of up to `n` lines from the end of the file, returned in chronological (not reverse) order. Raises: FileNotFoundError: If the file does not exist. OSError: If the file cannot be read due to permission or I/O issues. ValueError: If `path` is None or empty. Notes: - This method uses the `file_read_backwards` library, which performs disk-efficient reverse reads by buffering from the end. - Handles variable-length lines and multi-byte encodings gracefully. - Does not assume file fits in memory \u2014 ideal for large logs. Examples: Input : \"/var/log/syslog\", n=100 Output: Returns the last 100 lines in chronological order Input : None, n=100 Output: ValueError Input : \"\", n=100 Output: ValueError \"\"\" from file_read_backwards import FileReadBackwards file_path = Path ( path ) lines : list [ str ] = [] with FileReadBackwards ( file_path , encoding = encoding ) as f : for i , line in enumerate ( f ): lines . append ( line ) if i + 1 >= n : break return list ( reversed ( lines )) resolve_project_root ( file_path , candidate_structures = None ) Attempt to locate the project root directory using known folder sequences. Parameters: file_path ( str | Path ) \u2013 The file path to begin traversal from (typically __file__ ). If None or empty, raises ValueError. candidate_structures ( list [ list [ str ]] | None , default: None ) \u2013 List of folder name sequences to match. If None, uses defaults. Returns: Path ( Path ) \u2013 Path to the root of the matched folder chain. Raises: ProjectRootNotFoundError \u2013 If no matching sequence is found. ValueError \u2013 If file_path is None or empty. Examples: Input : file Output: Path to the resolved project root, such as Path(\"/Users/tony/dev/feedback_portal\") Input : None Output: ValueError Input : \"\" Output: ValueError Notes If file_path is None or empty, raises ValueError. If candidate_structures is None, uses default structures. Source code in arb\\utils\\file_io.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def resolve_project_root ( file_path : str | Path , candidate_structures : list [ list [ str ]] | None = None ) -> Path : \"\"\" Attempt to locate the project root directory using known folder sequences. Args: file_path (str | Path): The file path to begin traversal from (typically `__file__`). If None or empty, raises ValueError. candidate_structures (list[list[str]] | None): List of folder name sequences to match. If None, uses defaults. Returns: Path: Path to the root of the matched folder chain. Raises: ProjectRootNotFoundError: If no matching sequence is found. ValueError: If `file_path` is None or empty. Examples: Input : __file__ Output: Path to the resolved project root, such as Path(\"/Users/tony/dev/feedback_portal\") Input : None Output: ValueError Input : \"\" Output: ValueError Notes: - If `file_path` is None or empty, raises ValueError. - If `candidate_structures` is None, uses default structures. \"\"\" if candidate_structures is None : candidate_structures = [ [ 'feedback_portal' , 'source' , 'production' , 'arb' , 'utils' , 'excel' ], [ 'feedback_portal' , 'source' , 'production' , 'arb' , 'portal' ], ] errors = [] for structure in candidate_structures : try : root = get_project_root_dir ( file_path , structure ) logger . debug ( f \" { root =} , based on structure { structure =} \" ) return root except ValueError as e : errors . append ( f \" { structure } : { e } \" ) raise ProjectRootNotFoundError ( \"Unable to determine project root. Tried the following structures: \\n \" + \" \\n \" . join ( errors ) )","title":"arb.utils.file_io"},{"location":"reference/arb/utils/file_io/#arbutilsfile_io","text":"File and path utility functions for the ARB Feedback Portal. This module provides helpers for directory creation, secure file name generation, project root resolution, and efficient file reading. These utilities are designed to support robust file handling and diagnostics across ARB portal workflows. Features: - Ensures parent and target directories exist - Generates secure, timestamped file names using UTC - Dynamically resolves the project root based on directory structure - Efficiently reads the last N lines of large files Notes: - Uses werkzeug.utils.secure_filename to sanitize input filenames - Timestamps are formatted in UTC using the DATETIME_WITH_SECONDS pattern Intended use: - Shared helpers for ARB portal and related utilities - Promotes DRY principles and robust file handling Version: 1.0.0","title":"arb.utils.file_io"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.ProjectRootNotFoundError","text":"Bases: ValueError Raised when no matching project root can be determined from the provided file path and candidate folder sequences. Source code in arb\\utils\\file_io.py 138 139 140 141 142 143 class ProjectRootNotFoundError ( ValueError ): \"\"\" Raised when no matching project root can be determined from the provided file path and candidate folder sequences. \"\"\" pass","title":"ProjectRootNotFoundError"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.ensure_dir_exists","text":"Ensure that the specified directory exists, creating it if necessary. Parameters: dir_path ( str | Path ) \u2013 Path to the directory. If None or empty, no action is taken. Raises: ValueError \u2013 If the path exists but is not a directory. Returns: None \u2013 None Examples: Input : \"logs/output\" Output: Creates the directory and parents if needed Input : None Output: No action Input : \"\" Output: No action Notes If dir_path is None or empty, no action is taken. Source code in arb\\utils\\file_io.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def ensure_dir_exists ( dir_path : str | Path ) -> None : \"\"\" Ensure that the specified directory exists, creating it if necessary. Args: dir_path (str | Path): Path to the directory. If None or empty, no action is taken. Raises: ValueError: If the path exists but is not a directory. Returns: None Examples: Input : \"logs/output\" Output: Creates the directory and parents if needed Input : None Output: No action Input : \"\" Output: No action Notes: - If `dir_path` is None or empty, no action is taken. \"\"\" logger . debug ( f \"ensure_dir_exists() called for: { dir_path =} \" ) if not dir_path : return dir_path = Path ( dir_path ) if dir_path . exists () and not dir_path . is_dir (): raise ValueError ( f \"The path { dir_path } exists and is not a directory.\" ) dir_path . mkdir ( parents = True , exist_ok = True )","title":"ensure_dir_exists"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.ensure_parent_dirs","text":"Ensure that the parent directories for a given file path exist. Parameters: file_name ( str | Path ) \u2013 The full path to a file. Parent folders will be created if needed. If None or empty, no action is taken. Returns: None \u2013 None Examples: Input : \"/tmp/some/deep/file.txt\" Output: Ensures intermediate directories exist Input : \"local_file.txt\" Output: No error if directory already exists or is current Input : None Output: No action Input : \"\" Output: No action Notes If file_name is None or empty, no action is taken. Source code in arb\\utils\\file_io.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def ensure_parent_dirs ( file_name : str | Path ) -> None : \"\"\" Ensure that the parent directories for a given file path exist. Args: file_name (str | Path): The full path to a file. Parent folders will be created if needed. If None or empty, no action is taken. Returns: None Examples: Input : \"/tmp/some/deep/file.txt\" Output: Ensures intermediate directories exist Input : \"local_file.txt\" Output: No error if directory already exists or is current Input : None Output: No action Input : \"\" Output: No action Notes: - If `file_name` is None or empty, no action is taken. \"\"\" logger . debug ( f \"ensure_parent_dirs() called for: { file_name =} \" ) if not file_name : return file_path = Path ( file_name ) file_path . parent . mkdir ( parents = True , exist_ok = True )","title":"ensure_parent_dirs"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.get_project_root_dir","text":"Traverse up the directory tree from a file path to locate the root of a known structure. Parameters: file ( str | Path ) \u2013 The starting file path, typically __file__ . If None or empty, raises ValueError. match_parts ( list [ str ] ) \u2013 Folder names expected in the path, ordered from root to leaf. If None or empty, raises ValueError. Returns: Path ( Path ) \u2013 Path to the top of the matched folder chain. Raises: ValueError \u2013 If no matching structure is found in the parent hierarchy, or if arguments are None or empty. Examples: Input : \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\", [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"] Output: Path(\"/Users/tony/dev/feedback_portal\") Input : None, [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"] Output: ValueError Input : \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\", [] Output: ValueError Notes If file or match_parts is None or empty, raises ValueError. Source code in arb\\utils\\file_io.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def get_project_root_dir ( file : str | Path , match_parts : list [ str ]) -> Path : \"\"\" Traverse up the directory tree from a file path to locate the root of a known structure. Args: file (str | Path): The starting file path, typically `__file__`. If None or empty, raises ValueError. match_parts (list[str]): Folder names expected in the path, ordered from root to leaf. If None or empty, raises ValueError. Returns: Path: Path to the top of the matched folder chain. Raises: ValueError: If no matching structure is found in the parent hierarchy, or if arguments are None or empty. Examples: Input : \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\", [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"] Output: Path(\"/Users/tony/dev/feedback_portal\") Input : None, [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"] Output: ValueError Input : \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\", [] Output: ValueError Notes: - If `file` or `match_parts` is None or empty, raises ValueError. \"\"\" path = Path ( file ) . resolve () match_len = len ( match_parts ) current = path while current != current . parent : if list ( current . parts [ - match_len :]) == match_parts : return Path ( * current . parts [: len ( current . parts ) - match_len + 1 ]) current = current . parent raise ValueError ( f \"Could not locate project root using match_parts= { match_parts } from path= { path } \" )","title":"get_project_root_dir"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.get_secure_timestamped_file_name","text":"Generate a sanitized file name in the given directory, appending a UTC timestamp. Parameters: directory ( str | Path ) \u2013 Target directory where the file will be saved. If None or empty, uses the home directory. file_name ( str ) \u2013 Proposed name for the file, possibly unsafe. If None or empty, raises ValueError. Returns: Path ( Path ) \u2013 The full secure, timestamped file path. Examples: Input : \"/tmp\", \"user report.xlsx\" Output: Path(\"/home/user/tmp/user_report_ts_2025-05-05T12-30-00Z.xlsx\") Input : None, \"user report.xlsx\" Output: Path(\"/home/user/user_report_ts_2025-05-05T12-30-00Z.xlsx\") Input : \"/tmp\", None Output: ValueError Input : \"/tmp\", \"\" Output: ValueError Raises: ValueError \u2013 If file_name is None or empty. Notes Uses werkzeug.utils.secure_filename to sanitize input filenames. If directory is None or empty, uses the home directory. Source code in arb\\utils\\file_io.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def get_secure_timestamped_file_name ( directory : str | Path , file_name : str ) -> Path : \"\"\" Generate a sanitized file name in the given directory, appending a UTC timestamp. Args: directory (str | Path): Target directory where the file will be saved. If None or empty, uses the home directory. file_name (str): Proposed name for the file, possibly unsafe. If None or empty, raises ValueError. Returns: Path: The full secure, timestamped file path. Examples: Input : \"/tmp\", \"user report.xlsx\" Output: Path(\"/home/user/tmp/user_report_ts_2025-05-05T12-30-00Z.xlsx\") Input : None, \"user report.xlsx\" Output: Path(\"/home/user/user_report_ts_2025-05-05T12-30-00Z.xlsx\") Input : \"/tmp\", None Output: ValueError Input : \"/tmp\", \"\" Output: ValueError Raises: ValueError: If `file_name` is None or empty. Notes: - Uses `werkzeug.utils.secure_filename` to sanitize input filenames. - If `directory` is None or empty, uses the home directory. \"\"\" if not file_name : raise ValueError ( \"file_name must not be None or empty\" ) file_name_clean = secure_filename ( file_name ) full_path = Path . home () / directory / file_name_clean timestamp = datetime . now ( ZoneInfo ( \"UTC\" )) . strftime ( DATETIME_WITH_SECONDS ) new_name = f \" { full_path . stem } _ts_ { timestamp }{ full_path . suffix } \" return full_path . with_name ( new_name )","title":"get_secure_timestamped_file_name"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.read_file_reverse","text":"Efficiently read the last n lines of a text file in reverse order, returning the result in normal top-down order (oldest to newest). This function is optimized for large files by avoiding full memory loads. It uses streaming reads from the end of the file, making it suitable for real-time diagnostics, log viewers, or tail-style interfaces. Parameters: path ( str | Path ) \u2013 Path to the log or text file. If None or empty, raises ValueError. n ( int , default: 1000 ) \u2013 Number of lines to read from the end of the file (default is 1000). encoding ( str , default: 'utf-8' ) \u2013 Text encoding used to decode the file (default is \"utf-8\"). Returns: list [ str ] \u2013 list[str]: A list of up to n lines from the end of the file, returned in chronological (not reverse) order. Raises: FileNotFoundError \u2013 If the file does not exist. OSError \u2013 If the file cannot be read due to permission or I/O issues. ValueError \u2013 If path is None or empty. Notes This method uses the file_read_backwards library, which performs disk-efficient reverse reads by buffering from the end. Handles variable-length lines and multi-byte encodings gracefully. Does not assume file fits in memory \u2014 ideal for large logs. Examples: Input : \"/var/log/syslog\", n=100 Output: Returns the last 100 lines in chronological order Input : None, n=100 Output: ValueError Input : \"\", n=100 Output: ValueError Source code in arb\\utils\\file_io.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 def read_file_reverse ( path : str | Path , n : int = 1000 , encoding : str = \"utf-8\" ) -> list [ str ]: \"\"\" Efficiently read the last `n` lines of a text file in reverse order, returning the result in normal top-down order (oldest to newest). This function is optimized for large files by avoiding full memory loads. It uses streaming reads from the end of the file, making it suitable for real-time diagnostics, log viewers, or tail-style interfaces. Args: path (str | Path): Path to the log or text file. If None or empty, raises ValueError. n (int): Number of lines to read from the end of the file (default is 1000). encoding (str): Text encoding used to decode the file (default is \"utf-8\"). Returns: list[str]: A list of up to `n` lines from the end of the file, returned in chronological (not reverse) order. Raises: FileNotFoundError: If the file does not exist. OSError: If the file cannot be read due to permission or I/O issues. ValueError: If `path` is None or empty. Notes: - This method uses the `file_read_backwards` library, which performs disk-efficient reverse reads by buffering from the end. - Handles variable-length lines and multi-byte encodings gracefully. - Does not assume file fits in memory \u2014 ideal for large logs. Examples: Input : \"/var/log/syslog\", n=100 Output: Returns the last 100 lines in chronological order Input : None, n=100 Output: ValueError Input : \"\", n=100 Output: ValueError \"\"\" from file_read_backwards import FileReadBackwards file_path = Path ( path ) lines : list [ str ] = [] with FileReadBackwards ( file_path , encoding = encoding ) as f : for i , line in enumerate ( f ): lines . append ( line ) if i + 1 >= n : break return list ( reversed ( lines ))","title":"read_file_reverse"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.resolve_project_root","text":"Attempt to locate the project root directory using known folder sequences. Parameters: file_path ( str | Path ) \u2013 The file path to begin traversal from (typically __file__ ). If None or empty, raises ValueError. candidate_structures ( list [ list [ str ]] | None , default: None ) \u2013 List of folder name sequences to match. If None, uses defaults. Returns: Path ( Path ) \u2013 Path to the root of the matched folder chain. Raises: ProjectRootNotFoundError \u2013 If no matching sequence is found. ValueError \u2013 If file_path is None or empty. Examples: Input : file Output: Path to the resolved project root, such as Path(\"/Users/tony/dev/feedback_portal\") Input : None Output: ValueError Input : \"\" Output: ValueError Notes If file_path is None or empty, raises ValueError. If candidate_structures is None, uses default structures. Source code in arb\\utils\\file_io.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def resolve_project_root ( file_path : str | Path , candidate_structures : list [ list [ str ]] | None = None ) -> Path : \"\"\" Attempt to locate the project root directory using known folder sequences. Args: file_path (str | Path): The file path to begin traversal from (typically `__file__`). If None or empty, raises ValueError. candidate_structures (list[list[str]] | None): List of folder name sequences to match. If None, uses defaults. Returns: Path: Path to the root of the matched folder chain. Raises: ProjectRootNotFoundError: If no matching sequence is found. ValueError: If `file_path` is None or empty. Examples: Input : __file__ Output: Path to the resolved project root, such as Path(\"/Users/tony/dev/feedback_portal\") Input : None Output: ValueError Input : \"\" Output: ValueError Notes: - If `file_path` is None or empty, raises ValueError. - If `candidate_structures` is None, uses default structures. \"\"\" if candidate_structures is None : candidate_structures = [ [ 'feedback_portal' , 'source' , 'production' , 'arb' , 'utils' , 'excel' ], [ 'feedback_portal' , 'source' , 'production' , 'arb' , 'portal' ], ] errors = [] for structure in candidate_structures : try : root = get_project_root_dir ( file_path , structure ) logger . debug ( f \" { root =} , based on structure { structure =} \" ) return root except ValueError as e : errors . append ( f \" { structure } : { e } \" ) raise ProjectRootNotFoundError ( \"Unable to determine project root. Tried the following structures: \\n \" + \" \\n \" . join ( errors ) )","title":"resolve_project_root"},{"location":"reference/arb/utils/io_wrappers/","text":"arb.utils.io_wrappers File I/O utility wrappers for the ARB Feedback Portal. This module provides reusable helpers for safely reading and writing JSON and text files, as well as copying files with directory creation. These functions isolate file system side effects and should be used anywhere file persistence is required in ARB portal utilities or scripts. Features: - Simplifies error handling and directory setup - Makes code easier to test and mock - Promotes DRY principles for common file tasks Typical usage from arb.utils.io_wrappers import save_json_safely, read_json_file save_json_safely(data, Path(\"/tmp/output.json\")) contents = read_json_file(Path(\"/tmp/output.json\")) Version: 1.0.0 copy_file_safe ( src , dst ) Copy a file from src to dst , creating the destination directory if needed. Parameters: src ( Path ) \u2013 Source file path. If None, raises ValueError. dst ( Path ) \u2013 Destination file path. If None, raises ValueError. Raises: FileNotFoundError \u2013 If the source file does not exist. OSError \u2013 If the copy fails. ValueError \u2013 If src or dst is None. Examples: Input : Path(\"/tmp/a.txt\"), Path(\"/tmp/b.txt\") Output: Copies a.txt to b.txt Notes If src or dst is None, raises ValueError. Source code in arb\\utils\\io_wrappers.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def copy_file_safe ( src : Path , dst : Path ) -> None : \"\"\" Copy a file from `src` to `dst`, creating the destination directory if needed. Args: src (Path): Source file path. If None, raises ValueError. dst (Path): Destination file path. If None, raises ValueError. Raises: FileNotFoundError: If the source file does not exist. OSError: If the copy fails. ValueError: If `src` or `dst` is None. Examples: Input : Path(\"/tmp/a.txt\"), Path(\"/tmp/b.txt\") Output: Copies a.txt to b.txt Notes: - If `src` or `dst` is None, raises ValueError. \"\"\" if src is None or dst is None : raise ValueError ( \"src and dst must not be None.\" ) dst . parent . mkdir ( parents = True , exist_ok = True ) copy2 ( src , dst ) read_json_file ( path , encoding = 'utf-8-sig' , json_options = None ) Load and return the contents of a JSON file. Parameters: path ( Path ) \u2013 Path to the JSON file. If None, raises ValueError. encoding ( str , default: 'utf-8-sig' ) \u2013 File encoding (default: \"utf-8-sig\" to handle BOM). json_options ( dict | None , default: None ) \u2013 Options passed to json.load (e.g., object_hook). Returns: dict ( dict ) \u2013 Parsed JSON contents. Raises: FileNotFoundError \u2013 If the file does not exist. JSONDecodeError \u2013 If the file is not valid JSON. ValueError \u2013 If path is None. Examples: Input : Path(\"/tmp/test.json\") Output: Returns parsed JSON dict Notes If path is None, raises ValueError. Source code in arb\\utils\\io_wrappers.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def read_json_file ( path : Path , encoding : str = \"utf-8-sig\" , json_options : dict | None = None ) -> dict : \"\"\" Load and return the contents of a JSON file. Args: path (Path): Path to the JSON file. If None, raises ValueError. encoding (str): File encoding (default: \"utf-8-sig\" to handle BOM). json_options (dict | None): Options passed to `json.load` (e.g., object_hook). Returns: dict: Parsed JSON contents. Raises: FileNotFoundError: If the file does not exist. json.JSONDecodeError: If the file is not valid JSON. ValueError: If `path` is None. Examples: Input : Path(\"/tmp/test.json\") Output: Returns parsed JSON dict Notes: - If `path` is None, raises ValueError. \"\"\" if path is None : raise ValueError ( \"Path must not be None.\" ) json_options = json_options or {} with path . open ( \"r\" , encoding = encoding ) as f : return json . load ( f , ** json_options ) save_json_safely ( data , path , encoding = 'utf-8' , json_options = None ) Write a dictionary as JSON to the specified path with optional encoding and json options. Parameters: data ( object ) \u2013 Data to serialize. If None, writes 'null' to the file. path ( Path ) \u2013 Destination file path. If None, raises ValueError. encoding ( str , default: 'utf-8' ) \u2013 File encoding (default: \"utf-8\"). json_options ( dict | None , default: None ) \u2013 Options passed to json.dump (e.g., indent, default). Raises: OSError \u2013 If the file or directory cannot be written. ValueError \u2013 If path is None. Examples: Input : {\"a\": 1}, Path(\"/tmp/test.json\") Output: Writes JSON to /tmp/test.json Notes If data is None, writes 'null' to the file. If path is None, raises ValueError. Source code in arb\\utils\\io_wrappers.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def save_json_safely ( data : object , path : Path , encoding : str = \"utf-8\" , json_options : dict | None = None ) -> None : \"\"\" Write a dictionary as JSON to the specified path with optional encoding and json options. Args: data (object): Data to serialize. If None, writes 'null' to the file. path (Path): Destination file path. If None, raises ValueError. encoding (str): File encoding (default: \"utf-8\"). json_options (dict | None): Options passed to `json.dump` (e.g., indent, default). Raises: OSError: If the file or directory cannot be written. ValueError: If `path` is None. Examples: Input : {\"a\": 1}, Path(\"/tmp/test.json\") Output: Writes JSON to /tmp/test.json Notes: - If `data` is None, writes 'null' to the file. - If `path` is None, raises ValueError. \"\"\" if path is None : raise ValueError ( \"Path must not be None.\" ) json_options = json_options or { \"indent\" : 2 } path . parent . mkdir ( parents = True , exist_ok = True ) with path . open ( \"w\" , encoding = encoding ) as f : json . dump ( data , f , ** json_options ) write_text_file ( text , path , encoding = 'utf-8' ) Write plain text to a file, overwriting if it exists. Parameters: text ( str ) \u2013 Text content to write. If None, writes an empty file. path ( Path ) \u2013 Destination file path. If None, raises ValueError. encoding ( str , default: 'utf-8' ) \u2013 File encoding (default: \"utf-8\"). Raises: OSError \u2013 If the file cannot be written. ValueError \u2013 If path is None. Examples: Input : \"Hello, world!\", Path(\"/tmp/hello.txt\") Output: Writes text to /tmp/hello.txt Notes If text is None, writes an empty file. If path is None, raises ValueError. Source code in arb\\utils\\io_wrappers.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def write_text_file ( text : str , path : Path , encoding : str = \"utf-8\" ) -> None : \"\"\" Write plain text to a file, overwriting if it exists. Args: text (str): Text content to write. If None, writes an empty file. path (Path): Destination file path. If None, raises ValueError. encoding (str): File encoding (default: \"utf-8\"). Raises: OSError: If the file cannot be written. ValueError: If `path` is None. Examples: Input : \"Hello, world!\", Path(\"/tmp/hello.txt\") Output: Writes text to /tmp/hello.txt Notes: - If `text` is None, writes an empty file. - If `path` is None, raises ValueError. \"\"\" if path is None : raise ValueError ( \"Path must not be None.\" ) if text is None : text = \"\" path . parent . mkdir ( parents = True , exist_ok = True ) path . write_text ( text , encoding = encoding )","title":"arb.utils.io_wrappers"},{"location":"reference/arb/utils/io_wrappers/#arbutilsio_wrappers","text":"File I/O utility wrappers for the ARB Feedback Portal. This module provides reusable helpers for safely reading and writing JSON and text files, as well as copying files with directory creation. These functions isolate file system side effects and should be used anywhere file persistence is required in ARB portal utilities or scripts. Features: - Simplifies error handling and directory setup - Makes code easier to test and mock - Promotes DRY principles for common file tasks Typical usage from arb.utils.io_wrappers import save_json_safely, read_json_file save_json_safely(data, Path(\"/tmp/output.json\")) contents = read_json_file(Path(\"/tmp/output.json\")) Version: 1.0.0","title":"arb.utils.io_wrappers"},{"location":"reference/arb/utils/io_wrappers/#arb.utils.io_wrappers.copy_file_safe","text":"Copy a file from src to dst , creating the destination directory if needed. Parameters: src ( Path ) \u2013 Source file path. If None, raises ValueError. dst ( Path ) \u2013 Destination file path. If None, raises ValueError. Raises: FileNotFoundError \u2013 If the source file does not exist. OSError \u2013 If the copy fails. ValueError \u2013 If src or dst is None. Examples: Input : Path(\"/tmp/a.txt\"), Path(\"/tmp/b.txt\") Output: Copies a.txt to b.txt Notes If src or dst is None, raises ValueError. Source code in arb\\utils\\io_wrappers.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def copy_file_safe ( src : Path , dst : Path ) -> None : \"\"\" Copy a file from `src` to `dst`, creating the destination directory if needed. Args: src (Path): Source file path. If None, raises ValueError. dst (Path): Destination file path. If None, raises ValueError. Raises: FileNotFoundError: If the source file does not exist. OSError: If the copy fails. ValueError: If `src` or `dst` is None. Examples: Input : Path(\"/tmp/a.txt\"), Path(\"/tmp/b.txt\") Output: Copies a.txt to b.txt Notes: - If `src` or `dst` is None, raises ValueError. \"\"\" if src is None or dst is None : raise ValueError ( \"src and dst must not be None.\" ) dst . parent . mkdir ( parents = True , exist_ok = True ) copy2 ( src , dst )","title":"copy_file_safe"},{"location":"reference/arb/utils/io_wrappers/#arb.utils.io_wrappers.read_json_file","text":"Load and return the contents of a JSON file. Parameters: path ( Path ) \u2013 Path to the JSON file. If None, raises ValueError. encoding ( str , default: 'utf-8-sig' ) \u2013 File encoding (default: \"utf-8-sig\" to handle BOM). json_options ( dict | None , default: None ) \u2013 Options passed to json.load (e.g., object_hook). Returns: dict ( dict ) \u2013 Parsed JSON contents. Raises: FileNotFoundError \u2013 If the file does not exist. JSONDecodeError \u2013 If the file is not valid JSON. ValueError \u2013 If path is None. Examples: Input : Path(\"/tmp/test.json\") Output: Returns parsed JSON dict Notes If path is None, raises ValueError. Source code in arb\\utils\\io_wrappers.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def read_json_file ( path : Path , encoding : str = \"utf-8-sig\" , json_options : dict | None = None ) -> dict : \"\"\" Load and return the contents of a JSON file. Args: path (Path): Path to the JSON file. If None, raises ValueError. encoding (str): File encoding (default: \"utf-8-sig\" to handle BOM). json_options (dict | None): Options passed to `json.load` (e.g., object_hook). Returns: dict: Parsed JSON contents. Raises: FileNotFoundError: If the file does not exist. json.JSONDecodeError: If the file is not valid JSON. ValueError: If `path` is None. Examples: Input : Path(\"/tmp/test.json\") Output: Returns parsed JSON dict Notes: - If `path` is None, raises ValueError. \"\"\" if path is None : raise ValueError ( \"Path must not be None.\" ) json_options = json_options or {} with path . open ( \"r\" , encoding = encoding ) as f : return json . load ( f , ** json_options )","title":"read_json_file"},{"location":"reference/arb/utils/io_wrappers/#arb.utils.io_wrappers.save_json_safely","text":"Write a dictionary as JSON to the specified path with optional encoding and json options. Parameters: data ( object ) \u2013 Data to serialize. If None, writes 'null' to the file. path ( Path ) \u2013 Destination file path. If None, raises ValueError. encoding ( str , default: 'utf-8' ) \u2013 File encoding (default: \"utf-8\"). json_options ( dict | None , default: None ) \u2013 Options passed to json.dump (e.g., indent, default). Raises: OSError \u2013 If the file or directory cannot be written. ValueError \u2013 If path is None. Examples: Input : {\"a\": 1}, Path(\"/tmp/test.json\") Output: Writes JSON to /tmp/test.json Notes If data is None, writes 'null' to the file. If path is None, raises ValueError. Source code in arb\\utils\\io_wrappers.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def save_json_safely ( data : object , path : Path , encoding : str = \"utf-8\" , json_options : dict | None = None ) -> None : \"\"\" Write a dictionary as JSON to the specified path with optional encoding and json options. Args: data (object): Data to serialize. If None, writes 'null' to the file. path (Path): Destination file path. If None, raises ValueError. encoding (str): File encoding (default: \"utf-8\"). json_options (dict | None): Options passed to `json.dump` (e.g., indent, default). Raises: OSError: If the file or directory cannot be written. ValueError: If `path` is None. Examples: Input : {\"a\": 1}, Path(\"/tmp/test.json\") Output: Writes JSON to /tmp/test.json Notes: - If `data` is None, writes 'null' to the file. - If `path` is None, raises ValueError. \"\"\" if path is None : raise ValueError ( \"Path must not be None.\" ) json_options = json_options or { \"indent\" : 2 } path . parent . mkdir ( parents = True , exist_ok = True ) with path . open ( \"w\" , encoding = encoding ) as f : json . dump ( data , f , ** json_options )","title":"save_json_safely"},{"location":"reference/arb/utils/io_wrappers/#arb.utils.io_wrappers.write_text_file","text":"Write plain text to a file, overwriting if it exists. Parameters: text ( str ) \u2013 Text content to write. If None, writes an empty file. path ( Path ) \u2013 Destination file path. If None, raises ValueError. encoding ( str , default: 'utf-8' ) \u2013 File encoding (default: \"utf-8\"). Raises: OSError \u2013 If the file cannot be written. ValueError \u2013 If path is None. Examples: Input : \"Hello, world!\", Path(\"/tmp/hello.txt\") Output: Writes text to /tmp/hello.txt Notes If text is None, writes an empty file. If path is None, raises ValueError. Source code in arb\\utils\\io_wrappers.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def write_text_file ( text : str , path : Path , encoding : str = \"utf-8\" ) -> None : \"\"\" Write plain text to a file, overwriting if it exists. Args: text (str): Text content to write. If None, writes an empty file. path (Path): Destination file path. If None, raises ValueError. encoding (str): File encoding (default: \"utf-8\"). Raises: OSError: If the file cannot be written. ValueError: If `path` is None. Examples: Input : \"Hello, world!\", Path(\"/tmp/hello.txt\") Output: Writes text to /tmp/hello.txt Notes: - If `text` is None, writes an empty file. - If `path` is None, raises ValueError. \"\"\" if path is None : raise ValueError ( \"Path must not be None.\" ) if text is None : text = \"\" path . parent . mkdir ( parents = True , exist_ok = True ) path . write_text ( text , encoding = encoding )","title":"write_text_file"},{"location":"reference/arb/utils/json/","text":"arb.utils.json JSON utilities for the ARB Feedback Portal. This module centralizes JSON serialization, deserialization, metadata handling, and diagnostics for ARB portal utilities. It ensures consistent handling of ISO 8601 datetimes, decimals, and custom types, and provides helpers for file-based workflows, WTForms integration, and contract-compliant value normalization. Features: - Custom serialization/deserialization for datetime, decimal, and class/type objects - Metadata support for enhanced JSON files (with data and metadata fields) - File-based diagnostics, JSON comparison, and safe file I/O - WTForms integration for extracting and casting form data - Value normalization and diffing for ingestion and audit workflows Contract notes: - Emphasizes ISO 8601 datetime formats and Pacific Time handling where required - Designed for robust, structured JSON handling across ARB portal utilities - Logging is used for diagnostics, warnings, and error reporting Extensible for new types and workflows as the portal evolves. add_metadata_to_json ( file_name_in , file_name_out = None ) Add metadata to an existing JSON file or overwrite it in-place. Parameters: file_name_in ( str | Path ) \u2013 Input JSON file path. If None or empty, raises ValueError. file_name_out ( str | Path | None , default: None ) \u2013 Output file path. If None, overwrites input. If empty, raises ValueError. Returns: None \u2013 None Raises: ValueError \u2013 If file_name_in is None or empty, or if file_name_out is empty. FileNotFoundError \u2013 If the input file does not exist. OSError \u2013 If the file cannot be written. Examples: Input : \"schema.json\" Output: Adds metadata and writes back to same file Input : \"input.json\", \"output.json\" Output: Adds metadata and writes to output.json Notes If file_name_out is None, input file is overwritten. If file_name_in or file_name_out is empty, raises ValueError. Source code in arb\\utils\\json.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def add_metadata_to_json ( file_name_in : str | pathlib . Path , file_name_out : str | pathlib . Path | None = None ) -> None : \"\"\" Add metadata to an existing JSON file or overwrite it in-place. Args: file_name_in (str | Path): Input JSON file path. If None or empty, raises ValueError. file_name_out (str | Path | None): Output file path. If None, overwrites input. If empty, raises ValueError. Returns: None Raises: ValueError: If file_name_in is None or empty, or if file_name_out is empty. FileNotFoundError: If the input file does not exist. OSError: If the file cannot be written. Examples: Input : \"schema.json\" Output: Adds metadata and writes back to same file Input : \"input.json\", \"output.json\" Output: Adds metadata and writes to output.json Notes: - If file_name_out is None, input file is overwritten. - If file_name_in or file_name_out is empty, raises ValueError. \"\"\" logger . debug ( f \"add_metadata_to_json() called with { file_name_in =} , { file_name_out =} \" ) file_name_in = pathlib . Path ( file_name_in ) if file_name_out is not None : file_name_out = pathlib . Path ( file_name_out ) else : file_name_out = file_name_in data = json_load ( file_name_in ) json_save_with_meta ( file_name_out , data = data ) cast_model_value ( value , value_type , convert_time_to_ca = False ) Cast a stringified JSON value into a Python object of the expected type. Parameters: value ( str ) \u2013 Input value to cast. If None, raises ValueError. value_type ( type ) \u2013 Python type to cast to (str, int, float, bool, datetime, decimal). If None, raises ValueError. convert_time_to_ca ( bool , default: False ) \u2013 If True, convert UTC to California naive datetime. Returns: Any ( Any ) \u2013 Value converted to the target Python type. Raises: ValueError \u2013 If the value cannot be cast to the given type, type is unsupported, or value_type is None. Examples: Input : \"2025-01-01T12:00:00Z\", datetime.datetime Output: datetime.datetime(2025, 1, 1, 12, 0, 0, tzinfo=ZoneInfo(\"UTC\")) Input : \"123.45\", decimal.Decimal Output: decimal.Decimal(\"123.45\") Input : \"true\", bool Output: True Notes If value_type is None, raises ValueError. If value is None, raises ValueError. If type is unsupported, raises ValueError. Source code in arb\\utils\\json.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 def cast_model_value ( value : str , value_type : type , convert_time_to_ca : bool = False ) -> Any : \"\"\" Cast a stringified JSON value into a Python object of the expected type. Args: value (str): Input value to cast. If None, raises ValueError. value_type (type): Python type to cast to (str, int, float, bool, datetime, decimal). If None, raises ValueError. convert_time_to_ca (bool): If True, convert UTC to California naive datetime. Returns: Any: Value converted to the target Python type. Raises: ValueError: If the value cannot be cast to the given type, type is unsupported, or value_type is None. Examples: Input : \"2025-01-01T12:00:00Z\", datetime.datetime Output: datetime.datetime(2025, 1, 1, 12, 0, 0, tzinfo=ZoneInfo(\"UTC\")) Input : \"123.45\", decimal.Decimal Output: decimal.Decimal(\"123.45\") Input : \"true\", bool Output: True Notes: - If value_type is None, raises ValueError. - If value is None, raises ValueError. - If type is unsupported, raises ValueError. \"\"\" # todo - datetime - may need to update try : if value_type == str : # No need to cast a string return value elif value_type in [ bool , int , float ]: return value_type ( value ) elif value_type == datetime . datetime : dt = iso_str_to_utc_datetime ( value ) return utc_datetime_to_ca_naive_datetime ( dt ) if convert_time_to_ca else dt elif value_type == decimal . Decimal : return decimal . Decimal ( value ) else : raise ValueError ( f \"Unsupported type for casting: { value_type } \" ) except Exception as e : raise ValueError ( f \"Failed to cast { value !r} to { value_type } : { e } \" ) compare_json_files ( file_name_1 , file_name_2 ) Compare the contents of two JSON files including metadata and values. Parameters: file_name_1 ( str | Path ) \u2013 Path to the first file. If None or empty, raises ValueError. file_name_2 ( str | Path ) \u2013 Path to the second file. If None or empty, raises ValueError. Returns: None \u2013 None Logs Differences or matches are logged at debug level. Raises: ValueError \u2013 If file_name_1 or file_name_2 is None or empty. FileNotFoundError \u2013 If either file does not exist. JSONDecodeError \u2013 If either file is not valid JSON. Examples: Input : \"old.json\", \"new.json\" Output: Logs any differences or confirms matches Notes If either file is missing or invalid, raises an exception. Only logs differences; does not return a value. Source code in arb\\utils\\json.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 def compare_json_files ( file_name_1 : str | pathlib . Path , file_name_2 : str | pathlib . Path ) -> None : \"\"\" Compare the contents of two JSON files including metadata and values. Args: file_name_1 (str | Path): Path to the first file. If None or empty, raises ValueError. file_name_2 (str | Path): Path to the second file. If None or empty, raises ValueError. Returns: None Logs: Differences or matches are logged at debug level. Raises: ValueError: If file_name_1 or file_name_2 is None or empty. FileNotFoundError: If either file does not exist. json.JSONDecodeError: If either file is not valid JSON. Examples: Input : \"old.json\", \"new.json\" Output: Logs any differences or confirms matches Notes: - If either file is missing or invalid, raises an exception. - Only logs differences; does not return a value. \"\"\" logger . debug ( f \"compare_json_files() comparing { file_name_1 } and { file_name_2 } \" ) data_1 , meta_1 = json_load_with_meta ( file_name_1 ) data_2 , meta_2 = json_load_with_meta ( file_name_2 ) logger . debug ( f \"Comparing metadata\" ) if compare_dicts ( meta_1 , meta_2 , \"metadata_01\" , \"metadata_02\" ) is True : logger . debug ( f \"Metadata are equivalent\" ) else : logger . debug ( f \"Metadata differ\" ) logger . debug ( f \"Comparing data\" ) if compare_dicts ( data_1 , data_2 , \"data_01\" , \"data_02\" ) is True : logger . debug ( f \"Data are equivalent\" ) else : logger . debug ( f \"Data differ\" ) compute_field_differences ( new_data , existing_data ) Generate a field-by-field diff between two dictionaries using keys from new_data . Parameters: new_data ( dict ) \u2013 The incoming or modified dictionary (e.g., staged JSON). If None, treated as empty dict. existing_data ( dict ) \u2013 The reference or baseline dictionary (e.g., DB row). If None, treated as empty dict. Returns: list [ dict ] \u2013 list[dict]: List of field diffs, each with: - 'key': The dictionary key being compared - 'old': The normalized value from existing_data - 'new': The normalized value from new_data - 'changed': True if the values differ after normalization - 'is_same': True if values are unchanged - 'from_upload': Always True, indicating this field came from uploaded JSON - 'requires_confirmation': True if the update adds or overwrites non-trivial data Examples: Input : {\"a\": 1}, {\"a\": 2} Output: [{\"key\": \"a\", \"old\": \"2\", \"new\": \"1\", \"changed\": True, ...}] Input : {\"a\": None}, {\"a\": \"\"} Output: [{\"key\": \"a\", \"old\": \"\", \"new\": \"\", \"changed\": False, ...}] Notes Normalization uses normalize_value() for consistent formatting, especially for empty strings, None, and datetimes. Keys present in existing_data but not in new_data are ignored. A field requires confirmation if it adds or overwrites non-empty data. If either input dict is None, it is treated as empty dict. Source code in arb\\utils\\json.py 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 def compute_field_differences ( new_data : dict , existing_data : dict ) -> list [ dict ]: \"\"\" Generate a field-by-field diff between two dictionaries using keys from `new_data`. Args: new_data (dict): The incoming or modified dictionary (e.g., staged JSON). If None, treated as empty dict. existing_data (dict): The reference or baseline dictionary (e.g., DB row). If None, treated as empty dict. Returns: list[dict]: List of field diffs, each with: - 'key': The dictionary key being compared - 'old': The normalized value from `existing_data` - 'new': The normalized value from `new_data` - 'changed': True if the values differ after normalization - 'is_same': True if values are unchanged - 'from_upload': Always True, indicating this field came from uploaded JSON - 'requires_confirmation': True if the update adds or overwrites non-trivial data Examples: Input : {\"a\": 1}, {\"a\": 2} Output: [{\"key\": \"a\", \"old\": \"2\", \"new\": \"1\", \"changed\": True, ...}] Input : {\"a\": None}, {\"a\": \"\"} Output: [{\"key\": \"a\", \"old\": \"\", \"new\": \"\", \"changed\": False, ...}] Notes: - Normalization uses `normalize_value()` for consistent formatting, especially for empty strings, None, and datetimes. - Keys present in `existing_data` but *not* in `new_data` are ignored. - A field requires confirmation if it adds or overwrites non-empty data. - If either input dict is None, it is treated as empty dict. \"\"\" differences = [] for key in sorted ( new_data . keys ()): new_value = new_data . get ( key ) old_value = existing_data . get ( key ) norm_new = normalize_value ( new_value ) norm_old = normalize_value ( old_value ) is_same = norm_old == norm_new requires_confirmation = ( norm_new not in ( None , \"\" , []) and not is_same ) differences . append ({ \"key\" : key , \"old\" : norm_old , \"new\" : norm_new , \"changed\" : not is_same , \"is_same\" : is_same , \"from_upload\" : True , \"requires_confirmation\" : requires_confirmation , }) logger . debug ( f \"DIFF KEY= { key !r} | DB= { type ( old_value ) . __name__ } : { norm_old !r} \" f \"| NEW= { type ( new_value ) . __name__ } : { norm_new !r} \" f \"| SAME= { is_same } | CONFIRM= { requires_confirmation } \" ) return differences deserialize_dict ( input_dict , type_map , convert_time_to_ca = False ) Deserialize a dictionary of raw values using a type map. Parameters: input_dict ( dict ) \u2013 Dictionary of raw values. If None, raises ValueError. type_map ( dict [ str , type ] ) \u2013 Field-to-type mapping for deserialization. If None or empty, no casting is performed. convert_time_to_ca ( bool , default: False ) \u2013 If True, converts datetime to CA time. Returns: dict ( dict ) \u2013 Fully deserialized dictionary. Raises: TypeError \u2013 If any key is not a string. ValueError \u2013 If value casting fails for a key or if input_dict is None. Examples: Input : {\"dt\": \"2025-01-01T12:00:00Z\"}, {\"dt\": datetime.datetime} Output: {\"dt\": datetime.datetime(2025, 1, 1, 12, 0, 0, tzinfo=ZoneInfo(\"UTC\"))} Notes If input_dict is None, raises ValueError. If type_map is None or empty, no casting is performed. Source code in arb\\utils\\json.py 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 def deserialize_dict ( input_dict : dict , type_map : dict [ str , type ], convert_time_to_ca : bool = False ) -> dict : \"\"\" Deserialize a dictionary of raw values using a type map. Args: input_dict (dict): Dictionary of raw values. If None, raises ValueError. type_map (dict[str, type]): Field-to-type mapping for deserialization. If None or empty, no casting is performed. convert_time_to_ca (bool): If True, converts datetime to CA time. Returns: dict: Fully deserialized dictionary. Raises: TypeError: If any key is not a string. ValueError: If value casting fails for a key or if input_dict is None. Examples: Input : {\"dt\": \"2025-01-01T12:00:00Z\"}, {\"dt\": datetime.datetime} Output: {\"dt\": datetime.datetime(2025, 1, 1, 12, 0, 0, tzinfo=ZoneInfo(\"UTC\"))} Notes: - If input_dict is None, raises ValueError. - If type_map is None or empty, no casting is performed. \"\"\" result = {} for key , value in input_dict . items (): if not isinstance ( key , str ): raise TypeError ( f \"All keys must be strings. Invalid key: { key } ( { type ( key ) } )\" ) if key in type_map and value is not None : result [ key ] = cast_model_value ( value , type_map [ key ], convert_time_to_ca ) else : result [ key ] = value return result extract_id_from_json ( json_data , tab_name = 'Feedback Form' , key_name = 'id_incidence' ) Safely extract a numeric key (like id_incidence) from a specified tab in Excel-parsed JSON. Parameters: json_data ( dict ) \u2013 JSON dictionary as parsed from the Excel upload. If None, returns None. tab_name ( str , default: 'Feedback Form' ) \u2013 Name of the tab in 'tab_contents'. Defaults to \"Feedback Form\". If None or empty, uses default. key_name ( str , default: 'id_incidence' ) \u2013 Key to extract from the tab. Defaults to \"id_incidence\". If None or empty, uses default. Returns: int | None \u2013 int | None: Parsed integer value, or None if not found or invalid. Examples: Input : {\"tab_contents\": {\"Feedback Form\": {\"id_incidence\": \"123\"}}}, tab_name=\"Feedback Form\", key_name=\"id_incidence\" Output: 123 Input : {\"tab_contents\": {\"Feedback Form\": {}}}, tab_name=\"Feedback Form\", key_name=\"id_incidence\" Output: None Notes Unlike full ingestion, this does not validate schema or write to the DB. Handles string digits and trims whitespace safely. Logs warning if structure is invalid. If json_data is None, returns None. Source code in arb\\utils\\json.py 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 def extract_id_from_json ( json_data : dict , tab_name : str = \"Feedback Form\" , key_name : str = \"id_incidence\" ) -> int | None : \"\"\" Safely extract a numeric key (like id_incidence) from a specified tab in Excel-parsed JSON. Args: json_data (dict): JSON dictionary as parsed from the Excel upload. If None, returns None. tab_name (str, optional): Name of the tab in 'tab_contents'. Defaults to \"Feedback Form\". If None or empty, uses default. key_name (str, optional): Key to extract from the tab. Defaults to \"id_incidence\". If None or empty, uses default. Returns: int | None: Parsed integer value, or None if not found or invalid. Examples: Input : {\"tab_contents\": {\"Feedback Form\": {\"id_incidence\": \"123\"}}}, tab_name=\"Feedback Form\", key_name=\"id_incidence\" Output: 123 Input : {\"tab_contents\": {\"Feedback Form\": {}}}, tab_name=\"Feedback Form\", key_name=\"id_incidence\" Output: None Notes: - Unlike full ingestion, this does not validate schema or write to the DB. - Handles string digits and trims whitespace safely. - Logs warning if structure is invalid. - If json_data is None, returns None. \"\"\" try : tab_data = json_data . get ( \"tab_contents\" , {}) . get ( tab_name , {}) val = tab_data . get ( key_name ) if isinstance ( val , int ): return val if isinstance ( val , str ) and val . strip () . isdigit (): return int ( val . strip ()) except Exception as e : logger . warning ( f \"Failed to extract key ' { key_name } ' from tab ' { tab_name } ': { e } \" ) return None extract_tab_payload ( json_data , tab_name = 'Feedback Form' ) Extract the contents of a specific tab from a parsed Excel JSON structure. Parameters: json_data ( dict ) \u2013 Parsed JSON dictionary, typically from json_load_with_meta() . If None, returns empty dict. tab_name ( str , default: 'Feedback Form' ) \u2013 Tab name whose contents to extract. Defaults to \"Feedback Form\". If None or empty, uses default. Returns: dict ( dict ) \u2013 The dictionary of field keys/values for the specified tab, or an empty dict if the tab is missing or malformed. Examples: Input : {\"tab_contents\": {\"Feedback Form\": {\"field1\": \"value1\"}}}, tab_name=\"Feedback Form\" Output: {\"field1\": \"value1\"} Input : {\"tab_contents\": {}}, tab_name=\"Feedback Form\" Output: {} Input : None, tab_name=\"Feedback Form\" Output: {} Notes If json_data is None, returns empty dict. If tab_name is None or empty, uses default. If the tab is missing or malformed, returns empty dict. Source code in arb\\utils\\json.py 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 def extract_tab_payload ( json_data : dict , tab_name : str = \"Feedback Form\" ) -> dict : \"\"\" Extract the contents of a specific tab from a parsed Excel JSON structure. Args: json_data (dict): Parsed JSON dictionary, typically from `json_load_with_meta()`. If None, returns empty dict. tab_name (str): Tab name whose contents to extract. Defaults to \"Feedback Form\". If None or empty, uses default. Returns: dict: The dictionary of field keys/values for the specified tab, or an empty dict if the tab is missing or malformed. Examples: Input : {\"tab_contents\": {\"Feedback Form\": {\"field1\": \"value1\"}}}, tab_name=\"Feedback Form\" Output: {\"field1\": \"value1\"} Input : {\"tab_contents\": {}}, tab_name=\"Feedback Form\" Output: {} Input : None, tab_name=\"Feedback Form\" Output: {} Notes: - If json_data is None, returns empty dict. - If tab_name is None or empty, uses default. - If the tab is missing or malformed, returns empty dict. \"\"\" try : return json_data . get ( \"tab_contents\" , {}) . get ( tab_name , {}) except Exception as e : logger . warning ( f \"extract_tab_payload() failed for tab ' { tab_name } ': { e } \" ) return {} json_deserializer ( obj ) Custom JSON deserializer for class/type representations created by json_serializer . Parameters: obj ( dict [ str , Any ] ) \u2013 Dictionary object from JSON with special tags for known types. If None, returns None. Returns: Any ( Any ) \u2013 Reconstructed Python object (datetime, decimal, or class/type), or original dict if no tags found. Raises: TypeError \u2013 If the type tag is unknown or unsupported. Examples: Input : {\" type \": \"datetime.datetime\", \"value\": \"2025-07-04T12:34:56.789012\"} Output: datetime.datetime(2025, 7, 4, 12, 34, 56, 789012) Input : {\" class \": \"int\", \" module \": \"builtins\"} Output: int Notes If obj is None, returns None. If no recognized tags, returns obj unchanged. Source code in arb\\utils\\json.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def json_deserializer ( obj : dict [ str , Any ]) -> Any : \"\"\" Custom JSON deserializer for class/type representations created by `json_serializer`. Args: obj (dict[str, Any]): Dictionary object from JSON with special tags for known types. If None, returns None. Returns: Any: Reconstructed Python object (datetime, decimal, or class/type), or original dict if no tags found. Raises: TypeError: If the type tag is unknown or unsupported. Examples: Input : {\"__type__\": \"datetime.datetime\", \"value\": \"2025-07-04T12:34:56.789012\"} Output: datetime.datetime(2025, 7, 4, 12, 34, 56, 789012) Input : {\"__class__\": \"int\", \"__module__\": \"builtins\"} Output: int Notes: - If obj is None, returns None. - If no recognized tags, returns obj unchanged. \"\"\" new_obj = obj if \"__class__\" in obj : # logger.debug(f\"{obj['__class__']=} detected in object deserializer.\") if obj [ \"__class__\" ] == \"str\" : new_obj = str elif obj [ \"__class__\" ] == \"int\" : new_obj = int elif obj [ \"__class__\" ] == \"float\" : new_obj = float elif obj [ \"__class__\" ] == \"bool\" : new_obj = bool elif obj [ \"__class__\" ] == \"datetime\" : new_obj = datetime . datetime else : raise TypeError ( f \"Object of type { type ( obj ) . __name__ } is not JSON deserializable\" ) elif \"__type__\" in obj : type_tag = obj [ \"__type__\" ] if type_tag == \"datetime.datetime\" : new_obj = datetime . datetime . fromisoformat ( obj [ \"value\" ]) elif type_tag == \"decimal.Decimal\" : new_obj = decimal . Decimal ( obj [ \"value\" ]) else : logger . debug ( f \"No known conversion type for type { obj [ '__type__' ] } \" ) # logger.debug(f\"deserializer() returning type= {type(new_obj)}, new_obj= {new_obj}\") return new_obj json_load ( file_path , json_options = None ) Load and deserialize data from a JSON file. Parameters: file_path ( str | Path ) \u2013 Path to the JSON file. If None or empty, raises ValueError. json_options ( dict | None , default: None ) \u2013 Optional options passed to json.load . Returns: object ( object ) \u2013 Deserialized Python object (dict, list, etc.). Raises: ValueError \u2013 If file_path is None or empty. FileNotFoundError \u2013 If the file does not exist. JSONDecodeError \u2013 If the file is not valid JSON. Examples: Input : \"data.json\" Output: Deserialized Python object from JSON Notes Uses utf-8-sig encoding to handle BOM if present. If the file is not valid JSON, an exception is raised. If file_path is None or empty, raises ValueError. Source code in arb\\utils\\json.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def json_load ( file_path : str | pathlib . Path , json_options : dict | None = None ) -> object : \"\"\" Load and deserialize data from a JSON file. Args: file_path (str | Path): Path to the JSON file. If None or empty, raises ValueError. json_options (dict | None): Optional options passed to `json.load`. Returns: object: Deserialized Python object (dict, list, etc.). Raises: ValueError: If file_path is None or empty. FileNotFoundError: If the file does not exist. json.JSONDecodeError: If the file is not valid JSON. Examples: Input : \"data.json\" Output: Deserialized Python object from JSON Notes: - Uses utf-8-sig encoding to handle BOM if present. - If the file is not valid JSON, an exception is raised. - If file_path is None or empty, raises ValueError. \"\"\" logger . debug ( f \"json_load() called with { file_path =} , { json_options =} \" ) file_path = pathlib . Path ( file_path ) if json_options is None : json_options = { \"object_hook\" : json_deserializer } return read_json_file ( file_path , encoding = \"utf-8-sig\" , json_options = json_options ) json_load_with_meta ( file_path , json_options = None ) Load a JSON file and return both data and metadata if present. Parameters: file_path ( str | Path ) \u2013 Path to the JSON file. If None or empty, raises ValueError. json_options ( dict | None , default: None ) \u2013 Optional options passed to json.load . Returns: tuple ( tuple [ Any , dict ] ) \u2013 Any: Deserialized data from \" data \" (or the entire file if \" data \" is not present). dict: Deserialized metadata from \" metadata \" (or empty dict if not present). Raises: ValueError \u2013 If file_path is None or empty. FileNotFoundError \u2013 If the file does not exist. JSONDecodeError \u2013 If the file is not valid JSON. Examples: Input : \"example.json\" Output: tuple (data, metadata) extracted from the file Notes If the JSON file is a dictionary with a key data , then the data and metadata (if possible) will be extracted. Otherwise, the data is assumed to be the entire file contents and metadata is an empty dict. If file_path is None or empty, raises ValueError. Source code in arb\\utils\\json.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def json_load_with_meta ( file_path : str | pathlib . Path , json_options : dict | None = None ) -> tuple [ Any , dict ]: \"\"\" Load a JSON file and return both data and metadata if present. Args: file_path (str | Path): Path to the JSON file. If None or empty, raises ValueError. json_options (dict | None): Optional options passed to `json.load`. Returns: tuple: - Any: Deserialized data from \"_data_\" (or the entire file if \"_data_\" is not present). - dict: Deserialized metadata from \"_metadata_\" (or empty dict if not present). Raises: ValueError: If file_path is None or empty. FileNotFoundError: If the file does not exist. json.JSONDecodeError: If the file is not valid JSON. Examples: Input : \"example.json\" Output: tuple (data, metadata) extracted from the file Notes: - If the JSON file is a dictionary with a key _data_, then the _data_ and _metadata_ (if possible) will be extracted. Otherwise, the data is assumed to be the entire file contents and metadata is an empty dict. - If file_path is None or empty, raises ValueError. \"\"\" logger . debug ( f \"json_load_with_meta() called with { file_path =} , { json_options =} \" ) file_path = pathlib . Path ( file_path ) all_data = json_load ( file_path , json_options = json_options ) if isinstance ( all_data , dict ) and \"_data_\" in all_data : return all_data [ \"_data_\" ], all_data . get ( \"_metadata_\" , {}) return all_data , {} json_save ( file_path , data , json_options = None ) Save a Python object to a JSON file with optional serialization settings. Parameters: file_path ( str | Path ) \u2013 Path to write the JSON file. If None or empty, raises ValueError. data ( object ) \u2013 Data to serialize and write. If None, writes 'null' to the file. json_options ( dict | None , default: None ) \u2013 Options to pass to json.dump (e.g., indent, default). Returns: None \u2013 None Raises: ValueError \u2013 If file_path is None or empty. OSError \u2013 If the file cannot be written. Examples: Input : \"output.json\", {\"x\": decimal.Decimal(\"1.23\")} Output: Creates a JSON file with serialized data at the given path Notes If file_path is None or empty, raises ValueError. If data is None, writes 'null' to the file. Source code in arb\\utils\\json.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def json_save ( file_path : str | pathlib . Path , data : object , json_options : dict | None = None ) -> None : \"\"\" Save a Python object to a JSON file with optional serialization settings. Args: file_path (str | Path): Path to write the JSON file. If None or empty, raises ValueError. data (object): Data to serialize and write. If None, writes 'null' to the file. json_options (dict | None): Options to pass to `json.dump` (e.g., indent, default). Returns: None Raises: ValueError: If file_path is None or empty. OSError: If the file cannot be written. Examples: Input : \"output.json\", {\"x\": decimal.Decimal(\"1.23\")} Output: Creates a JSON file with serialized data at the given path Notes: - If file_path is None or empty, raises ValueError. - If data is None, writes 'null' to the file. \"\"\" logger . debug ( f \"json_save() called with { file_path =} , { json_options =} , { data =} \" ) if not file_path : raise ValueError ( \"file_path must not be None or empty.\" ) file_path = pathlib . Path ( file_path ) if json_options is None : json_options = { \"default\" : json_serializer , \"indent\" : 4 } save_json_safely ( data , file_path , encoding = \"utf-8\" , json_options = json_options ) logger . debug ( f \"JSON saved to file: ' { file_path } '.\" ) json_save_with_meta ( file_path , data , metadata = None , json_options = None ) Save data with metadata to a JSON file under special keys ( data , metadata ). Parameters: file_path ( str | Path ) \u2013 Output JSON file path. If None or empty, raises ValueError. data ( object ) \u2013 Primary data to store under \" data \". If None, writes 'null' under \" data \". metadata ( dict | None , default: None ) \u2013 Optional metadata under \" metadata \". If None, auto-generated. json_options ( dict | None , default: None ) \u2013 Options for json.dump . Returns: None \u2013 None Raises: ValueError \u2013 If file_path is None or empty. OSError \u2013 If the file cannot be written. Examples: Input : \"log.json\", {\"key\": \"value\"}, metadata={\"source\": \"generated\"} Output: Writes JSON with data and metadata fields Notes If metadata is None, a default metadata dict is generated. If file_path is None or empty, raises ValueError. Source code in arb\\utils\\json.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def json_save_with_meta ( file_path : str | pathlib . Path , data : object , metadata : dict | None = None , json_options : dict | None = None ) -> None : \"\"\" Save data with metadata to a JSON file under special keys (_data_, _metadata_). Args: file_path (str | Path): Output JSON file path. If None or empty, raises ValueError. data (object): Primary data to store under \"_data_\". If None, writes 'null' under \"_data_\". metadata (dict | None): Optional metadata under \"_metadata_\". If None, auto-generated. json_options (dict | None): Options for `json.dump`. Returns: None Raises: ValueError: If file_path is None or empty. OSError: If the file cannot be written. Examples: Input : \"log.json\", {\"key\": \"value\"}, metadata={\"source\": \"generated\"} Output: Writes JSON with _data_ and _metadata_ fields Notes: - If metadata is None, a default metadata dict is generated. - If file_path is None or empty, raises ValueError. \"\"\" logger . debug ( f \"json_save_with_meta() called with { file_path =} , { json_options =} , { metadata =} , { data =} \" ) if not file_path : raise ValueError ( \"file_path must not be None or empty.\" ) file_path = pathlib . Path ( file_path ) if metadata is None : metadata = {} metadata . update ({ \"File created at\" : datetime . datetime . now ( ZoneInfo ( \"UTC\" )) . isoformat (), \"Serialized with\" : \"utils.json.json_save_with_meta\" , \"Deserialize with\" : \"utils.json.json_load_with_meta\" , }) wrapped = { \"_metadata_\" : metadata , \"_data_\" : data , } json_save ( file_path , wrapped , json_options = json_options ) json_serializer ( obj ) Custom JSON serializer for objects not natively serializable by json.dump . Parameters: obj ( object ) \u2013 The object to serialize. Supported: datetime, decimal, class/type objects. If None, raises TypeError. Returns: dict ( dict ) \u2013 A JSON-compatible dictionary representation of the object. Raises: TypeError \u2013 If the object type is unsupported or if obj is None. Examples: Input : datetime.datetime.now() Output: {\" type \": \"datetime.datetime\", \"value\": \"2025-07-04T12:34:56.789012\"} Input : decimal.Decimal(\"1.23\") Output: {\" type \": \"decimal.Decimal\", \"value\": \"1.23\"} Notes Only supports specific types; all others raise TypeError. If obj is None, raises TypeError. Source code in arb\\utils\\json.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def json_serializer ( obj : object ) -> dict : \"\"\" Custom JSON serializer for objects not natively serializable by `json.dump`. Args: obj (object): The object to serialize. Supported: datetime, decimal, class/type objects. If None, raises TypeError. Returns: dict: A JSON-compatible dictionary representation of the object. Raises: TypeError: If the object type is unsupported or if obj is None. Examples: Input : datetime.datetime.now() Output: {\"__type__\": \"datetime.datetime\", \"value\": \"2025-07-04T12:34:56.789012\"} Input : decimal.Decimal(\"1.23\") Output: {\"__type__\": \"decimal.Decimal\", \"value\": \"1.23\"} Notes: - Only supports specific types; all others raise TypeError. - If obj is None, raises TypeError. \"\"\" if isinstance ( obj , type ): return { \"__class__\" : obj . __name__ , \"__module__\" : obj . __module__ } elif isinstance ( obj , datetime . datetime ): return { \"__type__\" : \"datetime.datetime\" , \"value\" : obj . isoformat ()} elif isinstance ( obj , decimal . Decimal ): return { \"__type__\" : \"decimal.Decimal\" , \"value\" : str ( obj )} raise TypeError ( f \"Object of type { type ( obj ) . __name__ } is not JSON serializable\" ) make_dict_serializeable ( input_dict , type_map = None , convert_time_to_ca = False ) Transform a dictionary to ensure JSON compatibility of its values. Parameters: input_dict ( dict ) \u2013 Original dictionary to process. If None, raises ValueError. type_map ( dict [ str , type ] | None , default: None ) \u2013 Optional field-to-type map for casting. If not provided, no casting is performed. convert_time_to_ca ( bool , default: False ) \u2013 Convert datetimes to CA time before serialization. Returns: dict ( dict ) \u2013 Dictionary with all values JSON-serializable. Raises: ValueError \u2013 If input_dict is None. Examples: Input : {\"amount\": decimal.Decimal(\"1.23\"), \"date\": datetime.datetime(2025, 7, 4, 12, 0)} Output: {\"amount\": \"1.23\", \"date\": \"2025-07-04T12:00:00\"} Notes If input_dict is None, raises ValueError. If type_map is provided, values are cast to the specified types before serialization. Source code in arb\\utils\\json.py 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 def make_dict_serializeable ( input_dict : dict , type_map : dict [ str , type ] | None = None , convert_time_to_ca : bool = False ) -> dict : \"\"\" Transform a dictionary to ensure JSON compatibility of its values. Args: input_dict (dict): Original dictionary to process. If None, raises ValueError. type_map (dict[str, type] | None): Optional field-to-type map for casting. If not provided, no casting is performed. convert_time_to_ca (bool): Convert datetimes to CA time before serialization. Returns: dict: Dictionary with all values JSON-serializable. Raises: ValueError: If input_dict is None. Examples: Input : {\"amount\": decimal.Decimal(\"1.23\"), \"date\": datetime.datetime(2025, 7, 4, 12, 0)} Output: {\"amount\": \"1.23\", \"date\": \"2025-07-04T12:00:00\"} Notes: - If input_dict is None, raises ValueError. - If type_map is provided, values are cast to the specified types before serialization. \"\"\" result = {} for key , value in input_dict . items (): if not isinstance ( key , str ): raise TypeError ( f \"All keys must be strings. Invalid key: { key } ( { type ( key ) } )\" ) if type_map and key in type_map : try : value = safe_cast ( value , type_map [ key ]) except Exception as e : raise ValueError ( f \"Failed to cast key ' { key } ' to { type_map [ key ] } : { e } \" ) # todo - datetime - change this to ensure datetime is iso and fail if it is not if isinstance ( value , datetime . datetime ): if convert_time_to_ca : value = ca_naive_datetime_to_utc_datetime ( value ) value = value . isoformat () elif isinstance ( value , decimal . Decimal ): value = float ( value ) result [ key ] = value return result normalize_value ( val ) Normalize a value for string-based diffing or comparison. Parameters: val ( Any ) \u2013 Value to normalize (any type). If None or empty string, returns \"\". If datetime, returns ISO string. Returns: str ( str ) \u2013 Normalized string value. None and empty string become \"\". Datetimes are ISO strings. Examples: Input : None Output: \"\" Input : \"\" Output: \"\" Input : datetime.datetime(2025, 1, 1, 12, 0) Output: \"2025-01-01T12:00:00\" Input : 123 Output: \"123\" Notes None and empty strings (\"\") are treated identically, returning \"\". Naive datetime values are assumed to be in California time and converted to UTC. All other types are stringified using str(val). Ensures fields that were previously None but now filled with an empty string (or vice versa) are not falsely flagged as changed. Datetime normalization is contract-compliant. Source code in arb\\utils\\json.py 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 def normalize_value ( val : Any ) -> str : \"\"\" Normalize a value for string-based diffing or comparison. Args: val: Value to normalize (any type). If None or empty string, returns \"\". If datetime, returns ISO string. Returns: str: Normalized string value. None and empty string become \"\". Datetimes are ISO strings. Examples: Input : None Output: \"\" Input : \"\" Output: \"\" Input : datetime.datetime(2025, 1, 1, 12, 0) Output: \"2025-01-01T12:00:00\" Input : 123 Output: \"123\" Notes: - None and empty strings (\"\") are treated identically, returning \"\". - Naive datetime values are assumed to be in California time and converted to UTC. - All other types are stringified using str(val). - Ensures fields that were previously None but now filled with an empty string (or vice versa) are not falsely flagged as changed. Datetime normalization is contract-compliant. \"\"\" from datetime import datetime from arb.utils.date_and_time import is_datetime_naive , ca_naive_datetime_to_utc_datetime if val is None or val == \"\" : return \"\" if isinstance ( val , datetime ): if is_datetime_naive ( val ): val = ca_naive_datetime_to_utc_datetime ( val ) return val . isoformat () return str ( val ) safe_json_loads ( value , context_label = '' ) Safely parse a JSON string into a Python dictionary. Parameters: value ( str | dict | None ) \u2013 JSON-formatted string, pre-decoded dict, or None. If not str, dict, or None, raises TypeError. context_label ( str , default: '' ) \u2013 Optional label for diagnostics/logging. Returns: dict ( dict ) \u2013 Parsed dictionary from the input, or empty dict if input is None, invalid, or already a valid dict. Raises: TypeError \u2013 If value is not a str, dict, or None. Examples: Input : '{\"a\": 1, \"b\": 2}' Output: {\"a\": 1, \"b\": 2} Input : {\"a\": 1, \"b\": 2} Output: {\"a\": 1, \"b\": 2} Input : None Output: {} Input : \"\" Output: {} Input : \"not valid json\" Output: {} Notes If value is already a dict, it is returned unchanged. If value is None, empty, or invalid JSON, an empty dict is returned. If decoding fails, a warning is logged including the context_label. Source code in arb\\utils\\json.py 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 def safe_json_loads ( value : str | dict | None , context_label : str = \"\" ) -> dict : \"\"\" Safely parse a JSON string into a Python dictionary. Args: value (str | dict | None): JSON-formatted string, pre-decoded dict, or None. If not str, dict, or None, raises TypeError. context_label (str): Optional label for diagnostics/logging. Returns: dict: Parsed dictionary from the input, or empty dict if input is None, invalid, or already a valid dict. Raises: TypeError: If `value` is not a str, dict, or None. Examples: Input : '{\"a\": 1, \"b\": 2}' Output: {\"a\": 1, \"b\": 2} Input : {\"a\": 1, \"b\": 2} Output: {\"a\": 1, \"b\": 2} Input : None Output: {} Input : \"\" Output: {} Input : \"not valid json\" Output: {} Notes: - If `value` is already a dict, it is returned unchanged. - If `value` is None, empty, or invalid JSON, an empty dict is returned. - If decoding fails, a warning is logged including the context_label. \"\"\" if value is None or ( isinstance ( value , str ) and value . strip () == \"\" ): return {} if isinstance ( value , dict ): return value if not isinstance ( value , str ): raise TypeError ( f \"Expected str, dict, or None; got { type ( value ) . __name__ } \" ) try : return json . loads ( value ) except json . JSONDecodeError : label_msg = f \" ( { context_label } )\" if context_label else \"\" logger . warning ( f \"Corrupt or invalid JSON string encountered { label_msg } ; returning empty dict.\" ) return {} wtform_types_and_values ( wtform ) Extract field types and current data values from a WTForm. Parameters: wtform ( FlaskForm ) \u2013 WTForms instance. Must not be None. Returns: tuple ( tuple [ dict [ str , type ], dict [ str , object ]] ) \u2013 dict[str, type]: Field name to type mapping for deserialization. dict[str, object]: Field name to current value mapping (may include 'Please Select'). Raises: ValueError \u2013 If wtform is None or does not have _fields attribute. Examples: Input : form (WTForms instance) Output: (type_map, field_data) where type_map is a dict of field types and field_data is a dict of field values Notes If wtform is None or invalid, raises ValueError. 'Please Select' is a valid value for SelectField. Source code in arb\\utils\\json.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 def wtform_types_and_values ( wtform ) -> tuple [ dict [ str , type ], dict [ str , object ]]: \"\"\" Extract field types and current data values from a WTForm. Args: wtform (FlaskForm): WTForms instance. Must not be None. Returns: tuple: - dict[str, type]: Field name to type mapping for deserialization. - dict[str, object]: Field name to current value mapping (may include 'Please Select'). Raises: ValueError: If wtform is None or does not have _fields attribute. Examples: Input : form (WTForms instance) Output: (type_map, field_data) where type_map is a dict of field types and field_data is a dict of field values Notes: - If wtform is None or invalid, raises ValueError. - 'Please Select' is a valid value for SelectField. \"\"\" if wtform is None or not hasattr ( wtform , '_fields' ): raise ValueError ( \"wtform must have a _fields attribute\" ) type_map = {} field_data = {} for name , field in wtform . _fields . items (): value = field . data field_data [ name ] = value if isinstance ( field , DateTimeField ): type_map [ name ] = datetime . datetime elif isinstance ( field , DecimalField ): type_map [ name ] = decimal . Decimal elif isinstance ( field , BooleanField ): type_map [ name ] = bool elif isinstance ( field , IntegerField ): type_map [ name ] = int elif isinstance ( field , SelectField ): type_map [ name ] = str # 'Please Select' is valid return type_map , field_data","title":"arb.utils.json"},{"location":"reference/arb/utils/json/#arbutilsjson","text":"JSON utilities for the ARB Feedback Portal. This module centralizes JSON serialization, deserialization, metadata handling, and diagnostics for ARB portal utilities. It ensures consistent handling of ISO 8601 datetimes, decimals, and custom types, and provides helpers for file-based workflows, WTForms integration, and contract-compliant value normalization. Features: - Custom serialization/deserialization for datetime, decimal, and class/type objects - Metadata support for enhanced JSON files (with data and metadata fields) - File-based diagnostics, JSON comparison, and safe file I/O - WTForms integration for extracting and casting form data - Value normalization and diffing for ingestion and audit workflows Contract notes: - Emphasizes ISO 8601 datetime formats and Pacific Time handling where required - Designed for robust, structured JSON handling across ARB portal utilities - Logging is used for diagnostics, warnings, and error reporting Extensible for new types and workflows as the portal evolves.","title":"arb.utils.json"},{"location":"reference/arb/utils/json/#arb.utils.json.add_metadata_to_json","text":"Add metadata to an existing JSON file or overwrite it in-place. Parameters: file_name_in ( str | Path ) \u2013 Input JSON file path. If None or empty, raises ValueError. file_name_out ( str | Path | None , default: None ) \u2013 Output file path. If None, overwrites input. If empty, raises ValueError. Returns: None \u2013 None Raises: ValueError \u2013 If file_name_in is None or empty, or if file_name_out is empty. FileNotFoundError \u2013 If the input file does not exist. OSError \u2013 If the file cannot be written. Examples: Input : \"schema.json\" Output: Adds metadata and writes back to same file Input : \"input.json\", \"output.json\" Output: Adds metadata and writes to output.json Notes If file_name_out is None, input file is overwritten. If file_name_in or file_name_out is empty, raises ValueError. Source code in arb\\utils\\json.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 def add_metadata_to_json ( file_name_in : str | pathlib . Path , file_name_out : str | pathlib . Path | None = None ) -> None : \"\"\" Add metadata to an existing JSON file or overwrite it in-place. Args: file_name_in (str | Path): Input JSON file path. If None or empty, raises ValueError. file_name_out (str | Path | None): Output file path. If None, overwrites input. If empty, raises ValueError. Returns: None Raises: ValueError: If file_name_in is None or empty, or if file_name_out is empty. FileNotFoundError: If the input file does not exist. OSError: If the file cannot be written. Examples: Input : \"schema.json\" Output: Adds metadata and writes back to same file Input : \"input.json\", \"output.json\" Output: Adds metadata and writes to output.json Notes: - If file_name_out is None, input file is overwritten. - If file_name_in or file_name_out is empty, raises ValueError. \"\"\" logger . debug ( f \"add_metadata_to_json() called with { file_name_in =} , { file_name_out =} \" ) file_name_in = pathlib . Path ( file_name_in ) if file_name_out is not None : file_name_out = pathlib . Path ( file_name_out ) else : file_name_out = file_name_in data = json_load ( file_name_in ) json_save_with_meta ( file_name_out , data = data )","title":"add_metadata_to_json"},{"location":"reference/arb/utils/json/#arb.utils.json.cast_model_value","text":"Cast a stringified JSON value into a Python object of the expected type. Parameters: value ( str ) \u2013 Input value to cast. If None, raises ValueError. value_type ( type ) \u2013 Python type to cast to (str, int, float, bool, datetime, decimal). If None, raises ValueError. convert_time_to_ca ( bool , default: False ) \u2013 If True, convert UTC to California naive datetime. Returns: Any ( Any ) \u2013 Value converted to the target Python type. Raises: ValueError \u2013 If the value cannot be cast to the given type, type is unsupported, or value_type is None. Examples: Input : \"2025-01-01T12:00:00Z\", datetime.datetime Output: datetime.datetime(2025, 1, 1, 12, 0, 0, tzinfo=ZoneInfo(\"UTC\")) Input : \"123.45\", decimal.Decimal Output: decimal.Decimal(\"123.45\") Input : \"true\", bool Output: True Notes If value_type is None, raises ValueError. If value is None, raises ValueError. If type is unsupported, raises ValueError. Source code in arb\\utils\\json.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 def cast_model_value ( value : str , value_type : type , convert_time_to_ca : bool = False ) -> Any : \"\"\" Cast a stringified JSON value into a Python object of the expected type. Args: value (str): Input value to cast. If None, raises ValueError. value_type (type): Python type to cast to (str, int, float, bool, datetime, decimal). If None, raises ValueError. convert_time_to_ca (bool): If True, convert UTC to California naive datetime. Returns: Any: Value converted to the target Python type. Raises: ValueError: If the value cannot be cast to the given type, type is unsupported, or value_type is None. Examples: Input : \"2025-01-01T12:00:00Z\", datetime.datetime Output: datetime.datetime(2025, 1, 1, 12, 0, 0, tzinfo=ZoneInfo(\"UTC\")) Input : \"123.45\", decimal.Decimal Output: decimal.Decimal(\"123.45\") Input : \"true\", bool Output: True Notes: - If value_type is None, raises ValueError. - If value is None, raises ValueError. - If type is unsupported, raises ValueError. \"\"\" # todo - datetime - may need to update try : if value_type == str : # No need to cast a string return value elif value_type in [ bool , int , float ]: return value_type ( value ) elif value_type == datetime . datetime : dt = iso_str_to_utc_datetime ( value ) return utc_datetime_to_ca_naive_datetime ( dt ) if convert_time_to_ca else dt elif value_type == decimal . Decimal : return decimal . Decimal ( value ) else : raise ValueError ( f \"Unsupported type for casting: { value_type } \" ) except Exception as e : raise ValueError ( f \"Failed to cast { value !r} to { value_type } : { e } \" )","title":"cast_model_value"},{"location":"reference/arb/utils/json/#arb.utils.json.compare_json_files","text":"Compare the contents of two JSON files including metadata and values. Parameters: file_name_1 ( str | Path ) \u2013 Path to the first file. If None or empty, raises ValueError. file_name_2 ( str | Path ) \u2013 Path to the second file. If None or empty, raises ValueError. Returns: None \u2013 None Logs Differences or matches are logged at debug level. Raises: ValueError \u2013 If file_name_1 or file_name_2 is None or empty. FileNotFoundError \u2013 If either file does not exist. JSONDecodeError \u2013 If either file is not valid JSON. Examples: Input : \"old.json\", \"new.json\" Output: Logs any differences or confirms matches Notes If either file is missing or invalid, raises an exception. Only logs differences; does not return a value. Source code in arb\\utils\\json.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 def compare_json_files ( file_name_1 : str | pathlib . Path , file_name_2 : str | pathlib . Path ) -> None : \"\"\" Compare the contents of two JSON files including metadata and values. Args: file_name_1 (str | Path): Path to the first file. If None or empty, raises ValueError. file_name_2 (str | Path): Path to the second file. If None or empty, raises ValueError. Returns: None Logs: Differences or matches are logged at debug level. Raises: ValueError: If file_name_1 or file_name_2 is None or empty. FileNotFoundError: If either file does not exist. json.JSONDecodeError: If either file is not valid JSON. Examples: Input : \"old.json\", \"new.json\" Output: Logs any differences or confirms matches Notes: - If either file is missing or invalid, raises an exception. - Only logs differences; does not return a value. \"\"\" logger . debug ( f \"compare_json_files() comparing { file_name_1 } and { file_name_2 } \" ) data_1 , meta_1 = json_load_with_meta ( file_name_1 ) data_2 , meta_2 = json_load_with_meta ( file_name_2 ) logger . debug ( f \"Comparing metadata\" ) if compare_dicts ( meta_1 , meta_2 , \"metadata_01\" , \"metadata_02\" ) is True : logger . debug ( f \"Metadata are equivalent\" ) else : logger . debug ( f \"Metadata differ\" ) logger . debug ( f \"Comparing data\" ) if compare_dicts ( data_1 , data_2 , \"data_01\" , \"data_02\" ) is True : logger . debug ( f \"Data are equivalent\" ) else : logger . debug ( f \"Data differ\" )","title":"compare_json_files"},{"location":"reference/arb/utils/json/#arb.utils.json.compute_field_differences","text":"Generate a field-by-field diff between two dictionaries using keys from new_data . Parameters: new_data ( dict ) \u2013 The incoming or modified dictionary (e.g., staged JSON). If None, treated as empty dict. existing_data ( dict ) \u2013 The reference or baseline dictionary (e.g., DB row). If None, treated as empty dict. Returns: list [ dict ] \u2013 list[dict]: List of field diffs, each with: - 'key': The dictionary key being compared - 'old': The normalized value from existing_data - 'new': The normalized value from new_data - 'changed': True if the values differ after normalization - 'is_same': True if values are unchanged - 'from_upload': Always True, indicating this field came from uploaded JSON - 'requires_confirmation': True if the update adds or overwrites non-trivial data Examples: Input : {\"a\": 1}, {\"a\": 2} Output: [{\"key\": \"a\", \"old\": \"2\", \"new\": \"1\", \"changed\": True, ...}] Input : {\"a\": None}, {\"a\": \"\"} Output: [{\"key\": \"a\", \"old\": \"\", \"new\": \"\", \"changed\": False, ...}] Notes Normalization uses normalize_value() for consistent formatting, especially for empty strings, None, and datetimes. Keys present in existing_data but not in new_data are ignored. A field requires confirmation if it adds or overwrites non-empty data. If either input dict is None, it is treated as empty dict. Source code in arb\\utils\\json.py 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 def compute_field_differences ( new_data : dict , existing_data : dict ) -> list [ dict ]: \"\"\" Generate a field-by-field diff between two dictionaries using keys from `new_data`. Args: new_data (dict): The incoming or modified dictionary (e.g., staged JSON). If None, treated as empty dict. existing_data (dict): The reference or baseline dictionary (e.g., DB row). If None, treated as empty dict. Returns: list[dict]: List of field diffs, each with: - 'key': The dictionary key being compared - 'old': The normalized value from `existing_data` - 'new': The normalized value from `new_data` - 'changed': True if the values differ after normalization - 'is_same': True if values are unchanged - 'from_upload': Always True, indicating this field came from uploaded JSON - 'requires_confirmation': True if the update adds or overwrites non-trivial data Examples: Input : {\"a\": 1}, {\"a\": 2} Output: [{\"key\": \"a\", \"old\": \"2\", \"new\": \"1\", \"changed\": True, ...}] Input : {\"a\": None}, {\"a\": \"\"} Output: [{\"key\": \"a\", \"old\": \"\", \"new\": \"\", \"changed\": False, ...}] Notes: - Normalization uses `normalize_value()` for consistent formatting, especially for empty strings, None, and datetimes. - Keys present in `existing_data` but *not* in `new_data` are ignored. - A field requires confirmation if it adds or overwrites non-empty data. - If either input dict is None, it is treated as empty dict. \"\"\" differences = [] for key in sorted ( new_data . keys ()): new_value = new_data . get ( key ) old_value = existing_data . get ( key ) norm_new = normalize_value ( new_value ) norm_old = normalize_value ( old_value ) is_same = norm_old == norm_new requires_confirmation = ( norm_new not in ( None , \"\" , []) and not is_same ) differences . append ({ \"key\" : key , \"old\" : norm_old , \"new\" : norm_new , \"changed\" : not is_same , \"is_same\" : is_same , \"from_upload\" : True , \"requires_confirmation\" : requires_confirmation , }) logger . debug ( f \"DIFF KEY= { key !r} | DB= { type ( old_value ) . __name__ } : { norm_old !r} \" f \"| NEW= { type ( new_value ) . __name__ } : { norm_new !r} \" f \"| SAME= { is_same } | CONFIRM= { requires_confirmation } \" ) return differences","title":"compute_field_differences"},{"location":"reference/arb/utils/json/#arb.utils.json.deserialize_dict","text":"Deserialize a dictionary of raw values using a type map. Parameters: input_dict ( dict ) \u2013 Dictionary of raw values. If None, raises ValueError. type_map ( dict [ str , type ] ) \u2013 Field-to-type mapping for deserialization. If None or empty, no casting is performed. convert_time_to_ca ( bool , default: False ) \u2013 If True, converts datetime to CA time. Returns: dict ( dict ) \u2013 Fully deserialized dictionary. Raises: TypeError \u2013 If any key is not a string. ValueError \u2013 If value casting fails for a key or if input_dict is None. Examples: Input : {\"dt\": \"2025-01-01T12:00:00Z\"}, {\"dt\": datetime.datetime} Output: {\"dt\": datetime.datetime(2025, 1, 1, 12, 0, 0, tzinfo=ZoneInfo(\"UTC\"))} Notes If input_dict is None, raises ValueError. If type_map is None or empty, no casting is performed. Source code in arb\\utils\\json.py 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 def deserialize_dict ( input_dict : dict , type_map : dict [ str , type ], convert_time_to_ca : bool = False ) -> dict : \"\"\" Deserialize a dictionary of raw values using a type map. Args: input_dict (dict): Dictionary of raw values. If None, raises ValueError. type_map (dict[str, type]): Field-to-type mapping for deserialization. If None or empty, no casting is performed. convert_time_to_ca (bool): If True, converts datetime to CA time. Returns: dict: Fully deserialized dictionary. Raises: TypeError: If any key is not a string. ValueError: If value casting fails for a key or if input_dict is None. Examples: Input : {\"dt\": \"2025-01-01T12:00:00Z\"}, {\"dt\": datetime.datetime} Output: {\"dt\": datetime.datetime(2025, 1, 1, 12, 0, 0, tzinfo=ZoneInfo(\"UTC\"))} Notes: - If input_dict is None, raises ValueError. - If type_map is None or empty, no casting is performed. \"\"\" result = {} for key , value in input_dict . items (): if not isinstance ( key , str ): raise TypeError ( f \"All keys must be strings. Invalid key: { key } ( { type ( key ) } )\" ) if key in type_map and value is not None : result [ key ] = cast_model_value ( value , type_map [ key ], convert_time_to_ca ) else : result [ key ] = value return result","title":"deserialize_dict"},{"location":"reference/arb/utils/json/#arb.utils.json.extract_id_from_json","text":"Safely extract a numeric key (like id_incidence) from a specified tab in Excel-parsed JSON. Parameters: json_data ( dict ) \u2013 JSON dictionary as parsed from the Excel upload. If None, returns None. tab_name ( str , default: 'Feedback Form' ) \u2013 Name of the tab in 'tab_contents'. Defaults to \"Feedback Form\". If None or empty, uses default. key_name ( str , default: 'id_incidence' ) \u2013 Key to extract from the tab. Defaults to \"id_incidence\". If None or empty, uses default. Returns: int | None \u2013 int | None: Parsed integer value, or None if not found or invalid. Examples: Input : {\"tab_contents\": {\"Feedback Form\": {\"id_incidence\": \"123\"}}}, tab_name=\"Feedback Form\", key_name=\"id_incidence\" Output: 123 Input : {\"tab_contents\": {\"Feedback Form\": {}}}, tab_name=\"Feedback Form\", key_name=\"id_incidence\" Output: None Notes Unlike full ingestion, this does not validate schema or write to the DB. Handles string digits and trims whitespace safely. Logs warning if structure is invalid. If json_data is None, returns None. Source code in arb\\utils\\json.py 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 def extract_id_from_json ( json_data : dict , tab_name : str = \"Feedback Form\" , key_name : str = \"id_incidence\" ) -> int | None : \"\"\" Safely extract a numeric key (like id_incidence) from a specified tab in Excel-parsed JSON. Args: json_data (dict): JSON dictionary as parsed from the Excel upload. If None, returns None. tab_name (str, optional): Name of the tab in 'tab_contents'. Defaults to \"Feedback Form\". If None or empty, uses default. key_name (str, optional): Key to extract from the tab. Defaults to \"id_incidence\". If None or empty, uses default. Returns: int | None: Parsed integer value, or None if not found or invalid. Examples: Input : {\"tab_contents\": {\"Feedback Form\": {\"id_incidence\": \"123\"}}}, tab_name=\"Feedback Form\", key_name=\"id_incidence\" Output: 123 Input : {\"tab_contents\": {\"Feedback Form\": {}}}, tab_name=\"Feedback Form\", key_name=\"id_incidence\" Output: None Notes: - Unlike full ingestion, this does not validate schema or write to the DB. - Handles string digits and trims whitespace safely. - Logs warning if structure is invalid. - If json_data is None, returns None. \"\"\" try : tab_data = json_data . get ( \"tab_contents\" , {}) . get ( tab_name , {}) val = tab_data . get ( key_name ) if isinstance ( val , int ): return val if isinstance ( val , str ) and val . strip () . isdigit (): return int ( val . strip ()) except Exception as e : logger . warning ( f \"Failed to extract key ' { key_name } ' from tab ' { tab_name } ': { e } \" ) return None","title":"extract_id_from_json"},{"location":"reference/arb/utils/json/#arb.utils.json.extract_tab_payload","text":"Extract the contents of a specific tab from a parsed Excel JSON structure. Parameters: json_data ( dict ) \u2013 Parsed JSON dictionary, typically from json_load_with_meta() . If None, returns empty dict. tab_name ( str , default: 'Feedback Form' ) \u2013 Tab name whose contents to extract. Defaults to \"Feedback Form\". If None or empty, uses default. Returns: dict ( dict ) \u2013 The dictionary of field keys/values for the specified tab, or an empty dict if the tab is missing or malformed. Examples: Input : {\"tab_contents\": {\"Feedback Form\": {\"field1\": \"value1\"}}}, tab_name=\"Feedback Form\" Output: {\"field1\": \"value1\"} Input : {\"tab_contents\": {}}, tab_name=\"Feedback Form\" Output: {} Input : None, tab_name=\"Feedback Form\" Output: {} Notes If json_data is None, returns empty dict. If tab_name is None or empty, uses default. If the tab is missing or malformed, returns empty dict. Source code in arb\\utils\\json.py 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 def extract_tab_payload ( json_data : dict , tab_name : str = \"Feedback Form\" ) -> dict : \"\"\" Extract the contents of a specific tab from a parsed Excel JSON structure. Args: json_data (dict): Parsed JSON dictionary, typically from `json_load_with_meta()`. If None, returns empty dict. tab_name (str): Tab name whose contents to extract. Defaults to \"Feedback Form\". If None or empty, uses default. Returns: dict: The dictionary of field keys/values for the specified tab, or an empty dict if the tab is missing or malformed. Examples: Input : {\"tab_contents\": {\"Feedback Form\": {\"field1\": \"value1\"}}}, tab_name=\"Feedback Form\" Output: {\"field1\": \"value1\"} Input : {\"tab_contents\": {}}, tab_name=\"Feedback Form\" Output: {} Input : None, tab_name=\"Feedback Form\" Output: {} Notes: - If json_data is None, returns empty dict. - If tab_name is None or empty, uses default. - If the tab is missing or malformed, returns empty dict. \"\"\" try : return json_data . get ( \"tab_contents\" , {}) . get ( tab_name , {}) except Exception as e : logger . warning ( f \"extract_tab_payload() failed for tab ' { tab_name } ': { e } \" ) return {}","title":"extract_tab_payload"},{"location":"reference/arb/utils/json/#arb.utils.json.json_deserializer","text":"Custom JSON deserializer for class/type representations created by json_serializer . Parameters: obj ( dict [ str , Any ] ) \u2013 Dictionary object from JSON with special tags for known types. If None, returns None. Returns: Any ( Any ) \u2013 Reconstructed Python object (datetime, decimal, or class/type), or original dict if no tags found. Raises: TypeError \u2013 If the type tag is unknown or unsupported. Examples: Input : {\" type \": \"datetime.datetime\", \"value\": \"2025-07-04T12:34:56.789012\"} Output: datetime.datetime(2025, 7, 4, 12, 34, 56, 789012) Input : {\" class \": \"int\", \" module \": \"builtins\"} Output: int Notes If obj is None, returns None. If no recognized tags, returns obj unchanged. Source code in arb\\utils\\json.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def json_deserializer ( obj : dict [ str , Any ]) -> Any : \"\"\" Custom JSON deserializer for class/type representations created by `json_serializer`. Args: obj (dict[str, Any]): Dictionary object from JSON with special tags for known types. If None, returns None. Returns: Any: Reconstructed Python object (datetime, decimal, or class/type), or original dict if no tags found. Raises: TypeError: If the type tag is unknown or unsupported. Examples: Input : {\"__type__\": \"datetime.datetime\", \"value\": \"2025-07-04T12:34:56.789012\"} Output: datetime.datetime(2025, 7, 4, 12, 34, 56, 789012) Input : {\"__class__\": \"int\", \"__module__\": \"builtins\"} Output: int Notes: - If obj is None, returns None. - If no recognized tags, returns obj unchanged. \"\"\" new_obj = obj if \"__class__\" in obj : # logger.debug(f\"{obj['__class__']=} detected in object deserializer.\") if obj [ \"__class__\" ] == \"str\" : new_obj = str elif obj [ \"__class__\" ] == \"int\" : new_obj = int elif obj [ \"__class__\" ] == \"float\" : new_obj = float elif obj [ \"__class__\" ] == \"bool\" : new_obj = bool elif obj [ \"__class__\" ] == \"datetime\" : new_obj = datetime . datetime else : raise TypeError ( f \"Object of type { type ( obj ) . __name__ } is not JSON deserializable\" ) elif \"__type__\" in obj : type_tag = obj [ \"__type__\" ] if type_tag == \"datetime.datetime\" : new_obj = datetime . datetime . fromisoformat ( obj [ \"value\" ]) elif type_tag == \"decimal.Decimal\" : new_obj = decimal . Decimal ( obj [ \"value\" ]) else : logger . debug ( f \"No known conversion type for type { obj [ '__type__' ] } \" ) # logger.debug(f\"deserializer() returning type= {type(new_obj)}, new_obj= {new_obj}\") return new_obj","title":"json_deserializer"},{"location":"reference/arb/utils/json/#arb.utils.json.json_load","text":"Load and deserialize data from a JSON file. Parameters: file_path ( str | Path ) \u2013 Path to the JSON file. If None or empty, raises ValueError. json_options ( dict | None , default: None ) \u2013 Optional options passed to json.load . Returns: object ( object ) \u2013 Deserialized Python object (dict, list, etc.). Raises: ValueError \u2013 If file_path is None or empty. FileNotFoundError \u2013 If the file does not exist. JSONDecodeError \u2013 If the file is not valid JSON. Examples: Input : \"data.json\" Output: Deserialized Python object from JSON Notes Uses utf-8-sig encoding to handle BOM if present. If the file is not valid JSON, an exception is raised. If file_path is None or empty, raises ValueError. Source code in arb\\utils\\json.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def json_load ( file_path : str | pathlib . Path , json_options : dict | None = None ) -> object : \"\"\" Load and deserialize data from a JSON file. Args: file_path (str | Path): Path to the JSON file. If None or empty, raises ValueError. json_options (dict | None): Optional options passed to `json.load`. Returns: object: Deserialized Python object (dict, list, etc.). Raises: ValueError: If file_path is None or empty. FileNotFoundError: If the file does not exist. json.JSONDecodeError: If the file is not valid JSON. Examples: Input : \"data.json\" Output: Deserialized Python object from JSON Notes: - Uses utf-8-sig encoding to handle BOM if present. - If the file is not valid JSON, an exception is raised. - If file_path is None or empty, raises ValueError. \"\"\" logger . debug ( f \"json_load() called with { file_path =} , { json_options =} \" ) file_path = pathlib . Path ( file_path ) if json_options is None : json_options = { \"object_hook\" : json_deserializer } return read_json_file ( file_path , encoding = \"utf-8-sig\" , json_options = json_options )","title":"json_load"},{"location":"reference/arb/utils/json/#arb.utils.json.json_load_with_meta","text":"Load a JSON file and return both data and metadata if present. Parameters: file_path ( str | Path ) \u2013 Path to the JSON file. If None or empty, raises ValueError. json_options ( dict | None , default: None ) \u2013 Optional options passed to json.load . Returns: tuple ( tuple [ Any , dict ] ) \u2013 Any: Deserialized data from \" data \" (or the entire file if \" data \" is not present). dict: Deserialized metadata from \" metadata \" (or empty dict if not present). Raises: ValueError \u2013 If file_path is None or empty. FileNotFoundError \u2013 If the file does not exist. JSONDecodeError \u2013 If the file is not valid JSON. Examples: Input : \"example.json\" Output: tuple (data, metadata) extracted from the file Notes If the JSON file is a dictionary with a key data , then the data and metadata (if possible) will be extracted. Otherwise, the data is assumed to be the entire file contents and metadata is an empty dict. If file_path is None or empty, raises ValueError. Source code in arb\\utils\\json.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def json_load_with_meta ( file_path : str | pathlib . Path , json_options : dict | None = None ) -> tuple [ Any , dict ]: \"\"\" Load a JSON file and return both data and metadata if present. Args: file_path (str | Path): Path to the JSON file. If None or empty, raises ValueError. json_options (dict | None): Optional options passed to `json.load`. Returns: tuple: - Any: Deserialized data from \"_data_\" (or the entire file if \"_data_\" is not present). - dict: Deserialized metadata from \"_metadata_\" (or empty dict if not present). Raises: ValueError: If file_path is None or empty. FileNotFoundError: If the file does not exist. json.JSONDecodeError: If the file is not valid JSON. Examples: Input : \"example.json\" Output: tuple (data, metadata) extracted from the file Notes: - If the JSON file is a dictionary with a key _data_, then the _data_ and _metadata_ (if possible) will be extracted. Otherwise, the data is assumed to be the entire file contents and metadata is an empty dict. - If file_path is None or empty, raises ValueError. \"\"\" logger . debug ( f \"json_load_with_meta() called with { file_path =} , { json_options =} \" ) file_path = pathlib . Path ( file_path ) all_data = json_load ( file_path , json_options = json_options ) if isinstance ( all_data , dict ) and \"_data_\" in all_data : return all_data [ \"_data_\" ], all_data . get ( \"_metadata_\" , {}) return all_data , {}","title":"json_load_with_meta"},{"location":"reference/arb/utils/json/#arb.utils.json.json_save","text":"Save a Python object to a JSON file with optional serialization settings. Parameters: file_path ( str | Path ) \u2013 Path to write the JSON file. If None or empty, raises ValueError. data ( object ) \u2013 Data to serialize and write. If None, writes 'null' to the file. json_options ( dict | None , default: None ) \u2013 Options to pass to json.dump (e.g., indent, default). Returns: None \u2013 None Raises: ValueError \u2013 If file_path is None or empty. OSError \u2013 If the file cannot be written. Examples: Input : \"output.json\", {\"x\": decimal.Decimal(\"1.23\")} Output: Creates a JSON file with serialized data at the given path Notes If file_path is None or empty, raises ValueError. If data is None, writes 'null' to the file. Source code in arb\\utils\\json.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def json_save ( file_path : str | pathlib . Path , data : object , json_options : dict | None = None ) -> None : \"\"\" Save a Python object to a JSON file with optional serialization settings. Args: file_path (str | Path): Path to write the JSON file. If None or empty, raises ValueError. data (object): Data to serialize and write. If None, writes 'null' to the file. json_options (dict | None): Options to pass to `json.dump` (e.g., indent, default). Returns: None Raises: ValueError: If file_path is None or empty. OSError: If the file cannot be written. Examples: Input : \"output.json\", {\"x\": decimal.Decimal(\"1.23\")} Output: Creates a JSON file with serialized data at the given path Notes: - If file_path is None or empty, raises ValueError. - If data is None, writes 'null' to the file. \"\"\" logger . debug ( f \"json_save() called with { file_path =} , { json_options =} , { data =} \" ) if not file_path : raise ValueError ( \"file_path must not be None or empty.\" ) file_path = pathlib . Path ( file_path ) if json_options is None : json_options = { \"default\" : json_serializer , \"indent\" : 4 } save_json_safely ( data , file_path , encoding = \"utf-8\" , json_options = json_options ) logger . debug ( f \"JSON saved to file: ' { file_path } '.\" )","title":"json_save"},{"location":"reference/arb/utils/json/#arb.utils.json.json_save_with_meta","text":"Save data with metadata to a JSON file under special keys ( data , metadata ). Parameters: file_path ( str | Path ) \u2013 Output JSON file path. If None or empty, raises ValueError. data ( object ) \u2013 Primary data to store under \" data \". If None, writes 'null' under \" data \". metadata ( dict | None , default: None ) \u2013 Optional metadata under \" metadata \". If None, auto-generated. json_options ( dict | None , default: None ) \u2013 Options for json.dump . Returns: None \u2013 None Raises: ValueError \u2013 If file_path is None or empty. OSError \u2013 If the file cannot be written. Examples: Input : \"log.json\", {\"key\": \"value\"}, metadata={\"source\": \"generated\"} Output: Writes JSON with data and metadata fields Notes If metadata is None, a default metadata dict is generated. If file_path is None or empty, raises ValueError. Source code in arb\\utils\\json.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 def json_save_with_meta ( file_path : str | pathlib . Path , data : object , metadata : dict | None = None , json_options : dict | None = None ) -> None : \"\"\" Save data with metadata to a JSON file under special keys (_data_, _metadata_). Args: file_path (str | Path): Output JSON file path. If None or empty, raises ValueError. data (object): Primary data to store under \"_data_\". If None, writes 'null' under \"_data_\". metadata (dict | None): Optional metadata under \"_metadata_\". If None, auto-generated. json_options (dict | None): Options for `json.dump`. Returns: None Raises: ValueError: If file_path is None or empty. OSError: If the file cannot be written. Examples: Input : \"log.json\", {\"key\": \"value\"}, metadata={\"source\": \"generated\"} Output: Writes JSON with _data_ and _metadata_ fields Notes: - If metadata is None, a default metadata dict is generated. - If file_path is None or empty, raises ValueError. \"\"\" logger . debug ( f \"json_save_with_meta() called with { file_path =} , { json_options =} , { metadata =} , { data =} \" ) if not file_path : raise ValueError ( \"file_path must not be None or empty.\" ) file_path = pathlib . Path ( file_path ) if metadata is None : metadata = {} metadata . update ({ \"File created at\" : datetime . datetime . now ( ZoneInfo ( \"UTC\" )) . isoformat (), \"Serialized with\" : \"utils.json.json_save_with_meta\" , \"Deserialize with\" : \"utils.json.json_load_with_meta\" , }) wrapped = { \"_metadata_\" : metadata , \"_data_\" : data , } json_save ( file_path , wrapped , json_options = json_options )","title":"json_save_with_meta"},{"location":"reference/arb/utils/json/#arb.utils.json.json_serializer","text":"Custom JSON serializer for objects not natively serializable by json.dump . Parameters: obj ( object ) \u2013 The object to serialize. Supported: datetime, decimal, class/type objects. If None, raises TypeError. Returns: dict ( dict ) \u2013 A JSON-compatible dictionary representation of the object. Raises: TypeError \u2013 If the object type is unsupported or if obj is None. Examples: Input : datetime.datetime.now() Output: {\" type \": \"datetime.datetime\", \"value\": \"2025-07-04T12:34:56.789012\"} Input : decimal.Decimal(\"1.23\") Output: {\" type \": \"decimal.Decimal\", \"value\": \"1.23\"} Notes Only supports specific types; all others raise TypeError. If obj is None, raises TypeError. Source code in arb\\utils\\json.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def json_serializer ( obj : object ) -> dict : \"\"\" Custom JSON serializer for objects not natively serializable by `json.dump`. Args: obj (object): The object to serialize. Supported: datetime, decimal, class/type objects. If None, raises TypeError. Returns: dict: A JSON-compatible dictionary representation of the object. Raises: TypeError: If the object type is unsupported or if obj is None. Examples: Input : datetime.datetime.now() Output: {\"__type__\": \"datetime.datetime\", \"value\": \"2025-07-04T12:34:56.789012\"} Input : decimal.Decimal(\"1.23\") Output: {\"__type__\": \"decimal.Decimal\", \"value\": \"1.23\"} Notes: - Only supports specific types; all others raise TypeError. - If obj is None, raises TypeError. \"\"\" if isinstance ( obj , type ): return { \"__class__\" : obj . __name__ , \"__module__\" : obj . __module__ } elif isinstance ( obj , datetime . datetime ): return { \"__type__\" : \"datetime.datetime\" , \"value\" : obj . isoformat ()} elif isinstance ( obj , decimal . Decimal ): return { \"__type__\" : \"decimal.Decimal\" , \"value\" : str ( obj )} raise TypeError ( f \"Object of type { type ( obj ) . __name__ } is not JSON serializable\" )","title":"json_serializer"},{"location":"reference/arb/utils/json/#arb.utils.json.make_dict_serializeable","text":"Transform a dictionary to ensure JSON compatibility of its values. Parameters: input_dict ( dict ) \u2013 Original dictionary to process. If None, raises ValueError. type_map ( dict [ str , type ] | None , default: None ) \u2013 Optional field-to-type map for casting. If not provided, no casting is performed. convert_time_to_ca ( bool , default: False ) \u2013 Convert datetimes to CA time before serialization. Returns: dict ( dict ) \u2013 Dictionary with all values JSON-serializable. Raises: ValueError \u2013 If input_dict is None. Examples: Input : {\"amount\": decimal.Decimal(\"1.23\"), \"date\": datetime.datetime(2025, 7, 4, 12, 0)} Output: {\"amount\": \"1.23\", \"date\": \"2025-07-04T12:00:00\"} Notes If input_dict is None, raises ValueError. If type_map is provided, values are cast to the specified types before serialization. Source code in arb\\utils\\json.py 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 def make_dict_serializeable ( input_dict : dict , type_map : dict [ str , type ] | None = None , convert_time_to_ca : bool = False ) -> dict : \"\"\" Transform a dictionary to ensure JSON compatibility of its values. Args: input_dict (dict): Original dictionary to process. If None, raises ValueError. type_map (dict[str, type] | None): Optional field-to-type map for casting. If not provided, no casting is performed. convert_time_to_ca (bool): Convert datetimes to CA time before serialization. Returns: dict: Dictionary with all values JSON-serializable. Raises: ValueError: If input_dict is None. Examples: Input : {\"amount\": decimal.Decimal(\"1.23\"), \"date\": datetime.datetime(2025, 7, 4, 12, 0)} Output: {\"amount\": \"1.23\", \"date\": \"2025-07-04T12:00:00\"} Notes: - If input_dict is None, raises ValueError. - If type_map is provided, values are cast to the specified types before serialization. \"\"\" result = {} for key , value in input_dict . items (): if not isinstance ( key , str ): raise TypeError ( f \"All keys must be strings. Invalid key: { key } ( { type ( key ) } )\" ) if type_map and key in type_map : try : value = safe_cast ( value , type_map [ key ]) except Exception as e : raise ValueError ( f \"Failed to cast key ' { key } ' to { type_map [ key ] } : { e } \" ) # todo - datetime - change this to ensure datetime is iso and fail if it is not if isinstance ( value , datetime . datetime ): if convert_time_to_ca : value = ca_naive_datetime_to_utc_datetime ( value ) value = value . isoformat () elif isinstance ( value , decimal . Decimal ): value = float ( value ) result [ key ] = value return result","title":"make_dict_serializeable"},{"location":"reference/arb/utils/json/#arb.utils.json.normalize_value","text":"Normalize a value for string-based diffing or comparison. Parameters: val ( Any ) \u2013 Value to normalize (any type). If None or empty string, returns \"\". If datetime, returns ISO string. Returns: str ( str ) \u2013 Normalized string value. None and empty string become \"\". Datetimes are ISO strings. Examples: Input : None Output: \"\" Input : \"\" Output: \"\" Input : datetime.datetime(2025, 1, 1, 12, 0) Output: \"2025-01-01T12:00:00\" Input : 123 Output: \"123\" Notes None and empty strings (\"\") are treated identically, returning \"\". Naive datetime values are assumed to be in California time and converted to UTC. All other types are stringified using str(val). Ensures fields that were previously None but now filled with an empty string (or vice versa) are not falsely flagged as changed. Datetime normalization is contract-compliant. Source code in arb\\utils\\json.py 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 def normalize_value ( val : Any ) -> str : \"\"\" Normalize a value for string-based diffing or comparison. Args: val: Value to normalize (any type). If None or empty string, returns \"\". If datetime, returns ISO string. Returns: str: Normalized string value. None and empty string become \"\". Datetimes are ISO strings. Examples: Input : None Output: \"\" Input : \"\" Output: \"\" Input : datetime.datetime(2025, 1, 1, 12, 0) Output: \"2025-01-01T12:00:00\" Input : 123 Output: \"123\" Notes: - None and empty strings (\"\") are treated identically, returning \"\". - Naive datetime values are assumed to be in California time and converted to UTC. - All other types are stringified using str(val). - Ensures fields that were previously None but now filled with an empty string (or vice versa) are not falsely flagged as changed. Datetime normalization is contract-compliant. \"\"\" from datetime import datetime from arb.utils.date_and_time import is_datetime_naive , ca_naive_datetime_to_utc_datetime if val is None or val == \"\" : return \"\" if isinstance ( val , datetime ): if is_datetime_naive ( val ): val = ca_naive_datetime_to_utc_datetime ( val ) return val . isoformat () return str ( val )","title":"normalize_value"},{"location":"reference/arb/utils/json/#arb.utils.json.safe_json_loads","text":"Safely parse a JSON string into a Python dictionary. Parameters: value ( str | dict | None ) \u2013 JSON-formatted string, pre-decoded dict, or None. If not str, dict, or None, raises TypeError. context_label ( str , default: '' ) \u2013 Optional label for diagnostics/logging. Returns: dict ( dict ) \u2013 Parsed dictionary from the input, or empty dict if input is None, invalid, or already a valid dict. Raises: TypeError \u2013 If value is not a str, dict, or None. Examples: Input : '{\"a\": 1, \"b\": 2}' Output: {\"a\": 1, \"b\": 2} Input : {\"a\": 1, \"b\": 2} Output: {\"a\": 1, \"b\": 2} Input : None Output: {} Input : \"\" Output: {} Input : \"not valid json\" Output: {} Notes If value is already a dict, it is returned unchanged. If value is None, empty, or invalid JSON, an empty dict is returned. If decoding fails, a warning is logged including the context_label. Source code in arb\\utils\\json.py 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 def safe_json_loads ( value : str | dict | None , context_label : str = \"\" ) -> dict : \"\"\" Safely parse a JSON string into a Python dictionary. Args: value (str | dict | None): JSON-formatted string, pre-decoded dict, or None. If not str, dict, or None, raises TypeError. context_label (str): Optional label for diagnostics/logging. Returns: dict: Parsed dictionary from the input, or empty dict if input is None, invalid, or already a valid dict. Raises: TypeError: If `value` is not a str, dict, or None. Examples: Input : '{\"a\": 1, \"b\": 2}' Output: {\"a\": 1, \"b\": 2} Input : {\"a\": 1, \"b\": 2} Output: {\"a\": 1, \"b\": 2} Input : None Output: {} Input : \"\" Output: {} Input : \"not valid json\" Output: {} Notes: - If `value` is already a dict, it is returned unchanged. - If `value` is None, empty, or invalid JSON, an empty dict is returned. - If decoding fails, a warning is logged including the context_label. \"\"\" if value is None or ( isinstance ( value , str ) and value . strip () == \"\" ): return {} if isinstance ( value , dict ): return value if not isinstance ( value , str ): raise TypeError ( f \"Expected str, dict, or None; got { type ( value ) . __name__ } \" ) try : return json . loads ( value ) except json . JSONDecodeError : label_msg = f \" ( { context_label } )\" if context_label else \"\" logger . warning ( f \"Corrupt or invalid JSON string encountered { label_msg } ; returning empty dict.\" ) return {}","title":"safe_json_loads"},{"location":"reference/arb/utils/json/#arb.utils.json.wtform_types_and_values","text":"Extract field types and current data values from a WTForm. Parameters: wtform ( FlaskForm ) \u2013 WTForms instance. Must not be None. Returns: tuple ( tuple [ dict [ str , type ], dict [ str , object ]] ) \u2013 dict[str, type]: Field name to type mapping for deserialization. dict[str, object]: Field name to current value mapping (may include 'Please Select'). Raises: ValueError \u2013 If wtform is None or does not have _fields attribute. Examples: Input : form (WTForms instance) Output: (type_map, field_data) where type_map is a dict of field types and field_data is a dict of field values Notes If wtform is None or invalid, raises ValueError. 'Please Select' is a valid value for SelectField. Source code in arb\\utils\\json.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 def wtform_types_and_values ( wtform ) -> tuple [ dict [ str , type ], dict [ str , object ]]: \"\"\" Extract field types and current data values from a WTForm. Args: wtform (FlaskForm): WTForms instance. Must not be None. Returns: tuple: - dict[str, type]: Field name to type mapping for deserialization. - dict[str, object]: Field name to current value mapping (may include 'Please Select'). Raises: ValueError: If wtform is None or does not have _fields attribute. Examples: Input : form (WTForms instance) Output: (type_map, field_data) where type_map is a dict of field types and field_data is a dict of field values Notes: - If wtform is None or invalid, raises ValueError. - 'Please Select' is a valid value for SelectField. \"\"\" if wtform is None or not hasattr ( wtform , '_fields' ): raise ValueError ( \"wtform must have a _fields attribute\" ) type_map = {} field_data = {} for name , field in wtform . _fields . items (): value = field . data field_data [ name ] = value if isinstance ( field , DateTimeField ): type_map [ name ] = datetime . datetime elif isinstance ( field , DecimalField ): type_map [ name ] = decimal . Decimal elif isinstance ( field , BooleanField ): type_map [ name ] = bool elif isinstance ( field , IntegerField ): type_map [ name ] = int elif isinstance ( field , SelectField ): type_map [ name ] = str # 'Please Select' is valid return type_map , field_data","title":"wtform_types_and_values"},{"location":"reference/arb/utils/log_util/","text":"arb.utils.log_util log_util.py Logging utilities to trace function calls and parameter values across the application. This module provides two main tools for logging function arguments: 1. log_function_parameters(): Logs the name and arguments of the current function. 2. log_parameters(): A decorator that logs all arguments of decorated functions. It also includes a logging filter, FlaskUserContextFilter, to inject the current Flask user into all log records when inside a request context. Features Logs arguments from both positional and keyword inputs. Automatically derives the correct logger based on caller/module context. Supports optional printing to stdout for real-time debugging. Integrates Flask g.user context when available, aiding request traceability. Intended Use Diagnostic tracing and observability in Flask applications. Debugging individual functions without modifying logic. Enhancing structured logging with contextual request user information. Dependencies Python standard library (inspect, logging) Flask (optional, for FlaskUserContextFilter) Version 1.0.0 Example Usage: Input : import arb.utils.log_util as log_util logging.basicConfig(level=logging.DEBUG) @log_parameters(print_to_console=True) def greet(name, lang=\"en\"): return f\"Hello {name} [{lang}]\" def example(): log_function_parameters(print_to_console=True) greet(\"Alice\", lang=\"fr\") example() Output greet(name='Alice', lang='fr') example() Recommendations: Use log_parameters for consistent tracing of function calls across modules. Use log_function_parameters for temporary instrumentation or detailed inline debugging. This module is safe to import in any Python project and requires only the standard library. FlaskUserContextFilter Bases: Filter Logging filter that injects Flask's g.user into log records, if available. This allows log formats to include the active Flask user for traceability. Adds record.user (str): User identifier from Flask's request context, or \"n/a\" if unavailable. Examples: Input : filter = FlaskUserContextFilter() logger.addFilter(filter) Output: Log records include a 'user' attribute with the Flask user or 'n/a'. Notes If Flask's request context is not available or g.user is missing, sets user to 'n/a'. Safe to use in non-Flask contexts; always sets a user attribute. Source code in arb\\utils\\log_util.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 class FlaskUserContextFilter ( logging . Filter ): \"\"\" Logging filter that injects Flask's `g.user` into log records, if available. This allows log formats to include the active Flask user for traceability. Adds: record.user (str): User identifier from Flask's request context, or \"n/a\" if unavailable. Examples: Input : filter = FlaskUserContextFilter() logger.addFilter(filter) Output: Log records include a 'user' attribute with the Flask user or 'n/a'. Notes: - If Flask's request context is not available or g.user is missing, sets user to 'n/a'. - Safe to use in non-Flask contexts; always sets a user attribute. \"\"\" def filter ( self , record ): if has_request_context () and hasattr ( g , \"user\" ): record . user = g . user else : record . user = \"n/a\" return True log_function_parameters ( logger = None , print_to_console = False ) Log the current function's name and arguments using debug-level logging. Parameters: logger ( Logger | None , default: None ) \u2013 Optional logger. If None, derives one from caller's module. If not a Logger instance, raises TypeError. print_to_console ( bool , default: False ) \u2013 If True, also print the message to stdout. Examples: Input : def example(a, b=2): log_function_parameters() example(1) Output: Logs: example(a=1, b=2) Input : logger=None, print_to_console=True Output: Prints and logs the function call Notes If logger is None, uses the caller's module logger. If logger is not a Logger instance, raises TypeError. Source code in arb\\utils\\log_util.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def log_function_parameters ( logger : logging . Logger | None = None , print_to_console : bool = False ) -> None : \"\"\" Log the current function's name and arguments using debug-level logging. Args: logger (logging.Logger | None): Optional logger. If None, derives one from caller's module. If not a Logger instance, raises TypeError. print_to_console (bool): If True, also print the message to stdout. Examples: Input : def example(a, b=2): log_function_parameters() example(1) Output: Logs: example(a=1, b=2) Input : logger=None, print_to_console=True Output: Prints and logs the function call Notes: - If logger is None, uses the caller's module logger. - If logger is not a Logger instance, raises TypeError. \"\"\" frame = inspect . currentframe () if frame is None or frame . f_back is None : logging . getLogger ( __name__ ) . warning ( \"log_function_parameters: Unable to access caller frame.\" ) return frame = frame . f_back func_name = frame . f_code . co_name if logger is None : module_name = frame . f_globals . get ( \"__name__\" , \"default_logger\" ) logger = logging . getLogger ( module_name ) args_info = inspect . getargvalues ( frame ) params = [] for arg in args_info . args : value = args_info . locals . get ( arg ) params . append ( f \" { arg } = { value !r} \" ) if args_info . varargs : value = args_info . locals . get ( args_info . varargs ) params . append ( f \" { args_info . varargs } = { value !r} \" ) if args_info . keywords : kwargs = args_info . locals . get ( args_info . keywords , {}) for k , v in kwargs . items (): params . append ( f \" { k } = { v !r} \" ) log_line = f \" { func_name } ( { ', ' . join ( params ) } )\" logger . debug ( log_line ) if print_to_console : print ( log_line ) log_parameters ( logger = None , print_to_console = False ) Decorator to log all arguments passed to a function upon each invocation. Parameters: logger ( Logger | None , default: None ) \u2013 Optional logger instance. Defaults to caller's module logger. If not a Logger instance, raises TypeError. print_to_console ( bool , default: False ) \u2013 If True, also prints the log message to stdout. Returns: Callable ( Callable ) \u2013 A decorator that logs parameter values each time the function is called. Examples: Input : @log_parameters(print_to_console=True) def greet(name, lang=\"en\"): return f\"Hello {name} [{lang}]\" greet(\"Alice\", lang=\"fr\") Output: greet(name='Alice', lang='fr') (logged and printed on each invocation) Input : logger=None, print_to_console=False Output: Only logs the function call Notes If logger is None, uses the function's module logger. If logger is not a Logger instance, raises TypeError. Source code in arb\\utils\\log_util.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def log_parameters ( logger : logging . Logger | None = None , print_to_console : bool = False ) -> Callable : \"\"\" Decorator to log all arguments passed to a function upon each invocation. Args: logger (logging.Logger | None): Optional logger instance. Defaults to caller's module logger. If not a Logger instance, raises TypeError. print_to_console (bool): If True, also prints the log message to stdout. Returns: Callable: A decorator that logs parameter values each time the function is called. Examples: Input : @log_parameters(print_to_console=True) def greet(name, lang=\"en\"): return f\"Hello {name} [{lang}]\" greet(\"Alice\", lang=\"fr\") Output: greet(name='Alice', lang='fr') (logged and printed on each invocation) Input : logger=None, print_to_console=False Output: Only logs the function call Notes: - If logger is None, uses the function's module logger. - If logger is not a Logger instance, raises TypeError. \"\"\" def decorator ( func : Callable ) -> Callable : @wraps ( func ) def wrapper ( * args , ** kwargs ): nonlocal logger if logger is None : logger = logging . getLogger ( func . __module__ ) bound = inspect . signature ( func ) . bind ( * args , ** kwargs ) bound . apply_defaults () param_str = \", \" . join ( f \" { k } = { v !r} \" for k , v in bound . arguments . items ()) log_line = f \" { func . __name__ } ( { param_str } )\" logger . debug ( log_line ) if print_to_console : print ( log_line ) return func ( * args , ** kwargs ) return wrapper return decorator","title":"arb.utils.log_util"},{"location":"reference/arb/utils/log_util/#arbutilslog_util","text":"log_util.py Logging utilities to trace function calls and parameter values across the application. This module provides two main tools for logging function arguments: 1. log_function_parameters(): Logs the name and arguments of the current function. 2. log_parameters(): A decorator that logs all arguments of decorated functions. It also includes a logging filter, FlaskUserContextFilter, to inject the current Flask user into all log records when inside a request context. Features Logs arguments from both positional and keyword inputs. Automatically derives the correct logger based on caller/module context. Supports optional printing to stdout for real-time debugging. Integrates Flask g.user context when available, aiding request traceability. Intended Use Diagnostic tracing and observability in Flask applications. Debugging individual functions without modifying logic. Enhancing structured logging with contextual request user information. Dependencies Python standard library (inspect, logging) Flask (optional, for FlaskUserContextFilter) Version 1.0.0","title":"arb.utils.log_util"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util--example-usage","text":"Input : import arb.utils.log_util as log_util logging.basicConfig(level=logging.DEBUG) @log_parameters(print_to_console=True) def greet(name, lang=\"en\"): return f\"Hello {name} [{lang}]\" def example(): log_function_parameters(print_to_console=True) greet(\"Alice\", lang=\"fr\") example() Output greet(name='Alice', lang='fr') example()","title":"Example Usage:"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util--recommendations","text":"Use log_parameters for consistent tracing of function calls across modules. Use log_function_parameters for temporary instrumentation or detailed inline debugging. This module is safe to import in any Python project and requires only the standard library.","title":"Recommendations:"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util.FlaskUserContextFilter","text":"Bases: Filter Logging filter that injects Flask's g.user into log records, if available. This allows log formats to include the active Flask user for traceability. Adds record.user (str): User identifier from Flask's request context, or \"n/a\" if unavailable. Examples: Input : filter = FlaskUserContextFilter() logger.addFilter(filter) Output: Log records include a 'user' attribute with the Flask user or 'n/a'. Notes If Flask's request context is not available or g.user is missing, sets user to 'n/a'. Safe to use in non-Flask contexts; always sets a user attribute. Source code in arb\\utils\\log_util.py 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 class FlaskUserContextFilter ( logging . Filter ): \"\"\" Logging filter that injects Flask's `g.user` into log records, if available. This allows log formats to include the active Flask user for traceability. Adds: record.user (str): User identifier from Flask's request context, or \"n/a\" if unavailable. Examples: Input : filter = FlaskUserContextFilter() logger.addFilter(filter) Output: Log records include a 'user' attribute with the Flask user or 'n/a'. Notes: - If Flask's request context is not available or g.user is missing, sets user to 'n/a'. - Safe to use in non-Flask contexts; always sets a user attribute. \"\"\" def filter ( self , record ): if has_request_context () and hasattr ( g , \"user\" ): record . user = g . user else : record . user = \"n/a\" return True","title":"FlaskUserContextFilter"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util.log_function_parameters","text":"Log the current function's name and arguments using debug-level logging. Parameters: logger ( Logger | None , default: None ) \u2013 Optional logger. If None, derives one from caller's module. If not a Logger instance, raises TypeError. print_to_console ( bool , default: False ) \u2013 If True, also print the message to stdout. Examples: Input : def example(a, b=2): log_function_parameters() example(1) Output: Logs: example(a=1, b=2) Input : logger=None, print_to_console=True Output: Prints and logs the function call Notes If logger is None, uses the caller's module logger. If logger is not a Logger instance, raises TypeError. Source code in arb\\utils\\log_util.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def log_function_parameters ( logger : logging . Logger | None = None , print_to_console : bool = False ) -> None : \"\"\" Log the current function's name and arguments using debug-level logging. Args: logger (logging.Logger | None): Optional logger. If None, derives one from caller's module. If not a Logger instance, raises TypeError. print_to_console (bool): If True, also print the message to stdout. Examples: Input : def example(a, b=2): log_function_parameters() example(1) Output: Logs: example(a=1, b=2) Input : logger=None, print_to_console=True Output: Prints and logs the function call Notes: - If logger is None, uses the caller's module logger. - If logger is not a Logger instance, raises TypeError. \"\"\" frame = inspect . currentframe () if frame is None or frame . f_back is None : logging . getLogger ( __name__ ) . warning ( \"log_function_parameters: Unable to access caller frame.\" ) return frame = frame . f_back func_name = frame . f_code . co_name if logger is None : module_name = frame . f_globals . get ( \"__name__\" , \"default_logger\" ) logger = logging . getLogger ( module_name ) args_info = inspect . getargvalues ( frame ) params = [] for arg in args_info . args : value = args_info . locals . get ( arg ) params . append ( f \" { arg } = { value !r} \" ) if args_info . varargs : value = args_info . locals . get ( args_info . varargs ) params . append ( f \" { args_info . varargs } = { value !r} \" ) if args_info . keywords : kwargs = args_info . locals . get ( args_info . keywords , {}) for k , v in kwargs . items (): params . append ( f \" { k } = { v !r} \" ) log_line = f \" { func_name } ( { ', ' . join ( params ) } )\" logger . debug ( log_line ) if print_to_console : print ( log_line )","title":"log_function_parameters"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util.log_parameters","text":"Decorator to log all arguments passed to a function upon each invocation. Parameters: logger ( Logger | None , default: None ) \u2013 Optional logger instance. Defaults to caller's module logger. If not a Logger instance, raises TypeError. print_to_console ( bool , default: False ) \u2013 If True, also prints the log message to stdout. Returns: Callable ( Callable ) \u2013 A decorator that logs parameter values each time the function is called. Examples: Input : @log_parameters(print_to_console=True) def greet(name, lang=\"en\"): return f\"Hello {name} [{lang}]\" greet(\"Alice\", lang=\"fr\") Output: greet(name='Alice', lang='fr') (logged and printed on each invocation) Input : logger=None, print_to_console=False Output: Only logs the function call Notes If logger is None, uses the function's module logger. If logger is not a Logger instance, raises TypeError. Source code in arb\\utils\\log_util.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def log_parameters ( logger : logging . Logger | None = None , print_to_console : bool = False ) -> Callable : \"\"\" Decorator to log all arguments passed to a function upon each invocation. Args: logger (logging.Logger | None): Optional logger instance. Defaults to caller's module logger. If not a Logger instance, raises TypeError. print_to_console (bool): If True, also prints the log message to stdout. Returns: Callable: A decorator that logs parameter values each time the function is called. Examples: Input : @log_parameters(print_to_console=True) def greet(name, lang=\"en\"): return f\"Hello {name} [{lang}]\" greet(\"Alice\", lang=\"fr\") Output: greet(name='Alice', lang='fr') (logged and printed on each invocation) Input : logger=None, print_to_console=False Output: Only logs the function call Notes: - If logger is None, uses the function's module logger. - If logger is not a Logger instance, raises TypeError. \"\"\" def decorator ( func : Callable ) -> Callable : @wraps ( func ) def wrapper ( * args , ** kwargs ): nonlocal logger if logger is None : logger = logging . getLogger ( func . __module__ ) bound = inspect . signature ( func ) . bind ( * args , ** kwargs ) bound . apply_defaults () param_str = \", \" . join ( f \" { k } = { v !r} \" for k , v in bound . arguments . items ()) log_line = f \" { func . __name__ } ( { param_str } )\" logger . debug ( log_line ) if print_to_console : print ( log_line ) return func ( * args , ** kwargs ) return wrapper return decorator","title":"log_parameters"},{"location":"reference/arb/utils/misc/","text":"arb.utils.misc Miscellaneous utility functions for the ARB Feedback Portal. This module provides reusable helpers for dictionary traversal, default injection, argument formatting, exception logging, and safe type casting. These utilities are designed for both Flask and CLI-based Python applications, improving code reuse and diagnostic traceability. Features: - Safe access to deeply nested dictionary values - Default key/value injection for sub-dictionaries - In-place replacement of list values using a mapping - Argument list formatting for CLI or logging - Exception logging with full traceback - Type-safe value casting Intended use: - Shared helpers for ARB portal and related utilities - Promotes DRY principles and robust error handling Dependencies: - Python standard library - Logging provided by arb.__get_logger Version: 1.0.0 args_to_string ( args ) Convert a list or tuple of arguments into a single space-separated string with padding. Parameters: args ( list | tuple | None ) \u2013 Arguments to convert. If None or empty, returns an empty string. Returns: str ( str ) \u2013 Space-separated string representation, or empty string if args is None or empty. Examples: Input : [\"--debug\", \"--log\", \"file.txt\"] Output: \" --debug --log file.txt \" Input : None Output: \"\" Input : [] Output: \"\" Notes If args is None or empty, returns an empty string. Source code in arb\\utils\\misc.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def args_to_string ( args : list | tuple | None ) -> str : \"\"\" Convert a list or tuple of arguments into a single space-separated string with padding. Args: args (list | tuple | None): Arguments to convert. If None or empty, returns an empty string. Returns: str: Space-separated string representation, or empty string if args is None or empty. Examples: Input : [\"--debug\", \"--log\", \"file.txt\"] Output: \" --debug --log file.txt \" Input : None Output: \"\" Input : [] Output: \"\" Notes: - If `args` is None or empty, returns an empty string. \"\"\" if not args : return '' else : args = [ str ( arg ) for arg in args ] return_string = \" \" + \" \" . join ( args ) + \" \" return return_string ensure_key_value_pair ( dict_ , default_dict , sub_key ) Ensure each sub-dictionary in dict_ has a given key, populating it from default_dict if missing. Parameters: dict_ ( dict [ str , dict ] ) \u2013 A dictionary whose values are sub-dictionaries. Must not be None. default_dict ( dict ) \u2013 A lookup dictionary to supply missing key-value pairs. Must not be None. sub_key ( str ) \u2013 The key that must exist in each sub-dictionary. If None or empty, raises ValueError. Raises: TypeError \u2013 If the sub_key is missing, and no fallback is found in default_dict. ValueError \u2013 If sub_key is None or empty. Examples: Input : dict_ = {'a': {'x': 1}, 'b': {'x': 2}, 'c': {}}, default_dict = {'c': 99}, sub_key = 'x' Output: dict_['c']['x'] == 99 Notes If sub_key is None or empty, raises ValueError. If dict_ or default_dict is None, raises TypeError. Source code in arb\\utils\\misc.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def ensure_key_value_pair ( dict_ : dict [ str , dict ], default_dict : dict , sub_key : str ) -> None : \"\"\" Ensure each sub-dictionary in dict_ has a given key, populating it from default_dict if missing. Args: dict_ (dict[str, dict]): A dictionary whose values are sub-dictionaries. Must not be None. default_dict (dict): A lookup dictionary to supply missing key-value pairs. Must not be None. sub_key (str): The key that must exist in each sub-dictionary. If None or empty, raises ValueError. Raises: TypeError: If the sub_key is missing, and no fallback is found in default_dict. ValueError: If `sub_key` is None or empty. Examples: Input : dict_ = {'a': {'x': 1}, 'b': {'x': 2}, 'c': {}}, default_dict = {'c': 99}, sub_key = 'x' Output: dict_['c']['x'] == 99 Notes: - If `sub_key` is None or empty, raises ValueError. - If `dict_` or `default_dict` is None, raises TypeError. \"\"\" for key , sub_dict in dict_ . items (): logger . debug ( f \" { key =} , { sub_dict =} \" ) if sub_key not in sub_dict : if key in default_dict : sub_dict [ sub_key ] = default_dict [ key ] else : raise TypeError ( f \" { sub_key } is not present in sub dictionary for key ' { key } ' \" f \"and no default provided in default_dict\" ) get_nested_value ( nested_dict , keys ) Retrieve a value from a nested dictionary using a key path. Parameters: nested_dict ( dict ) \u2013 The dictionary to search. Must not be None. keys ( list | tuple | str ) \u2013 A sequence of keys to traverse the dictionary, or a single key. If None or empty, raises ValueError. Returns: object ( object ) \u2013 The value found at the specified key path. Raises: KeyError \u2013 If a key is missing at any level. TypeError \u2013 If a non-dictionary value is encountered mid-traversal. ValueError \u2013 If keys is None or empty. Examples: Input : {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99}, (\"a\", \"b\", \"c\") Output: 42 Input : {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99}, \"x\" Output: 99 Notes If keys is None or empty, raises ValueError. If nested_dict is None, raises TypeError. Source code in arb\\utils\\misc.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_nested_value ( nested_dict : dict , keys : list | tuple | str ) -> object : \"\"\" Retrieve a value from a nested dictionary using a key path. Args: nested_dict (dict): The dictionary to search. Must not be None. keys (list | tuple | str): A sequence of keys to traverse the dictionary, or a single key. If None or empty, raises ValueError. Returns: object: The value found at the specified key path. Raises: KeyError: If a key is missing at any level. TypeError: If a non-dictionary value is encountered mid-traversal. ValueError: If `keys` is None or empty. Examples: Input : {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99}, (\"a\", \"b\", \"c\") Output: 42 Input : {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99}, \"x\" Output: 99 Notes: - If `keys` is None or empty, raises ValueError. - If `nested_dict` is None, raises TypeError. \"\"\" if not isinstance ( keys , ( list , tuple )): # Single key case if keys not in nested_dict : raise KeyError ( f \"Key ' { keys } ' not found in the dictionary\" ) return nested_dict [ keys ] current = nested_dict for key in keys : if not isinstance ( current , dict ): raise TypeError ( f \"Expected a dictionary at key ' { key } ', found { type ( current ) . __name__ } \" ) if key not in current : raise KeyError ( f \"Key ' { key } ' not found in the dictionary\" ) current = current [ key ] return current log_error ( e ) Log an exception and its stack trace, then re-raise the exception. Parameters: e ( Exception ) \u2013 The exception to log. Must not be None. Raises: Exception \u2013 Always re-raises the input exception after logging. ValueError \u2013 If e is None. Examples: Input : ValueError(\"bad value\") Output: Logs error and traceback, then raises ValueError Notes Outputs full traceback to logger. Re-raises the original exception. If e is None, raises ValueError. Source code in arb\\utils\\misc.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def log_error ( e : Exception ) -> None : \"\"\" Log an exception and its stack trace, then re-raise the exception. Args: e (Exception): The exception to log. Must not be None. Raises: Exception: Always re-raises the input exception after logging. ValueError: If `e` is None. Examples: Input : ValueError(\"bad value\") Output: Logs error and traceback, then raises ValueError Notes: - Outputs full traceback to logger. - Re-raises the original exception. - If `e` is None, raises ValueError. \"\"\" logger . error ( e , exc_info = True ) stack = traceback . extract_stack () logger . error ( stack ) raise e replace_list_occurrences ( list_ , lookup_dict ) Replace elements of a list in-place using a lookup dictionary. Parameters: list_ ( list ) \u2013 The list whose elements may be replaced. If None, raises ValueError. lookup_dict ( dict ) \u2013 A dictionary mapping old values to new values. If None, raises ValueError. Raises: ValueError \u2013 If list_ or lookup_dict is None. Examples: Input : list_ = [\"cat\", \"dog\", \"bird\"], lookup_dict = {\"dog\": \"puppy\", \"bird\": \"parrot\"} Output: list_ becomes ['cat', 'puppy', 'parrot'] Notes If list_ or lookup_dict is None, raises ValueError. Source code in arb\\utils\\misc.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def replace_list_occurrences ( list_ : list , lookup_dict : dict ) -> None : \"\"\" Replace elements of a list in-place using a lookup dictionary. Args: list_ (list): The list whose elements may be replaced. If None, raises ValueError. lookup_dict (dict): A dictionary mapping old values to new values. If None, raises ValueError. Raises: ValueError: If `list_` or `lookup_dict` is None. Examples: Input : list_ = [\"cat\", \"dog\", \"bird\"], lookup_dict = {\"dog\": \"puppy\", \"bird\": \"parrot\"} Output: list_ becomes ['cat', 'puppy', 'parrot'] Notes: - If `list_` or `lookup_dict` is None, raises ValueError. \"\"\" for i in range ( len ( list_ )): if list_ [ i ] in lookup_dict : list_ [ i ] = lookup_dict [ list_ [ i ]] safe_cast ( value , expected_type ) Cast a value to the expected type only if it's not already of that type. Parameters: value ( Any ) \u2013 The value to check and potentially cast. If None, attempts to cast None. expected_type ( type ) \u2013 The target Python type to cast to. Must not be None. Returns: Any ( Any ) \u2013 The original or casted value. Raises: ValueError \u2013 If the cast fails or is inappropriate for the type, or if expected_type is None. Examples: Input : \"123\", int Output: 123 Input : 123, int Output: 123 Input : None, int Output: 0 (if int(None) is allowed, else raises ValueError) Notes If expected_type is None, raises ValueError. If value is already of expected_type , returns it unchanged. If cast fails, raises ValueError. Source code in arb\\utils\\misc.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def safe_cast ( value : Any , expected_type : type ) -> Any : \"\"\" Cast a value to the expected type only if it's not already of that type. Args: value (Any): The value to check and potentially cast. If None, attempts to cast None. expected_type (type): The target Python type to cast to. Must not be None. Returns: Any: The original or casted value. Raises: ValueError: If the cast fails or is inappropriate for the type, or if `expected_type` is None. Examples: Input : \"123\", int Output: 123 Input : 123, int Output: 123 Input : None, int Output: 0 (if int(None) is allowed, else raises ValueError) Notes: - If `expected_type` is None, raises ValueError. - If `value` is already of `expected_type`, returns it unchanged. - If cast fails, raises ValueError. \"\"\" try : if not isinstance ( value , expected_type ): value = expected_type ( value ) return value except Exception as e : raise ValueError ( f \"Failed to cast value { value !r} to { expected_type } : { e } \" )","title":"arb.utils.misc"},{"location":"reference/arb/utils/misc/#arbutilsmisc","text":"Miscellaneous utility functions for the ARB Feedback Portal. This module provides reusable helpers for dictionary traversal, default injection, argument formatting, exception logging, and safe type casting. These utilities are designed for both Flask and CLI-based Python applications, improving code reuse and diagnostic traceability. Features: - Safe access to deeply nested dictionary values - Default key/value injection for sub-dictionaries - In-place replacement of list values using a mapping - Argument list formatting for CLI or logging - Exception logging with full traceback - Type-safe value casting Intended use: - Shared helpers for ARB portal and related utilities - Promotes DRY principles and robust error handling Dependencies: - Python standard library - Logging provided by arb.__get_logger Version: 1.0.0","title":"arb.utils.misc"},{"location":"reference/arb/utils/misc/#arb.utils.misc.args_to_string","text":"Convert a list or tuple of arguments into a single space-separated string with padding. Parameters: args ( list | tuple | None ) \u2013 Arguments to convert. If None or empty, returns an empty string. Returns: str ( str ) \u2013 Space-separated string representation, or empty string if args is None or empty. Examples: Input : [\"--debug\", \"--log\", \"file.txt\"] Output: \" --debug --log file.txt \" Input : None Output: \"\" Input : [] Output: \"\" Notes If args is None or empty, returns an empty string. Source code in arb\\utils\\misc.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def args_to_string ( args : list | tuple | None ) -> str : \"\"\" Convert a list or tuple of arguments into a single space-separated string with padding. Args: args (list | tuple | None): Arguments to convert. If None or empty, returns an empty string. Returns: str: Space-separated string representation, or empty string if args is None or empty. Examples: Input : [\"--debug\", \"--log\", \"file.txt\"] Output: \" --debug --log file.txt \" Input : None Output: \"\" Input : [] Output: \"\" Notes: - If `args` is None or empty, returns an empty string. \"\"\" if not args : return '' else : args = [ str ( arg ) for arg in args ] return_string = \" \" + \" \" . join ( args ) + \" \" return return_string","title":"args_to_string"},{"location":"reference/arb/utils/misc/#arb.utils.misc.ensure_key_value_pair","text":"Ensure each sub-dictionary in dict_ has a given key, populating it from default_dict if missing. Parameters: dict_ ( dict [ str , dict ] ) \u2013 A dictionary whose values are sub-dictionaries. Must not be None. default_dict ( dict ) \u2013 A lookup dictionary to supply missing key-value pairs. Must not be None. sub_key ( str ) \u2013 The key that must exist in each sub-dictionary. If None or empty, raises ValueError. Raises: TypeError \u2013 If the sub_key is missing, and no fallback is found in default_dict. ValueError \u2013 If sub_key is None or empty. Examples: Input : dict_ = {'a': {'x': 1}, 'b': {'x': 2}, 'c': {}}, default_dict = {'c': 99}, sub_key = 'x' Output: dict_['c']['x'] == 99 Notes If sub_key is None or empty, raises ValueError. If dict_ or default_dict is None, raises TypeError. Source code in arb\\utils\\misc.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def ensure_key_value_pair ( dict_ : dict [ str , dict ], default_dict : dict , sub_key : str ) -> None : \"\"\" Ensure each sub-dictionary in dict_ has a given key, populating it from default_dict if missing. Args: dict_ (dict[str, dict]): A dictionary whose values are sub-dictionaries. Must not be None. default_dict (dict): A lookup dictionary to supply missing key-value pairs. Must not be None. sub_key (str): The key that must exist in each sub-dictionary. If None or empty, raises ValueError. Raises: TypeError: If the sub_key is missing, and no fallback is found in default_dict. ValueError: If `sub_key` is None or empty. Examples: Input : dict_ = {'a': {'x': 1}, 'b': {'x': 2}, 'c': {}}, default_dict = {'c': 99}, sub_key = 'x' Output: dict_['c']['x'] == 99 Notes: - If `sub_key` is None or empty, raises ValueError. - If `dict_` or `default_dict` is None, raises TypeError. \"\"\" for key , sub_dict in dict_ . items (): logger . debug ( f \" { key =} , { sub_dict =} \" ) if sub_key not in sub_dict : if key in default_dict : sub_dict [ sub_key ] = default_dict [ key ] else : raise TypeError ( f \" { sub_key } is not present in sub dictionary for key ' { key } ' \" f \"and no default provided in default_dict\" )","title":"ensure_key_value_pair"},{"location":"reference/arb/utils/misc/#arb.utils.misc.get_nested_value","text":"Retrieve a value from a nested dictionary using a key path. Parameters: nested_dict ( dict ) \u2013 The dictionary to search. Must not be None. keys ( list | tuple | str ) \u2013 A sequence of keys to traverse the dictionary, or a single key. If None or empty, raises ValueError. Returns: object ( object ) \u2013 The value found at the specified key path. Raises: KeyError \u2013 If a key is missing at any level. TypeError \u2013 If a non-dictionary value is encountered mid-traversal. ValueError \u2013 If keys is None or empty. Examples: Input : {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99}, (\"a\", \"b\", \"c\") Output: 42 Input : {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99}, \"x\" Output: 99 Notes If keys is None or empty, raises ValueError. If nested_dict is None, raises TypeError. Source code in arb\\utils\\misc.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_nested_value ( nested_dict : dict , keys : list | tuple | str ) -> object : \"\"\" Retrieve a value from a nested dictionary using a key path. Args: nested_dict (dict): The dictionary to search. Must not be None. keys (list | tuple | str): A sequence of keys to traverse the dictionary, or a single key. If None or empty, raises ValueError. Returns: object: The value found at the specified key path. Raises: KeyError: If a key is missing at any level. TypeError: If a non-dictionary value is encountered mid-traversal. ValueError: If `keys` is None or empty. Examples: Input : {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99}, (\"a\", \"b\", \"c\") Output: 42 Input : {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99}, \"x\" Output: 99 Notes: - If `keys` is None or empty, raises ValueError. - If `nested_dict` is None, raises TypeError. \"\"\" if not isinstance ( keys , ( list , tuple )): # Single key case if keys not in nested_dict : raise KeyError ( f \"Key ' { keys } ' not found in the dictionary\" ) return nested_dict [ keys ] current = nested_dict for key in keys : if not isinstance ( current , dict ): raise TypeError ( f \"Expected a dictionary at key ' { key } ', found { type ( current ) . __name__ } \" ) if key not in current : raise KeyError ( f \"Key ' { key } ' not found in the dictionary\" ) current = current [ key ] return current","title":"get_nested_value"},{"location":"reference/arb/utils/misc/#arb.utils.misc.log_error","text":"Log an exception and its stack trace, then re-raise the exception. Parameters: e ( Exception ) \u2013 The exception to log. Must not be None. Raises: Exception \u2013 Always re-raises the input exception after logging. ValueError \u2013 If e is None. Examples: Input : ValueError(\"bad value\") Output: Logs error and traceback, then raises ValueError Notes Outputs full traceback to logger. Re-raises the original exception. If e is None, raises ValueError. Source code in arb\\utils\\misc.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def log_error ( e : Exception ) -> None : \"\"\" Log an exception and its stack trace, then re-raise the exception. Args: e (Exception): The exception to log. Must not be None. Raises: Exception: Always re-raises the input exception after logging. ValueError: If `e` is None. Examples: Input : ValueError(\"bad value\") Output: Logs error and traceback, then raises ValueError Notes: - Outputs full traceback to logger. - Re-raises the original exception. - If `e` is None, raises ValueError. \"\"\" logger . error ( e , exc_info = True ) stack = traceback . extract_stack () logger . error ( stack ) raise e","title":"log_error"},{"location":"reference/arb/utils/misc/#arb.utils.misc.replace_list_occurrences","text":"Replace elements of a list in-place using a lookup dictionary. Parameters: list_ ( list ) \u2013 The list whose elements may be replaced. If None, raises ValueError. lookup_dict ( dict ) \u2013 A dictionary mapping old values to new values. If None, raises ValueError. Raises: ValueError \u2013 If list_ or lookup_dict is None. Examples: Input : list_ = [\"cat\", \"dog\", \"bird\"], lookup_dict = {\"dog\": \"puppy\", \"bird\": \"parrot\"} Output: list_ becomes ['cat', 'puppy', 'parrot'] Notes If list_ or lookup_dict is None, raises ValueError. Source code in arb\\utils\\misc.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def replace_list_occurrences ( list_ : list , lookup_dict : dict ) -> None : \"\"\" Replace elements of a list in-place using a lookup dictionary. Args: list_ (list): The list whose elements may be replaced. If None, raises ValueError. lookup_dict (dict): A dictionary mapping old values to new values. If None, raises ValueError. Raises: ValueError: If `list_` or `lookup_dict` is None. Examples: Input : list_ = [\"cat\", \"dog\", \"bird\"], lookup_dict = {\"dog\": \"puppy\", \"bird\": \"parrot\"} Output: list_ becomes ['cat', 'puppy', 'parrot'] Notes: - If `list_` or `lookup_dict` is None, raises ValueError. \"\"\" for i in range ( len ( list_ )): if list_ [ i ] in lookup_dict : list_ [ i ] = lookup_dict [ list_ [ i ]]","title":"replace_list_occurrences"},{"location":"reference/arb/utils/misc/#arb.utils.misc.safe_cast","text":"Cast a value to the expected type only if it's not already of that type. Parameters: value ( Any ) \u2013 The value to check and potentially cast. If None, attempts to cast None. expected_type ( type ) \u2013 The target Python type to cast to. Must not be None. Returns: Any ( Any ) \u2013 The original or casted value. Raises: ValueError \u2013 If the cast fails or is inappropriate for the type, or if expected_type is None. Examples: Input : \"123\", int Output: 123 Input : 123, int Output: 123 Input : None, int Output: 0 (if int(None) is allowed, else raises ValueError) Notes If expected_type is None, raises ValueError. If value is already of expected_type , returns it unchanged. If cast fails, raises ValueError. Source code in arb\\utils\\misc.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def safe_cast ( value : Any , expected_type : type ) -> Any : \"\"\" Cast a value to the expected type only if it's not already of that type. Args: value (Any): The value to check and potentially cast. If None, attempts to cast None. expected_type (type): The target Python type to cast to. Must not be None. Returns: Any: The original or casted value. Raises: ValueError: If the cast fails or is inappropriate for the type, or if `expected_type` is None. Examples: Input : \"123\", int Output: 123 Input : 123, int Output: 123 Input : None, int Output: 0 (if int(None) is allowed, else raises ValueError) Notes: - If `expected_type` is None, raises ValueError. - If `value` is already of `expected_type`, returns it unchanged. - If cast fails, raises ValueError. \"\"\" try : if not isinstance ( value , expected_type ): value = expected_type ( value ) return value except Exception as e : raise ValueError ( f \"Failed to cast value { value !r} to { expected_type } : { e } \" )","title":"safe_cast"},{"location":"reference/arb/utils/sql_alchemy/","text":"arb.utils.sql_alchemy SQLAlchemy utility functions for Flask applications using declarative or automap base models. This module provides introspection, diagnostics, and model-row operations for SQLAlchemy-based Flask apps. It supports both declarative and automapped models. Included Utilities: Model diagnostics ( sa_model_diagnostics ) Field and column type inspection ( get_sa_fields , get_sa_column_types ) Full automap type mapping ( get_sa_automap_types ) Dictionary conversions ( sa_model_to_dict , sa_model_dict_compare ) Table-to-dict exports ( table_to_list ) Table/class lookups ( get_class_from_table_name ) Row fetch and sort utilities ( get_rows_by_table_name ) Model add/delete with logging ( add_commit_and_log_model , delete_commit_and_log_model ) Foreign key traversal ( get_foreign_value ) PostgresQL sequence inspection ( find_auto_increment_value ) JSON column loader ( load_model_json_column ) Type Hints: db (SQLAlchemy) : Flask-SQLAlchemy database instance with .session and .engine. base (DeclarativeMeta) : Declarative or automapped SQLAlchemy base (e.g., via automap_base()). model (DeclarativeMeta) : SQLAlchemy ORM model class or instance. session (Session) : SQLAlchemy session object. Usage Notes: Supports PostgresQL features like sequence inspection via pg_get_serial_sequence . Logging is integrated for debugging and auditing. Compatible with Python 3.10+ syntax (PEP 604 union types). Version 1.0.0 add_commit_and_log_model ( db , model_row , comment = '' , model_before = None ) Add or update a model instance in the database, log changes, and commit. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. Must not be None. model_row ( AutomapBase ) \u2013 ORM model instance to add or update. Must not be None. comment ( str , default: '' ) \u2013 Optional log comment for auditing. model_before ( dict | None , default: None ) \u2013 Optional snapshot of the model before changes. Returns: None \u2013 None Examples: Input : db, user_row, comment=\"Adding user\" Output: Adds or updates user_row in the database and logs changes Input : db, None Output: Exception Input : None, user_row Output: Exception Notes If db or model_row is None, an exception will be raised. Source code in arb\\utils\\sql_alchemy.py 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def add_commit_and_log_model ( db : SQLAlchemy , model_row : AutomapBase , comment : str = \"\" , model_before : dict | None = None ) -> None : \"\"\" Add or update a model instance in the database, log changes, and commit. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. Must not be None. model_row (AutomapBase): ORM model instance to add or update. Must not be None. comment (str): Optional log comment for auditing. model_before (dict | None): Optional snapshot of the model before changes. Returns: None Examples: Input : db, user_row, comment=\"Adding user\" Output: Adds or updates user_row in the database and logs changes Input : db, None Output: Exception Input : None, user_row Output: Exception Notes: - If `db` or `model_row` is None, an exception will be raised. \"\"\" # todo (update) - use the payload routine apply_json_patch_and_log if model_before : logger . info ( f \"Before commit { comment =} : { model_before } \" ) try : db . session . add ( model_row ) db . session . commit () model_after = sa_model_to_dict ( model_row ) logger . info ( f \"After commit: { model_after } \" ) if model_before : changes = sa_model_dict_compare ( model_before , model_after ) logger . info ( f \"Changed values: { changes } \" ) except Exception as e : log_error ( e ) delete_commit_and_log_model ( db , model_row , comment = '' ) Delete a model instance from the database, log the operation, and commit the change. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. Must not be None. model_row ( AutomapBase ) \u2013 ORM model instance to delete. Must not be None. comment ( str , default: '' ) \u2013 Optional log comment for auditing. Returns: None \u2013 None Examples: Input : db, user_row, comment=\"Deleting user\" Output: Deletes user_row from the database and logs the operation Input : db, None Output: Exception Input : None, user_row Output: Exception Notes If db or model_row is None, an exception will be raised. Source code in arb\\utils\\sql_alchemy.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 def delete_commit_and_log_model ( db : SQLAlchemy , model_row : AutomapBase , comment : str = \"\" ) -> None : \"\"\" Delete a model instance from the database, log the operation, and commit the change. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. Must not be None. model_row (AutomapBase): ORM model instance to delete. Must not be None. comment (str): Optional log comment for auditing. Returns: None Examples: Input : db, user_row, comment=\"Deleting user\" Output: Deletes user_row from the database and logs the operation Input : db, None Output: Exception Input : None, user_row Output: Exception Notes: - If `db` or `model_row` is None, an exception will be raised. \"\"\" # todo (update) - use the payload routine apply_json_patch_and_log and or some way to track change logger . info ( f \"Deleting model { comment =} : { sa_model_to_dict ( model_row ) } \" ) try : db . session . delete ( model_row ) db . session . commit () except Exception as e : log_error ( e ) find_auto_increment_value ( db , table_name , column_name ) Find the next auto-increment value for a table column (PostgresQL only). Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. table_name ( str ) \u2013 Table name. column_name ( str ) \u2013 Column name. Returns: str ( str ) \u2013 Human-readable summary of the next sequence value. Example summary = find_auto_increment_value(db, 'users', 'id') Source code in arb\\utils\\sql_alchemy.py 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 def find_auto_increment_value ( db : SQLAlchemy , table_name : str , column_name : str ) -> str : \"\"\" Find the next auto-increment value for a table column (PostgresQL only). Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. table_name (str): Table name. column_name (str): Column name. Returns: str: Human-readable summary of the next sequence value. Example: summary = find_auto_increment_value(db, 'users', 'id') \"\"\" with db . engine . connect () as connection : sql_seq = f \"SELECT pg_get_serial_sequence(' { table_name } ', ' { column_name } ')\" sequence_name = connection . execute ( text ( sql_seq )) . scalar () sql_nextval = f \"SELECT nextval(' { sequence_name } ')\" next_val = connection . execute ( text ( sql_nextval )) . scalar () return f \"Table ' { table_name } ' column ' { column_name } ' sequence ' { sequence_name } ' next value is ' { next_val } '\" get_class_from_table_name ( base , table_name ) Retrieve the mapped ORM class for a given table name from an automap base. Parameters: base ( AutomapBase ) \u2013 SQLAlchemy AutomapBase. Must not be None. table_name ( str ) \u2013 Database table name. If None or empty, returns None. Returns: AutomapBase | None \u2013 AutomapBase | None: Mapped SQLAlchemy ORM class, or None if not found. Examples: Input : base, 'users' Output: Input : base, None Output: None Input : None, 'users' Output: None Notes Uses Base.metadata and registry to find the mapped class. If the table is not found, returns None. If base is None, returns None. If table_name is None or empty, returns None. Source code in arb\\utils\\sql_alchemy.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 def get_class_from_table_name ( base : AutomapBase | None , table_name : str ) -> AutomapBase | None : \"\"\" Retrieve the mapped ORM class for a given table name from an automap base. Args: base (AutomapBase): SQLAlchemy AutomapBase. Must not be None. table_name (str): Database table name. If None or empty, returns None. Returns: AutomapBase | None: Mapped SQLAlchemy ORM class, or None if not found. Examples: Input : base, 'users' Output: <User ORM class> Input : base, None Output: None Input : None, 'users' Output: None Notes: - Uses Base.metadata and registry to find the mapped class. - If the table is not found, returns None. - If `base` is None, returns None. - If `table_name` is None or empty, returns None. \"\"\" try : # Look up the table in metadata and find its mapped class table = base . metadata . tables . get ( table_name ) # type: ignore if table is not None : for mapper in base . registry . mappers : # type: ignore if mapper . local_table == table : return mapper . class_ return None except Exception as e : msg = f \"Exception occurred when trying to get table named { table_name } \" logger . error ( msg , exc_info = True ) logger . error ( f \"exception info: { e } \" ) return None get_foreign_value ( db , base , primary_table_name , foreign_table_name , primary_table_fk_name , foreign_table_column_name , primary_table_pk_value , primary_table_pk_name = None , foreign_table_pk_name = None ) Resolve a foreign key reference and return the referenced value from the foreign table. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. Must not be None. base ( AutomapBase ) \u2013 Declarative base. Must not be None. primary_table_name ( str ) \u2013 Table with the foreign key. If None or empty, raises ValueError. foreign_table_name ( str ) \u2013 Table containing the referenced value. If None or empty, raises ValueError. primary_table_fk_name ( str ) \u2013 Foreign key field name in the primary table. If None or empty, raises ValueError. foreign_table_column_name ( str ) \u2013 Target field name in the foreign table. If None or empty, raises ValueError. primary_table_pk_value ( int ) \u2013 Primary key value of the row in the primary table. primary_table_pk_name ( str | None , default: None ) \u2013 Optional primary key field override in the primary table. foreign_table_pk_name ( str | None , default: None ) \u2013 Optional primary key field override in the foreign table. Returns: str | None \u2013 str | None: The referenced value from the foreign table, or None if not found. Examples: Input : db, base, 'orders', 'users', 'user_id', 'email', 123 Output: 'user@example.com' Input : db, base, None, 'users', 'user_id', 'email', 123 Output: ValueError Input : db, base, 'orders', None, 'user_id', 'email', 123 Output: ValueError Notes If any required argument is None or empty, raises ValueError. If the referenced value is not found, returns None. Source code in arb\\utils\\sql_alchemy.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 def get_foreign_value ( db : SQLAlchemy , base : AutomapBase , primary_table_name : str , foreign_table_name : str , primary_table_fk_name : str , foreign_table_column_name : str , primary_table_pk_value : int , primary_table_pk_name : str | None = None , foreign_table_pk_name : str | None = None ) -> str | None : \"\"\" Resolve a foreign key reference and return the referenced value from the foreign table. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. Must not be None. base (AutomapBase): Declarative base. Must not be None. primary_table_name (str): Table with the foreign key. If None or empty, raises ValueError. foreign_table_name (str): Table containing the referenced value. If None or empty, raises ValueError. primary_table_fk_name (str): Foreign key field name in the primary table. If None or empty, raises ValueError. foreign_table_column_name (str): Target field name in the foreign table. If None or empty, raises ValueError. primary_table_pk_value (int): Primary key value of the row in the primary table. primary_table_pk_name (str | None): Optional primary key field override in the primary table. foreign_table_pk_name (str | None): Optional primary key field override in the foreign table. Returns: str | None: The referenced value from the foreign table, or None if not found. Examples: Input : db, base, 'orders', 'users', 'user_id', 'email', 123 Output: 'user@example.com' Input : db, base, None, 'users', 'user_id', 'email', 123 Output: ValueError Input : db, base, 'orders', None, 'user_id', 'email', 123 Output: ValueError Notes: - If any required argument is None or empty, raises ValueError. - If the referenced value is not found, returns None. \"\"\" logger . debug ( f \"Looking up foreign value: { locals () } \" ) result = None primary_table = get_class_from_table_name ( base , primary_table_name ) foreign_table = get_class_from_table_name ( base , foreign_table_name ) if primary_table_pk_name : pk_column = getattr ( primary_table , primary_table_pk_name ) primary_row = db . session . query ( primary_table ) . filter ( pk_column == primary_table_pk_value ) . first () # type: ignore else : primary_row = db . session . query ( primary_table ) . get ( primary_table_pk_value ) # type: ignore if primary_row : fk_value = getattr ( primary_row , primary_table_fk_name ) if foreign_table_pk_name : fk_column = getattr ( foreign_table , foreign_table_pk_name ) foreign_row = db . session . query ( foreign_table ) . filter ( fk_column == fk_value ) . first () # type: ignore else : foreign_row = db . session . query ( foreign_table ) . get ( fk_value ) # type: ignore if foreign_row : result = getattr ( foreign_row , foreign_table_column_name ) logger . debug ( f \"Foreign key result: { result } \" ) return result get_rows_by_table_name ( db , base , table_name , colum_name_pk = None , ascending = True ) Retrieve all rows from a table, optionally sorted by a column. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy db object. Must not be None. base ( AutomapBase ) \u2013 Declarative base. Must not be None. table_name ( str ) \u2013 Table name. If None or empty, raises ValueError. colum_name_pk ( str | None , default: None ) \u2013 Column to sort by (primary key or other). If None, no sorting is applied. ascending ( bool , default: True ) \u2013 Sort order (True for ascending, False for descending). Returns: list \u2013 list[DeclarativeMeta]: List of ORM model instances. Examples: Input : db, Base, 'users', 'id', ascending=False Output: List of user ORM instances sorted by id descending Input : db, Base, None Output: ValueError Input : db, Base, '' Output: ValueError Notes If table_name is None or empty, raises ValueError. If table is not found, returns an empty list and logs a warning. Source code in arb\\utils\\sql_alchemy.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 def get_rows_by_table_name ( db : SQLAlchemy , base : AutomapBase , table_name : str , colum_name_pk : str | None = None , ascending : bool = True ) -> list : \"\"\" Retrieve all rows from a table, optionally sorted by a column. Args: db (SQLAlchemy): SQLAlchemy db object. Must not be None. base (AutomapBase): Declarative base. Must not be None. table_name (str): Table name. If None or empty, raises ValueError. colum_name_pk (str | None): Column to sort by (primary key or other). If None, no sorting is applied. ascending (bool): Sort order (True for ascending, False for descending). Returns: list[DeclarativeMeta]: List of ORM model instances. Examples: Input : db, Base, 'users', 'id', ascending=False Output: List of user ORM instances sorted by id descending Input : db, Base, None Output: ValueError Input : db, Base, '' Output: ValueError Notes: - If `table_name` is None or empty, raises ValueError. - If table is not found, returns an empty list and logs a warning. \"\"\" table = get_class_from_table_name ( base , table_name ) logger . info ( f \" { type ( table ) =} \" ) query = db . session . query ( table ) if colum_name_pk : column = getattr ( table , colum_name_pk ) query = query . order_by ( column if ascending else desc ( column )) rows = query . all () logger . debug ( f \"Query result: { type ( rows ) =} \" ) return rows get_sa_automap_types ( engine , base ) Return column type metadata for all mapped classes in an automap base. Parameters: engine ( Engine ) \u2013 SQLAlchemy engine instance. Must not be None. base ( AutomapBase ) \u2013 Automap base prepared with reflected metadata. Must not be None. Returns: dict [ str , dict ] \u2013 dict[str, dict]: Nested mapping: table -> column -> type category. Structure ( dict [ str , dict ] ) \u2013 result[table_name][column_name][kind] = type # table_name, column_name, kind are placeholders where kind can be 'database_type', 'sqlalchemy_type', or 'python_type'. Examples: Input : engine, base Output: {'users': {'id': {'python_type': int, ...}, ...}, ...} Notes base is typically created with: base = automap_base() base.prepare(db.engine, reflect=True) If engine or base is None, a TypeError or AttributeError will occur. Source code in arb\\utils\\sql_alchemy.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def get_sa_automap_types ( engine : Engine , base : AutomapBase ) -> dict [ str , dict ]: \"\"\" Return column type metadata for all mapped classes in an automap base. Args: engine (Engine): SQLAlchemy engine instance. Must not be None. base (AutomapBase): Automap base prepared with reflected metadata. Must not be None. Returns: dict[str, dict]: Nested mapping: table -> column -> type category. Structure: result[table_name][column_name][kind] = type # table_name, column_name, kind are placeholders where kind can be 'database_type', 'sqlalchemy_type', or 'python_type'. Examples: Input : engine, base Output: {'users': {'id': {'python_type': int, ...}, ...}, ...} Notes: - `base` is typically created with: base = automap_base() base.prepare(db.engine, reflect=True) - If `engine` or `base` is None, a TypeError or AttributeError will occur. \"\"\" logger . debug ( f \"calling get_sa_automap_types()\" ) result = {} inspector = inspect ( engine ) # Loop through all the mapped classes (tables) # print(f\"{type(base)=}\") # print(f\"{type(base.classes)=}\") for class_name , mapped_class in base . classes . items (): # type: ignore # print(f\"Table: {class_name}\") result [ class_name ] = {} # Get the table columns columns = mapped_class . __table__ . columns # Loop through columns to get types for column in columns : # print(f\"Column: {column.name}\") result [ class_name ][ column . name ] = {} db_type = None sa_type = None py_type = None # Database (SQL) column type db_column_type = inspector . get_columns ( class_name ) for col in db_column_type : if col [ 'name' ] == column . name : db_type = col [ 'type' ] # print(f\" Database type (SQL): {db_type}\") # SQLAlchemy type sa_type = type ( db_type ) . __name__ # print(f\" SQLAlchemy type: {sa_type}\") # Python type try : py_type = column . type . python_type except Exception as e : msg = f \" { column . name } is of type: { sa_type } that is not implemented in python. Setting python type to None.\" logger . warning ( msg ) logger . warning ( e ) # print(f\" Python type: {py_type}\") result [ class_name ][ column . name ][ \"python_type\" ] = py_type result [ class_name ][ column . name ][ \"database_type\" ] = db_type result [ class_name ][ column . name ][ \"sqlalchemy_type\" ] = sa_type logger . debug ( f \"returning from get_sa_automap_types()\" ) return result get_sa_column_types ( model , is_instance = False ) Return a mapping of each column to its SQLAlchemy and Python types for a model. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy model instance or class. Must not be None. is_instance ( bool , default: False ) \u2013 True if model is an instance, False if a class. Returns: dict [ str , dict ] \u2013 dict[str, dict]: Mapping from column names to a dict with 'sqlalchemy_type' and 'python_type'. Examples: Input : user_model Output: {'id': {'sqlalchemy_type': Integer, 'python_type': int}, ...} Raises: AttributeError \u2013 If model is None or does not have valid metadata. Source code in arb\\utils\\sql_alchemy.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def get_sa_column_types ( model : AutomapBase , is_instance : bool = False ) -> dict [ str , dict ]: \"\"\" Return a mapping of each column to its SQLAlchemy and Python types for a model. Args: model (AutomapBase): SQLAlchemy model instance or class. Must not be None. is_instance (bool): True if `model` is an instance, False if a class. Returns: dict[str, dict]: Mapping from column names to a dict with 'sqlalchemy_type' and 'python_type'. Examples: Input : user_model Output: {'id': {'sqlalchemy_type': Integer, 'python_type': int}, ...} Raises: AttributeError: If `model` is None or does not have valid metadata. \"\"\" # Get the table inspector for the model if is_instance : inspector = inspect ( type ( model )) else : inspector = inspect ( model ) logger . debug ( f \" \\t { model =} \" ) logger . debug ( f \" \\t { inspector =} \" ) columns_info = {} for column in inspector . columns : # type: ignore col_name = column . name try : columns_info [ col_name ] = { 'sqlalchemy_type' : column . type , 'python_type' : column . type . python_type } except Exception as e : logger . warning ( f \" { col_name } has unsupported Python type.\" ) logger . warning ( e ) columns_info [ col_name ] = { 'sqlalchemy_type' : column . type , 'python_type' : None } raise # Re-raises the current exception with original traceback - comment out if you don't to warn rather than fail return columns_info get_sa_fields ( model ) Return a sorted list of column attribute names for a SQLAlchemy model. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy AutomapBase model instance. Must not be None. Returns: list [ str ] \u2013 list[str]: Alphabetically sorted list of column attribute names. Examples: Input : user_model Output: ['email', 'id', 'name'] Notes If model is None, a TypeError or AttributeError will occur. Source code in arb\\utils\\sql_alchemy.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def get_sa_fields ( model : AutomapBase ) -> list [ str ]: \"\"\" Return a sorted list of column attribute names for a SQLAlchemy model. Args: model (AutomapBase): SQLAlchemy AutomapBase model instance. Must not be None. Returns: list[str]: Alphabetically sorted list of column attribute names. Examples: Input : user_model Output: ['email', 'id', 'name'] Notes: - If `model` is None, a TypeError or AttributeError will occur. \"\"\" inst = inspect ( model ) # type: ignore model_fields = [ c_attr . key for c_attr in inst . mapper . column_attrs ] # type: ignore model_fields . sort () return model_fields get_table_row_and_column ( db , base , table_name , column_name , id_ ) Fetch a row and a specific column value given table name and primary key value. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. Must not be None. base ( AutomapBase ) \u2013 AutomapBase. Must not be None. table_name ( str ) \u2013 Table name. If None or empty, raises ValueError. column_name ( str ) \u2013 Column of interest. If None or empty, raises ValueError. id_ ( int ) \u2013 Primary key value. Returns: tuple | None \u2013 tuple | None: (row, value) if found, else (None, None). Examples: Input : db, Base, 'users', 'email', 1 Output: (user_row, user_row.email) Input : db, Base, None, 'email', 1 Output: ValueError Input : db, Base, 'users', None, 1 Output: ValueError Notes If table_name or column_name is None or empty, raises ValueError. If row is not found, returns (None, None). Source code in arb\\utils\\sql_alchemy.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 def get_table_row_and_column ( db : SQLAlchemy , base : AutomapBase , table_name : str , column_name : str , id_ : int ) -> tuple | None : \"\"\" Fetch a row and a specific column value given table name and primary key value. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. Must not be None. base (AutomapBase): AutomapBase. Must not be None. table_name (str): Table name. If None or empty, raises ValueError. column_name (str): Column of interest. If None or empty, raises ValueError. id_ (int): Primary key value. Returns: tuple | None: (row, value) if found, else (None, None). Examples: Input : db, Base, 'users', 'email', 1 Output: (user_row, user_row.email) Input : db, Base, None, 'email', 1 Output: ValueError Input : db, Base, 'users', None, 1 Output: ValueError Notes: - If `table_name` or `column_name` is None or empty, raises ValueError. - If row is not found, returns (None, None). \"\"\" logger . debug ( f \"Getting { column_name } from { table_name } where pk= { id_ } \" ) column_value = None table = get_class_from_table_name ( base , table_name ) if table is None : return None , None row = db . session . query ( table ) . get ( id_ ) # type: ignore if row : column_value = getattr ( row , column_name ) logger . debug ( f \" { row =} , { column_value =} \" ) return row , column_value load_model_json_column ( model , column_name ) Safely extract and normalize a JSON dictionary from a model's column. This helper ensures that the value stored in a model's JSON column is returned as a Python dictionary, regardless of whether it's stored as a JSON string or a native dict in the database. If the value is a malformed JSON string, a warning is logged and an empty dict is returned. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy ORM model instance. column_name ( str ) \u2013 Name of the attribute on the model (e.g., 'misc_json'). Returns: dict ( dict ) \u2013 Parsed dictionary from the JSON column. Returns {} on failure, None, or invalid input type. Raises: TypeError \u2013 If the value is not a string, dict, or None. Example misc = load_model_json_column(user_model, 'misc_json') Source code in arb\\utils\\sql_alchemy.py 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 def load_model_json_column ( model : AutomapBase , column_name : str ) -> dict : \"\"\" Safely extract and normalize a JSON dictionary from a model's column. This helper ensures that the value stored in a model's JSON column is returned as a Python dictionary, regardless of whether it's stored as a JSON string or a native dict in the database. If the value is a malformed JSON string, a warning is logged and an empty dict is returned. Args: model (AutomapBase): SQLAlchemy ORM model instance. column_name (str): Name of the attribute on the model (e.g., 'misc_json'). Returns: dict: Parsed dictionary from the JSON column. Returns {} on failure, None, or invalid input type. Raises: TypeError: If the value is not a string, dict, or None. Example: misc = load_model_json_column(user_model, 'misc_json') \"\"\" raw_value = getattr ( model , column_name ) if isinstance ( raw_value , dict ): return raw_value elif isinstance ( raw_value , str ) or raw_value is None : return safe_json_loads ( raw_value ) else : raise TypeError ( f \"Expected str, dict, or None for { column_name } , got { type ( raw_value ) . __name__ } \" ) sa_model_diagnostics ( model , comment = '' ) Log diagnostic details about a SQLAlchemy model instance, including all field names and values. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy model instance to inspect. Must not be None. comment ( str , default: '' ) \u2013 Optional comment header for log output. Returns: None \u2013 None Examples: Input : user_model, comment=\"User diagnostics\" Output: Logs all field names and values for user_model Notes If model is None, a TypeError or AttributeError will occur. Source code in arb\\utils\\sql_alchemy.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def sa_model_diagnostics ( model : AutomapBase , comment : str = \"\" ) -> None : \"\"\" Log diagnostic details about a SQLAlchemy model instance, including all field names and values. Args: model (AutomapBase): SQLAlchemy model instance to inspect. Must not be None. comment (str): Optional comment header for log output. Returns: None Examples: Input : user_model, comment=\"User diagnostics\" Output: Logs all field names and values for user_model Notes: - If `model` is None, a TypeError or AttributeError will occur. \"\"\" logger . debug ( f \"Diagnostics for model of type { type ( model ) =} \" ) if comment : logger . debug ( f \" { comment } \" ) logger . debug ( f \" { model =} \" ) fields = get_sa_fields ( model ) for key in fields : value = getattr ( model , key ) logger . debug ( f \" { key } { type ( value ) } = ( { value } )\" ) sa_model_dict_compare ( model_before , model_after ) Compare two model dictionaries and return a dict of changed fields and their new values. Parameters: model_before ( dict ) \u2013 Original model state as a dictionary. If None, treated as empty dict. model_after ( dict ) \u2013 New model state as a dictionary. If None, treated as empty dict. Returns: dict ( dict ) \u2013 Dictionary of changed fields and their new values. Examples: Input : {'email': 'old@example.com'}, {'email': 'new@example.com'} Output: {'email': 'new@example.com'} Input : None, {'email': 'new@example.com'} Output: {'email': 'new@example.com'} Input : {'email': 'old@example.com'}, None Output: {} Notes If either input is None, it is treated as an empty dict. Source code in arb\\utils\\sql_alchemy.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def sa_model_dict_compare ( model_before : dict , model_after : dict ) -> dict : \"\"\" Compare two model dictionaries and return a dict of changed fields and their new values. Args: model_before (dict): Original model state as a dictionary. If None, treated as empty dict. model_after (dict): New model state as a dictionary. If None, treated as empty dict. Returns: dict: Dictionary of changed fields and their new values. Examples: Input : {'email': 'old@example.com'}, {'email': 'new@example.com'} Output: {'email': 'new@example.com'} Input : None, {'email': 'new@example.com'} Output: {'email': 'new@example.com'} Input : {'email': 'old@example.com'}, None Output: {} Notes: - If either input is None, it is treated as an empty dict. \"\"\" changes = {} for field in model_after : if field not in model_before or model_before [ field ] != model_after [ field ]: changes [ field ] = model_after [ field ] return changes sa_model_to_dict ( model ) Convert a SQLAlchemy model instance to a Python dictionary. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy model instance. Must not be None. Returns: dict ( dict ) \u2013 Dictionary with column names as keys and model values as values. Examples: Input : user_model Output: {'id': 1, 'email': 'user@example.com'} Notes If model is None, a TypeError or AttributeError will occur. Source code in arb\\utils\\sql_alchemy.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def sa_model_to_dict ( model : AutomapBase ) -> dict : \"\"\" Convert a SQLAlchemy model instance to a Python dictionary. Args: model (AutomapBase): SQLAlchemy model instance. Must not be None. Returns: dict: Dictionary with column names as keys and model values as values. Examples: Input : user_model Output: {'id': 1, 'email': 'user@example.com'} Notes: - If `model` is None, a TypeError or AttributeError will occur. \"\"\" model_as_dict = {} fields = get_sa_fields ( model ) for field in fields : value = getattr ( model , field ) model_as_dict [ field ] = value return model_as_dict table_to_list ( base , session , table_name ) Convert all rows of a mapped table to a list of dictionaries. Parameters: base ( AutomapBase ) \u2013 Automap base. Must not be None. session ( Session ) \u2013 SQLAlchemy session. Must not be None. table_name ( str ) \u2013 Table name to query. If None or empty, raises ValueError. Returns: list [ dict ] \u2013 list[dict]: List of row dictionaries, one per row in the table. Examples: Input : Base, session, 'users' Output: [{'id': 1, 'email': ...}, ...] Input : Base, session, None Output: ValueError Input : Base, session, '' Output: ValueError Notes If table_name is None or empty, raises ValueError. If table is not found, returns an empty list and logs a warning. Source code in arb\\utils\\sql_alchemy.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 def table_to_list ( base : AutomapBase , session : Session , table_name : str ) -> list [ dict ]: \"\"\" Convert all rows of a mapped table to a list of dictionaries. Args: base (AutomapBase): Automap base. Must not be None. session (Session): SQLAlchemy session. Must not be None. table_name (str): Table name to query. If None or empty, raises ValueError. Returns: list[dict]: List of row dictionaries, one per row in the table. Examples: Input : Base, session, 'users' Output: [{'id': 1, 'email': ...}, ...] Input : Base, session, None Output: ValueError Input : Base, session, '' Output: ValueError Notes: - If `table_name` is None or empty, raises ValueError. - If table is not found, returns an empty list and logs a warning. \"\"\" result = [] table = base . classes . get ( table_name ) if table : logger . debug ( f \"Selecting data from: { table_name } \" ) rows = session . query ( table ) . all () col_names = table . __table__ . columns . keys () for row in rows : row_data = { col : getattr ( row , col ) for col in col_names } result . append ( row_data ) else : logger . warning ( f \"Table ' { table_name } ' not found in metadata.\" ) return result","title":"arb.utils.sql_alchemy"},{"location":"reference/arb/utils/sql_alchemy/#arbutilssql_alchemy","text":"SQLAlchemy utility functions for Flask applications using declarative or automap base models. This module provides introspection, diagnostics, and model-row operations for SQLAlchemy-based Flask apps. It supports both declarative and automapped models.","title":"arb.utils.sql_alchemy"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy--included-utilities","text":"Model diagnostics ( sa_model_diagnostics ) Field and column type inspection ( get_sa_fields , get_sa_column_types ) Full automap type mapping ( get_sa_automap_types ) Dictionary conversions ( sa_model_to_dict , sa_model_dict_compare ) Table-to-dict exports ( table_to_list ) Table/class lookups ( get_class_from_table_name ) Row fetch and sort utilities ( get_rows_by_table_name ) Model add/delete with logging ( add_commit_and_log_model , delete_commit_and_log_model ) Foreign key traversal ( get_foreign_value ) PostgresQL sequence inspection ( find_auto_increment_value ) JSON column loader ( load_model_json_column )","title":"Included Utilities:"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy--type-hints","text":"db (SQLAlchemy) : Flask-SQLAlchemy database instance with .session and .engine. base (DeclarativeMeta) : Declarative or automapped SQLAlchemy base (e.g., via automap_base()). model (DeclarativeMeta) : SQLAlchemy ORM model class or instance. session (Session) : SQLAlchemy session object.","title":"Type Hints:"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy--usage-notes","text":"Supports PostgresQL features like sequence inspection via pg_get_serial_sequence . Logging is integrated for debugging and auditing. Compatible with Python 3.10+ syntax (PEP 604 union types). Version 1.0.0","title":"Usage Notes:"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.add_commit_and_log_model","text":"Add or update a model instance in the database, log changes, and commit. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. Must not be None. model_row ( AutomapBase ) \u2013 ORM model instance to add or update. Must not be None. comment ( str , default: '' ) \u2013 Optional log comment for auditing. model_before ( dict | None , default: None ) \u2013 Optional snapshot of the model before changes. Returns: None \u2013 None Examples: Input : db, user_row, comment=\"Adding user\" Output: Adds or updates user_row in the database and logs changes Input : db, None Output: Exception Input : None, user_row Output: Exception Notes If db or model_row is None, an exception will be raised. Source code in arb\\utils\\sql_alchemy.py 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def add_commit_and_log_model ( db : SQLAlchemy , model_row : AutomapBase , comment : str = \"\" , model_before : dict | None = None ) -> None : \"\"\" Add or update a model instance in the database, log changes, and commit. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. Must not be None. model_row (AutomapBase): ORM model instance to add or update. Must not be None. comment (str): Optional log comment for auditing. model_before (dict | None): Optional snapshot of the model before changes. Returns: None Examples: Input : db, user_row, comment=\"Adding user\" Output: Adds or updates user_row in the database and logs changes Input : db, None Output: Exception Input : None, user_row Output: Exception Notes: - If `db` or `model_row` is None, an exception will be raised. \"\"\" # todo (update) - use the payload routine apply_json_patch_and_log if model_before : logger . info ( f \"Before commit { comment =} : { model_before } \" ) try : db . session . add ( model_row ) db . session . commit () model_after = sa_model_to_dict ( model_row ) logger . info ( f \"After commit: { model_after } \" ) if model_before : changes = sa_model_dict_compare ( model_before , model_after ) logger . info ( f \"Changed values: { changes } \" ) except Exception as e : log_error ( e )","title":"add_commit_and_log_model"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.delete_commit_and_log_model","text":"Delete a model instance from the database, log the operation, and commit the change. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. Must not be None. model_row ( AutomapBase ) \u2013 ORM model instance to delete. Must not be None. comment ( str , default: '' ) \u2013 Optional log comment for auditing. Returns: None \u2013 None Examples: Input : db, user_row, comment=\"Deleting user\" Output: Deletes user_row from the database and logs the operation Input : db, None Output: Exception Input : None, user_row Output: Exception Notes If db or model_row is None, an exception will be raised. Source code in arb\\utils\\sql_alchemy.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 def delete_commit_and_log_model ( db : SQLAlchemy , model_row : AutomapBase , comment : str = \"\" ) -> None : \"\"\" Delete a model instance from the database, log the operation, and commit the change. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. Must not be None. model_row (AutomapBase): ORM model instance to delete. Must not be None. comment (str): Optional log comment for auditing. Returns: None Examples: Input : db, user_row, comment=\"Deleting user\" Output: Deletes user_row from the database and logs the operation Input : db, None Output: Exception Input : None, user_row Output: Exception Notes: - If `db` or `model_row` is None, an exception will be raised. \"\"\" # todo (update) - use the payload routine apply_json_patch_and_log and or some way to track change logger . info ( f \"Deleting model { comment =} : { sa_model_to_dict ( model_row ) } \" ) try : db . session . delete ( model_row ) db . session . commit () except Exception as e : log_error ( e )","title":"delete_commit_and_log_model"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.find_auto_increment_value","text":"Find the next auto-increment value for a table column (PostgresQL only). Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. table_name ( str ) \u2013 Table name. column_name ( str ) \u2013 Column name. Returns: str ( str ) \u2013 Human-readable summary of the next sequence value. Example summary = find_auto_increment_value(db, 'users', 'id') Source code in arb\\utils\\sql_alchemy.py 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 def find_auto_increment_value ( db : SQLAlchemy , table_name : str , column_name : str ) -> str : \"\"\" Find the next auto-increment value for a table column (PostgresQL only). Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. table_name (str): Table name. column_name (str): Column name. Returns: str: Human-readable summary of the next sequence value. Example: summary = find_auto_increment_value(db, 'users', 'id') \"\"\" with db . engine . connect () as connection : sql_seq = f \"SELECT pg_get_serial_sequence(' { table_name } ', ' { column_name } ')\" sequence_name = connection . execute ( text ( sql_seq )) . scalar () sql_nextval = f \"SELECT nextval(' { sequence_name } ')\" next_val = connection . execute ( text ( sql_nextval )) . scalar () return f \"Table ' { table_name } ' column ' { column_name } ' sequence ' { sequence_name } ' next value is ' { next_val } '\"","title":"find_auto_increment_value"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_class_from_table_name","text":"Retrieve the mapped ORM class for a given table name from an automap base. Parameters: base ( AutomapBase ) \u2013 SQLAlchemy AutomapBase. Must not be None. table_name ( str ) \u2013 Database table name. If None or empty, returns None. Returns: AutomapBase | None \u2013 AutomapBase | None: Mapped SQLAlchemy ORM class, or None if not found. Examples: Input : base, 'users' Output: Input : base, None Output: None Input : None, 'users' Output: None Notes Uses Base.metadata and registry to find the mapped class. If the table is not found, returns None. If base is None, returns None. If table_name is None or empty, returns None. Source code in arb\\utils\\sql_alchemy.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 def get_class_from_table_name ( base : AutomapBase | None , table_name : str ) -> AutomapBase | None : \"\"\" Retrieve the mapped ORM class for a given table name from an automap base. Args: base (AutomapBase): SQLAlchemy AutomapBase. Must not be None. table_name (str): Database table name. If None or empty, returns None. Returns: AutomapBase | None: Mapped SQLAlchemy ORM class, or None if not found. Examples: Input : base, 'users' Output: <User ORM class> Input : base, None Output: None Input : None, 'users' Output: None Notes: - Uses Base.metadata and registry to find the mapped class. - If the table is not found, returns None. - If `base` is None, returns None. - If `table_name` is None or empty, returns None. \"\"\" try : # Look up the table in metadata and find its mapped class table = base . metadata . tables . get ( table_name ) # type: ignore if table is not None : for mapper in base . registry . mappers : # type: ignore if mapper . local_table == table : return mapper . class_ return None except Exception as e : msg = f \"Exception occurred when trying to get table named { table_name } \" logger . error ( msg , exc_info = True ) logger . error ( f \"exception info: { e } \" ) return None","title":"get_class_from_table_name"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_foreign_value","text":"Resolve a foreign key reference and return the referenced value from the foreign table. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. Must not be None. base ( AutomapBase ) \u2013 Declarative base. Must not be None. primary_table_name ( str ) \u2013 Table with the foreign key. If None or empty, raises ValueError. foreign_table_name ( str ) \u2013 Table containing the referenced value. If None or empty, raises ValueError. primary_table_fk_name ( str ) \u2013 Foreign key field name in the primary table. If None or empty, raises ValueError. foreign_table_column_name ( str ) \u2013 Target field name in the foreign table. If None or empty, raises ValueError. primary_table_pk_value ( int ) \u2013 Primary key value of the row in the primary table. primary_table_pk_name ( str | None , default: None ) \u2013 Optional primary key field override in the primary table. foreign_table_pk_name ( str | None , default: None ) \u2013 Optional primary key field override in the foreign table. Returns: str | None \u2013 str | None: The referenced value from the foreign table, or None if not found. Examples: Input : db, base, 'orders', 'users', 'user_id', 'email', 123 Output: 'user@example.com' Input : db, base, None, 'users', 'user_id', 'email', 123 Output: ValueError Input : db, base, 'orders', None, 'user_id', 'email', 123 Output: ValueError Notes If any required argument is None or empty, raises ValueError. If the referenced value is not found, returns None. Source code in arb\\utils\\sql_alchemy.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 def get_foreign_value ( db : SQLAlchemy , base : AutomapBase , primary_table_name : str , foreign_table_name : str , primary_table_fk_name : str , foreign_table_column_name : str , primary_table_pk_value : int , primary_table_pk_name : str | None = None , foreign_table_pk_name : str | None = None ) -> str | None : \"\"\" Resolve a foreign key reference and return the referenced value from the foreign table. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. Must not be None. base (AutomapBase): Declarative base. Must not be None. primary_table_name (str): Table with the foreign key. If None or empty, raises ValueError. foreign_table_name (str): Table containing the referenced value. If None or empty, raises ValueError. primary_table_fk_name (str): Foreign key field name in the primary table. If None or empty, raises ValueError. foreign_table_column_name (str): Target field name in the foreign table. If None or empty, raises ValueError. primary_table_pk_value (int): Primary key value of the row in the primary table. primary_table_pk_name (str | None): Optional primary key field override in the primary table. foreign_table_pk_name (str | None): Optional primary key field override in the foreign table. Returns: str | None: The referenced value from the foreign table, or None if not found. Examples: Input : db, base, 'orders', 'users', 'user_id', 'email', 123 Output: 'user@example.com' Input : db, base, None, 'users', 'user_id', 'email', 123 Output: ValueError Input : db, base, 'orders', None, 'user_id', 'email', 123 Output: ValueError Notes: - If any required argument is None or empty, raises ValueError. - If the referenced value is not found, returns None. \"\"\" logger . debug ( f \"Looking up foreign value: { locals () } \" ) result = None primary_table = get_class_from_table_name ( base , primary_table_name ) foreign_table = get_class_from_table_name ( base , foreign_table_name ) if primary_table_pk_name : pk_column = getattr ( primary_table , primary_table_pk_name ) primary_row = db . session . query ( primary_table ) . filter ( pk_column == primary_table_pk_value ) . first () # type: ignore else : primary_row = db . session . query ( primary_table ) . get ( primary_table_pk_value ) # type: ignore if primary_row : fk_value = getattr ( primary_row , primary_table_fk_name ) if foreign_table_pk_name : fk_column = getattr ( foreign_table , foreign_table_pk_name ) foreign_row = db . session . query ( foreign_table ) . filter ( fk_column == fk_value ) . first () # type: ignore else : foreign_row = db . session . query ( foreign_table ) . get ( fk_value ) # type: ignore if foreign_row : result = getattr ( foreign_row , foreign_table_column_name ) logger . debug ( f \"Foreign key result: { result } \" ) return result","title":"get_foreign_value"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_rows_by_table_name","text":"Retrieve all rows from a table, optionally sorted by a column. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy db object. Must not be None. base ( AutomapBase ) \u2013 Declarative base. Must not be None. table_name ( str ) \u2013 Table name. If None or empty, raises ValueError. colum_name_pk ( str | None , default: None ) \u2013 Column to sort by (primary key or other). If None, no sorting is applied. ascending ( bool , default: True ) \u2013 Sort order (True for ascending, False for descending). Returns: list \u2013 list[DeclarativeMeta]: List of ORM model instances. Examples: Input : db, Base, 'users', 'id', ascending=False Output: List of user ORM instances sorted by id descending Input : db, Base, None Output: ValueError Input : db, Base, '' Output: ValueError Notes If table_name is None or empty, raises ValueError. If table is not found, returns an empty list and logs a warning. Source code in arb\\utils\\sql_alchemy.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 def get_rows_by_table_name ( db : SQLAlchemy , base : AutomapBase , table_name : str , colum_name_pk : str | None = None , ascending : bool = True ) -> list : \"\"\" Retrieve all rows from a table, optionally sorted by a column. Args: db (SQLAlchemy): SQLAlchemy db object. Must not be None. base (AutomapBase): Declarative base. Must not be None. table_name (str): Table name. If None or empty, raises ValueError. colum_name_pk (str | None): Column to sort by (primary key or other). If None, no sorting is applied. ascending (bool): Sort order (True for ascending, False for descending). Returns: list[DeclarativeMeta]: List of ORM model instances. Examples: Input : db, Base, 'users', 'id', ascending=False Output: List of user ORM instances sorted by id descending Input : db, Base, None Output: ValueError Input : db, Base, '' Output: ValueError Notes: - If `table_name` is None or empty, raises ValueError. - If table is not found, returns an empty list and logs a warning. \"\"\" table = get_class_from_table_name ( base , table_name ) logger . info ( f \" { type ( table ) =} \" ) query = db . session . query ( table ) if colum_name_pk : column = getattr ( table , colum_name_pk ) query = query . order_by ( column if ascending else desc ( column )) rows = query . all () logger . debug ( f \"Query result: { type ( rows ) =} \" ) return rows","title":"get_rows_by_table_name"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_sa_automap_types","text":"Return column type metadata for all mapped classes in an automap base. Parameters: engine ( Engine ) \u2013 SQLAlchemy engine instance. Must not be None. base ( AutomapBase ) \u2013 Automap base prepared with reflected metadata. Must not be None. Returns: dict [ str , dict ] \u2013 dict[str, dict]: Nested mapping: table -> column -> type category. Structure ( dict [ str , dict ] ) \u2013 result[table_name][column_name][kind] = type # table_name, column_name, kind are placeholders where kind can be 'database_type', 'sqlalchemy_type', or 'python_type'. Examples: Input : engine, base Output: {'users': {'id': {'python_type': int, ...}, ...}, ...} Notes base is typically created with: base = automap_base() base.prepare(db.engine, reflect=True) If engine or base is None, a TypeError or AttributeError will occur. Source code in arb\\utils\\sql_alchemy.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def get_sa_automap_types ( engine : Engine , base : AutomapBase ) -> dict [ str , dict ]: \"\"\" Return column type metadata for all mapped classes in an automap base. Args: engine (Engine): SQLAlchemy engine instance. Must not be None. base (AutomapBase): Automap base prepared with reflected metadata. Must not be None. Returns: dict[str, dict]: Nested mapping: table -> column -> type category. Structure: result[table_name][column_name][kind] = type # table_name, column_name, kind are placeholders where kind can be 'database_type', 'sqlalchemy_type', or 'python_type'. Examples: Input : engine, base Output: {'users': {'id': {'python_type': int, ...}, ...}, ...} Notes: - `base` is typically created with: base = automap_base() base.prepare(db.engine, reflect=True) - If `engine` or `base` is None, a TypeError or AttributeError will occur. \"\"\" logger . debug ( f \"calling get_sa_automap_types()\" ) result = {} inspector = inspect ( engine ) # Loop through all the mapped classes (tables) # print(f\"{type(base)=}\") # print(f\"{type(base.classes)=}\") for class_name , mapped_class in base . classes . items (): # type: ignore # print(f\"Table: {class_name}\") result [ class_name ] = {} # Get the table columns columns = mapped_class . __table__ . columns # Loop through columns to get types for column in columns : # print(f\"Column: {column.name}\") result [ class_name ][ column . name ] = {} db_type = None sa_type = None py_type = None # Database (SQL) column type db_column_type = inspector . get_columns ( class_name ) for col in db_column_type : if col [ 'name' ] == column . name : db_type = col [ 'type' ] # print(f\" Database type (SQL): {db_type}\") # SQLAlchemy type sa_type = type ( db_type ) . __name__ # print(f\" SQLAlchemy type: {sa_type}\") # Python type try : py_type = column . type . python_type except Exception as e : msg = f \" { column . name } is of type: { sa_type } that is not implemented in python. Setting python type to None.\" logger . warning ( msg ) logger . warning ( e ) # print(f\" Python type: {py_type}\") result [ class_name ][ column . name ][ \"python_type\" ] = py_type result [ class_name ][ column . name ][ \"database_type\" ] = db_type result [ class_name ][ column . name ][ \"sqlalchemy_type\" ] = sa_type logger . debug ( f \"returning from get_sa_automap_types()\" ) return result","title":"get_sa_automap_types"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_sa_column_types","text":"Return a mapping of each column to its SQLAlchemy and Python types for a model. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy model instance or class. Must not be None. is_instance ( bool , default: False ) \u2013 True if model is an instance, False if a class. Returns: dict [ str , dict ] \u2013 dict[str, dict]: Mapping from column names to a dict with 'sqlalchemy_type' and 'python_type'. Examples: Input : user_model Output: {'id': {'sqlalchemy_type': Integer, 'python_type': int}, ...} Raises: AttributeError \u2013 If model is None or does not have valid metadata. Source code in arb\\utils\\sql_alchemy.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 def get_sa_column_types ( model : AutomapBase , is_instance : bool = False ) -> dict [ str , dict ]: \"\"\" Return a mapping of each column to its SQLAlchemy and Python types for a model. Args: model (AutomapBase): SQLAlchemy model instance or class. Must not be None. is_instance (bool): True if `model` is an instance, False if a class. Returns: dict[str, dict]: Mapping from column names to a dict with 'sqlalchemy_type' and 'python_type'. Examples: Input : user_model Output: {'id': {'sqlalchemy_type': Integer, 'python_type': int}, ...} Raises: AttributeError: If `model` is None or does not have valid metadata. \"\"\" # Get the table inspector for the model if is_instance : inspector = inspect ( type ( model )) else : inspector = inspect ( model ) logger . debug ( f \" \\t { model =} \" ) logger . debug ( f \" \\t { inspector =} \" ) columns_info = {} for column in inspector . columns : # type: ignore col_name = column . name try : columns_info [ col_name ] = { 'sqlalchemy_type' : column . type , 'python_type' : column . type . python_type } except Exception as e : logger . warning ( f \" { col_name } has unsupported Python type.\" ) logger . warning ( e ) columns_info [ col_name ] = { 'sqlalchemy_type' : column . type , 'python_type' : None } raise # Re-raises the current exception with original traceback - comment out if you don't to warn rather than fail return columns_info","title":"get_sa_column_types"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_sa_fields","text":"Return a sorted list of column attribute names for a SQLAlchemy model. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy AutomapBase model instance. Must not be None. Returns: list [ str ] \u2013 list[str]: Alphabetically sorted list of column attribute names. Examples: Input : user_model Output: ['email', 'id', 'name'] Notes If model is None, a TypeError or AttributeError will occur. Source code in arb\\utils\\sql_alchemy.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 def get_sa_fields ( model : AutomapBase ) -> list [ str ]: \"\"\" Return a sorted list of column attribute names for a SQLAlchemy model. Args: model (AutomapBase): SQLAlchemy AutomapBase model instance. Must not be None. Returns: list[str]: Alphabetically sorted list of column attribute names. Examples: Input : user_model Output: ['email', 'id', 'name'] Notes: - If `model` is None, a TypeError or AttributeError will occur. \"\"\" inst = inspect ( model ) # type: ignore model_fields = [ c_attr . key for c_attr in inst . mapper . column_attrs ] # type: ignore model_fields . sort () return model_fields","title":"get_sa_fields"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_table_row_and_column","text":"Fetch a row and a specific column value given table name and primary key value. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. Must not be None. base ( AutomapBase ) \u2013 AutomapBase. Must not be None. table_name ( str ) \u2013 Table name. If None or empty, raises ValueError. column_name ( str ) \u2013 Column of interest. If None or empty, raises ValueError. id_ ( int ) \u2013 Primary key value. Returns: tuple | None \u2013 tuple | None: (row, value) if found, else (None, None). Examples: Input : db, Base, 'users', 'email', 1 Output: (user_row, user_row.email) Input : db, Base, None, 'email', 1 Output: ValueError Input : db, Base, 'users', None, 1 Output: ValueError Notes If table_name or column_name is None or empty, raises ValueError. If row is not found, returns (None, None). Source code in arb\\utils\\sql_alchemy.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 def get_table_row_and_column ( db : SQLAlchemy , base : AutomapBase , table_name : str , column_name : str , id_ : int ) -> tuple | None : \"\"\" Fetch a row and a specific column value given table name and primary key value. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. Must not be None. base (AutomapBase): AutomapBase. Must not be None. table_name (str): Table name. If None or empty, raises ValueError. column_name (str): Column of interest. If None or empty, raises ValueError. id_ (int): Primary key value. Returns: tuple | None: (row, value) if found, else (None, None). Examples: Input : db, Base, 'users', 'email', 1 Output: (user_row, user_row.email) Input : db, Base, None, 'email', 1 Output: ValueError Input : db, Base, 'users', None, 1 Output: ValueError Notes: - If `table_name` or `column_name` is None or empty, raises ValueError. - If row is not found, returns (None, None). \"\"\" logger . debug ( f \"Getting { column_name } from { table_name } where pk= { id_ } \" ) column_value = None table = get_class_from_table_name ( base , table_name ) if table is None : return None , None row = db . session . query ( table ) . get ( id_ ) # type: ignore if row : column_value = getattr ( row , column_name ) logger . debug ( f \" { row =} , { column_value =} \" ) return row , column_value","title":"get_table_row_and_column"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.load_model_json_column","text":"Safely extract and normalize a JSON dictionary from a model's column. This helper ensures that the value stored in a model's JSON column is returned as a Python dictionary, regardless of whether it's stored as a JSON string or a native dict in the database. If the value is a malformed JSON string, a warning is logged and an empty dict is returned. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy ORM model instance. column_name ( str ) \u2013 Name of the attribute on the model (e.g., 'misc_json'). Returns: dict ( dict ) \u2013 Parsed dictionary from the JSON column. Returns {} on failure, None, or invalid input type. Raises: TypeError \u2013 If the value is not a string, dict, or None. Example misc = load_model_json_column(user_model, 'misc_json') Source code in arb\\utils\\sql_alchemy.py 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 def load_model_json_column ( model : AutomapBase , column_name : str ) -> dict : \"\"\" Safely extract and normalize a JSON dictionary from a model's column. This helper ensures that the value stored in a model's JSON column is returned as a Python dictionary, regardless of whether it's stored as a JSON string or a native dict in the database. If the value is a malformed JSON string, a warning is logged and an empty dict is returned. Args: model (AutomapBase): SQLAlchemy ORM model instance. column_name (str): Name of the attribute on the model (e.g., 'misc_json'). Returns: dict: Parsed dictionary from the JSON column. Returns {} on failure, None, or invalid input type. Raises: TypeError: If the value is not a string, dict, or None. Example: misc = load_model_json_column(user_model, 'misc_json') \"\"\" raw_value = getattr ( model , column_name ) if isinstance ( raw_value , dict ): return raw_value elif isinstance ( raw_value , str ) or raw_value is None : return safe_json_loads ( raw_value ) else : raise TypeError ( f \"Expected str, dict, or None for { column_name } , got { type ( raw_value ) . __name__ } \" )","title":"load_model_json_column"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.sa_model_diagnostics","text":"Log diagnostic details about a SQLAlchemy model instance, including all field names and values. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy model instance to inspect. Must not be None. comment ( str , default: '' ) \u2013 Optional comment header for log output. Returns: None \u2013 None Examples: Input : user_model, comment=\"User diagnostics\" Output: Logs all field names and values for user_model Notes If model is None, a TypeError or AttributeError will occur. Source code in arb\\utils\\sql_alchemy.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def sa_model_diagnostics ( model : AutomapBase , comment : str = \"\" ) -> None : \"\"\" Log diagnostic details about a SQLAlchemy model instance, including all field names and values. Args: model (AutomapBase): SQLAlchemy model instance to inspect. Must not be None. comment (str): Optional comment header for log output. Returns: None Examples: Input : user_model, comment=\"User diagnostics\" Output: Logs all field names and values for user_model Notes: - If `model` is None, a TypeError or AttributeError will occur. \"\"\" logger . debug ( f \"Diagnostics for model of type { type ( model ) =} \" ) if comment : logger . debug ( f \" { comment } \" ) logger . debug ( f \" { model =} \" ) fields = get_sa_fields ( model ) for key in fields : value = getattr ( model , key ) logger . debug ( f \" { key } { type ( value ) } = ( { value } )\" )","title":"sa_model_diagnostics"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.sa_model_dict_compare","text":"Compare two model dictionaries and return a dict of changed fields and their new values. Parameters: model_before ( dict ) \u2013 Original model state as a dictionary. If None, treated as empty dict. model_after ( dict ) \u2013 New model state as a dictionary. If None, treated as empty dict. Returns: dict ( dict ) \u2013 Dictionary of changed fields and their new values. Examples: Input : {'email': 'old@example.com'}, {'email': 'new@example.com'} Output: {'email': 'new@example.com'} Input : None, {'email': 'new@example.com'} Output: {'email': 'new@example.com'} Input : {'email': 'old@example.com'}, None Output: {} Notes If either input is None, it is treated as an empty dict. Source code in arb\\utils\\sql_alchemy.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def sa_model_dict_compare ( model_before : dict , model_after : dict ) -> dict : \"\"\" Compare two model dictionaries and return a dict of changed fields and their new values. Args: model_before (dict): Original model state as a dictionary. If None, treated as empty dict. model_after (dict): New model state as a dictionary. If None, treated as empty dict. Returns: dict: Dictionary of changed fields and their new values. Examples: Input : {'email': 'old@example.com'}, {'email': 'new@example.com'} Output: {'email': 'new@example.com'} Input : None, {'email': 'new@example.com'} Output: {'email': 'new@example.com'} Input : {'email': 'old@example.com'}, None Output: {} Notes: - If either input is None, it is treated as an empty dict. \"\"\" changes = {} for field in model_after : if field not in model_before or model_before [ field ] != model_after [ field ]: changes [ field ] = model_after [ field ] return changes","title":"sa_model_dict_compare"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.sa_model_to_dict","text":"Convert a SQLAlchemy model instance to a Python dictionary. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy model instance. Must not be None. Returns: dict ( dict ) \u2013 Dictionary with column names as keys and model values as values. Examples: Input : user_model Output: {'id': 1, 'email': 'user@example.com'} Notes If model is None, a TypeError or AttributeError will occur. Source code in arb\\utils\\sql_alchemy.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def sa_model_to_dict ( model : AutomapBase ) -> dict : \"\"\" Convert a SQLAlchemy model instance to a Python dictionary. Args: model (AutomapBase): SQLAlchemy model instance. Must not be None. Returns: dict: Dictionary with column names as keys and model values as values. Examples: Input : user_model Output: {'id': 1, 'email': 'user@example.com'} Notes: - If `model` is None, a TypeError or AttributeError will occur. \"\"\" model_as_dict = {} fields = get_sa_fields ( model ) for field in fields : value = getattr ( model , field ) model_as_dict [ field ] = value return model_as_dict","title":"sa_model_to_dict"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.table_to_list","text":"Convert all rows of a mapped table to a list of dictionaries. Parameters: base ( AutomapBase ) \u2013 Automap base. Must not be None. session ( Session ) \u2013 SQLAlchemy session. Must not be None. table_name ( str ) \u2013 Table name to query. If None or empty, raises ValueError. Returns: list [ dict ] \u2013 list[dict]: List of row dictionaries, one per row in the table. Examples: Input : Base, session, 'users' Output: [{'id': 1, 'email': ...}, ...] Input : Base, session, None Output: ValueError Input : Base, session, '' Output: ValueError Notes If table_name is None or empty, raises ValueError. If table is not found, returns an empty list and logs a warning. Source code in arb\\utils\\sql_alchemy.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 def table_to_list ( base : AutomapBase , session : Session , table_name : str ) -> list [ dict ]: \"\"\" Convert all rows of a mapped table to a list of dictionaries. Args: base (AutomapBase): Automap base. Must not be None. session (Session): SQLAlchemy session. Must not be None. table_name (str): Table name to query. If None or empty, raises ValueError. Returns: list[dict]: List of row dictionaries, one per row in the table. Examples: Input : Base, session, 'users' Output: [{'id': 1, 'email': ...}, ...] Input : Base, session, None Output: ValueError Input : Base, session, '' Output: ValueError Notes: - If `table_name` is None or empty, raises ValueError. - If table is not found, returns an empty list and logs a warning. \"\"\" result = [] table = base . classes . get ( table_name ) if table : logger . debug ( f \"Selecting data from: { table_name } \" ) rows = session . query ( table ) . all () col_names = table . __table__ . columns . keys () for row in rows : row_data = { col : getattr ( row , col ) for col in col_names } result . append ( row_data ) else : logger . warning ( f \"Table ' { table_name } ' not found in metadata.\" ) return result","title":"table_to_list"},{"location":"reference/arb/utils/web_html/","text":"arb.utils.web_html HTML and WTForms utility functions for form handling and file uploads. This module provides helper functions for Uploading user files with sanitized names Generating WTForms-compatible selector lists Managing triple tuples for dynamic dropdown metadata Notes Avoid circular imports by not depending on other utility modules. Other utility modules (e.g., Excel, DB) may safely import this one. Adds \"Please Select\" logic to dropdowns using arb.utils.constants . Examples: Input : file = request.files['data'], upload_dir = \"/data/uploads\" Output: Path object pointing to a securely saved file ensure_placeholder_option ( tuple_list , item = PLEASE_SELECT , item_dict = None , ensure_first = True ) Ensure a placeholder entry is present in the tuple list. This function ensures that a specified \"placeholder\" option (typically used to prompt users to select a value, such as \"Please Select\") exists in the given list of selector options. If the placeholder is not present, it is inserted at the top. If it exists but is not the first item, it is optionally moved to the first position. Parameters: tuple_list ( list [ tuple [ str , str , dict ]] ) \u2013 Original selector list. If None or empty, returns a list with only the placeholder. item ( str , default: PLEASE_SELECT ) \u2013 Value for the placeholder. Default is \"Please Select\". If None, uses default. item_dict ( dict , default: None ) \u2013 Metadata for the placeholder. Default disables the option. If None, uses default. ensure_first ( bool , default: True ) \u2013 If True, move placeholder to top if found elsewhere. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Updated tuple list with ensured placeholder. Examples: Input : [(\"A\", \"A\", {})] Output: [('Please Select', 'Please Select', {'disabled': True}), ('A', 'A', {})] Input : None Output: [('Please Select', 'Please Select', {'disabled': True})] Input : [], item=None Output: [('Please Select', 'Please Select', {'disabled': True})] Notes If tuple_list is None or empty, returns a list with only the placeholder. If item is None, uses \"Please Select\". If item_dict is None, uses {\"disabled\": True}. Source code in arb\\utils\\web_html.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def ensure_placeholder_option ( tuple_list : list [ tuple [ str , str , dict ]], item : str = PLEASE_SELECT , item_dict : dict = None , ensure_first : bool = True ) -> list [ tuple [ str , str , dict ]]: \"\"\" Ensure a placeholder entry is present in the tuple list. This function ensures that a specified \"placeholder\" option (typically used to prompt users to select a value, such as \"Please Select\") exists in the given list of selector options. If the placeholder is not present, it is inserted at the top. If it exists but is not the first item, it is optionally moved to the first position. Args: tuple_list (list[tuple[str, str, dict]]): Original selector list. If None or empty, returns a list with only the placeholder. item (str): Value for the placeholder. Default is \"Please Select\". If None, uses default. item_dict (dict): Metadata for the placeholder. Default disables the option. If None, uses default. ensure_first (bool): If True, move placeholder to top if found elsewhere. Returns: list[tuple[str, str, dict]]: Updated tuple list with ensured placeholder. Examples: Input : [(\"A\", \"A\", {})] Output: [('Please Select', 'Please Select', {'disabled': True}), ('A', 'A', {})] Input : None Output: [('Please Select', 'Please Select', {'disabled': True})] Input : [], item=None Output: [('Please Select', 'Please Select', {'disabled': True})] Notes: - If `tuple_list` is None or empty, returns a list with only the placeholder. - If `item` is None, uses \"Please Select\". - If `item_dict` is None, uses {\"disabled\": True}. \"\"\" if item is None : item = PLEASE_SELECT if item_dict is None : item_dict = { \"disabled\" : True } placeholder = ( item , item , item_dict ) # Find the index of any existing placeholder (based on value match) # Explanation: # - `enumerate(tuple_list)` produces (index, tuple) pairs. # - `t[0] == item` checks if the first element (value) matches the placeholder value. # - `next(...)` returns the index of the first match, or `None` if no match is found. index = next (( i for i , t in enumerate ( tuple_list ) if t [ 0 ] == item ), None ) if index is None : # The Placeholder is not found; insert it at the beginning of the list. return [ placeholder ] + tuple_list elif ensure_first and index != 0 : # Placeholder found but not in first position and `ensure_first` is True. # Move it to the front while preserving the order of the rest. reordered = [ t for i , t in enumerate ( tuple_list ) if i != index ] return [ tuple_list [ index ]] + reordered # Placeholder exists and is already in the correct position; return unchanged. return tuple_list list_to_triple_tuple ( values ) Convert a list of strings into WTForms triple tuples. Each tuple contains (value, label, metadata). Parameters: values ( list [ str ] ) \u2013 List of form options. If None or empty, returns an empty list. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Triple tuples for WTForms SelectField. Examples: Input : [\"A\", \"B\"] Output: [('A', 'A', {}), ('B', 'B', {})] Input : [] Output: [] Input : None Output: [] Notes If values is None or empty, returns an empty list. Source code in arb\\utils\\web_html.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def list_to_triple_tuple ( values : list [ str ]) -> list [ tuple [ str , str , dict ]]: \"\"\" Convert a list of strings into WTForms triple tuples. Each tuple contains (value, label, metadata). Args: values (list[str]): List of form options. If None or empty, returns an empty list. Returns: list[tuple[str, str, dict]]: Triple tuples for WTForms SelectField. Examples: Input : [\"A\", \"B\"] Output: [('A', 'A', {}), ('B', 'B', {})] Input : [] Output: [] Input : None Output: [] Notes: - If `values` is None or empty, returns an empty list. \"\"\" return [( v , v , {}) for v in values ] remove_items ( tuple_list , remove_items ) Remove one or more values from a tuple list by matching the first element. Parameters: tuple_list ( list [ tuple [ str , str , dict ]] ) \u2013 Selector tuples. If None or empty, returns empty list. remove_items ( str | list [ str ] ) \u2013 One or more values to remove by key match. If None or empty, returns the original list. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Filtered list excluding the removed values. Examples: Input : [(\"A\", \"A\", {}), (\"B\", \"B\", {})], remove_items=\"B\" Output: [('A', 'A', {})] Input : [(\"A\", \"A\", {}), (\"B\", \"B\", {})], remove_items=[\"A\", \"B\"] Output: [] Input : [], remove_items=\"A\" Output: [] Input : None, remove_items=\"A\" Output: [] Notes If tuple_list is None or empty, returns empty list. If remove_items is None or empty, returns the original list. Source code in arb\\utils\\web_html.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def remove_items ( tuple_list : list [ tuple [ str , str , dict ]], remove_items : str | list [ str ] ) -> list [ tuple [ str , str , dict ]]: \"\"\" Remove one or more values from a tuple list by matching the first element. Args: tuple_list (list[tuple[str, str, dict]]): Selector tuples. If None or empty, returns empty list. remove_items (str | list[str]): One or more values to remove by key match. If None or empty, returns the original list. Returns: list[tuple[str, str, dict]]: Filtered list excluding the removed values. Examples: Input : [(\"A\", \"A\", {}), (\"B\", \"B\", {})], remove_items=\"B\" Output: [('A', 'A', {})] Input : [(\"A\", \"A\", {}), (\"B\", \"B\", {})], remove_items=[\"A\", \"B\"] Output: [] Input : [], remove_items=\"A\" Output: [] Input : None, remove_items=\"A\" Output: [] Notes: - If `tuple_list` is None or empty, returns empty list. - If `remove_items` is None or empty, returns the original list. \"\"\" remove_set = { remove_items } if isinstance ( remove_items , str ) else set ( remove_items ) return [ t for t in tuple_list if t [ 0 ] not in remove_set ] run_diagnostics () Run assertions to validate selector utility behavior. Tests Conversion of string lists to selector tuples Tuple updating with metadata Placeholder insertion Value removal from selector lists Dict transformation to tuple selectors Returns: None \u2013 None Source code in arb\\utils\\web_html.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def run_diagnostics () -> None : \"\"\" Run assertions to validate selector utility behavior. Tests: - Conversion of string lists to selector tuples - Tuple updating with metadata - Placeholder insertion - Value removal from selector lists - Dict transformation to tuple selectors Returns: None \"\"\" print ( \"Running diagnostics for web_html.py...\" ) test_values = [ \"A\" , \"B\" , \"C\" ] # Test selector_list_to_tuples selector = selector_list_to_tuples ( test_values ) assert selector [ 0 ][ 0 ] == PLEASE_SELECT assert ( \"A\" , \"A\" ) in selector # Test list_to_triple_tuple triple = list_to_triple_tuple ([ \"X\" , \"Y\" ]) assert triple == [( \"X\" , \"X\" , {}), ( \"Y\" , \"Y\" , {})] # Test update_triple_tuple_dict updated = update_triple_tuple_dict ( triple , [ \"Y\" ], { \"selected\" : True }) assert updated [ 1 ][ 2 ] . get ( \"selected\" ) is True # Test update_selector_dict test_dict = { \"colors\" : [ \"red\" , \"green\" ]} updated_dict = update_selector_dict ( test_dict ) assert PLEASE_SELECT in [ x [ 0 ] for x in updated_dict [ \"colors\" ]] # Test ensure_placeholder_option reordered = ensure_placeholder_option ([( \"X\" , \"X\" , {})]) assert reordered [ 0 ][ 0 ] == PLEASE_SELECT # Test remove_items cleaned = remove_items ( triple , \"X\" ) assert all ( t [ 0 ] != \"X\" for t in cleaned ) print ( \"All selector diagnostics passed.\" ) selector_list_to_tuples ( values ) Convert a list of values into WTForms-compatible dropdown tuples. Adds a disabled \"Please Select\" entry at the top of the list. Parameters: values ( list [ str ] ) \u2013 Dropdown options (excluding \"Please Select\"). If None or empty, returns only the placeholder. Returns: list [ tuple [ str , str ] | tuple [ str , str , dict ]] \u2013 list[tuple[str, str] | tuple[str, str, dict]]: WTForms selector list including a disabled \"Please Select\" entry. Examples: Input : [\"Red\", \"Green\"] Output: [('Please Select', 'Please Select', {'disabled': True}), ('Red', 'Red'), ('Green', 'Green')] Input : [] Output: [('Please Select', 'Please Select', {'disabled': True})] Input : None Output: [('Please Select', 'Please Select', {'disabled': True})] Notes If values is None or empty, returns only the placeholder. Source code in arb\\utils\\web_html.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def selector_list_to_tuples ( values : list [ str ]) -> list [ tuple [ str , str ] | tuple [ str , str , dict ]]: \"\"\" Convert a list of values into WTForms-compatible dropdown tuples. Adds a disabled \"Please Select\" entry at the top of the list. Args: values (list[str]): Dropdown options (excluding \"Please Select\"). If None or empty, returns only the placeholder. Returns: list[tuple[str, str] | tuple[str, str, dict]]: WTForms selector list including a disabled \"Please Select\" entry. Examples: Input : [\"Red\", \"Green\"] Output: [('Please Select', 'Please Select', {'disabled': True}), ('Red', 'Red'), ('Green', 'Green')] Input : [] Output: [('Please Select', 'Please Select', {'disabled': True})] Input : None Output: [('Please Select', 'Please Select', {'disabled': True})] Notes: - If `values` is None or empty, returns only the placeholder. \"\"\" result = [( PLEASE_SELECT , PLEASE_SELECT , { \"disabled\" : True })] result += [( v , v ) for v in values ] return result update_selector_dict ( input_dict ) Convert dictionary of string lists into selector-style tuple lists. Each list is transformed to include a \"Please Select\" disabled option followed by (value, label) tuples. Parameters: input_dict ( dict [ str , list [ str ]] ) \u2013 Dict of dropdown options per field. If None or empty, returns empty dict. Returns: dict [ str , list [ tuple [ str , str ] | tuple [ str , str , dict ]]] \u2013 dict[str, list[tuple[str, str] | tuple[str, str, dict]]]: Dict with WTForms-ready selector tuples. Examples: Input : {\"colors\": [\"Red\", \"Blue\"]} Output: { \"colors\": [ (\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"Red\", \"Red\"), (\"Blue\", \"Blue\") ] } Input : {} Output: {} Input : None Output: {} Notes If input_dict is None or empty, returns empty dict. Source code in arb\\utils\\web_html.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def update_selector_dict ( input_dict : dict [ str , list [ str ]]) -> dict [ str , list [ tuple [ str , str ] | tuple [ str , str , dict ]]]: \"\"\" Convert dictionary of string lists into selector-style tuple lists. Each list is transformed to include a \"Please Select\" disabled option followed by (value, label) tuples. Args: input_dict (dict[str, list[str]]): Dict of dropdown options per field. If None or empty, returns empty dict. Returns: dict[str, list[tuple[str, str] | tuple[str, str, dict]]]: Dict with WTForms-ready selector tuples. Examples: Input : {\"colors\": [\"Red\", \"Blue\"]} Output: { \"colors\": [ (\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"Red\", \"Red\"), (\"Blue\", \"Blue\") ] } Input : {} Output: {} Input : None Output: {} Notes: - If `input_dict` is None or empty, returns empty dict. \"\"\" return { key : selector_list_to_tuples ( values ) for key , values in input_dict . items ()} update_triple_tuple_dict ( tuple_list , match_list , match_update_dict , unmatch_update_dict = None ) Update the metadata dict of each WTForms triple tuple based on value match. Parameters: tuple_list ( list [ tuple [ str , str , dict ]] ) \u2013 Existing list of selector tuples. If None or empty, returns empty list. match_list ( list [ str ] ) \u2013 Values to match against. If None or empty, no matches will occur. match_update_dict ( dict ) \u2013 Metadata to apply if value is in match_list . If None, no update is applied. unmatch_update_dict ( dict | None , default: None ) \u2013 Metadata to apply otherwise (optional). If None, no update is applied to unmatched. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Updated list of selector tuples. Examples: Input : tuple_list = [('A', 'A', {}), ('B', 'B', {})] match_list = ['A'] match_update_dict = {'disabled': True} unmatch_update_dict = {'class': 'available'} Output: [('A', 'A', {'disabled': True}), ('B', 'B', {'class': 'available'})] Input : [], ['A'], {'disabled': True}, None Output: [] Input : None, ['A'], {'disabled': True}, None Output: [] Notes If tuple_list is None or empty, returns empty list. If match_list is None or empty, no matches will occur. Source code in arb\\utils\\web_html.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def update_triple_tuple_dict ( tuple_list : list [ tuple [ str , str , dict ]], match_list : list [ str ], match_update_dict : dict , unmatch_update_dict : dict | None = None ) -> list [ tuple [ str , str , dict ]]: \"\"\" Update the metadata dict of each WTForms triple tuple based on value match. Args: tuple_list (list[tuple[str, str, dict]]): Existing list of selector tuples. If None or empty, returns empty list. match_list (list[str]): Values to match against. If None or empty, no matches will occur. match_update_dict (dict): Metadata to apply if value is in `match_list`. If None, no update is applied. unmatch_update_dict (dict | None): Metadata to apply otherwise (optional). If None, no update is applied to unmatched. Returns: list[tuple[str, str, dict]]: Updated list of selector tuples. Examples: Input : tuple_list = [('A', 'A', {}), ('B', 'B', {})] match_list = ['A'] match_update_dict = {'disabled': True} unmatch_update_dict = {'class': 'available'} Output: [('A', 'A', {'disabled': True}), ('B', 'B', {'class': 'available'})] Input : [], ['A'], {'disabled': True}, None Output: [] Input : None, ['A'], {'disabled': True}, None Output: [] Notes: - If `tuple_list` is None or empty, returns empty list. - If `match_list` is None or empty, no matches will occur. \"\"\" if unmatch_update_dict is None : unmatch_update_dict = {} result = [] for key , value , meta in tuple_list : meta . update ( match_update_dict if key in match_list else unmatch_update_dict ) result . append (( key , value , meta )) return result upload_single_file ( upload_dir , request_file ) Save a user-uploaded file to the server using a secure, timestamped filename. Parameters: upload_dir ( str | Path ) \u2013 Directory to save the uploaded file. If None or empty, raises ValueError. request_file ( FileStorage ) \u2013 Werkzeug object from request.files['<field>'] . If None, raises ValueError. Returns: Path ( Path ) \u2013 Full path to the uploaded file on disk. Raises: OSError \u2013 If the file cannot be written to disk. ValueError \u2013 If upload_dir or request_file is None or empty. Examples: Input : file = request.files['data'], upload_dir = \"/data/uploads\" Output: Path object pointing to a securely saved file Input : upload_dir=None, request_file=file Output: ValueError Input : upload_dir=\"/data/uploads\", request_file=None Output: ValueError Notes Uses a secure, timestamped filename for storage. If upload_dir or request_file is None or empty, raises ValueError. Source code in arb\\utils\\web_html.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def upload_single_file ( upload_dir : str | Path , request_file : FileStorage ) -> Path : \"\"\" Save a user-uploaded file to the server using a secure, timestamped filename. Args: upload_dir (str | Path): Directory to save the uploaded file. If None or empty, raises ValueError. request_file (FileStorage): Werkzeug object from `request.files['<field>']`. If None, raises ValueError. Returns: Path: Full path to the uploaded file on disk. Raises: OSError: If the file cannot be written to disk. ValueError: If `upload_dir` or `request_file` is None or empty. Examples: Input : file = request.files['data'], upload_dir = \"/data/uploads\" Output: Path object pointing to a securely saved file Input : upload_dir=None, request_file=file Output: ValueError Input : upload_dir=\"/data/uploads\", request_file=None Output: ValueError Notes: - Uses a secure, timestamped filename for storage. - If `upload_dir` or `request_file` is None or empty, raises ValueError. \"\"\" logger . debug ( f \"Attempting to upload { request_file . filename =} \" ) if not request_file . filename : raise ValueError ( \"request_file.filename must not be None or empty\" ) file_name = get_secure_timestamped_file_name ( upload_dir , request_file . filename ) logger . debug ( f \"Upload single file as: { file_name } \" ) request_file . save ( file_name ) return file_name","title":"arb.utils.web_html"},{"location":"reference/arb/utils/web_html/#arbutilsweb_html","text":"HTML and WTForms utility functions for form handling and file uploads. This module provides helper functions for Uploading user files with sanitized names Generating WTForms-compatible selector lists Managing triple tuples for dynamic dropdown metadata Notes Avoid circular imports by not depending on other utility modules. Other utility modules (e.g., Excel, DB) may safely import this one. Adds \"Please Select\" logic to dropdowns using arb.utils.constants . Examples: Input : file = request.files['data'], upload_dir = \"/data/uploads\" Output: Path object pointing to a securely saved file","title":"arb.utils.web_html"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.ensure_placeholder_option","text":"Ensure a placeholder entry is present in the tuple list. This function ensures that a specified \"placeholder\" option (typically used to prompt users to select a value, such as \"Please Select\") exists in the given list of selector options. If the placeholder is not present, it is inserted at the top. If it exists but is not the first item, it is optionally moved to the first position. Parameters: tuple_list ( list [ tuple [ str , str , dict ]] ) \u2013 Original selector list. If None or empty, returns a list with only the placeholder. item ( str , default: PLEASE_SELECT ) \u2013 Value for the placeholder. Default is \"Please Select\". If None, uses default. item_dict ( dict , default: None ) \u2013 Metadata for the placeholder. Default disables the option. If None, uses default. ensure_first ( bool , default: True ) \u2013 If True, move placeholder to top if found elsewhere. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Updated tuple list with ensured placeholder. Examples: Input : [(\"A\", \"A\", {})] Output: [('Please Select', 'Please Select', {'disabled': True}), ('A', 'A', {})] Input : None Output: [('Please Select', 'Please Select', {'disabled': True})] Input : [], item=None Output: [('Please Select', 'Please Select', {'disabled': True})] Notes If tuple_list is None or empty, returns a list with only the placeholder. If item is None, uses \"Please Select\". If item_dict is None, uses {\"disabled\": True}. Source code in arb\\utils\\web_html.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def ensure_placeholder_option ( tuple_list : list [ tuple [ str , str , dict ]], item : str = PLEASE_SELECT , item_dict : dict = None , ensure_first : bool = True ) -> list [ tuple [ str , str , dict ]]: \"\"\" Ensure a placeholder entry is present in the tuple list. This function ensures that a specified \"placeholder\" option (typically used to prompt users to select a value, such as \"Please Select\") exists in the given list of selector options. If the placeholder is not present, it is inserted at the top. If it exists but is not the first item, it is optionally moved to the first position. Args: tuple_list (list[tuple[str, str, dict]]): Original selector list. If None or empty, returns a list with only the placeholder. item (str): Value for the placeholder. Default is \"Please Select\". If None, uses default. item_dict (dict): Metadata for the placeholder. Default disables the option. If None, uses default. ensure_first (bool): If True, move placeholder to top if found elsewhere. Returns: list[tuple[str, str, dict]]: Updated tuple list with ensured placeholder. Examples: Input : [(\"A\", \"A\", {})] Output: [('Please Select', 'Please Select', {'disabled': True}), ('A', 'A', {})] Input : None Output: [('Please Select', 'Please Select', {'disabled': True})] Input : [], item=None Output: [('Please Select', 'Please Select', {'disabled': True})] Notes: - If `tuple_list` is None or empty, returns a list with only the placeholder. - If `item` is None, uses \"Please Select\". - If `item_dict` is None, uses {\"disabled\": True}. \"\"\" if item is None : item = PLEASE_SELECT if item_dict is None : item_dict = { \"disabled\" : True } placeholder = ( item , item , item_dict ) # Find the index of any existing placeholder (based on value match) # Explanation: # - `enumerate(tuple_list)` produces (index, tuple) pairs. # - `t[0] == item` checks if the first element (value) matches the placeholder value. # - `next(...)` returns the index of the first match, or `None` if no match is found. index = next (( i for i , t in enumerate ( tuple_list ) if t [ 0 ] == item ), None ) if index is None : # The Placeholder is not found; insert it at the beginning of the list. return [ placeholder ] + tuple_list elif ensure_first and index != 0 : # Placeholder found but not in first position and `ensure_first` is True. # Move it to the front while preserving the order of the rest. reordered = [ t for i , t in enumerate ( tuple_list ) if i != index ] return [ tuple_list [ index ]] + reordered # Placeholder exists and is already in the correct position; return unchanged. return tuple_list","title":"ensure_placeholder_option"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.list_to_triple_tuple","text":"Convert a list of strings into WTForms triple tuples. Each tuple contains (value, label, metadata). Parameters: values ( list [ str ] ) \u2013 List of form options. If None or empty, returns an empty list. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Triple tuples for WTForms SelectField. Examples: Input : [\"A\", \"B\"] Output: [('A', 'A', {}), ('B', 'B', {})] Input : [] Output: [] Input : None Output: [] Notes If values is None or empty, returns an empty list. Source code in arb\\utils\\web_html.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def list_to_triple_tuple ( values : list [ str ]) -> list [ tuple [ str , str , dict ]]: \"\"\" Convert a list of strings into WTForms triple tuples. Each tuple contains (value, label, metadata). Args: values (list[str]): List of form options. If None or empty, returns an empty list. Returns: list[tuple[str, str, dict]]: Triple tuples for WTForms SelectField. Examples: Input : [\"A\", \"B\"] Output: [('A', 'A', {}), ('B', 'B', {})] Input : [] Output: [] Input : None Output: [] Notes: - If `values` is None or empty, returns an empty list. \"\"\" return [( v , v , {}) for v in values ]","title":"list_to_triple_tuple"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.remove_items","text":"Remove one or more values from a tuple list by matching the first element. Parameters: tuple_list ( list [ tuple [ str , str , dict ]] ) \u2013 Selector tuples. If None or empty, returns empty list. remove_items ( str | list [ str ] ) \u2013 One or more values to remove by key match. If None or empty, returns the original list. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Filtered list excluding the removed values. Examples: Input : [(\"A\", \"A\", {}), (\"B\", \"B\", {})], remove_items=\"B\" Output: [('A', 'A', {})] Input : [(\"A\", \"A\", {}), (\"B\", \"B\", {})], remove_items=[\"A\", \"B\"] Output: [] Input : [], remove_items=\"A\" Output: [] Input : None, remove_items=\"A\" Output: [] Notes If tuple_list is None or empty, returns empty list. If remove_items is None or empty, returns the original list. Source code in arb\\utils\\web_html.py 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 def remove_items ( tuple_list : list [ tuple [ str , str , dict ]], remove_items : str | list [ str ] ) -> list [ tuple [ str , str , dict ]]: \"\"\" Remove one or more values from a tuple list by matching the first element. Args: tuple_list (list[tuple[str, str, dict]]): Selector tuples. If None or empty, returns empty list. remove_items (str | list[str]): One or more values to remove by key match. If None or empty, returns the original list. Returns: list[tuple[str, str, dict]]: Filtered list excluding the removed values. Examples: Input : [(\"A\", \"A\", {}), (\"B\", \"B\", {})], remove_items=\"B\" Output: [('A', 'A', {})] Input : [(\"A\", \"A\", {}), (\"B\", \"B\", {})], remove_items=[\"A\", \"B\"] Output: [] Input : [], remove_items=\"A\" Output: [] Input : None, remove_items=\"A\" Output: [] Notes: - If `tuple_list` is None or empty, returns empty list. - If `remove_items` is None or empty, returns the original list. \"\"\" remove_set = { remove_items } if isinstance ( remove_items , str ) else set ( remove_items ) return [ t for t in tuple_list if t [ 0 ] not in remove_set ]","title":"remove_items"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.run_diagnostics","text":"Run assertions to validate selector utility behavior. Tests Conversion of string lists to selector tuples Tuple updating with metadata Placeholder insertion Value removal from selector lists Dict transformation to tuple selectors Returns: None \u2013 None Source code in arb\\utils\\web_html.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def run_diagnostics () -> None : \"\"\" Run assertions to validate selector utility behavior. Tests: - Conversion of string lists to selector tuples - Tuple updating with metadata - Placeholder insertion - Value removal from selector lists - Dict transformation to tuple selectors Returns: None \"\"\" print ( \"Running diagnostics for web_html.py...\" ) test_values = [ \"A\" , \"B\" , \"C\" ] # Test selector_list_to_tuples selector = selector_list_to_tuples ( test_values ) assert selector [ 0 ][ 0 ] == PLEASE_SELECT assert ( \"A\" , \"A\" ) in selector # Test list_to_triple_tuple triple = list_to_triple_tuple ([ \"X\" , \"Y\" ]) assert triple == [( \"X\" , \"X\" , {}), ( \"Y\" , \"Y\" , {})] # Test update_triple_tuple_dict updated = update_triple_tuple_dict ( triple , [ \"Y\" ], { \"selected\" : True }) assert updated [ 1 ][ 2 ] . get ( \"selected\" ) is True # Test update_selector_dict test_dict = { \"colors\" : [ \"red\" , \"green\" ]} updated_dict = update_selector_dict ( test_dict ) assert PLEASE_SELECT in [ x [ 0 ] for x in updated_dict [ \"colors\" ]] # Test ensure_placeholder_option reordered = ensure_placeholder_option ([( \"X\" , \"X\" , {})]) assert reordered [ 0 ][ 0 ] == PLEASE_SELECT # Test remove_items cleaned = remove_items ( triple , \"X\" ) assert all ( t [ 0 ] != \"X\" for t in cleaned ) print ( \"All selector diagnostics passed.\" )","title":"run_diagnostics"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.selector_list_to_tuples","text":"Convert a list of values into WTForms-compatible dropdown tuples. Adds a disabled \"Please Select\" entry at the top of the list. Parameters: values ( list [ str ] ) \u2013 Dropdown options (excluding \"Please Select\"). If None or empty, returns only the placeholder. Returns: list [ tuple [ str , str ] | tuple [ str , str , dict ]] \u2013 list[tuple[str, str] | tuple[str, str, dict]]: WTForms selector list including a disabled \"Please Select\" entry. Examples: Input : [\"Red\", \"Green\"] Output: [('Please Select', 'Please Select', {'disabled': True}), ('Red', 'Red'), ('Green', 'Green')] Input : [] Output: [('Please Select', 'Please Select', {'disabled': True})] Input : None Output: [('Please Select', 'Please Select', {'disabled': True})] Notes If values is None or empty, returns only the placeholder. Source code in arb\\utils\\web_html.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def selector_list_to_tuples ( values : list [ str ]) -> list [ tuple [ str , str ] | tuple [ str , str , dict ]]: \"\"\" Convert a list of values into WTForms-compatible dropdown tuples. Adds a disabled \"Please Select\" entry at the top of the list. Args: values (list[str]): Dropdown options (excluding \"Please Select\"). If None or empty, returns only the placeholder. Returns: list[tuple[str, str] | tuple[str, str, dict]]: WTForms selector list including a disabled \"Please Select\" entry. Examples: Input : [\"Red\", \"Green\"] Output: [('Please Select', 'Please Select', {'disabled': True}), ('Red', 'Red'), ('Green', 'Green')] Input : [] Output: [('Please Select', 'Please Select', {'disabled': True})] Input : None Output: [('Please Select', 'Please Select', {'disabled': True})] Notes: - If `values` is None or empty, returns only the placeholder. \"\"\" result = [( PLEASE_SELECT , PLEASE_SELECT , { \"disabled\" : True })] result += [( v , v ) for v in values ] return result","title":"selector_list_to_tuples"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.update_selector_dict","text":"Convert dictionary of string lists into selector-style tuple lists. Each list is transformed to include a \"Please Select\" disabled option followed by (value, label) tuples. Parameters: input_dict ( dict [ str , list [ str ]] ) \u2013 Dict of dropdown options per field. If None or empty, returns empty dict. Returns: dict [ str , list [ tuple [ str , str ] | tuple [ str , str , dict ]]] \u2013 dict[str, list[tuple[str, str] | tuple[str, str, dict]]]: Dict with WTForms-ready selector tuples. Examples: Input : {\"colors\": [\"Red\", \"Blue\"]} Output: { \"colors\": [ (\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"Red\", \"Red\"), (\"Blue\", \"Blue\") ] } Input : {} Output: {} Input : None Output: {} Notes If input_dict is None or empty, returns empty dict. Source code in arb\\utils\\web_html.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def update_selector_dict ( input_dict : dict [ str , list [ str ]]) -> dict [ str , list [ tuple [ str , str ] | tuple [ str , str , dict ]]]: \"\"\" Convert dictionary of string lists into selector-style tuple lists. Each list is transformed to include a \"Please Select\" disabled option followed by (value, label) tuples. Args: input_dict (dict[str, list[str]]): Dict of dropdown options per field. If None or empty, returns empty dict. Returns: dict[str, list[tuple[str, str] | tuple[str, str, dict]]]: Dict with WTForms-ready selector tuples. Examples: Input : {\"colors\": [\"Red\", \"Blue\"]} Output: { \"colors\": [ (\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"Red\", \"Red\"), (\"Blue\", \"Blue\") ] } Input : {} Output: {} Input : None Output: {} Notes: - If `input_dict` is None or empty, returns empty dict. \"\"\" return { key : selector_list_to_tuples ( values ) for key , values in input_dict . items ()}","title":"update_selector_dict"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.update_triple_tuple_dict","text":"Update the metadata dict of each WTForms triple tuple based on value match. Parameters: tuple_list ( list [ tuple [ str , str , dict ]] ) \u2013 Existing list of selector tuples. If None or empty, returns empty list. match_list ( list [ str ] ) \u2013 Values to match against. If None or empty, no matches will occur. match_update_dict ( dict ) \u2013 Metadata to apply if value is in match_list . If None, no update is applied. unmatch_update_dict ( dict | None , default: None ) \u2013 Metadata to apply otherwise (optional). If None, no update is applied to unmatched. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Updated list of selector tuples. Examples: Input : tuple_list = [('A', 'A', {}), ('B', 'B', {})] match_list = ['A'] match_update_dict = {'disabled': True} unmatch_update_dict = {'class': 'available'} Output: [('A', 'A', {'disabled': True}), ('B', 'B', {'class': 'available'})] Input : [], ['A'], {'disabled': True}, None Output: [] Input : None, ['A'], {'disabled': True}, None Output: [] Notes If tuple_list is None or empty, returns empty list. If match_list is None or empty, no matches will occur. Source code in arb\\utils\\web_html.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def update_triple_tuple_dict ( tuple_list : list [ tuple [ str , str , dict ]], match_list : list [ str ], match_update_dict : dict , unmatch_update_dict : dict | None = None ) -> list [ tuple [ str , str , dict ]]: \"\"\" Update the metadata dict of each WTForms triple tuple based on value match. Args: tuple_list (list[tuple[str, str, dict]]): Existing list of selector tuples. If None or empty, returns empty list. match_list (list[str]): Values to match against. If None or empty, no matches will occur. match_update_dict (dict): Metadata to apply if value is in `match_list`. If None, no update is applied. unmatch_update_dict (dict | None): Metadata to apply otherwise (optional). If None, no update is applied to unmatched. Returns: list[tuple[str, str, dict]]: Updated list of selector tuples. Examples: Input : tuple_list = [('A', 'A', {}), ('B', 'B', {})] match_list = ['A'] match_update_dict = {'disabled': True} unmatch_update_dict = {'class': 'available'} Output: [('A', 'A', {'disabled': True}), ('B', 'B', {'class': 'available'})] Input : [], ['A'], {'disabled': True}, None Output: [] Input : None, ['A'], {'disabled': True}, None Output: [] Notes: - If `tuple_list` is None or empty, returns empty list. - If `match_list` is None or empty, no matches will occur. \"\"\" if unmatch_update_dict is None : unmatch_update_dict = {} result = [] for key , value , meta in tuple_list : meta . update ( match_update_dict if key in match_list else unmatch_update_dict ) result . append (( key , value , meta )) return result","title":"update_triple_tuple_dict"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.upload_single_file","text":"Save a user-uploaded file to the server using a secure, timestamped filename. Parameters: upload_dir ( str | Path ) \u2013 Directory to save the uploaded file. If None or empty, raises ValueError. request_file ( FileStorage ) \u2013 Werkzeug object from request.files['<field>'] . If None, raises ValueError. Returns: Path ( Path ) \u2013 Full path to the uploaded file on disk. Raises: OSError \u2013 If the file cannot be written to disk. ValueError \u2013 If upload_dir or request_file is None or empty. Examples: Input : file = request.files['data'], upload_dir = \"/data/uploads\" Output: Path object pointing to a securely saved file Input : upload_dir=None, request_file=file Output: ValueError Input : upload_dir=\"/data/uploads\", request_file=None Output: ValueError Notes Uses a secure, timestamped filename for storage. If upload_dir or request_file is None or empty, raises ValueError. Source code in arb\\utils\\web_html.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def upload_single_file ( upload_dir : str | Path , request_file : FileStorage ) -> Path : \"\"\" Save a user-uploaded file to the server using a secure, timestamped filename. Args: upload_dir (str | Path): Directory to save the uploaded file. If None or empty, raises ValueError. request_file (FileStorage): Werkzeug object from `request.files['<field>']`. If None, raises ValueError. Returns: Path: Full path to the uploaded file on disk. Raises: OSError: If the file cannot be written to disk. ValueError: If `upload_dir` or `request_file` is None or empty. Examples: Input : file = request.files['data'], upload_dir = \"/data/uploads\" Output: Path object pointing to a securely saved file Input : upload_dir=None, request_file=file Output: ValueError Input : upload_dir=\"/data/uploads\", request_file=None Output: ValueError Notes: - Uses a secure, timestamped filename for storage. - If `upload_dir` or `request_file` is None or empty, raises ValueError. \"\"\" logger . debug ( f \"Attempting to upload { request_file . filename =} \" ) if not request_file . filename : raise ValueError ( \"request_file.filename must not be None or empty\" ) file_name = get_secure_timestamped_file_name ( upload_dir , request_file . filename ) logger . debug ( f \"Upload single file as: { file_name } \" ) request_file . save ( file_name ) return file_name","title":"upload_single_file"},{"location":"reference/arb/utils/wtf_forms_util/","text":"arb.utils.wtf_forms_util Functions and helper classes to support WTForms models. Notes WTForm model classes should remain adjacent to Flask views (e.g., in wtf_landfill.py ) This module is for shared utilities, validators, and form-to-model conversion logic. build_choices ( header , items ) Combine header and dynamic items into a list of triple-tuples for WTForms SelectFields. Parameters: header ( list [ tuple [ str , str , dict ]] ) \u2013 Static options to appear first in the dropdown. Must not be None. items ( list [ str ] ) \u2013 Dynamic option values to convert into (value, label, {}) format. If None or empty, only header is returned. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Combined list of header and generated item tuples. Examples: Input : build_choices([(\"Please Select\", \"Please Select\", {\"disabled\": True})], [\"One\", \"Two\"]) Output: [(\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"One\", \"One\", {}), (\"Two\", \"Two\", {})] Input : build_choices([(\"Please Select\", \"Please Select\", {\"disabled\": True})], []) Output: [(\"Please Select\", \"Please Select\", {\"disabled\": True})] Input : build_choices([], [\"A\"]) Output: [(\"A\", \"A\", {})] Notes If items is None or empty, only the header is returned. If header is None, raises an exception. Source code in arb\\utils\\wtf_forms_util.py 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 def build_choices ( header : list [ tuple [ str , str , dict ]], items : list [ str ]) -> list [ tuple [ str , str , dict ]]: \"\"\" Combine header and dynamic items into a list of triple-tuples for WTForms SelectFields. Args: header (list[tuple[str, str, dict]]): Static options to appear first in the dropdown. Must not be None. items (list[str]): Dynamic option values to convert into (value, label, {}) format. If None or empty, only header is returned. Returns: list[tuple[str, str, dict]]: Combined list of header and generated item tuples. Examples: Input : build_choices([(\"Please Select\", \"Please Select\", {\"disabled\": True})], [\"One\", \"Two\"]) Output: [(\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"One\", \"One\", {}), (\"Two\", \"Two\", {})] Input : build_choices([(\"Please Select\", \"Please Select\", {\"disabled\": True})], []) Output: [(\"Please Select\", \"Please Select\", {\"disabled\": True})] Input : build_choices([], [\"A\"]) Output: [(\"A\", \"A\", {})] Notes: - If items is None or empty, only the header is returned. - If header is None, raises an exception. \"\"\" footer = [( item , item , {}) for item in items ] return header + footer change_validators ( form , field_names_to_change , old_validator , new_validator ) Replace one validator type with another on a list of WTForms fields. Parameters: form ( FlaskForm ) \u2013 WTForms form instance. Must not be None. field_names_to_change ( list [ str ] ) \u2013 List of fields to alter. If None or empty, no action is taken. old_validator ( type ) \u2013 Validator class to remove (e.g., Optional). Must not be None. new_validator ( type ) \u2013 Validator class to add (e.g., InputRequired). Must not be None. Returns: None \u2013 None Examples: Input : change_validators(form, [\"name\"], Optional, InputRequired) Output: Replaces Optional with InputRequired on the 'name' field Input : change_validators(form, None, Optional, InputRequired) Output: No action Input : change_validators(form, [\"name\"], None, InputRequired) Output: No action Notes If field_names_to_change is None or empty, no action is taken. If old_validator or new_validator is None, no action is taken. Source code in arb\\utils\\wtf_forms_util.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def change_validators ( form : FlaskForm , field_names_to_change : list [ str ], old_validator : type , new_validator : type ) -> None : \"\"\" Replace one validator type with another on a list of WTForms fields. Args: form (FlaskForm): WTForms form instance. Must not be None. field_names_to_change (list[str]): List of fields to alter. If None or empty, no action is taken. old_validator (type): Validator class to remove (e.g., Optional). Must not be None. new_validator (type): Validator class to add (e.g., InputRequired). Must not be None. Returns: None Examples: Input : change_validators(form, [\"name\"], Optional, InputRequired) Output: Replaces Optional with InputRequired on the 'name' field Input : change_validators(form, None, Optional, InputRequired) Output: No action Input : change_validators(form, [\"name\"], None, InputRequired) Output: No action Notes: - If `field_names_to_change` is None or empty, no action is taken. - If `old_validator` or `new_validator` is None, no action is taken. \"\"\" field_names = get_wtforms_fields ( form , include_csrf_token = False ) for field_name in field_names : if field_name in field_names_to_change : validators = form [ field_name ] . validators # Replace old_validator with new_validator, reassign as list form [ field_name ] . validators = [ new_validator () if isinstance ( v , old_validator ) else v for v in validators ] change_validators_on_test ( form , bool_test , required_if_true , optional_if_true = None ) Conditionally switch validators on selected form fields based on a boolean test. If bool_test is True Fields in required_if_true become required (InputRequired). Fields in optional_if_true become optional (Optional). If bool_test is False Fields in required_if_true become optional. Fields in optional_if_true become required. Parameters: form ( FlaskForm ) \u2013 The form to update. Must not be None. bool_test ( bool ) \u2013 If True, required/optional fields are swapped accordingly. required_if_true ( list [ str ] ) \u2013 Field names that become required when bool_test is True. If None or empty, no action is taken. optional_if_true ( list [ str ] | None , default: None ) \u2013 Field names that become optional when bool_test is True. If None, treated as empty list. Returns: None \u2013 None Examples: Input : change_validators_on_test(form, True, [\"name\"], [\"email\"]) Output: 'name' becomes required, 'email' becomes optional Input : change_validators_on_test(form, False, [\"name\"], [\"email\"]) Output: 'name' becomes optional, 'email' becomes required Input : change_validators_on_test(form, True, None, None) Output: No action Notes If required_if_true is None or empty, no action is taken. If optional_if_true is None, treated as empty list. Source code in arb\\utils\\wtf_forms_util.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def change_validators_on_test ( form : FlaskForm , bool_test : bool , required_if_true : list [ str ], optional_if_true : list [ str ] | None = None ) -> None : \"\"\" Conditionally switch validators on selected form fields based on a boolean test. If bool_test is True: - Fields in required_if_true become required (InputRequired). - Fields in optional_if_true become optional (Optional). If bool_test is False: - Fields in required_if_true become optional. - Fields in optional_if_true become required. Args: form (FlaskForm): The form to update. Must not be None. bool_test (bool): If True, required/optional fields are swapped accordingly. required_if_true (list[str]): Field names that become required when bool_test is True. If None or empty, no action is taken. optional_if_true (list[str] | None): Field names that become optional when bool_test is True. If None, treated as empty list. Returns: None Examples: Input : change_validators_on_test(form, True, [\"name\"], [\"email\"]) Output: 'name' becomes required, 'email' becomes optional Input : change_validators_on_test(form, False, [\"name\"], [\"email\"]) Output: 'name' becomes optional, 'email' becomes required Input : change_validators_on_test(form, True, None, None) Output: No action Notes: - If `required_if_true` is None or empty, no action is taken. - If `optional_if_true` is None, treated as empty list. \"\"\" if optional_if_true is None : optional_if_true = [] if bool_test : change_validators ( form , field_names_to_change = required_if_true , old_validator = Optional , new_validator = InputRequired , ) change_validators ( form , field_names_to_change = optional_if_true , old_validator = InputRequired , new_validator = Optional , ) else : change_validators ( form , field_names_to_change = required_if_true , old_validator = InputRequired , new_validator = Optional , ) change_validators ( form , field_names_to_change = optional_if_true , old_validator = Optional , new_validator = InputRequired , ) coerce_choices ( val ) Convert various dropdown data formats to a list of (str, str) tuples for WTForms SelectField. WTForms SelectField expects choices as a list of (value, label) tuples. This helper ensures compatibility regardless of the input format (dict, list of tuples, or list of strings). Parameters: val ( Any ) \u2013 The dropdown data, which may be a dict, list of tuples, or list of strings. Returns: list [ tuple [ str , str ]] \u2013 List[Tuple[str, str]]: A list of (value, label) tuples. Source code in arb\\utils\\wtf_forms_util.py 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 def coerce_choices ( val : Any ) -> list [ tuple [ str , str ]]: \"\"\" Convert various dropdown data formats to a list of (str, str) tuples for WTForms SelectField. WTForms SelectField expects choices as a list of (value, label) tuples. This helper ensures compatibility regardless of the input format (dict, list of tuples, or list of strings). Args: val: The dropdown data, which may be a dict, list of tuples, or list of strings. Returns: List[Tuple[str, str]]: A list of (value, label) tuples. \"\"\" if not val : return [] if isinstance ( val , dict ): return [( str ( k ), str ( v )) for k , v in val . items ()] if isinstance ( val , list ): # If already a list of tuples, convert to (str, str) using only first two elements if all ( isinstance ( x , tuple ) and len ( x ) >= 2 for x in val ): return [( str ( x [ 0 ]), str ( x [ 1 ])) for x in val ] # If a list of strings, convert to (str, str) if all ( isinstance ( x , str ) for x in val ): return [( x , x ) for x in val ] return [] ensure_field_choice ( field_name , field , choices = None ) Ensure a field's current value is among its valid choices, or reset it to a placeholder. Parameters: field_name ( str ) \u2013 Name of the WTForms field (for logging purposes). Must not be None. field ( Field ) \u2013 WTForms-compatible field (typically a SelectField). Must not be None. choices ( list [ tuple [ str , str ]] | list [ tuple [ str , str , dict ]] | None , default: None ) \u2013 Valid choices to enforce. If None, uses the field's existing choices. If both are None, uses an empty list. Returns: None \u2013 None Examples: Input : ensure_field_choice(\"sector\", field, [(\"A\", \"A\"), (\"B\", \"B\")]) Output: Resets field.data to placeholder if not in [\"A\", \"B\"] Input : ensure_field_choice(\"sector\", field, None) Output: Uses field.choices for validation Input : ensure_field_choice(\"sector\", None, [(\"A\", \"A\")]) Output: Exception Notes If choices is provided, this function sets field.choices to the new list. If choices is None, it uses the field's existing .choices. If both are None, uses an empty list. Resets field.data and field.raw_data to the placeholder if the value is invalid. If field is None, raises an exception. Source code in arb\\utils\\wtf_forms_util.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 def ensure_field_choice ( field_name : str , field , choices : list [ tuple [ str , str ] | tuple [ str , str , dict ]] | None = None ) -> None : \"\"\" Ensure a field's current value is among its valid choices, or reset it to a placeholder. Args: field_name (str): Name of the WTForms field (for logging purposes). Must not be None. field (Field): WTForms-compatible field (typically a SelectField). Must not be None. choices (list[tuple[str, str]] | list[tuple[str, str, dict]] | None): Valid choices to enforce. If None, uses the field's existing choices. If both are None, uses an empty list. Returns: None Examples: Input : ensure_field_choice(\"sector\", field, [(\"A\", \"A\"), (\"B\", \"B\")]) Output: Resets field.data to placeholder if not in [\"A\", \"B\"] Input : ensure_field_choice(\"sector\", field, None) Output: Uses field.choices for validation Input : ensure_field_choice(\"sector\", None, [(\"A\", \"A\")]) Output: Exception Notes: - If choices is provided, this function sets field.choices to the new list. - If choices is None, it uses the field's existing .choices. If both are None, uses an empty list. - Resets field.data and field.raw_data to the placeholder if the value is invalid. - If field is None, raises an exception. \"\"\" if choices is None : # Use existing field choices if none are supplied choices = field . choices if field . choices is not None else [] else : # Apply a new set of choices to the field field . choices = choices valid_values = { c [ 0 ] for c in choices } if field . data not in valid_values : logger . debug ( f \" { field_name } .data= { field . data !r} not in valid options, resetting to ' { PLEASE_SELECT } '\" ) field . data = PLEASE_SELECT field . raw_data = [ field . data ] format_raw_data ( field , value ) Convert a field value to a format suitable for WTForms .raw_data . Parameters: field ( Field ) \u2013 A WTForms field instance (e.g., DecimalField, DateTimeField). Must not be None. value ( str | int | float | Decimal | datetime | None ) \u2013 The field's data value. If None, returns an empty list. Returns: list [ str ] \u2013 list[str]: List of string values to assign to field.raw_data . Returns an empty list if value is None. Raises: ValueError \u2013 If the value type is unsupported (not str, int, float, Decimal, or datetime.datetime). Examples: Input : format_raw_data(field, Decimal(\"10.5\")) Output: ['10.5'] Input : format_raw_data(field, None) Output: [] Input : format_raw_data(field, object()) Output: ValueError Notes If value is None, returns an empty list. If value is a Decimal, casts to float before converting to string. If value is an unsupported type, raises ValueError. Source code in arb\\utils\\wtf_forms_util.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 def format_raw_data ( field : Field , value ) -> list [ str ]: \"\"\" Convert a field value to a format suitable for WTForms `.raw_data`. Args: field (Field): A WTForms field instance (e.g., DecimalField, DateTimeField). Must not be None. value (str | int | float | Decimal | datetime.datetime | None): The field's data value. If None, returns an empty list. Returns: list[str]: List of string values to assign to `field.raw_data`. Returns an empty list if value is None. Raises: ValueError: If the value type is unsupported (not str, int, float, Decimal, or datetime.datetime). Examples: Input : format_raw_data(field, Decimal(\"10.5\")) Output: ['10.5'] Input : format_raw_data(field, None) Output: [] Input : format_raw_data(field, object()) Output: ValueError Notes: - If value is None, returns an empty list. - If value is a Decimal, casts to float before converting to string. - If value is an unsupported type, raises ValueError. \"\"\" if value is None : return [] elif isinstance ( value , ( str , int , float )): return [ str ( value )] elif isinstance ( value , Decimal ): return [ str ( float ( value ))] # Cast to float before converting to string elif isinstance ( value , datetime . datetime ): return [ value . isoformat ()] else : raise ValueError ( f \"Unsupported type for raw_data: { type ( value ) } with value { value } \" ) get_payloads ( model , wtform , ignore_fields = None ) DEPRECATED: Use wtform_to_model() instead. Extract all field values and changed values from a WTForm. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model with JSON column misc_json . Must not be None. wtform ( FlaskForm ) \u2013 The form to extract values from. Must not be None. ignore_fields ( list [ str ] | None , default: None ) \u2013 List of fields to skip during comparison. If None, no fields are skipped. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: Tuple of (payload_all, payload_changes) - payload_all: All form fields - payload_changes: Subset of fields with changed values vs. model Examples: Input : get_payloads(model, form) Output: (all_fields_dict, changed_fields_dict) Input : get_payloads(model, form, ignore_fields=[\"id\"]) Output: (all_fields_dict, changed_fields_dict) excluding 'id' Input : get_payloads(None, form) Output: AttributeError Notes Performs a naive comparison (==) without deserializing types. Use skip_empty_fields = True to suppress null-like values. If model or wtform is None, raises AttributeError. Source code in arb\\utils\\wtf_forms_util.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 def get_payloads ( model : DeclarativeMeta , wtform : FlaskForm , ignore_fields : list [ str ] | None = None ) -> tuple [ dict , dict ]: \"\"\" DEPRECATED: Use `wtform_to_model()` instead. Extract all field values and changed values from a WTForm. Args: model (DeclarativeMeta): SQLAlchemy model with JSON column `misc_json`. Must not be None. wtform (FlaskForm): The form to extract values from. Must not be None. ignore_fields (list[str] | None): List of fields to skip during comparison. If None, no fields are skipped. Returns: tuple[dict, dict]: Tuple of (payload_all, payload_changes) - payload_all: All form fields - payload_changes: Subset of fields with changed values vs. model Examples: Input : get_payloads(model, form) Output: (all_fields_dict, changed_fields_dict) Input : get_payloads(model, form, ignore_fields=[\"id\"]) Output: (all_fields_dict, changed_fields_dict) excluding 'id' Input : get_payloads(None, form) Output: AttributeError Notes: - Performs a naive comparison (==) without deserializing types. - Use skip_empty_fields = True to suppress null-like values. - If model or wtform is None, raises AttributeError. \"\"\" if ignore_fields is None : ignore_fields = [] skip_empty_fields = False # Yes: if you wish to skip blank fields from being updated when feasible payload_all = {} payload_changes = {} model_json_dict = getattr ( model , \"misc_json\" ) or {} logger . debug ( f \" { model_json_dict =} \" ) model_field_names = list ( model_json_dict . keys ()) form_field_names = get_wtforms_fields ( wtform ) list_differences ( model_field_names , form_field_names , iterable_01_name = \"SQLAlchemy Model\" , iterable_02_name = \"WTForm Fields\" , print_warning = False , ) for form_field_name in form_field_names : field = getattr ( wtform , form_field_name ) field_value = field . data model_value = model_json_dict . get ( form_field_name ) if form_field_name in ignore_fields : continue if skip_empty_fields is True : # skipping empty strings if the model is \"\" or None if field_value == \"\" : if model_value in [ None , \"\" ]: continue # Only persist \"Please Select\" if overwriting a meaningful value. if isinstance ( field , SelectField ) and field_value == PLEASE_SELECT : if model_value in [ None , \"\" ]: continue payload_all [ form_field_name ] = field_value # todo (depreciated) - object types are not being seen as equivalent (because they are serialized strings) # need to update logic - check out prep_payload_for_json for uniform approach if model_value != field_value : payload_changes [ form_field_name ] = field_value return payload_all , payload_changes get_wtforms_fields ( form , include_csrf_token = False ) Return the sorted field names associated with a WTForms form. Parameters: form ( FlaskForm ) \u2013 The WTForms form instance. Must not be None. include_csrf_token ( bool , default: False ) \u2013 If True, include 'csrf_token' in the result. Defaults to False. Returns: list [ str ] \u2013 list[str]: Alphabetically sorted list of field names in the form. Returns an empty list if form is None. Examples: Input : get_wtforms_fields(form) Output: ['name', 'sector'] Input : get_wtforms_fields(None) Output: [] Notes If form is None, returns an empty list. Field names are sorted alphabetically. If include_csrf_token is False, 'csrf_token' is excluded from the result. Source code in arb\\utils\\wtf_forms_util.py 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 def get_wtforms_fields ( form : FlaskForm , include_csrf_token : bool = False ) -> list [ str ]: \"\"\" Return the sorted field names associated with a WTForms form. Args: form (FlaskForm): The WTForms form instance. Must not be None. include_csrf_token (bool): If True, include 'csrf_token' in the result. Defaults to False. Returns: list[str]: Alphabetically sorted list of field names in the form. Returns an empty list if form is None. Examples: Input : get_wtforms_fields(form) Output: ['name', 'sector'] Input : get_wtforms_fields(None) Output: [] Notes: - If form is None, returns an empty list. - Field names are sorted alphabetically. - If include_csrf_token is False, 'csrf_token' is excluded from the result. \"\"\" field_names = [ name for name in form . data if include_csrf_token or name != \"csrf_token\" ] field_names . sort () return field_names initialize_drop_downs ( form , default = None ) Set default values for uninitialized WTForms SelectFields. Parameters: form ( FlaskForm ) \u2013 The form containing SelectField fields to be initialized. Must not be None. default ( str | None , default: None ) \u2013 The value to assign to a field if its current value is None. If None, uses the application's global placeholder (e.g., \"Please Select\"). Returns: None \u2013 None Examples: Input : initialize_drop_downs(form, default=\"Please Select\") Output: Sets all SelectField fields to default if not initialized Input : initialize_drop_downs(form, default=None) Output: Sets all SelectField fields to the global placeholder if not initialized Input : initialize_drop_downs(None) Output: Exception Notes Fields that already have a value (even a falsy one like an empty string) are not modified. Only fields of type SelectField are affected. This function is typically used after form construction but before rendering or validation. If form is None, raises an exception. Source code in arb\\utils\\wtf_forms_util.py 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 def initialize_drop_downs ( form : FlaskForm , default : str | None = None ) -> None : \"\"\" Set default values for uninitialized WTForms SelectFields. Args: form (FlaskForm): The form containing SelectField fields to be initialized. Must not be None. default (str | None): The value to assign to a field if its current value is None. If None, uses the application's global placeholder (e.g., \"Please Select\"). Returns: None Examples: Input : initialize_drop_downs(form, default=\"Please Select\") Output: Sets all SelectField fields to default if not initialized Input : initialize_drop_downs(form, default=None) Output: Sets all SelectField fields to the global placeholder if not initialized Input : initialize_drop_downs(None) Output: Exception Notes: - Fields that already have a value (even a falsy one like an empty string) are not modified. - Only fields of type `SelectField` are affected. - This function is typically used after form construction but before rendering or validation. - If form is None, raises an exception. \"\"\" if default is None : default = PLEASE_SELECT logger . debug ( f \"Initializing drop-downs...\" ) for field in form : if isinstance ( field , SelectField ) and field . data is None : logger . debug ( f \" { field . name } set to default value: { default } \" ) field . data = default min_decimal_precision ( min_digits ) Return a validator for WTForms DecimalField enforcing minimum decimal precision. Parameters: min_digits ( int ) \u2013 Minimum number of digits required after the decimal. If None or less than 0, raises ValueError. Returns: Callable ( Callable ) \u2013 WTForms-compatible validator that raises ValidationError if decimal places are insufficient. Examples: Input : field = DecimalField(\"Amount\", validators=[min_decimal_precision(2)]) Output: Raises ValidationError if fewer than 2 decimal places are entered Input : min_digits=None Output: ValueError Input : min_digits=-1 Output: ValueError Notes If min_digits is None or less than 0, raises ValueError. Used as a custom validator in WTForms field definitions. Source code in arb\\utils\\wtf_forms_util.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def min_decimal_precision ( min_digits : int ) -> Callable : \"\"\" Return a validator for WTForms DecimalField enforcing minimum decimal precision. Args: min_digits (int): Minimum number of digits required after the decimal. If None or less than 0, raises ValueError. Returns: Callable: WTForms-compatible validator that raises ValidationError if decimal places are insufficient. Examples: Input : field = DecimalField(\"Amount\", validators=[min_decimal_precision(2)]) Output: Raises ValidationError if fewer than 2 decimal places are entered Input : min_digits=None Output: ValueError Input : min_digits=-1 Output: ValueError Notes: - If `min_digits` is None or less than 0, raises ValueError. - Used as a custom validator in WTForms field definitions. \"\"\" if min_digits is None or min_digits < 0 : raise ValueError ( \"min_digits must be a non-negative integer.\" ) def _min_decimal_precision ( form , field ): \"\"\" WTForms validator to enforce a minimum number of decimal places on a DecimalField. Args: form (FlaskForm): The form instance being validated (unused, required by WTForms signature). field (Field): The field instance to validate. Should have a .data attribute containing the value. Raises: ValidationError: If the value does not have the required number of decimal places or is not a valid number. Notes: - This is an internal helper returned by min_decimal_precision(). - Used as a custom validator in WTForms field definitions. \"\"\" logger . debug ( f \"_min_decimal_precision called with { form =} , { field =} \" ) if field . data is None : return try : value_str = str ( field . data ) if '.' in value_str : _ , decimals = value_str . split ( '.' ) if len ( decimals ) < min_digits : raise ValidationError () elif min_digits > 0 : raise ValidationError () except ( ValueError , TypeError ): raise ValidationError ( f \"Field must be a valid numeric value with at least { min_digits } decimal places.\" ) return _min_decimal_precision model_to_wtform ( model , wtform , json_column = 'misc_json' ) Populate a WTForm from a SQLAlchemy model's JSON column. This function loads the model's JSON field (typically 'misc_json') and sets WTForms field .data and .raw_data accordingly. Required for correct rendering and validation of pre-filled forms. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy model instance containing a JSON column. wtform ( FlaskForm ) \u2013 The WTForm instance to populate. json_column ( str , default: 'misc_json' ) \u2013 The attribute name of the JSON column. Defaults to \"misc_json\". Raises: ValueError \u2013 If a datetime value cannot be parsed. TypeError \u2013 If the field type is unsupported. Notes Supports preloading DateTimeField and DecimalField types. Converts ISO8601 UTC \u2192 localized Pacific time. Ignores JSON fields that don't map to WTForm fields. Source code in arb\\utils\\wtf_forms_util.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 def model_to_wtform ( model : AutomapBase , wtform : FlaskForm , json_column : str = \"misc_json\" ) -> None : \"\"\" Populate a WTForm from a SQLAlchemy model's JSON column. This function loads the model's JSON field (typically 'misc_json') and sets WTForms field `.data` and `.raw_data` accordingly. Required for correct rendering and validation of pre-filled forms. Args: model (AutomapBase): SQLAlchemy model instance containing a JSON column. wtform (FlaskForm): The WTForm instance to populate. json_column (str): The attribute name of the JSON column. Defaults to \"misc_json\". Raises: ValueError: If a datetime value cannot be parsed. TypeError: If the field type is unsupported. Notes: - Supports preloading DateTimeField and DecimalField types. - Converts ISO8601 UTC \u2192 localized Pacific time. - Ignores JSON fields that don't map to WTForm fields. \"\"\" model_json_dict = getattr ( model , json_column ) logger . debug ( f \"model_to_wtform called with model= { model } , json= { model_json_dict } \" ) # # Ensure dict, not str # if isinstance(model_json_dict, str): # try: # model_json_dict = json.loads(model_json_dict) # logger.debug(f\"Parsed JSON string into dict.\") # except json.JSONDecodeError: # logger.warning(f\"Invalid JSON in model's '{json_column}' column.\") # model_json_dict = {} if isinstance ( model_json_dict , str ) or model_json_dict is None : model_json_dict = safe_json_loads ( model_json_dict , context_label = f \"model's ' { json_column } ' column\" ) if model_json_dict is None : model_json_dict = {} model_id_incidence = getattr ( model , \"id_incidence\" , None ) if \"id_incidence\" in model_json_dict and model_json_dict [ \"id_incidence\" ] != model_id_incidence : logger . warning ( f \"[model_to_wtform] MISMATCH: model.id_incidence= { model_id_incidence } \" f \"!= misc_json['id_incidence']= { model_json_dict [ 'id_incidence' ] } \" ) form_fields = get_wtforms_fields ( wtform ) model_fields = list ( model_json_dict . keys ()) list_differences ( model_fields , form_fields , iterable_01_name = \"SQLAlchemy Model JSON\" , iterable_02_name = \"WTForm Fields\" , print_warning = False ) # Use utilities to get type map and convert model dict type_map , _ = wtform_types_and_values ( wtform ) parsed_dict = deserialize_dict ( model_json_dict , type_map , convert_time_to_ca = True ) for field_name in form_fields : field = getattr ( wtform , field_name ) model_value = parsed_dict . get ( field_name ) # Set field data and raw_data for proper rendering/validation field . data = model_value field . raw_data = format_raw_data ( field , model_value ) logger . debug ( f \"Set { field_name =} , data= { field . data } , raw_data= { field . raw_data } \" ) prep_payload_for_json ( payload , type_matching_dict = None ) Prepare a payload dictionary for JSON-safe serialization. Parameters: payload ( dict ) \u2013 Key-value updates extracted from a WTForm or another source. Must not be None. type_matching_dict ( dict [ str , type ] | None , default: None ) \u2013 Optional type coercion rules. If None, uses default type map. e.g., {\"id_incidence\": int, \"some_flag\": bool} Returns: dict ( dict ) \u2013 Transformed version of the payload, suitable for use in a model's JSON field. Examples: Input : prep_payload_for_json({\"id_incidence\": \"123\"}, {\"id_incidence\": int}) Output: {\"id_incidence\": 123} Input : prep_payload_for_json({}, None) Output: {} Input : prep_payload_for_json(None) Output: TypeError Notes Applies datetime to ISO, Decimal to float. Respects \"Please Select\" for placeholders. Values in type_matching_dict are explicitly cast to the specified types. If payload is None, raises TypeError. Source code in arb\\utils\\wtf_forms_util.py 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 def prep_payload_for_json ( payload : dict , type_matching_dict : dict [ str , type ] | None = None ) -> dict : \"\"\" Prepare a payload dictionary for JSON-safe serialization. Args: payload (dict): Key-value updates extracted from a WTForm or another source. Must not be None. type_matching_dict (dict[str, type] | None): Optional type coercion rules. If None, uses default type map. e.g., {\"id_incidence\": int, \"some_flag\": bool} Returns: dict: Transformed version of the payload, suitable for use in a model's JSON field. Examples: Input : prep_payload_for_json({\"id_incidence\": \"123\"}, {\"id_incidence\": int}) Output: {\"id_incidence\": 123} Input : prep_payload_for_json({}, None) Output: {} Input : prep_payload_for_json(None) Output: TypeError Notes: - Applies datetime to ISO, Decimal to float. - Respects \"Please Select\" for placeholders. - Values in `type_matching_dict` are explicitly cast to the specified types. - If payload is None, raises TypeError. \"\"\" type_matching_dict = type_matching_dict or { \"id_incidence\" : int } return make_dict_serializeable ( payload , type_map = type_matching_dict , convert_time_to_ca = True ) remove_validators ( form , field_names , validators_to_remove = None ) Remove specified validators from selected WTForms fields. Parameters: form ( FlaskForm ) \u2013 The WTForms form instance. Must not be None. field_names ( list [ str ] ) \u2013 List of field names to examine and modify. If None or empty, no action is taken. validators_to_remove ( list [ type ] | None , default: None ) \u2013 Validator classes to remove. Default to [InputRequired] if not provided. Returns: None \u2013 None Examples: Input : remove_validators(form, [\"name\", \"email\"], [InputRequired]) Output: Removes InputRequired validators from 'name' and 'email' fields Input : remove_validators(form, None, [InputRequired]) Output: No action Input : remove_validators(form, [], [InputRequired]) Output: No action Notes If field_names is None or empty, no action is taken. Useful when validator logic depends on user input or view context. Source code in arb\\utils\\wtf_forms_util.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def remove_validators ( form : FlaskForm , field_names : list [ str ], validators_to_remove : list [ type ] | None = None ) -> None : \"\"\" Remove specified validators from selected WTForms fields. Args: form (FlaskForm): The WTForms form instance. Must not be None. field_names (list[str]): List of field names to examine and modify. If None or empty, no action is taken. validators_to_remove (list[type] | None): Validator classes to remove. Default to [InputRequired] if not provided. Returns: None Examples: Input : remove_validators(form, [\"name\", \"email\"], [InputRequired]) Output: Removes InputRequired validators from 'name' and 'email' fields Input : remove_validators(form, None, [InputRequired]) Output: No action Input : remove_validators(form, [], [InputRequired]) Output: No action Notes: - If `field_names` is None or empty, no action is taken. - Useful when validator logic depends on user input or view context. \"\"\" if validators_to_remove is None : validators_to_remove = [ InputRequired ] fields = get_wtforms_fields ( form , include_csrf_token = False ) for field in fields : if field in field_names : validators = form [ field ] . validators # Reassign validators with those not matching types to remove form [ field ] . validators = [ v for v in validators if not any ( isinstance ( v , t ) for t in validators_to_remove )] update_model_with_payload ( model , payload , json_field = 'misc_json' , comment = '' ) Apply a JSON-safe payload to a model's JSON column and mark it as changed. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance to update. Must not be None. payload ( dict ) \u2013 Dictionary of updates to apply. Must not be None. json_field ( str , default: 'misc_json' ) \u2013 Name of the model's JSON column (default is \"misc_json\"). If None or invalid, raises AttributeError. comment ( str , default: '' ) \u2013 Optional comment to include with update logging. If None, treated as empty string. Returns: None \u2013 None Examples: Input : update_model_with_payload(model, {\"foo\": 1}) Output: Updates model's misc_json with foo=1 Input : update_model_with_payload(None, {\"foo\": 1}) Output: AttributeError Notes Calls prep_payload_for_json to ensure data integrity. Uses apply_json_patch_and_log to track and log changes. Deep-copies the existing JSON field to avoid side effects. If model or payload is None, raises AttributeError. Source code in arb\\utils\\wtf_forms_util.py 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 def update_model_with_payload ( model : DeclarativeMeta , payload : dict , json_field : str = \"misc_json\" , comment : str = \"\" ) -> None : \"\"\" Apply a JSON-safe payload to a model's JSON column and mark it as changed. Args: model (DeclarativeMeta): SQLAlchemy model instance to update. Must not be None. payload (dict): Dictionary of updates to apply. Must not be None. json_field (str): Name of the model's JSON column (default is \"misc_json\"). If None or invalid, raises AttributeError. comment (str): Optional comment to include with update logging. If None, treated as empty string. Returns: None Examples: Input : update_model_with_payload(model, {\"foo\": 1}) Output: Updates model's misc_json with foo=1 Input : update_model_with_payload(None, {\"foo\": 1}) Output: AttributeError Notes: - Calls `prep_payload_for_json` to ensure data integrity. - Uses `apply_json_patch_and_log` to track and log changes. - Deep-copies the existing JSON field to avoid side effects. - If model or payload is None, raises AttributeError. \"\"\" logger . debug ( f \"update_model_with_payload: { model =} , { payload =} \" ) model_json = copy . deepcopy ( getattr ( model , json_field ) or {}) new_payload = prep_payload_for_json ( payload ) model_json . update ( new_payload ) apply_json_patch_and_log ( model , json_field = json_field , updates = model_json , user = \"anonymous\" , comments = comment , ) logger . debug ( f \"Model JSON updated: { getattr ( model , json_field ) =} \" ) validate_no_csrf ( form , extra_validators = None ) Validate a WTForm while skipping CSRF errors (useful for GET-submitted forms). Parameters: form ( FlaskForm ) \u2013 The form to validate. Must not be None. extra_validators ( dict | None , default: None ) \u2013 Optional per-field validators to apply. If None, no extra validators are used. Returns: bool ( bool ) \u2013 True if the form is valid after removing CSRF errors, otherwise False. Examples: Input : validate_no_csrf(form) Output: True if valid, False if errors remain (except CSRF) Input : validate_no_csrf(None) Output: Exception Notes This allows validation to succeed even when CSRF tokens are missing or invalid. It logs before and after validation for debug purposes. If form is None, raises an exception. Source code in arb\\utils\\wtf_forms_util.py 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 def validate_no_csrf ( form : FlaskForm , extra_validators : dict | None = None ) -> bool : \"\"\" Validate a WTForm while skipping CSRF errors (useful for GET-submitted forms). Args: form (FlaskForm): The form to validate. Must not be None. extra_validators (dict | None): Optional per-field validators to apply. If None, no extra validators are used. Returns: bool: True if the form is valid after removing CSRF errors, otherwise False. Examples: Input : validate_no_csrf(form) Output: True if valid, False if errors remain (except CSRF) Input : validate_no_csrf(None) Output: Exception Notes: - This allows validation to succeed even when CSRF tokens are missing or invalid. - It logs before and after validation for debug purposes. - If form is None, raises an exception. \"\"\" logger . debug ( f \"validate_no_csrf() called:\" ) form . validate ( extra_validators = extra_validators ) if form . errors and 'csrf_token' in form . errors : del form . errors [ 'csrf_token' ] csrf_field = getattr ( form , 'csrf_token' , None ) if csrf_field : if csrf_field . errors : if 'The CSRF token is missing.' in csrf_field . errors : csrf_field . errors . remove ( 'The CSRF token is missing.' ) form_valid = not bool ( form . errors ) logger . debug ( f \"after validate_no_csrf() called: { form_valid =} , { form . errors =} \" ) return form_valid validate_selectors ( form , default = None ) Append validation errors for SelectFields left at default placeholder values. Parameters: form ( FlaskForm ) \u2013 WTForm instance containing SelectFields. Must not be None. default ( str | None , default: None ) \u2013 Placeholder value to treat as invalid (default: \"Please Select\"). If None, uses the global placeholder. Returns: None \u2013 None Examples: Input : validate_selectors(form, default=\"Please Select\") Output: Adds error to fields left at default Input : validate_selectors(form, default=None) Output: Adds error to fields left at the global placeholder Input : validate_selectors(None) Output: Exception Notes Typically used for GET-submitted forms where default values are not caught automatically. Adds \"This field is required.\" error to fields that are InputRequired but still at default. If form is None, raises an exception. Source code in arb\\utils\\wtf_forms_util.py 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 def validate_selectors ( form : FlaskForm , default : str | None = None ) -> None : \"\"\" Append validation errors for SelectFields left at default placeholder values. Args: form (FlaskForm): WTForm instance containing SelectFields. Must not be None. default (str | None): Placeholder value to treat as invalid (default: \"Please Select\"). If None, uses the global placeholder. Returns: None Examples: Input : validate_selectors(form, default=\"Please Select\") Output: Adds error to fields left at default Input : validate_selectors(form, default=None) Output: Adds error to fields left at the global placeholder Input : validate_selectors(None) Output: Exception Notes: - Typically used for GET-submitted forms where default values are not caught automatically. - Adds \"This field is required.\" error to fields that are InputRequired but still at default. - If form is None, raises an exception. \"\"\" if default is None : default = PLEASE_SELECT for field in form : if isinstance ( field , SelectField ): if field . data is None or field . data == default : for validator in field . validators : if isinstance ( validator , InputRequired ): msg = \"This field is required.\" if msg not in field . errors : field . errors . append ( msg ) # type: ignore wtf_count_errors ( form , log_errors = False ) Count validation errors on a WTForm instance. Parameters: form ( FlaskForm ) \u2013 The form to inspect. Must not be None. log_errors ( bool , default: False ) \u2013 If True, log the form's errors using debug log level. Returns: dict [ str , int ] \u2013 dict[str, int]: Dictionary with error counts: - 'elements_with_errors': number of fields that had one or more errors - 'element_error_count': total number of field-level errors - 'wtf_form_error_count': number of form-level (non-field) errors - 'total_error_count': sum of all error types Examples: Input : error_summary = wtf_count_errors(form) Output: error_summary[\"total_error_count\"] \u2192 total number of errors found Input : wtf_count_errors(None) Output: Exception Notes Ensure form.validate_on_submit() or form.validate() has been called first, or the error counts will be inaccurate. If form is None, an exception will be raised. Source code in arb\\utils\\wtf_forms_util.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def wtf_count_errors ( form : FlaskForm , log_errors : bool = False ) -> dict [ str , int ]: \"\"\" Count validation errors on a WTForm instance. Args: form (FlaskForm): The form to inspect. Must not be None. log_errors (bool): If True, log the form's errors using debug log level. Returns: dict[str, int]: Dictionary with error counts: - 'elements_with_errors': number of fields that had one or more errors - 'element_error_count': total number of field-level errors - 'wtf_form_error_count': number of form-level (non-field) errors - 'total_error_count': sum of all error types Examples: Input : error_summary = wtf_count_errors(form) Output: error_summary[\"total_error_count\"] \u2192 total number of errors found Input : wtf_count_errors(None) Output: Exception Notes: - Ensure `form.validate_on_submit()` or `form.validate()` has been called first, or the error counts will be inaccurate. - If `form` is None, an exception will be raised. \"\"\" error_count_dict = { 'elements_with_errors' : 0 , 'element_error_count' : 0 , 'wtf_form_error_count' : 0 , 'total_error_count' : 0 , } if log_errors : logger . debug ( f \"Form errors are: { form . errors } \" ) for field , error_list in form . errors . items (): if field is None : error_count_dict [ 'wtf_form_error_count' ] += len ( error_list ) else : error_count_dict [ 'elements_with_errors' ] += 1 error_count_dict [ 'element_error_count' ] += len ( error_list ) error_count_dict [ 'total_error_count' ] = ( error_count_dict [ 'element_error_count' ] + error_count_dict [ 'wtf_form_error_count' ] ) return error_count_dict wtform_to_model ( model , wtform , json_column = 'misc_json' , user = 'anonymous' , comments = '' , ignore_fields = None , type_matching_dict = None ) Extract data from a WTForm and update the model's JSON column. Logs all changes. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy model instance. Must not be None. wtform ( FlaskForm ) \u2013 WTForm with typed Python values. Must not be None. json_column ( str , default: 'misc_json' ) \u2013 JSON column name on the model. Defaults to \"misc_json\". If None or invalid, raises AttributeError. user ( str , default: 'anonymous' ) \u2013 Username for logging purposes. If None, defaults to \"anonymous\". comments ( str , default: '' ) \u2013 Optional comment for logging context. If None, treated as empty string. ignore_fields ( list [ str ] | None , default: None ) \u2013 Fields to exclude from update. If None, no fields are excluded. type_matching_dict ( dict [ str , type ] | None , default: None ) \u2013 Optional override for type enforcement. If None, uses default type map. Returns: None \u2013 None Examples: Input : wtform_to_model(model, form) Output: Updates model's JSON column with form data Input : wtform_to_model(model, form, ignore_fields=[\"id\"]) Output: Updates all fields except 'id' Input : wtform_to_model(None, form) Output: AttributeError Notes Uses make_dict_serializable and get_changed_fields to compare values. Delegates to apply_json_patch_and_log to persist and log changes. If model or wtform is None, raises AttributeError. Source code in arb\\utils\\wtf_forms_util.py 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 def wtform_to_model ( model : AutomapBase , wtform : FlaskForm , json_column : str = \"misc_json\" , user : str = \"anonymous\" , comments : str = \"\" , ignore_fields : list [ str ] | None = None , type_matching_dict : dict [ str , type ] | None = None ) -> None : \"\"\" Extract data from a WTForm and update the model's JSON column. Logs all changes. Args: model (AutomapBase): SQLAlchemy model instance. Must not be None. wtform (FlaskForm): WTForm with typed Python values. Must not be None. json_column (str): JSON column name on the model. Defaults to \"misc_json\". If None or invalid, raises AttributeError. user (str): Username for logging purposes. If None, defaults to \"anonymous\". comments (str): Optional comment for logging context. If None, treated as empty string. ignore_fields (list[str] | None): Fields to exclude from update. If None, no fields are excluded. type_matching_dict (dict[str, type] | None): Optional override for type enforcement. If None, uses default type map. Returns: None Examples: Input : wtform_to_model(model, form) Output: Updates model's JSON column with form data Input : wtform_to_model(model, form, ignore_fields=[\"id\"]) Output: Updates all fields except 'id' Input : wtform_to_model(None, form) Output: AttributeError Notes: - Uses make_dict_serializable and get_changed_fields to compare values. - Delegates to apply_json_patch_and_log to persist and log changes. - If model or wtform is None, raises AttributeError. \"\"\" ignore_fields_set = set ( ignore_fields or []) payload_all = { field_name : getattr ( wtform , field_name ) . data for field_name in get_wtforms_fields ( wtform ) if field_name not in ignore_fields_set } # Use manual overrides only \u2014 no type_map from form payload_all = make_dict_serializeable ( payload_all , type_map = type_matching_dict , convert_time_to_ca = True ) existing_json = load_model_json_column ( model , json_column ) # todo - shouldn't json already be serialized, not sure what the next line accomplishes existing_serialized = make_dict_serializeable ( existing_json , type_map = type_matching_dict , convert_time_to_ca = True ) payload_changes = get_changed_fields ( payload_all , existing_serialized ) if payload_changes : logger . info ( f \"wtform_to_model payload_changes: { payload_changes } \" ) apply_json_patch_and_log ( model , payload_changes , json_column , user = user , comments = comments ) logger . info ( f \"wtform_to_model payload_all: { payload_all } \" )","title":"arb.utils.wtf_forms_util"},{"location":"reference/arb/utils/wtf_forms_util/#arbutilswtf_forms_util","text":"Functions and helper classes to support WTForms models. Notes WTForm model classes should remain adjacent to Flask views (e.g., in wtf_landfill.py ) This module is for shared utilities, validators, and form-to-model conversion logic.","title":"arb.utils.wtf_forms_util"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.build_choices","text":"Combine header and dynamic items into a list of triple-tuples for WTForms SelectFields. Parameters: header ( list [ tuple [ str , str , dict ]] ) \u2013 Static options to appear first in the dropdown. Must not be None. items ( list [ str ] ) \u2013 Dynamic option values to convert into (value, label, {}) format. If None or empty, only header is returned. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Combined list of header and generated item tuples. Examples: Input : build_choices([(\"Please Select\", \"Please Select\", {\"disabled\": True})], [\"One\", \"Two\"]) Output: [(\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"One\", \"One\", {}), (\"Two\", \"Two\", {})] Input : build_choices([(\"Please Select\", \"Please Select\", {\"disabled\": True})], []) Output: [(\"Please Select\", \"Please Select\", {\"disabled\": True})] Input : build_choices([], [\"A\"]) Output: [(\"A\", \"A\", {})] Notes If items is None or empty, only the header is returned. If header is None, raises an exception. Source code in arb\\utils\\wtf_forms_util.py 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 def build_choices ( header : list [ tuple [ str , str , dict ]], items : list [ str ]) -> list [ tuple [ str , str , dict ]]: \"\"\" Combine header and dynamic items into a list of triple-tuples for WTForms SelectFields. Args: header (list[tuple[str, str, dict]]): Static options to appear first in the dropdown. Must not be None. items (list[str]): Dynamic option values to convert into (value, label, {}) format. If None or empty, only header is returned. Returns: list[tuple[str, str, dict]]: Combined list of header and generated item tuples. Examples: Input : build_choices([(\"Please Select\", \"Please Select\", {\"disabled\": True})], [\"One\", \"Two\"]) Output: [(\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"One\", \"One\", {}), (\"Two\", \"Two\", {})] Input : build_choices([(\"Please Select\", \"Please Select\", {\"disabled\": True})], []) Output: [(\"Please Select\", \"Please Select\", {\"disabled\": True})] Input : build_choices([], [\"A\"]) Output: [(\"A\", \"A\", {})] Notes: - If items is None or empty, only the header is returned. - If header is None, raises an exception. \"\"\" footer = [( item , item , {}) for item in items ] return header + footer","title":"build_choices"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.change_validators","text":"Replace one validator type with another on a list of WTForms fields. Parameters: form ( FlaskForm ) \u2013 WTForms form instance. Must not be None. field_names_to_change ( list [ str ] ) \u2013 List of fields to alter. If None or empty, no action is taken. old_validator ( type ) \u2013 Validator class to remove (e.g., Optional). Must not be None. new_validator ( type ) \u2013 Validator class to add (e.g., InputRequired). Must not be None. Returns: None \u2013 None Examples: Input : change_validators(form, [\"name\"], Optional, InputRequired) Output: Replaces Optional with InputRequired on the 'name' field Input : change_validators(form, None, Optional, InputRequired) Output: No action Input : change_validators(form, [\"name\"], None, InputRequired) Output: No action Notes If field_names_to_change is None or empty, no action is taken. If old_validator or new_validator is None, no action is taken. Source code in arb\\utils\\wtf_forms_util.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 def change_validators ( form : FlaskForm , field_names_to_change : list [ str ], old_validator : type , new_validator : type ) -> None : \"\"\" Replace one validator type with another on a list of WTForms fields. Args: form (FlaskForm): WTForms form instance. Must not be None. field_names_to_change (list[str]): List of fields to alter. If None or empty, no action is taken. old_validator (type): Validator class to remove (e.g., Optional). Must not be None. new_validator (type): Validator class to add (e.g., InputRequired). Must not be None. Returns: None Examples: Input : change_validators(form, [\"name\"], Optional, InputRequired) Output: Replaces Optional with InputRequired on the 'name' field Input : change_validators(form, None, Optional, InputRequired) Output: No action Input : change_validators(form, [\"name\"], None, InputRequired) Output: No action Notes: - If `field_names_to_change` is None or empty, no action is taken. - If `old_validator` or `new_validator` is None, no action is taken. \"\"\" field_names = get_wtforms_fields ( form , include_csrf_token = False ) for field_name in field_names : if field_name in field_names_to_change : validators = form [ field_name ] . validators # Replace old_validator with new_validator, reassign as list form [ field_name ] . validators = [ new_validator () if isinstance ( v , old_validator ) else v for v in validators ]","title":"change_validators"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.change_validators_on_test","text":"Conditionally switch validators on selected form fields based on a boolean test. If bool_test is True Fields in required_if_true become required (InputRequired). Fields in optional_if_true become optional (Optional). If bool_test is False Fields in required_if_true become optional. Fields in optional_if_true become required. Parameters: form ( FlaskForm ) \u2013 The form to update. Must not be None. bool_test ( bool ) \u2013 If True, required/optional fields are swapped accordingly. required_if_true ( list [ str ] ) \u2013 Field names that become required when bool_test is True. If None or empty, no action is taken. optional_if_true ( list [ str ] | None , default: None ) \u2013 Field names that become optional when bool_test is True. If None, treated as empty list. Returns: None \u2013 None Examples: Input : change_validators_on_test(form, True, [\"name\"], [\"email\"]) Output: 'name' becomes required, 'email' becomes optional Input : change_validators_on_test(form, False, [\"name\"], [\"email\"]) Output: 'name' becomes optional, 'email' becomes required Input : change_validators_on_test(form, True, None, None) Output: No action Notes If required_if_true is None or empty, no action is taken. If optional_if_true is None, treated as empty list. Source code in arb\\utils\\wtf_forms_util.py 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def change_validators_on_test ( form : FlaskForm , bool_test : bool , required_if_true : list [ str ], optional_if_true : list [ str ] | None = None ) -> None : \"\"\" Conditionally switch validators on selected form fields based on a boolean test. If bool_test is True: - Fields in required_if_true become required (InputRequired). - Fields in optional_if_true become optional (Optional). If bool_test is False: - Fields in required_if_true become optional. - Fields in optional_if_true become required. Args: form (FlaskForm): The form to update. Must not be None. bool_test (bool): If True, required/optional fields are swapped accordingly. required_if_true (list[str]): Field names that become required when bool_test is True. If None or empty, no action is taken. optional_if_true (list[str] | None): Field names that become optional when bool_test is True. If None, treated as empty list. Returns: None Examples: Input : change_validators_on_test(form, True, [\"name\"], [\"email\"]) Output: 'name' becomes required, 'email' becomes optional Input : change_validators_on_test(form, False, [\"name\"], [\"email\"]) Output: 'name' becomes optional, 'email' becomes required Input : change_validators_on_test(form, True, None, None) Output: No action Notes: - If `required_if_true` is None or empty, no action is taken. - If `optional_if_true` is None, treated as empty list. \"\"\" if optional_if_true is None : optional_if_true = [] if bool_test : change_validators ( form , field_names_to_change = required_if_true , old_validator = Optional , new_validator = InputRequired , ) change_validators ( form , field_names_to_change = optional_if_true , old_validator = InputRequired , new_validator = Optional , ) else : change_validators ( form , field_names_to_change = required_if_true , old_validator = InputRequired , new_validator = Optional , ) change_validators ( form , field_names_to_change = optional_if_true , old_validator = Optional , new_validator = InputRequired , )","title":"change_validators_on_test"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.coerce_choices","text":"Convert various dropdown data formats to a list of (str, str) tuples for WTForms SelectField. WTForms SelectField expects choices as a list of (value, label) tuples. This helper ensures compatibility regardless of the input format (dict, list of tuples, or list of strings). Parameters: val ( Any ) \u2013 The dropdown data, which may be a dict, list of tuples, or list of strings. Returns: list [ tuple [ str , str ]] \u2013 List[Tuple[str, str]]: A list of (value, label) tuples. Source code in arb\\utils\\wtf_forms_util.py 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 def coerce_choices ( val : Any ) -> list [ tuple [ str , str ]]: \"\"\" Convert various dropdown data formats to a list of (str, str) tuples for WTForms SelectField. WTForms SelectField expects choices as a list of (value, label) tuples. This helper ensures compatibility regardless of the input format (dict, list of tuples, or list of strings). Args: val: The dropdown data, which may be a dict, list of tuples, or list of strings. Returns: List[Tuple[str, str]]: A list of (value, label) tuples. \"\"\" if not val : return [] if isinstance ( val , dict ): return [( str ( k ), str ( v )) for k , v in val . items ()] if isinstance ( val , list ): # If already a list of tuples, convert to (str, str) using only first two elements if all ( isinstance ( x , tuple ) and len ( x ) >= 2 for x in val ): return [( str ( x [ 0 ]), str ( x [ 1 ])) for x in val ] # If a list of strings, convert to (str, str) if all ( isinstance ( x , str ) for x in val ): return [( x , x ) for x in val ] return []","title":"coerce_choices"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.ensure_field_choice","text":"Ensure a field's current value is among its valid choices, or reset it to a placeholder. Parameters: field_name ( str ) \u2013 Name of the WTForms field (for logging purposes). Must not be None. field ( Field ) \u2013 WTForms-compatible field (typically a SelectField). Must not be None. choices ( list [ tuple [ str , str ]] | list [ tuple [ str , str , dict ]] | None , default: None ) \u2013 Valid choices to enforce. If None, uses the field's existing choices. If both are None, uses an empty list. Returns: None \u2013 None Examples: Input : ensure_field_choice(\"sector\", field, [(\"A\", \"A\"), (\"B\", \"B\")]) Output: Resets field.data to placeholder if not in [\"A\", \"B\"] Input : ensure_field_choice(\"sector\", field, None) Output: Uses field.choices for validation Input : ensure_field_choice(\"sector\", None, [(\"A\", \"A\")]) Output: Exception Notes If choices is provided, this function sets field.choices to the new list. If choices is None, it uses the field's existing .choices. If both are None, uses an empty list. Resets field.data and field.raw_data to the placeholder if the value is invalid. If field is None, raises an exception. Source code in arb\\utils\\wtf_forms_util.py 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 def ensure_field_choice ( field_name : str , field , choices : list [ tuple [ str , str ] | tuple [ str , str , dict ]] | None = None ) -> None : \"\"\" Ensure a field's current value is among its valid choices, or reset it to a placeholder. Args: field_name (str): Name of the WTForms field (for logging purposes). Must not be None. field (Field): WTForms-compatible field (typically a SelectField). Must not be None. choices (list[tuple[str, str]] | list[tuple[str, str, dict]] | None): Valid choices to enforce. If None, uses the field's existing choices. If both are None, uses an empty list. Returns: None Examples: Input : ensure_field_choice(\"sector\", field, [(\"A\", \"A\"), (\"B\", \"B\")]) Output: Resets field.data to placeholder if not in [\"A\", \"B\"] Input : ensure_field_choice(\"sector\", field, None) Output: Uses field.choices for validation Input : ensure_field_choice(\"sector\", None, [(\"A\", \"A\")]) Output: Exception Notes: - If choices is provided, this function sets field.choices to the new list. - If choices is None, it uses the field's existing .choices. If both are None, uses an empty list. - Resets field.data and field.raw_data to the placeholder if the value is invalid. - If field is None, raises an exception. \"\"\" if choices is None : # Use existing field choices if none are supplied choices = field . choices if field . choices is not None else [] else : # Apply a new set of choices to the field field . choices = choices valid_values = { c [ 0 ] for c in choices } if field . data not in valid_values : logger . debug ( f \" { field_name } .data= { field . data !r} not in valid options, resetting to ' { PLEASE_SELECT } '\" ) field . data = PLEASE_SELECT field . raw_data = [ field . data ]","title":"ensure_field_choice"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.format_raw_data","text":"Convert a field value to a format suitable for WTForms .raw_data . Parameters: field ( Field ) \u2013 A WTForms field instance (e.g., DecimalField, DateTimeField). Must not be None. value ( str | int | float | Decimal | datetime | None ) \u2013 The field's data value. If None, returns an empty list. Returns: list [ str ] \u2013 list[str]: List of string values to assign to field.raw_data . Returns an empty list if value is None. Raises: ValueError \u2013 If the value type is unsupported (not str, int, float, Decimal, or datetime.datetime). Examples: Input : format_raw_data(field, Decimal(\"10.5\")) Output: ['10.5'] Input : format_raw_data(field, None) Output: [] Input : format_raw_data(field, object()) Output: ValueError Notes If value is None, returns an empty list. If value is a Decimal, casts to float before converting to string. If value is an unsupported type, raises ValueError. Source code in arb\\utils\\wtf_forms_util.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 def format_raw_data ( field : Field , value ) -> list [ str ]: \"\"\" Convert a field value to a format suitable for WTForms `.raw_data`. Args: field (Field): A WTForms field instance (e.g., DecimalField, DateTimeField). Must not be None. value (str | int | float | Decimal | datetime.datetime | None): The field's data value. If None, returns an empty list. Returns: list[str]: List of string values to assign to `field.raw_data`. Returns an empty list if value is None. Raises: ValueError: If the value type is unsupported (not str, int, float, Decimal, or datetime.datetime). Examples: Input : format_raw_data(field, Decimal(\"10.5\")) Output: ['10.5'] Input : format_raw_data(field, None) Output: [] Input : format_raw_data(field, object()) Output: ValueError Notes: - If value is None, returns an empty list. - If value is a Decimal, casts to float before converting to string. - If value is an unsupported type, raises ValueError. \"\"\" if value is None : return [] elif isinstance ( value , ( str , int , float )): return [ str ( value )] elif isinstance ( value , Decimal ): return [ str ( float ( value ))] # Cast to float before converting to string elif isinstance ( value , datetime . datetime ): return [ value . isoformat ()] else : raise ValueError ( f \"Unsupported type for raw_data: { type ( value ) } with value { value } \" )","title":"format_raw_data"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.get_payloads","text":"DEPRECATED: Use wtform_to_model() instead. Extract all field values and changed values from a WTForm. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model with JSON column misc_json . Must not be None. wtform ( FlaskForm ) \u2013 The form to extract values from. Must not be None. ignore_fields ( list [ str ] | None , default: None ) \u2013 List of fields to skip during comparison. If None, no fields are skipped. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: Tuple of (payload_all, payload_changes) - payload_all: All form fields - payload_changes: Subset of fields with changed values vs. model Examples: Input : get_payloads(model, form) Output: (all_fields_dict, changed_fields_dict) Input : get_payloads(model, form, ignore_fields=[\"id\"]) Output: (all_fields_dict, changed_fields_dict) excluding 'id' Input : get_payloads(None, form) Output: AttributeError Notes Performs a naive comparison (==) without deserializing types. Use skip_empty_fields = True to suppress null-like values. If model or wtform is None, raises AttributeError. Source code in arb\\utils\\wtf_forms_util.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 def get_payloads ( model : DeclarativeMeta , wtform : FlaskForm , ignore_fields : list [ str ] | None = None ) -> tuple [ dict , dict ]: \"\"\" DEPRECATED: Use `wtform_to_model()` instead. Extract all field values and changed values from a WTForm. Args: model (DeclarativeMeta): SQLAlchemy model with JSON column `misc_json`. Must not be None. wtform (FlaskForm): The form to extract values from. Must not be None. ignore_fields (list[str] | None): List of fields to skip during comparison. If None, no fields are skipped. Returns: tuple[dict, dict]: Tuple of (payload_all, payload_changes) - payload_all: All form fields - payload_changes: Subset of fields with changed values vs. model Examples: Input : get_payloads(model, form) Output: (all_fields_dict, changed_fields_dict) Input : get_payloads(model, form, ignore_fields=[\"id\"]) Output: (all_fields_dict, changed_fields_dict) excluding 'id' Input : get_payloads(None, form) Output: AttributeError Notes: - Performs a naive comparison (==) without deserializing types. - Use skip_empty_fields = True to suppress null-like values. - If model or wtform is None, raises AttributeError. \"\"\" if ignore_fields is None : ignore_fields = [] skip_empty_fields = False # Yes: if you wish to skip blank fields from being updated when feasible payload_all = {} payload_changes = {} model_json_dict = getattr ( model , \"misc_json\" ) or {} logger . debug ( f \" { model_json_dict =} \" ) model_field_names = list ( model_json_dict . keys ()) form_field_names = get_wtforms_fields ( wtform ) list_differences ( model_field_names , form_field_names , iterable_01_name = \"SQLAlchemy Model\" , iterable_02_name = \"WTForm Fields\" , print_warning = False , ) for form_field_name in form_field_names : field = getattr ( wtform , form_field_name ) field_value = field . data model_value = model_json_dict . get ( form_field_name ) if form_field_name in ignore_fields : continue if skip_empty_fields is True : # skipping empty strings if the model is \"\" or None if field_value == \"\" : if model_value in [ None , \"\" ]: continue # Only persist \"Please Select\" if overwriting a meaningful value. if isinstance ( field , SelectField ) and field_value == PLEASE_SELECT : if model_value in [ None , \"\" ]: continue payload_all [ form_field_name ] = field_value # todo (depreciated) - object types are not being seen as equivalent (because they are serialized strings) # need to update logic - check out prep_payload_for_json for uniform approach if model_value != field_value : payload_changes [ form_field_name ] = field_value return payload_all , payload_changes","title":"get_payloads"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.get_wtforms_fields","text":"Return the sorted field names associated with a WTForms form. Parameters: form ( FlaskForm ) \u2013 The WTForms form instance. Must not be None. include_csrf_token ( bool , default: False ) \u2013 If True, include 'csrf_token' in the result. Defaults to False. Returns: list [ str ] \u2013 list[str]: Alphabetically sorted list of field names in the form. Returns an empty list if form is None. Examples: Input : get_wtforms_fields(form) Output: ['name', 'sector'] Input : get_wtforms_fields(None) Output: [] Notes If form is None, returns an empty list. Field names are sorted alphabetically. If include_csrf_token is False, 'csrf_token' is excluded from the result. Source code in arb\\utils\\wtf_forms_util.py 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 def get_wtforms_fields ( form : FlaskForm , include_csrf_token : bool = False ) -> list [ str ]: \"\"\" Return the sorted field names associated with a WTForms form. Args: form (FlaskForm): The WTForms form instance. Must not be None. include_csrf_token (bool): If True, include 'csrf_token' in the result. Defaults to False. Returns: list[str]: Alphabetically sorted list of field names in the form. Returns an empty list if form is None. Examples: Input : get_wtforms_fields(form) Output: ['name', 'sector'] Input : get_wtforms_fields(None) Output: [] Notes: - If form is None, returns an empty list. - Field names are sorted alphabetically. - If include_csrf_token is False, 'csrf_token' is excluded from the result. \"\"\" field_names = [ name for name in form . data if include_csrf_token or name != \"csrf_token\" ] field_names . sort () return field_names","title":"get_wtforms_fields"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.initialize_drop_downs","text":"Set default values for uninitialized WTForms SelectFields. Parameters: form ( FlaskForm ) \u2013 The form containing SelectField fields to be initialized. Must not be None. default ( str | None , default: None ) \u2013 The value to assign to a field if its current value is None. If None, uses the application's global placeholder (e.g., \"Please Select\"). Returns: None \u2013 None Examples: Input : initialize_drop_downs(form, default=\"Please Select\") Output: Sets all SelectField fields to default if not initialized Input : initialize_drop_downs(form, default=None) Output: Sets all SelectField fields to the global placeholder if not initialized Input : initialize_drop_downs(None) Output: Exception Notes Fields that already have a value (even a falsy one like an empty string) are not modified. Only fields of type SelectField are affected. This function is typically used after form construction but before rendering or validation. If form is None, raises an exception. Source code in arb\\utils\\wtf_forms_util.py 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 def initialize_drop_downs ( form : FlaskForm , default : str | None = None ) -> None : \"\"\" Set default values for uninitialized WTForms SelectFields. Args: form (FlaskForm): The form containing SelectField fields to be initialized. Must not be None. default (str | None): The value to assign to a field if its current value is None. If None, uses the application's global placeholder (e.g., \"Please Select\"). Returns: None Examples: Input : initialize_drop_downs(form, default=\"Please Select\") Output: Sets all SelectField fields to default if not initialized Input : initialize_drop_downs(form, default=None) Output: Sets all SelectField fields to the global placeholder if not initialized Input : initialize_drop_downs(None) Output: Exception Notes: - Fields that already have a value (even a falsy one like an empty string) are not modified. - Only fields of type `SelectField` are affected. - This function is typically used after form construction but before rendering or validation. - If form is None, raises an exception. \"\"\" if default is None : default = PLEASE_SELECT logger . debug ( f \"Initializing drop-downs...\" ) for field in form : if isinstance ( field , SelectField ) and field . data is None : logger . debug ( f \" { field . name } set to default value: { default } \" ) field . data = default","title":"initialize_drop_downs"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.min_decimal_precision","text":"Return a validator for WTForms DecimalField enforcing minimum decimal precision. Parameters: min_digits ( int ) \u2013 Minimum number of digits required after the decimal. If None or less than 0, raises ValueError. Returns: Callable ( Callable ) \u2013 WTForms-compatible validator that raises ValidationError if decimal places are insufficient. Examples: Input : field = DecimalField(\"Amount\", validators=[min_decimal_precision(2)]) Output: Raises ValidationError if fewer than 2 decimal places are entered Input : min_digits=None Output: ValueError Input : min_digits=-1 Output: ValueError Notes If min_digits is None or less than 0, raises ValueError. Used as a custom validator in WTForms field definitions. Source code in arb\\utils\\wtf_forms_util.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def min_decimal_precision ( min_digits : int ) -> Callable : \"\"\" Return a validator for WTForms DecimalField enforcing minimum decimal precision. Args: min_digits (int): Minimum number of digits required after the decimal. If None or less than 0, raises ValueError. Returns: Callable: WTForms-compatible validator that raises ValidationError if decimal places are insufficient. Examples: Input : field = DecimalField(\"Amount\", validators=[min_decimal_precision(2)]) Output: Raises ValidationError if fewer than 2 decimal places are entered Input : min_digits=None Output: ValueError Input : min_digits=-1 Output: ValueError Notes: - If `min_digits` is None or less than 0, raises ValueError. - Used as a custom validator in WTForms field definitions. \"\"\" if min_digits is None or min_digits < 0 : raise ValueError ( \"min_digits must be a non-negative integer.\" ) def _min_decimal_precision ( form , field ): \"\"\" WTForms validator to enforce a minimum number of decimal places on a DecimalField. Args: form (FlaskForm): The form instance being validated (unused, required by WTForms signature). field (Field): The field instance to validate. Should have a .data attribute containing the value. Raises: ValidationError: If the value does not have the required number of decimal places or is not a valid number. Notes: - This is an internal helper returned by min_decimal_precision(). - Used as a custom validator in WTForms field definitions. \"\"\" logger . debug ( f \"_min_decimal_precision called with { form =} , { field =} \" ) if field . data is None : return try : value_str = str ( field . data ) if '.' in value_str : _ , decimals = value_str . split ( '.' ) if len ( decimals ) < min_digits : raise ValidationError () elif min_digits > 0 : raise ValidationError () except ( ValueError , TypeError ): raise ValidationError ( f \"Field must be a valid numeric value with at least { min_digits } decimal places.\" ) return _min_decimal_precision","title":"min_decimal_precision"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.model_to_wtform","text":"Populate a WTForm from a SQLAlchemy model's JSON column. This function loads the model's JSON field (typically 'misc_json') and sets WTForms field .data and .raw_data accordingly. Required for correct rendering and validation of pre-filled forms. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy model instance containing a JSON column. wtform ( FlaskForm ) \u2013 The WTForm instance to populate. json_column ( str , default: 'misc_json' ) \u2013 The attribute name of the JSON column. Defaults to \"misc_json\". Raises: ValueError \u2013 If a datetime value cannot be parsed. TypeError \u2013 If the field type is unsupported. Notes Supports preloading DateTimeField and DecimalField types. Converts ISO8601 UTC \u2192 localized Pacific time. Ignores JSON fields that don't map to WTForm fields. Source code in arb\\utils\\wtf_forms_util.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 def model_to_wtform ( model : AutomapBase , wtform : FlaskForm , json_column : str = \"misc_json\" ) -> None : \"\"\" Populate a WTForm from a SQLAlchemy model's JSON column. This function loads the model's JSON field (typically 'misc_json') and sets WTForms field `.data` and `.raw_data` accordingly. Required for correct rendering and validation of pre-filled forms. Args: model (AutomapBase): SQLAlchemy model instance containing a JSON column. wtform (FlaskForm): The WTForm instance to populate. json_column (str): The attribute name of the JSON column. Defaults to \"misc_json\". Raises: ValueError: If a datetime value cannot be parsed. TypeError: If the field type is unsupported. Notes: - Supports preloading DateTimeField and DecimalField types. - Converts ISO8601 UTC \u2192 localized Pacific time. - Ignores JSON fields that don't map to WTForm fields. \"\"\" model_json_dict = getattr ( model , json_column ) logger . debug ( f \"model_to_wtform called with model= { model } , json= { model_json_dict } \" ) # # Ensure dict, not str # if isinstance(model_json_dict, str): # try: # model_json_dict = json.loads(model_json_dict) # logger.debug(f\"Parsed JSON string into dict.\") # except json.JSONDecodeError: # logger.warning(f\"Invalid JSON in model's '{json_column}' column.\") # model_json_dict = {} if isinstance ( model_json_dict , str ) or model_json_dict is None : model_json_dict = safe_json_loads ( model_json_dict , context_label = f \"model's ' { json_column } ' column\" ) if model_json_dict is None : model_json_dict = {} model_id_incidence = getattr ( model , \"id_incidence\" , None ) if \"id_incidence\" in model_json_dict and model_json_dict [ \"id_incidence\" ] != model_id_incidence : logger . warning ( f \"[model_to_wtform] MISMATCH: model.id_incidence= { model_id_incidence } \" f \"!= misc_json['id_incidence']= { model_json_dict [ 'id_incidence' ] } \" ) form_fields = get_wtforms_fields ( wtform ) model_fields = list ( model_json_dict . keys ()) list_differences ( model_fields , form_fields , iterable_01_name = \"SQLAlchemy Model JSON\" , iterable_02_name = \"WTForm Fields\" , print_warning = False ) # Use utilities to get type map and convert model dict type_map , _ = wtform_types_and_values ( wtform ) parsed_dict = deserialize_dict ( model_json_dict , type_map , convert_time_to_ca = True ) for field_name in form_fields : field = getattr ( wtform , field_name ) model_value = parsed_dict . get ( field_name ) # Set field data and raw_data for proper rendering/validation field . data = model_value field . raw_data = format_raw_data ( field , model_value ) logger . debug ( f \"Set { field_name =} , data= { field . data } , raw_data= { field . raw_data } \" )","title":"model_to_wtform"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.prep_payload_for_json","text":"Prepare a payload dictionary for JSON-safe serialization. Parameters: payload ( dict ) \u2013 Key-value updates extracted from a WTForm or another source. Must not be None. type_matching_dict ( dict [ str , type ] | None , default: None ) \u2013 Optional type coercion rules. If None, uses default type map. e.g., {\"id_incidence\": int, \"some_flag\": bool} Returns: dict ( dict ) \u2013 Transformed version of the payload, suitable for use in a model's JSON field. Examples: Input : prep_payload_for_json({\"id_incidence\": \"123\"}, {\"id_incidence\": int}) Output: {\"id_incidence\": 123} Input : prep_payload_for_json({}, None) Output: {} Input : prep_payload_for_json(None) Output: TypeError Notes Applies datetime to ISO, Decimal to float. Respects \"Please Select\" for placeholders. Values in type_matching_dict are explicitly cast to the specified types. If payload is None, raises TypeError. Source code in arb\\utils\\wtf_forms_util.py 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 def prep_payload_for_json ( payload : dict , type_matching_dict : dict [ str , type ] | None = None ) -> dict : \"\"\" Prepare a payload dictionary for JSON-safe serialization. Args: payload (dict): Key-value updates extracted from a WTForm or another source. Must not be None. type_matching_dict (dict[str, type] | None): Optional type coercion rules. If None, uses default type map. e.g., {\"id_incidence\": int, \"some_flag\": bool} Returns: dict: Transformed version of the payload, suitable for use in a model's JSON field. Examples: Input : prep_payload_for_json({\"id_incidence\": \"123\"}, {\"id_incidence\": int}) Output: {\"id_incidence\": 123} Input : prep_payload_for_json({}, None) Output: {} Input : prep_payload_for_json(None) Output: TypeError Notes: - Applies datetime to ISO, Decimal to float. - Respects \"Please Select\" for placeholders. - Values in `type_matching_dict` are explicitly cast to the specified types. - If payload is None, raises TypeError. \"\"\" type_matching_dict = type_matching_dict or { \"id_incidence\" : int } return make_dict_serializeable ( payload , type_map = type_matching_dict , convert_time_to_ca = True )","title":"prep_payload_for_json"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.remove_validators","text":"Remove specified validators from selected WTForms fields. Parameters: form ( FlaskForm ) \u2013 The WTForms form instance. Must not be None. field_names ( list [ str ] ) \u2013 List of field names to examine and modify. If None or empty, no action is taken. validators_to_remove ( list [ type ] | None , default: None ) \u2013 Validator classes to remove. Default to [InputRequired] if not provided. Returns: None \u2013 None Examples: Input : remove_validators(form, [\"name\", \"email\"], [InputRequired]) Output: Removes InputRequired validators from 'name' and 'email' fields Input : remove_validators(form, None, [InputRequired]) Output: No action Input : remove_validators(form, [], [InputRequired]) Output: No action Notes If field_names is None or empty, no action is taken. Useful when validator logic depends on user input or view context. Source code in arb\\utils\\wtf_forms_util.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def remove_validators ( form : FlaskForm , field_names : list [ str ], validators_to_remove : list [ type ] | None = None ) -> None : \"\"\" Remove specified validators from selected WTForms fields. Args: form (FlaskForm): The WTForms form instance. Must not be None. field_names (list[str]): List of field names to examine and modify. If None or empty, no action is taken. validators_to_remove (list[type] | None): Validator classes to remove. Default to [InputRequired] if not provided. Returns: None Examples: Input : remove_validators(form, [\"name\", \"email\"], [InputRequired]) Output: Removes InputRequired validators from 'name' and 'email' fields Input : remove_validators(form, None, [InputRequired]) Output: No action Input : remove_validators(form, [], [InputRequired]) Output: No action Notes: - If `field_names` is None or empty, no action is taken. - Useful when validator logic depends on user input or view context. \"\"\" if validators_to_remove is None : validators_to_remove = [ InputRequired ] fields = get_wtforms_fields ( form , include_csrf_token = False ) for field in fields : if field in field_names : validators = form [ field ] . validators # Reassign validators with those not matching types to remove form [ field ] . validators = [ v for v in validators if not any ( isinstance ( v , t ) for t in validators_to_remove )]","title":"remove_validators"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.update_model_with_payload","text":"Apply a JSON-safe payload to a model's JSON column and mark it as changed. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance to update. Must not be None. payload ( dict ) \u2013 Dictionary of updates to apply. Must not be None. json_field ( str , default: 'misc_json' ) \u2013 Name of the model's JSON column (default is \"misc_json\"). If None or invalid, raises AttributeError. comment ( str , default: '' ) \u2013 Optional comment to include with update logging. If None, treated as empty string. Returns: None \u2013 None Examples: Input : update_model_with_payload(model, {\"foo\": 1}) Output: Updates model's misc_json with foo=1 Input : update_model_with_payload(None, {\"foo\": 1}) Output: AttributeError Notes Calls prep_payload_for_json to ensure data integrity. Uses apply_json_patch_and_log to track and log changes. Deep-copies the existing JSON field to avoid side effects. If model or payload is None, raises AttributeError. Source code in arb\\utils\\wtf_forms_util.py 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 def update_model_with_payload ( model : DeclarativeMeta , payload : dict , json_field : str = \"misc_json\" , comment : str = \"\" ) -> None : \"\"\" Apply a JSON-safe payload to a model's JSON column and mark it as changed. Args: model (DeclarativeMeta): SQLAlchemy model instance to update. Must not be None. payload (dict): Dictionary of updates to apply. Must not be None. json_field (str): Name of the model's JSON column (default is \"misc_json\"). If None or invalid, raises AttributeError. comment (str): Optional comment to include with update logging. If None, treated as empty string. Returns: None Examples: Input : update_model_with_payload(model, {\"foo\": 1}) Output: Updates model's misc_json with foo=1 Input : update_model_with_payload(None, {\"foo\": 1}) Output: AttributeError Notes: - Calls `prep_payload_for_json` to ensure data integrity. - Uses `apply_json_patch_and_log` to track and log changes. - Deep-copies the existing JSON field to avoid side effects. - If model or payload is None, raises AttributeError. \"\"\" logger . debug ( f \"update_model_with_payload: { model =} , { payload =} \" ) model_json = copy . deepcopy ( getattr ( model , json_field ) or {}) new_payload = prep_payload_for_json ( payload ) model_json . update ( new_payload ) apply_json_patch_and_log ( model , json_field = json_field , updates = model_json , user = \"anonymous\" , comments = comment , ) logger . debug ( f \"Model JSON updated: { getattr ( model , json_field ) =} \" )","title":"update_model_with_payload"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.validate_no_csrf","text":"Validate a WTForm while skipping CSRF errors (useful for GET-submitted forms). Parameters: form ( FlaskForm ) \u2013 The form to validate. Must not be None. extra_validators ( dict | None , default: None ) \u2013 Optional per-field validators to apply. If None, no extra validators are used. Returns: bool ( bool ) \u2013 True if the form is valid after removing CSRF errors, otherwise False. Examples: Input : validate_no_csrf(form) Output: True if valid, False if errors remain (except CSRF) Input : validate_no_csrf(None) Output: Exception Notes This allows validation to succeed even when CSRF tokens are missing or invalid. It logs before and after validation for debug purposes. If form is None, raises an exception. Source code in arb\\utils\\wtf_forms_util.py 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 def validate_no_csrf ( form : FlaskForm , extra_validators : dict | None = None ) -> bool : \"\"\" Validate a WTForm while skipping CSRF errors (useful for GET-submitted forms). Args: form (FlaskForm): The form to validate. Must not be None. extra_validators (dict | None): Optional per-field validators to apply. If None, no extra validators are used. Returns: bool: True if the form is valid after removing CSRF errors, otherwise False. Examples: Input : validate_no_csrf(form) Output: True if valid, False if errors remain (except CSRF) Input : validate_no_csrf(None) Output: Exception Notes: - This allows validation to succeed even when CSRF tokens are missing or invalid. - It logs before and after validation for debug purposes. - If form is None, raises an exception. \"\"\" logger . debug ( f \"validate_no_csrf() called:\" ) form . validate ( extra_validators = extra_validators ) if form . errors and 'csrf_token' in form . errors : del form . errors [ 'csrf_token' ] csrf_field = getattr ( form , 'csrf_token' , None ) if csrf_field : if csrf_field . errors : if 'The CSRF token is missing.' in csrf_field . errors : csrf_field . errors . remove ( 'The CSRF token is missing.' ) form_valid = not bool ( form . errors ) logger . debug ( f \"after validate_no_csrf() called: { form_valid =} , { form . errors =} \" ) return form_valid","title":"validate_no_csrf"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.validate_selectors","text":"Append validation errors for SelectFields left at default placeholder values. Parameters: form ( FlaskForm ) \u2013 WTForm instance containing SelectFields. Must not be None. default ( str | None , default: None ) \u2013 Placeholder value to treat as invalid (default: \"Please Select\"). If None, uses the global placeholder. Returns: None \u2013 None Examples: Input : validate_selectors(form, default=\"Please Select\") Output: Adds error to fields left at default Input : validate_selectors(form, default=None) Output: Adds error to fields left at the global placeholder Input : validate_selectors(None) Output: Exception Notes Typically used for GET-submitted forms where default values are not caught automatically. Adds \"This field is required.\" error to fields that are InputRequired but still at default. If form is None, raises an exception. Source code in arb\\utils\\wtf_forms_util.py 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 def validate_selectors ( form : FlaskForm , default : str | None = None ) -> None : \"\"\" Append validation errors for SelectFields left at default placeholder values. Args: form (FlaskForm): WTForm instance containing SelectFields. Must not be None. default (str | None): Placeholder value to treat as invalid (default: \"Please Select\"). If None, uses the global placeholder. Returns: None Examples: Input : validate_selectors(form, default=\"Please Select\") Output: Adds error to fields left at default Input : validate_selectors(form, default=None) Output: Adds error to fields left at the global placeholder Input : validate_selectors(None) Output: Exception Notes: - Typically used for GET-submitted forms where default values are not caught automatically. - Adds \"This field is required.\" error to fields that are InputRequired but still at default. - If form is None, raises an exception. \"\"\" if default is None : default = PLEASE_SELECT for field in form : if isinstance ( field , SelectField ): if field . data is None or field . data == default : for validator in field . validators : if isinstance ( validator , InputRequired ): msg = \"This field is required.\" if msg not in field . errors : field . errors . append ( msg ) # type: ignore","title":"validate_selectors"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.wtf_count_errors","text":"Count validation errors on a WTForm instance. Parameters: form ( FlaskForm ) \u2013 The form to inspect. Must not be None. log_errors ( bool , default: False ) \u2013 If True, log the form's errors using debug log level. Returns: dict [ str , int ] \u2013 dict[str, int]: Dictionary with error counts: - 'elements_with_errors': number of fields that had one or more errors - 'element_error_count': total number of field-level errors - 'wtf_form_error_count': number of form-level (non-field) errors - 'total_error_count': sum of all error types Examples: Input : error_summary = wtf_count_errors(form) Output: error_summary[\"total_error_count\"] \u2192 total number of errors found Input : wtf_count_errors(None) Output: Exception Notes Ensure form.validate_on_submit() or form.validate() has been called first, or the error counts will be inaccurate. If form is None, an exception will be raised. Source code in arb\\utils\\wtf_forms_util.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def wtf_count_errors ( form : FlaskForm , log_errors : bool = False ) -> dict [ str , int ]: \"\"\" Count validation errors on a WTForm instance. Args: form (FlaskForm): The form to inspect. Must not be None. log_errors (bool): If True, log the form's errors using debug log level. Returns: dict[str, int]: Dictionary with error counts: - 'elements_with_errors': number of fields that had one or more errors - 'element_error_count': total number of field-level errors - 'wtf_form_error_count': number of form-level (non-field) errors - 'total_error_count': sum of all error types Examples: Input : error_summary = wtf_count_errors(form) Output: error_summary[\"total_error_count\"] \u2192 total number of errors found Input : wtf_count_errors(None) Output: Exception Notes: - Ensure `form.validate_on_submit()` or `form.validate()` has been called first, or the error counts will be inaccurate. - If `form` is None, an exception will be raised. \"\"\" error_count_dict = { 'elements_with_errors' : 0 , 'element_error_count' : 0 , 'wtf_form_error_count' : 0 , 'total_error_count' : 0 , } if log_errors : logger . debug ( f \"Form errors are: { form . errors } \" ) for field , error_list in form . errors . items (): if field is None : error_count_dict [ 'wtf_form_error_count' ] += len ( error_list ) else : error_count_dict [ 'elements_with_errors' ] += 1 error_count_dict [ 'element_error_count' ] += len ( error_list ) error_count_dict [ 'total_error_count' ] = ( error_count_dict [ 'element_error_count' ] + error_count_dict [ 'wtf_form_error_count' ] ) return error_count_dict","title":"wtf_count_errors"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.wtform_to_model","text":"Extract data from a WTForm and update the model's JSON column. Logs all changes. Parameters: model ( AutomapBase ) \u2013 SQLAlchemy model instance. Must not be None. wtform ( FlaskForm ) \u2013 WTForm with typed Python values. Must not be None. json_column ( str , default: 'misc_json' ) \u2013 JSON column name on the model. Defaults to \"misc_json\". If None or invalid, raises AttributeError. user ( str , default: 'anonymous' ) \u2013 Username for logging purposes. If None, defaults to \"anonymous\". comments ( str , default: '' ) \u2013 Optional comment for logging context. If None, treated as empty string. ignore_fields ( list [ str ] | None , default: None ) \u2013 Fields to exclude from update. If None, no fields are excluded. type_matching_dict ( dict [ str , type ] | None , default: None ) \u2013 Optional override for type enforcement. If None, uses default type map. Returns: None \u2013 None Examples: Input : wtform_to_model(model, form) Output: Updates model's JSON column with form data Input : wtform_to_model(model, form, ignore_fields=[\"id\"]) Output: Updates all fields except 'id' Input : wtform_to_model(None, form) Output: AttributeError Notes Uses make_dict_serializable and get_changed_fields to compare values. Delegates to apply_json_patch_and_log to persist and log changes. If model or wtform is None, raises AttributeError. Source code in arb\\utils\\wtf_forms_util.py 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 def wtform_to_model ( model : AutomapBase , wtform : FlaskForm , json_column : str = \"misc_json\" , user : str = \"anonymous\" , comments : str = \"\" , ignore_fields : list [ str ] | None = None , type_matching_dict : dict [ str , type ] | None = None ) -> None : \"\"\" Extract data from a WTForm and update the model's JSON column. Logs all changes. Args: model (AutomapBase): SQLAlchemy model instance. Must not be None. wtform (FlaskForm): WTForm with typed Python values. Must not be None. json_column (str): JSON column name on the model. Defaults to \"misc_json\". If None or invalid, raises AttributeError. user (str): Username for logging purposes. If None, defaults to \"anonymous\". comments (str): Optional comment for logging context. If None, treated as empty string. ignore_fields (list[str] | None): Fields to exclude from update. If None, no fields are excluded. type_matching_dict (dict[str, type] | None): Optional override for type enforcement. If None, uses default type map. Returns: None Examples: Input : wtform_to_model(model, form) Output: Updates model's JSON column with form data Input : wtform_to_model(model, form, ignore_fields=[\"id\"]) Output: Updates all fields except 'id' Input : wtform_to_model(None, form) Output: AttributeError Notes: - Uses make_dict_serializable and get_changed_fields to compare values. - Delegates to apply_json_patch_and_log to persist and log changes. - If model or wtform is None, raises AttributeError. \"\"\" ignore_fields_set = set ( ignore_fields or []) payload_all = { field_name : getattr ( wtform , field_name ) . data for field_name in get_wtforms_fields ( wtform ) if field_name not in ignore_fields_set } # Use manual overrides only \u2014 no type_map from form payload_all = make_dict_serializeable ( payload_all , type_map = type_matching_dict , convert_time_to_ca = True ) existing_json = load_model_json_column ( model , json_column ) # todo - shouldn't json already be serialized, not sure what the next line accomplishes existing_serialized = make_dict_serializeable ( existing_json , type_map = type_matching_dict , convert_time_to_ca = True ) payload_changes = get_changed_fields ( payload_all , existing_serialized ) if payload_changes : logger . info ( f \"wtform_to_model payload_changes: { payload_changes } \" ) apply_json_patch_and_log ( model , payload_changes , json_column , user = user , comments = comments ) logger . info ( f \"wtform_to_model payload_all: { payload_all } \" )","title":"wtform_to_model"},{"location":"reference/arb/utils/excel/excel_compare/","text":"arb.utils.excel.excel_compare excel_compare.py Limitations (openpyxl and Python-based Excel comparison): openpyxl does not fully capture all Excel formatting differences, especially: Theme, tint, and indexed colors may not be resolved to their true RGB values. Some border styles, conditional formatting, and advanced cell styles may not be detected. Data validation, dropdowns, and protection flags are only partially supported. Visual differences in Excel (as seen in the UI) may not be reflected in the Python object model. Some workbook/sheet protection settings and cell-level protection may not be fully compared. As a result, two files that look different in Excel may appear identical to this script, and vice versa. For full-fidelity Excel comparison (including all formatting, protection, and UI-visible differences), consider using the provided VBA-based tool (see excel_compare_vba.bas). This script is best used for basic content and common formatting checks, not for pixel-perfect or compliance-critical audits. compare_excel_content ( path1 , path2 , formatting_mode = 'common' ) Compare cell content, formulas, formatting, comments, dropdowns, and protection. Parameters: path1 ( Path ) \u2013 Path to first Excel file. path2 ( Path ) \u2013 Path to second Excel file. formatting_mode ( str , default: 'common' ) \u2013 'off', 'common', or 'full'. Returns: list [ str ] \u2013 list[str]: All differences grouped by category and sheet. Source code in arb\\utils\\excel\\excel_compare.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def compare_excel_content ( path1 : Path , path2 : Path , formatting_mode : str = \"common\" ) -> list [ str ]: \"\"\" Compare cell content, formulas, formatting, comments, dropdowns, and protection. Args: path1 (Path): Path to first Excel file. path2 (Path): Path to second Excel file. formatting_mode (str): 'off', 'common', or 'full'. Returns: list[str]: All differences grouped by category and sheet. \"\"\" wb1 = load_workbook ( path1 , data_only = False ) wb2 = load_workbook ( path2 , data_only = False ) output = [] # Workbook-level protection comparison workbook_protection_diffs = compare_workbook_protection ( wb1 , wb2 ) if workbook_protection_diffs : output . append ( \"[Workbook Protection Differences]\" ) output . extend ( workbook_protection_diffs ) sheets1 = set ( wb1 . sheetnames ) sheets2 = set ( wb2 . sheetnames ) only_in_a = sorted ( sheets1 - sheets2 ) only_in_b = sorted ( sheets2 - sheets1 ) in_both = sorted ( sheets1 & sheets2 ) if only_in_a : output . append ( \"[Sheets only in A]\" ) for name in only_in_a : output . append ( f \" { name } \" ) if only_in_b : output . append ( \"[Sheets only in B]\" ) for name in only_in_b : output . append ( f \" { name } \" ) for sheet in in_both : output . append ( f \" \\n === Sheet: { sheet } ===\" ) ws1 = wb1 [ sheet ] ws2 = wb2 [ sheet ] max_row = max ( ws1 . max_row , ws2 . max_row ) max_col = max ( ws1 . max_column , ws2 . max_column ) sheet_protection_diffs = compare_sheet_protection ( ws1 , ws2 , sheet ) if sheet_protection_diffs : output . extend ( sheet_protection_diffs ) content_diffs , formula_diffs = [], [] comment_diffs , formatting_diffs , validation_diffs , protection_diffs = [], [], [], [] validations1 = extract_data_validation_map ( ws1 ) validations2 = extract_data_validation_map ( ws2 ) for row in range ( 1 , max_row + 1 ): for col in range ( 1 , max_col + 1 ): c1 = ws1 . cell ( row = row , column = col ) c2 = ws2 . cell ( row = row , column = col ) coord = c1 . coordinate v1 = stringify_formula ( c1 ) v2 = stringify_formula ( c2 ) if v1 != v2 : content_diffs . append ( f \" { coord } : \\n A: { v1 } \\n B: { v2 } \" ) if c1 . data_type == 'f' or c2 . data_type == 'f' : if v1 != v2 : formula_diffs . append ( f \" { coord } : \\n A: { v1 } \\n B: { v2 } \" ) comm1 = c1 . comment . text if c1 . comment else None comm2 = c2 . comment . text if c2 . comment else None if comm1 != comm2 : comment_diffs . append ( f \" { coord } : \\n A: { comm1 } \\n B: { comm2 } \" ) # Protection attributes always checked for attr in PROTECTION_ATTRS : if getattr ( c1 . protection , attr ) != getattr ( c2 . protection , attr ): protection_diffs . append ( f \" { coord } : protection. { attr } changed\" ) if formatting_mode != \"off\" : for attr in ( COMMON_FONT_ATTRS if formatting_mode == \"common\" else FULL_FONT_ATTRS ): if getattr ( c1 . font , attr ) != getattr ( c2 . font , attr ): formatting_diffs . append ( f \" { coord } : font. { attr } changed\" ) for attr in FILL_ATTRS : if getattr ( c1 . fill , attr ) != getattr ( c2 . fill , attr ): formatting_diffs . append ( f \" { coord } : fill. { attr } changed\" ) for attr in ALIGNMENT_ATTRS : if getattr ( c1 . alignment , attr ) != getattr ( c2 . alignment , attr ): formatting_diffs . append ( f \" { coord } : alignment. { attr } changed\" ) if c1 . number_format != c2 . number_format : formatting_diffs . append ( f \" { coord } : number_format changed \\n A: { c1 . number_format } \\n B: { c2 . number_format } \" ) if formatting_mode == \"full\" : for side in BORDER_SIDES : b1 = getattr ( c1 . border , side ) b2 = getattr ( c2 . border , side ) if b1 . style != b2 . style or b1 . color != b2 . color : formatting_diffs . append ( f \" { coord } : border. { side } changed\" ) val1 = validations1 . get ( coord ) val2 = validations2 . get ( coord ) if val1 != val2 : validation_diffs . append ( f \" { coord } : \\n A: { val1 } \\n B: { val2 } \" ) if content_diffs : output . append ( \"[Content Differences]\" ) output . extend ( content_diffs ) if formula_diffs : output . append ( \"[Formula Differences]\" ) output . extend ( formula_diffs ) if comment_diffs : output . append ( \"[Comment Differences]\" ) output . extend ( comment_diffs ) if validation_diffs : output . append ( \"[Dropdown/Data Validation Differences]\" ) output . extend ( validation_diffs ) if protection_diffs : output . append ( \"[Cell Protection Differences]\" ) output . extend ( protection_diffs ) if formatting_diffs : output . append ( \"[Formatting Differences]\" ) output . extend ( formatting_diffs ) return output compare_excel_directories ( dir_a , dir_b , formatting_mode = 'common' ) Compare all Excel files in two directories by name, producing a timestamped report. Parameters: dir_a ( Path ) \u2013 Path to first directory. dir_b ( Path ) \u2013 Path to second directory. formatting_mode ( str , default: 'common' ) \u2013 Formatting check mode: 'off', 'common', or 'full'. Source code in arb\\utils\\excel\\excel_compare.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 def compare_excel_directories ( dir_a : Path , dir_b : Path , formatting_mode : str = \"common\" ) -> None : \"\"\" Compare all Excel files in two directories by name, producing a timestamped report. Args: dir_a (Path): Path to first directory. dir_b (Path): Path to second directory. formatting_mode (str): Formatting check mode: 'off', 'common', or 'full'. \"\"\" now = datetime . now () out_path = Path ( f \"comparison_at_ { now . strftime ( '%Y%m %d _%H%M%S' ) } .txt\" ) print ( f \"Creating comparison file: { out_path } \" ) excel_files_a = { f . name : f for f in dir_a . glob ( \"*.xlsx\" ) if f . is_file ()} excel_files_b = { f . name : f for f in dir_b . glob ( \"*.xlsx\" ) if f . is_file ()} only_in_a = sorted ( set ( excel_files_a ) - set ( excel_files_b )) only_in_b = sorted ( set ( excel_files_b ) - set ( excel_files_a )) in_both = sorted ( set ( excel_files_a ) & set ( excel_files_b )) with open ( out_path , \"w\" , encoding = \"utf-8\" ) as f : f . write ( f \"Comparing Excel files in: \\n A: { dir_a } \\n B: { dir_b } \\n\\n \" ) f . write ( f \"Files only in A ( { len ( only_in_a ) } ): \\n \" ) for name in only_in_a : f . write ( f \" { name } \\n \" ) f . write ( f \" \\n Files only in B ( { len ( only_in_b ) } ): \\n \" ) for name in only_in_b : f . write ( f \" { name } \\n \" ) f . write ( f \" \\n Files in both ( { len ( in_both ) } ): \\n \" ) for name in in_both : f . write ( f \" \\n === Comparing { name } === \\n \" ) result = compare_excel_files ( excel_files_a [ name ], excel_files_b [ name ], formatting_mode = formatting_mode ) for line in result : f . write ( line + \" \\n \" ) print ( f \"Comparison complete. Output saved to: { out_path . resolve () } \" ) compare_excel_files ( file_a_path , file_b_path , formatting_mode = 'common' , log_to_file = False ) Compare two Excel workbooks at the binary and content level. Parameters: file_a_path ( str | Path ) \u2013 Path to file A. file_b_path ( str | Path ) \u2013 Path to file B. formatting_mode ( str , default: 'common' ) \u2013 Formatting check mode: 'off', 'common', or 'full'. log_to_file ( bool , default: False ) \u2013 If true, then log to file Returns: list [ str ] \u2013 list[str]: List of human-readable comparison results. Source code in arb\\utils\\excel\\excel_compare.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def compare_excel_files ( file_a_path : Union [ str , Path ], file_b_path : Union [ str , Path ], formatting_mode : str = \"common\" , log_to_file : bool = False ) -> list [ str ]: \"\"\" Compare two Excel workbooks at the binary and content level. Args: file_a_path (str | Path): Path to file A. file_b_path (str | Path): Path to file B. formatting_mode (str): Formatting check mode: 'off', 'common', or 'full'. log_to_file (bool): If true, then log to file Returns: list[str]: List of human-readable comparison results. \"\"\" file_a = Path ( file_a_path ) file_b = Path ( file_b_path ) output = [ f \"Comparing: \\n A: { file_a . name } \\n B: { file_b . name } \\n \" ] hash_a = compute_sha256 ( file_a ) hash_b = compute_sha256 ( file_b ) output . append ( \"SHA-256 Hashes:\" ) output . append ( f \" A: { hash_a } \" ) output . append ( f \" B: { hash_b } \" ) if hash_a == hash_b : output . append ( \" \\u2714 Files are identical at the binary level. \\n \" ) return output output . append ( \" \\u2192 Hashes differ; comparing content... \\n \" ) output . extend ( compare_excel_content ( file_a , file_b , formatting_mode = formatting_mode )) if log_to_file : now = datetime . now () out_path = Path ( f \"comparison_at_ { now . strftime ( '%Y%m %d _%H%M%S' ) } .txt\" ) print ( f \"Creating comparison file: { out_path } \" ) with open ( out_path , \"w\" , encoding = \"utf-8\" ) as f : f . write ( f \"Comparing Excel files \\n\\t A: { file_a } \\n\\t B: { file_b } \\n\\n \" ) for line in output : f . write ( line + \" \\n \" ) return output compare_sheet_protection ( ws1 , ws2 , sheet_name ) Compare protection flags between two sheets. Returns: list [ str ] \u2013 list[str]: Differences under [Sheet Protection Differences] per sheet. Source code in arb\\utils\\excel\\excel_compare.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def compare_sheet_protection ( ws1 , ws2 , sheet_name : str ) -> list [ str ]: \"\"\" Compare protection flags between two sheets. Returns: list[str]: Differences under [Sheet Protection Differences] per sheet. \"\"\" attrs = [ \"sheet\" , \"objects\" , \"scenarios\" , \"formatCells\" , \"formatColumns\" , \"formatRows\" , \"insertColumns\" , \"insertRows\" , \"insertHyperlinks\" , \"deleteColumns\" , \"deleteRows\" , \"selectLockedCells\" , \"selectUnlockedCells\" , \"sort\" , \"autoFilter\" , \"pivotTables\" ] diffs = [] for attr in attrs : v1 = getattr ( ws1 . protection , attr , None ) v2 = getattr ( ws2 . protection , attr , None ) if v1 != v2 : diffs . append ( f \" { attr } : \\n A: { v1 } \\n B: { v2 } \" ) if diffs : return [ f \"[Sheet Protection Differences] ( { sheet_name } )\" ] + diffs return [] compare_workbook_protection ( wb1 , wb2 ) Compare top-level workbook protection settings. Returns: list [ str ] \u2013 list[str]: Differences under [Workbook Protection Differences]. Source code in arb\\utils\\excel\\excel_compare.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def compare_workbook_protection ( wb1 , wb2 ) -> list [ str ]: \"\"\" Compare top-level workbook protection settings. Returns: list[str]: Differences under [Workbook Protection Differences]. \"\"\" sec1 = getattr ( wb1 . security , \"lockStructure\" , None ), getattr ( wb1 . security , \"lockWindows\" , None ) sec2 = getattr ( wb2 . security , \"lockStructure\" , None ), getattr ( wb2 . security , \"lockWindows\" , None ) diffs = [] if sec1 [ 0 ] != sec2 [ 0 ]: diffs . append ( f \" lockStructure: \\n A: { sec1 [ 0 ] } \\n B: { sec2 [ 0 ] } \" ) if sec1 [ 1 ] != sec2 [ 1 ]: diffs . append ( f \" lockWindows: \\n A: { sec1 [ 1 ] } \\n B: { sec2 [ 1 ] } \" ) return diffs compute_sha256 ( path ) Compute the SHA-256 hash of a file to detect binary-level equality. Source code in arb\\utils\\excel\\excel_compare.py 55 56 57 58 def compute_sha256 ( path : Path ) -> str : \"\"\"Compute the SHA-256 hash of a file to detect binary-level equality.\"\"\" with open ( path , \"rb\" ) as f : return hashlib . sha256 ( f . read ()) . hexdigest () extract_data_validation_map ( ws ) Extract data validation rules from a worksheet and return a mapping from cell coordinate to validation settings. Returns: dict \u2013 dict[str, dict]: Maps cell coordinate to a dict of validation attributes. Source code in arb\\utils\\excel\\excel_compare.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def extract_data_validation_map ( ws ) -> dict : \"\"\" Extract data validation rules from a worksheet and return a mapping from cell coordinate to validation settings. Returns: dict[str, dict]: Maps cell coordinate to a dict of validation attributes. \"\"\" result = {} for dv in ws . data_validations . dataValidation : for cell_range in dv . cells : for cell in cell_range . cells : coord = ws . cell ( row = cell . row , column = cell . col_idx ) . coordinate result [ coord ] = { \"type\" : dv . type , \"formula1\" : dv . formula1 , \"formula2\" : dv . formula2 , \"allow_blank\" : dv . allow_blank , \"showDropDown\" : dv . showDropDown , \"operator\" : dv . operator , } return result stringify_formula ( cell ) Return a human-readable formula string or raw value for a cell. Source code in arb\\utils\\excel\\excel_compare.py 43 44 45 46 47 48 49 50 51 52 def stringify_formula ( cell ) -> str : \"\"\"Return a human-readable formula string or raw value for a cell.\"\"\" try : if isinstance ( cell . value , ArrayFormula ): return cell . value . formula if cell . data_type == 'f' : return str ( cell . formula or cell . _value or cell . value ) return str ( cell . value ) except Exception as e : return f \"[unreadable: { e } ]\"","title":"arb.utils.excel.excel_compare"},{"location":"reference/arb/utils/excel/excel_compare/#arbutilsexcelexcel_compare","text":"excel_compare.py","title":"arb.utils.excel.excel_compare"},{"location":"reference/arb/utils/excel/excel_compare/#arb.utils.excel.excel_compare--limitations-openpyxl-and-python-based-excel-comparison","text":"openpyxl does not fully capture all Excel formatting differences, especially: Theme, tint, and indexed colors may not be resolved to their true RGB values. Some border styles, conditional formatting, and advanced cell styles may not be detected. Data validation, dropdowns, and protection flags are only partially supported. Visual differences in Excel (as seen in the UI) may not be reflected in the Python object model. Some workbook/sheet protection settings and cell-level protection may not be fully compared. As a result, two files that look different in Excel may appear identical to this script, and vice versa. For full-fidelity Excel comparison (including all formatting, protection, and UI-visible differences), consider using the provided VBA-based tool (see excel_compare_vba.bas). This script is best used for basic content and common formatting checks, not for pixel-perfect or compliance-critical audits.","title":"Limitations (openpyxl and Python-based Excel comparison):"},{"location":"reference/arb/utils/excel/excel_compare/#arb.utils.excel.excel_compare.compare_excel_content","text":"Compare cell content, formulas, formatting, comments, dropdowns, and protection. Parameters: path1 ( Path ) \u2013 Path to first Excel file. path2 ( Path ) \u2013 Path to second Excel file. formatting_mode ( str , default: 'common' ) \u2013 'off', 'common', or 'full'. Returns: list [ str ] \u2013 list[str]: All differences grouped by category and sheet. Source code in arb\\utils\\excel\\excel_compare.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 def compare_excel_content ( path1 : Path , path2 : Path , formatting_mode : str = \"common\" ) -> list [ str ]: \"\"\" Compare cell content, formulas, formatting, comments, dropdowns, and protection. Args: path1 (Path): Path to first Excel file. path2 (Path): Path to second Excel file. formatting_mode (str): 'off', 'common', or 'full'. Returns: list[str]: All differences grouped by category and sheet. \"\"\" wb1 = load_workbook ( path1 , data_only = False ) wb2 = load_workbook ( path2 , data_only = False ) output = [] # Workbook-level protection comparison workbook_protection_diffs = compare_workbook_protection ( wb1 , wb2 ) if workbook_protection_diffs : output . append ( \"[Workbook Protection Differences]\" ) output . extend ( workbook_protection_diffs ) sheets1 = set ( wb1 . sheetnames ) sheets2 = set ( wb2 . sheetnames ) only_in_a = sorted ( sheets1 - sheets2 ) only_in_b = sorted ( sheets2 - sheets1 ) in_both = sorted ( sheets1 & sheets2 ) if only_in_a : output . append ( \"[Sheets only in A]\" ) for name in only_in_a : output . append ( f \" { name } \" ) if only_in_b : output . append ( \"[Sheets only in B]\" ) for name in only_in_b : output . append ( f \" { name } \" ) for sheet in in_both : output . append ( f \" \\n === Sheet: { sheet } ===\" ) ws1 = wb1 [ sheet ] ws2 = wb2 [ sheet ] max_row = max ( ws1 . max_row , ws2 . max_row ) max_col = max ( ws1 . max_column , ws2 . max_column ) sheet_protection_diffs = compare_sheet_protection ( ws1 , ws2 , sheet ) if sheet_protection_diffs : output . extend ( sheet_protection_diffs ) content_diffs , formula_diffs = [], [] comment_diffs , formatting_diffs , validation_diffs , protection_diffs = [], [], [], [] validations1 = extract_data_validation_map ( ws1 ) validations2 = extract_data_validation_map ( ws2 ) for row in range ( 1 , max_row + 1 ): for col in range ( 1 , max_col + 1 ): c1 = ws1 . cell ( row = row , column = col ) c2 = ws2 . cell ( row = row , column = col ) coord = c1 . coordinate v1 = stringify_formula ( c1 ) v2 = stringify_formula ( c2 ) if v1 != v2 : content_diffs . append ( f \" { coord } : \\n A: { v1 } \\n B: { v2 } \" ) if c1 . data_type == 'f' or c2 . data_type == 'f' : if v1 != v2 : formula_diffs . append ( f \" { coord } : \\n A: { v1 } \\n B: { v2 } \" ) comm1 = c1 . comment . text if c1 . comment else None comm2 = c2 . comment . text if c2 . comment else None if comm1 != comm2 : comment_diffs . append ( f \" { coord } : \\n A: { comm1 } \\n B: { comm2 } \" ) # Protection attributes always checked for attr in PROTECTION_ATTRS : if getattr ( c1 . protection , attr ) != getattr ( c2 . protection , attr ): protection_diffs . append ( f \" { coord } : protection. { attr } changed\" ) if formatting_mode != \"off\" : for attr in ( COMMON_FONT_ATTRS if formatting_mode == \"common\" else FULL_FONT_ATTRS ): if getattr ( c1 . font , attr ) != getattr ( c2 . font , attr ): formatting_diffs . append ( f \" { coord } : font. { attr } changed\" ) for attr in FILL_ATTRS : if getattr ( c1 . fill , attr ) != getattr ( c2 . fill , attr ): formatting_diffs . append ( f \" { coord } : fill. { attr } changed\" ) for attr in ALIGNMENT_ATTRS : if getattr ( c1 . alignment , attr ) != getattr ( c2 . alignment , attr ): formatting_diffs . append ( f \" { coord } : alignment. { attr } changed\" ) if c1 . number_format != c2 . number_format : formatting_diffs . append ( f \" { coord } : number_format changed \\n A: { c1 . number_format } \\n B: { c2 . number_format } \" ) if formatting_mode == \"full\" : for side in BORDER_SIDES : b1 = getattr ( c1 . border , side ) b2 = getattr ( c2 . border , side ) if b1 . style != b2 . style or b1 . color != b2 . color : formatting_diffs . append ( f \" { coord } : border. { side } changed\" ) val1 = validations1 . get ( coord ) val2 = validations2 . get ( coord ) if val1 != val2 : validation_diffs . append ( f \" { coord } : \\n A: { val1 } \\n B: { val2 } \" ) if content_diffs : output . append ( \"[Content Differences]\" ) output . extend ( content_diffs ) if formula_diffs : output . append ( \"[Formula Differences]\" ) output . extend ( formula_diffs ) if comment_diffs : output . append ( \"[Comment Differences]\" ) output . extend ( comment_diffs ) if validation_diffs : output . append ( \"[Dropdown/Data Validation Differences]\" ) output . extend ( validation_diffs ) if protection_diffs : output . append ( \"[Cell Protection Differences]\" ) output . extend ( protection_diffs ) if formatting_diffs : output . append ( \"[Formatting Differences]\" ) output . extend ( formatting_diffs ) return output","title":"compare_excel_content"},{"location":"reference/arb/utils/excel/excel_compare/#arb.utils.excel.excel_compare.compare_excel_directories","text":"Compare all Excel files in two directories by name, producing a timestamped report. Parameters: dir_a ( Path ) \u2013 Path to first directory. dir_b ( Path ) \u2013 Path to second directory. formatting_mode ( str , default: 'common' ) \u2013 Formatting check mode: 'off', 'common', or 'full'. Source code in arb\\utils\\excel\\excel_compare.py 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 def compare_excel_directories ( dir_a : Path , dir_b : Path , formatting_mode : str = \"common\" ) -> None : \"\"\" Compare all Excel files in two directories by name, producing a timestamped report. Args: dir_a (Path): Path to first directory. dir_b (Path): Path to second directory. formatting_mode (str): Formatting check mode: 'off', 'common', or 'full'. \"\"\" now = datetime . now () out_path = Path ( f \"comparison_at_ { now . strftime ( '%Y%m %d _%H%M%S' ) } .txt\" ) print ( f \"Creating comparison file: { out_path } \" ) excel_files_a = { f . name : f for f in dir_a . glob ( \"*.xlsx\" ) if f . is_file ()} excel_files_b = { f . name : f for f in dir_b . glob ( \"*.xlsx\" ) if f . is_file ()} only_in_a = sorted ( set ( excel_files_a ) - set ( excel_files_b )) only_in_b = sorted ( set ( excel_files_b ) - set ( excel_files_a )) in_both = sorted ( set ( excel_files_a ) & set ( excel_files_b )) with open ( out_path , \"w\" , encoding = \"utf-8\" ) as f : f . write ( f \"Comparing Excel files in: \\n A: { dir_a } \\n B: { dir_b } \\n\\n \" ) f . write ( f \"Files only in A ( { len ( only_in_a ) } ): \\n \" ) for name in only_in_a : f . write ( f \" { name } \\n \" ) f . write ( f \" \\n Files only in B ( { len ( only_in_b ) } ): \\n \" ) for name in only_in_b : f . write ( f \" { name } \\n \" ) f . write ( f \" \\n Files in both ( { len ( in_both ) } ): \\n \" ) for name in in_both : f . write ( f \" \\n === Comparing { name } === \\n \" ) result = compare_excel_files ( excel_files_a [ name ], excel_files_b [ name ], formatting_mode = formatting_mode ) for line in result : f . write ( line + \" \\n \" ) print ( f \"Comparison complete. Output saved to: { out_path . resolve () } \" )","title":"compare_excel_directories"},{"location":"reference/arb/utils/excel/excel_compare/#arb.utils.excel.excel_compare.compare_excel_files","text":"Compare two Excel workbooks at the binary and content level. Parameters: file_a_path ( str | Path ) \u2013 Path to file A. file_b_path ( str | Path ) \u2013 Path to file B. formatting_mode ( str , default: 'common' ) \u2013 Formatting check mode: 'off', 'common', or 'full'. log_to_file ( bool , default: False ) \u2013 If true, then log to file Returns: list [ str ] \u2013 list[str]: List of human-readable comparison results. Source code in arb\\utils\\excel\\excel_compare.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def compare_excel_files ( file_a_path : Union [ str , Path ], file_b_path : Union [ str , Path ], formatting_mode : str = \"common\" , log_to_file : bool = False ) -> list [ str ]: \"\"\" Compare two Excel workbooks at the binary and content level. Args: file_a_path (str | Path): Path to file A. file_b_path (str | Path): Path to file B. formatting_mode (str): Formatting check mode: 'off', 'common', or 'full'. log_to_file (bool): If true, then log to file Returns: list[str]: List of human-readable comparison results. \"\"\" file_a = Path ( file_a_path ) file_b = Path ( file_b_path ) output = [ f \"Comparing: \\n A: { file_a . name } \\n B: { file_b . name } \\n \" ] hash_a = compute_sha256 ( file_a ) hash_b = compute_sha256 ( file_b ) output . append ( \"SHA-256 Hashes:\" ) output . append ( f \" A: { hash_a } \" ) output . append ( f \" B: { hash_b } \" ) if hash_a == hash_b : output . append ( \" \\u2714 Files are identical at the binary level. \\n \" ) return output output . append ( \" \\u2192 Hashes differ; comparing content... \\n \" ) output . extend ( compare_excel_content ( file_a , file_b , formatting_mode = formatting_mode )) if log_to_file : now = datetime . now () out_path = Path ( f \"comparison_at_ { now . strftime ( '%Y%m %d _%H%M%S' ) } .txt\" ) print ( f \"Creating comparison file: { out_path } \" ) with open ( out_path , \"w\" , encoding = \"utf-8\" ) as f : f . write ( f \"Comparing Excel files \\n\\t A: { file_a } \\n\\t B: { file_b } \\n\\n \" ) for line in output : f . write ( line + \" \\n \" ) return output","title":"compare_excel_files"},{"location":"reference/arb/utils/excel/excel_compare/#arb.utils.excel.excel_compare.compare_sheet_protection","text":"Compare protection flags between two sheets. Returns: list [ str ] \u2013 list[str]: Differences under [Sheet Protection Differences] per sheet. Source code in arb\\utils\\excel\\excel_compare.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def compare_sheet_protection ( ws1 , ws2 , sheet_name : str ) -> list [ str ]: \"\"\" Compare protection flags between two sheets. Returns: list[str]: Differences under [Sheet Protection Differences] per sheet. \"\"\" attrs = [ \"sheet\" , \"objects\" , \"scenarios\" , \"formatCells\" , \"formatColumns\" , \"formatRows\" , \"insertColumns\" , \"insertRows\" , \"insertHyperlinks\" , \"deleteColumns\" , \"deleteRows\" , \"selectLockedCells\" , \"selectUnlockedCells\" , \"sort\" , \"autoFilter\" , \"pivotTables\" ] diffs = [] for attr in attrs : v1 = getattr ( ws1 . protection , attr , None ) v2 = getattr ( ws2 . protection , attr , None ) if v1 != v2 : diffs . append ( f \" { attr } : \\n A: { v1 } \\n B: { v2 } \" ) if diffs : return [ f \"[Sheet Protection Differences] ( { sheet_name } )\" ] + diffs return []","title":"compare_sheet_protection"},{"location":"reference/arb/utils/excel/excel_compare/#arb.utils.excel.excel_compare.compare_workbook_protection","text":"Compare top-level workbook protection settings. Returns: list [ str ] \u2013 list[str]: Differences under [Workbook Protection Differences]. Source code in arb\\utils\\excel\\excel_compare.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def compare_workbook_protection ( wb1 , wb2 ) -> list [ str ]: \"\"\" Compare top-level workbook protection settings. Returns: list[str]: Differences under [Workbook Protection Differences]. \"\"\" sec1 = getattr ( wb1 . security , \"lockStructure\" , None ), getattr ( wb1 . security , \"lockWindows\" , None ) sec2 = getattr ( wb2 . security , \"lockStructure\" , None ), getattr ( wb2 . security , \"lockWindows\" , None ) diffs = [] if sec1 [ 0 ] != sec2 [ 0 ]: diffs . append ( f \" lockStructure: \\n A: { sec1 [ 0 ] } \\n B: { sec2 [ 0 ] } \" ) if sec1 [ 1 ] != sec2 [ 1 ]: diffs . append ( f \" lockWindows: \\n A: { sec1 [ 1 ] } \\n B: { sec2 [ 1 ] } \" ) return diffs","title":"compare_workbook_protection"},{"location":"reference/arb/utils/excel/excel_compare/#arb.utils.excel.excel_compare.compute_sha256","text":"Compute the SHA-256 hash of a file to detect binary-level equality. Source code in arb\\utils\\excel\\excel_compare.py 55 56 57 58 def compute_sha256 ( path : Path ) -> str : \"\"\"Compute the SHA-256 hash of a file to detect binary-level equality.\"\"\" with open ( path , \"rb\" ) as f : return hashlib . sha256 ( f . read ()) . hexdigest ()","title":"compute_sha256"},{"location":"reference/arb/utils/excel/excel_compare/#arb.utils.excel.excel_compare.extract_data_validation_map","text":"Extract data validation rules from a worksheet and return a mapping from cell coordinate to validation settings. Returns: dict \u2013 dict[str, dict]: Maps cell coordinate to a dict of validation attributes. Source code in arb\\utils\\excel\\excel_compare.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def extract_data_validation_map ( ws ) -> dict : \"\"\" Extract data validation rules from a worksheet and return a mapping from cell coordinate to validation settings. Returns: dict[str, dict]: Maps cell coordinate to a dict of validation attributes. \"\"\" result = {} for dv in ws . data_validations . dataValidation : for cell_range in dv . cells : for cell in cell_range . cells : coord = ws . cell ( row = cell . row , column = cell . col_idx ) . coordinate result [ coord ] = { \"type\" : dv . type , \"formula1\" : dv . formula1 , \"formula2\" : dv . formula2 , \"allow_blank\" : dv . allow_blank , \"showDropDown\" : dv . showDropDown , \"operator\" : dv . operator , } return result","title":"extract_data_validation_map"},{"location":"reference/arb/utils/excel/excel_compare/#arb.utils.excel.excel_compare.stringify_formula","text":"Return a human-readable formula string or raw value for a cell. Source code in arb\\utils\\excel\\excel_compare.py 43 44 45 46 47 48 49 50 51 52 def stringify_formula ( cell ) -> str : \"\"\"Return a human-readable formula string or raw value for a cell.\"\"\" try : if isinstance ( cell . value , ArrayFormula ): return cell . value . formula if cell . data_type == 'f' : return str ( cell . formula or cell . _value or cell . value ) return str ( cell . value ) except Exception as e : return f \"[unreadable: { e } ]\"","title":"stringify_formula"},{"location":"reference/arb/utils/excel/excel_compare_04/","text":"arb.utils.excel.excel_compare_04 parse_comments ( zipf , sheet_file ) Parse comments for a given sheet if present. Source code in arb\\utils\\excel\\excel_compare_04.py 110 111 112 113 114 115 116 117 118 119 120 121 122 def parse_comments ( zipf : zipfile . ZipFile , sheet_file : str ) -> Dict [ str , str ]: \"\"\"Parse comments for a given sheet if present.\"\"\" comments = {} # Try to find the comments file for this sheet base = sheet_file . split ( '/' )[ - 1 ] . replace ( '.xml' , '' ) comments_file = f 'xl/comments { base [ 5 :] } .xml' # e.g., sheet1 -> comments1 if comments_file in zipf . namelist (): root = ET . fromstring ( zipf . read ( comments_file ) . decode ( 'utf-8' )) for comment in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}comment' ): ref = comment . attrib [ 'ref' ] text = '' . join ([ t . text or '' for t in comment . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}t' )]) comments [ ref ] = text return comments parse_data_validations ( sheet_xml ) Parse data validations (dropdowns) for a sheet. Source code in arb\\utils\\excel\\excel_compare_04.py 125 126 127 128 129 130 131 132 133 134 135 136 137 def parse_data_validations ( sheet_xml : str ) -> Dict [ str , str ]: \"\"\"Parse data validations (dropdowns) for a sheet.\"\"\" root = ET . fromstring ( sheet_xml ) ns = { 'main' : 'http://schemas.openxmlformats.org/spreadsheetml/2006/main' } validations = {} for dv in root . findall ( './/main:dataValidation' , ns ): sqref = dv . attrib . get ( 'sqref' , '' ) formula1 = dv . find ( 'main:formula1' , ns ) formula2 = dv . find ( 'main:formula2' , ns ) val_str = f \"type= { dv . attrib . get ( 'type' , '' ) } , formula1= { formula1 . text if formula1 is not None else '' } , formula2= { formula2 . text if formula2 is not None else '' } \" for cell in sqref . split (): validations [ cell ] = val_str return validations parse_hyperlinks ( sheet_xml ) Parse hyperlinks for a sheet. Source code in arb\\utils\\excel\\excel_compare_04.py 140 141 142 143 144 145 146 147 148 149 def parse_hyperlinks ( sheet_xml : str ) -> Dict [ str , str ]: \"\"\"Parse hyperlinks for a sheet.\"\"\" root = ET . fromstring ( sheet_xml ) ns = { 'main' : 'http://schemas.openxmlformats.org/spreadsheetml/2006/main' } links = {} for hl in root . findall ( './/main:hyperlink' , ns ): ref = hl . attrib . get ( 'ref' , '' ) target = hl . attrib . get ( 'display' , hl . attrib . get ( 'location' , hl . attrib . get ( 'tooltip' , '' ))) links [ ref ] = target return links parse_shared_strings ( xml ) Parse sharedStrings.xml and return a list of strings. Source code in arb\\utils\\excel\\excel_compare_04.py 13 14 15 16 17 18 19 20 21 22 23 24 def parse_shared_strings ( xml : str ) -> List [ str ]: \"\"\"Parse sharedStrings.xml and return a list of strings.\"\"\" root = ET . fromstring ( xml ) strings = [] for si in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}si' ): t = si . find ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}t' ) if t is not None : strings . append ( t . text or \"\" ) else : ts = si . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}t' ) strings . append ( '' . join ([ x . text or '' for x in ts ])) return strings parse_sheet ( xml , shared_strings , styles ) Parse a worksheet XML and return a dict: cell address -> {value, formula, style, ...} Source code in arb\\utils\\excel\\excel_compare_04.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def parse_sheet ( xml : str , shared_strings : List [ str ], styles : Dict [ int , Dict [ str , str ]]) -> Dict [ str , Dict [ str , str ]]: \"\"\"Parse a worksheet XML and return a dict: cell address -> {value, formula, style, ...}\"\"\" root = ET . fromstring ( xml ) ns = { 'main' : 'http://schemas.openxmlformats.org/spreadsheetml/2006/main' } cells = {} for c in root . findall ( './/main:c' , ns ): addr = c . attrib [ 'r' ] cell = {} v = c . find ( 'main:v' , ns ) f = c . find ( 'main:f' , ns ) t = c . attrib . get ( 't' , None ) s = c . attrib . get ( 's' , None ) if t == 's' and v is not None : idx = int ( v . text ) cell [ 'value' ] = shared_strings [ idx ] if idx < len ( shared_strings ) else '' elif v is not None : cell [ 'value' ] = v . text else : cell [ 'value' ] = '' if f is not None : cell [ 'formula' ] = f . text # Only attempt style lookup if s is a string and is a valid integer if isinstance ( s , str ): s_stripped = s . strip () if s_stripped and s_stripped . isdigit (): style_idx = int ( s_stripped ) cell [ 'style' ] = styles . get ( style_idx , {}) cells [ addr ] = cell return cells parse_styles ( xml ) Parse styles.xml and return a dict of style index -> formatting dict. Source code in arb\\utils\\excel\\excel_compare_04.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def parse_styles ( xml : str ) -> Dict [ int , Dict [ str , str ]]: \"\"\"Parse styles.xml and return a dict of style index -> formatting dict.\"\"\" root = ET . fromstring ( xml ) numFmts = {} for numFmt in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}numFmt' ): numFmts [ numFmt . attrib [ 'numFmtId' ]] = numFmt . attrib [ 'formatCode' ] fonts = [] for font in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}font' ): font_dict = {} for child in font : font_dict [ child . tag . split ( '}' )[ - 1 ]] = child . attrib . get ( 'val' , child . text ) fonts . append ( font_dict ) fills = [] for fill in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}fill' ): fill_dict = {} for child in fill : fill_dict [ child . tag . split ( '}' )[ - 1 ]] = child . attrib . get ( 'val' , child . text ) fills . append ( fill_dict ) borders = [] for border in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}border' ): border_dict = {} for child in border : border_dict [ child . tag . split ( '}' )[ - 1 ]] = child . attrib . get ( 'val' , child . text ) borders . append ( border_dict ) cellXfs = [] for xf in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}xf' ): cellXfs . append ( xf . attrib ) styles = {} for idx , xf in enumerate ( cellXfs ): style = {} if 'numFmtId' in xf : style [ 'numFmt' ] = numFmts . get ( xf [ 'numFmtId' ], xf [ 'numFmtId' ]) if 'fontId' in xf : try : style [ 'font' ] = fonts [ int ( xf [ 'fontId' ])] except Exception : style [ 'font' ] = {} if 'fillId' in xf : try : style [ 'fill' ] = fills [ int ( xf [ 'fillId' ])] except Exception : style [ 'fill' ] = {} if 'borderId' in xf : try : style [ 'border' ] = borders [ int ( xf [ 'borderId' ])] except Exception : style [ 'border' ] = {} style [ 'xf' ] = xf styles [ idx ] = style return styles","title":"arb.utils.excel.excel_compare_04"},{"location":"reference/arb/utils/excel/excel_compare_04/#arbutilsexcelexcel_compare_04","text":"","title":"arb.utils.excel.excel_compare_04"},{"location":"reference/arb/utils/excel/excel_compare_04/#arb.utils.excel.excel_compare_04.parse_comments","text":"Parse comments for a given sheet if present. Source code in arb\\utils\\excel\\excel_compare_04.py 110 111 112 113 114 115 116 117 118 119 120 121 122 def parse_comments ( zipf : zipfile . ZipFile , sheet_file : str ) -> Dict [ str , str ]: \"\"\"Parse comments for a given sheet if present.\"\"\" comments = {} # Try to find the comments file for this sheet base = sheet_file . split ( '/' )[ - 1 ] . replace ( '.xml' , '' ) comments_file = f 'xl/comments { base [ 5 :] } .xml' # e.g., sheet1 -> comments1 if comments_file in zipf . namelist (): root = ET . fromstring ( zipf . read ( comments_file ) . decode ( 'utf-8' )) for comment in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}comment' ): ref = comment . attrib [ 'ref' ] text = '' . join ([ t . text or '' for t in comment . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}t' )]) comments [ ref ] = text return comments","title":"parse_comments"},{"location":"reference/arb/utils/excel/excel_compare_04/#arb.utils.excel.excel_compare_04.parse_data_validations","text":"Parse data validations (dropdowns) for a sheet. Source code in arb\\utils\\excel\\excel_compare_04.py 125 126 127 128 129 130 131 132 133 134 135 136 137 def parse_data_validations ( sheet_xml : str ) -> Dict [ str , str ]: \"\"\"Parse data validations (dropdowns) for a sheet.\"\"\" root = ET . fromstring ( sheet_xml ) ns = { 'main' : 'http://schemas.openxmlformats.org/spreadsheetml/2006/main' } validations = {} for dv in root . findall ( './/main:dataValidation' , ns ): sqref = dv . attrib . get ( 'sqref' , '' ) formula1 = dv . find ( 'main:formula1' , ns ) formula2 = dv . find ( 'main:formula2' , ns ) val_str = f \"type= { dv . attrib . get ( 'type' , '' ) } , formula1= { formula1 . text if formula1 is not None else '' } , formula2= { formula2 . text if formula2 is not None else '' } \" for cell in sqref . split (): validations [ cell ] = val_str return validations","title":"parse_data_validations"},{"location":"reference/arb/utils/excel/excel_compare_04/#arb.utils.excel.excel_compare_04.parse_hyperlinks","text":"Parse hyperlinks for a sheet. Source code in arb\\utils\\excel\\excel_compare_04.py 140 141 142 143 144 145 146 147 148 149 def parse_hyperlinks ( sheet_xml : str ) -> Dict [ str , str ]: \"\"\"Parse hyperlinks for a sheet.\"\"\" root = ET . fromstring ( sheet_xml ) ns = { 'main' : 'http://schemas.openxmlformats.org/spreadsheetml/2006/main' } links = {} for hl in root . findall ( './/main:hyperlink' , ns ): ref = hl . attrib . get ( 'ref' , '' ) target = hl . attrib . get ( 'display' , hl . attrib . get ( 'location' , hl . attrib . get ( 'tooltip' , '' ))) links [ ref ] = target return links","title":"parse_hyperlinks"},{"location":"reference/arb/utils/excel/excel_compare_04/#arb.utils.excel.excel_compare_04.parse_shared_strings","text":"Parse sharedStrings.xml and return a list of strings. Source code in arb\\utils\\excel\\excel_compare_04.py 13 14 15 16 17 18 19 20 21 22 23 24 def parse_shared_strings ( xml : str ) -> List [ str ]: \"\"\"Parse sharedStrings.xml and return a list of strings.\"\"\" root = ET . fromstring ( xml ) strings = [] for si in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}si' ): t = si . find ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}t' ) if t is not None : strings . append ( t . text or \"\" ) else : ts = si . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}t' ) strings . append ( '' . join ([ x . text or '' for x in ts ])) return strings","title":"parse_shared_strings"},{"location":"reference/arb/utils/excel/excel_compare_04/#arb.utils.excel.excel_compare_04.parse_sheet","text":"Parse a worksheet XML and return a dict: cell address -> {value, formula, style, ...} Source code in arb\\utils\\excel\\excel_compare_04.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def parse_sheet ( xml : str , shared_strings : List [ str ], styles : Dict [ int , Dict [ str , str ]]) -> Dict [ str , Dict [ str , str ]]: \"\"\"Parse a worksheet XML and return a dict: cell address -> {value, formula, style, ...}\"\"\" root = ET . fromstring ( xml ) ns = { 'main' : 'http://schemas.openxmlformats.org/spreadsheetml/2006/main' } cells = {} for c in root . findall ( './/main:c' , ns ): addr = c . attrib [ 'r' ] cell = {} v = c . find ( 'main:v' , ns ) f = c . find ( 'main:f' , ns ) t = c . attrib . get ( 't' , None ) s = c . attrib . get ( 's' , None ) if t == 's' and v is not None : idx = int ( v . text ) cell [ 'value' ] = shared_strings [ idx ] if idx < len ( shared_strings ) else '' elif v is not None : cell [ 'value' ] = v . text else : cell [ 'value' ] = '' if f is not None : cell [ 'formula' ] = f . text # Only attempt style lookup if s is a string and is a valid integer if isinstance ( s , str ): s_stripped = s . strip () if s_stripped and s_stripped . isdigit (): style_idx = int ( s_stripped ) cell [ 'style' ] = styles . get ( style_idx , {}) cells [ addr ] = cell return cells","title":"parse_sheet"},{"location":"reference/arb/utils/excel/excel_compare_04/#arb.utils.excel.excel_compare_04.parse_styles","text":"Parse styles.xml and return a dict of style index -> formatting dict. Source code in arb\\utils\\excel\\excel_compare_04.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def parse_styles ( xml : str ) -> Dict [ int , Dict [ str , str ]]: \"\"\"Parse styles.xml and return a dict of style index -> formatting dict.\"\"\" root = ET . fromstring ( xml ) numFmts = {} for numFmt in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}numFmt' ): numFmts [ numFmt . attrib [ 'numFmtId' ]] = numFmt . attrib [ 'formatCode' ] fonts = [] for font in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}font' ): font_dict = {} for child in font : font_dict [ child . tag . split ( '}' )[ - 1 ]] = child . attrib . get ( 'val' , child . text ) fonts . append ( font_dict ) fills = [] for fill in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}fill' ): fill_dict = {} for child in fill : fill_dict [ child . tag . split ( '}' )[ - 1 ]] = child . attrib . get ( 'val' , child . text ) fills . append ( fill_dict ) borders = [] for border in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}border' ): border_dict = {} for child in border : border_dict [ child . tag . split ( '}' )[ - 1 ]] = child . attrib . get ( 'val' , child . text ) borders . append ( border_dict ) cellXfs = [] for xf in root . findall ( './/{http://schemas.openxmlformats.org/spreadsheetml/2006/main}xf' ): cellXfs . append ( xf . attrib ) styles = {} for idx , xf in enumerate ( cellXfs ): style = {} if 'numFmtId' in xf : style [ 'numFmt' ] = numFmts . get ( xf [ 'numFmtId' ], xf [ 'numFmtId' ]) if 'fontId' in xf : try : style [ 'font' ] = fonts [ int ( xf [ 'fontId' ])] except Exception : style [ 'font' ] = {} if 'fillId' in xf : try : style [ 'fill' ] = fills [ int ( xf [ 'fillId' ])] except Exception : style [ 'fill' ] = {} if 'borderId' in xf : try : style [ 'border' ] = borders [ int ( xf [ 'borderId' ])] except Exception : style [ 'border' ] = {} style [ 'xf' ] = xf styles [ idx ] = style return styles","title":"parse_styles"},{"location":"reference/arb/utils/excel/xl_create/","text":"arb.utils.excel.xl_create Module to prepare Excel templates and generate new Excel files using Jinja-rendered payloads. This module performs schema-based templating of Excel spreadsheets for feedback forms, injects metadata, applies default values, and renders Excel files based on structured JSON payloads. Typical Workflow Use a spreadsheet with named ranges and Jinja placeholders. Generate an initial JSON schema using a VBA macro. Refine the schema by injecting typing information. Generate default and test payloads. Populate Excel files using these payloads. Run this file directly to create all schema and payload artifacts for landfill, oil and gas, and energy templates. create_default_types_schema ( diagnostics = False ) Create a JSON file that maps variable names to their default Python value types. Parameters: diagnostics ( bool , default: False ) \u2013 If True, logs each variable name and its type to the debug logger. Returns: dict ( dict ) \u2013 Dictionary mapping variable names to Python types (e.g., str, int, datetime). Examples: Input : diagnostics=True Output: Dictionary of variable names \u2192 value types, logged if diagnostics enabled Notes Output is saved to 'xl_schemas/default_value_types_v01_00.json'. A backup is compared against the newly generated file if present. Field names and types are sourced from xl_hardcoded.default_value_types_v01_00 . Source code in arb\\utils\\excel\\xl_create.py 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 def create_default_types_schema ( diagnostics : bool = False ) -> dict : \"\"\" Create a JSON file that maps variable names to their default Python value types. Args: diagnostics (bool): If True, logs each variable name and its type to the debug logger. Returns: dict: Dictionary mapping variable names to Python types (e.g., str, int, datetime). Examples: Input : diagnostics=True Output: Dictionary of variable names \u2192 value types, logged if diagnostics enabled Notes: - Output is saved to 'xl_schemas/default_value_types_v01_00.json'. - A backup is compared against the newly generated file if present. - Field names and types are sourced from `xl_hardcoded.default_value_types_v01_00`. \"\"\" from arb.utils.excel.xl_hardcoded import default_value_types_v01_00 logger . debug ( f \"create_default_types_schema() called\" ) file_name = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00.json\" file_backup = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00_backup.json\" field_types = dict ( sorted ( default_value_types_v01_00 . items ())) if diagnostics : for name , typ in field_types . items (): logger . debug ( f \"' { name } ': { typ . __name__ } ,\" ) metadata = { \"schema_version\" : \"default_value_types_v01_00\" } json_save_with_meta ( file_name , field_types , metadata = metadata ) if file_name . is_file () and file_backup . is_file (): compare_json_files ( file_name , file_backup ) return field_types create_payload ( payload , file_name , schema_version , metadata = None ) Create a JSON payload file with embedded metadata describing the schema version. Parameters: payload ( dict ) \u2013 Dictionary of values to serialize to JSON. file_name ( Path ) \u2013 Path to output the payload JSON file. schema_version ( str ) \u2013 Identifier for the schema the payload conforms to. metadata ( dict , default: None ) \u2013 Additional metadata to embed. If None, a new dict is created. Returns: None \u2013 None Examples: Input : {\"id_case\": \"A42\"}, Path(\"payload.json\"), \"v01_00\" Output: JSON file with metadata saved to payload.json Notes Adds 'schema_version' and a default payload description to metadata. Uses json_save_with_meta() to embed metadata into the JSON file. Logs all key actions and file paths for diagnostics. Source code in arb\\utils\\excel\\xl_create.py 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 def create_payload ( payload : dict , file_name : Path , schema_version : str , metadata : dict | None = None ) -> None : \"\"\" Create a JSON payload file with embedded metadata describing the schema version. Args: payload (dict): Dictionary of values to serialize to JSON. file_name (Path): Path to output the payload JSON file. schema_version (str): Identifier for the schema the payload conforms to. metadata (dict, optional): Additional metadata to embed. If None, a new dict is created. Returns: None Examples: Input : {\"id_case\": \"A42\"}, Path(\"payload.json\"), \"v01_00\" Output: JSON file with metadata saved to payload.json Notes: - Adds 'schema_version' and a default payload description to metadata. - Uses `json_save_with_meta()` to embed metadata into the JSON file. - Logs all key actions and file paths for diagnostics. \"\"\" logger . debug ( f \"create_payload() called\" ) if metadata is None : metadata = {} metadata [ \"schema_version\" ] = schema_version metadata [ \"payload description\" ] = \"Test of Excel jinja templating system\" logger . debug ( f \"Writing payload to { file_name } with metadata: { metadata } \" ) json_save_with_meta ( file_name , data = payload , metadata = metadata ) create_payloads () Generate and save example payload files for each supported sector (landfill, oil and gas, energy). Returns: None \u2013 None Examples: Input : None Output: Sector payloads written to xl_payloads/*.json Notes Each payload is saved to xl_payloads/{schema_version}_payload_01.json . If a backup file exists, the new payload is compared against it for consistency. Uses create_payload() to handle serialization and metadata embedding. Source code in arb\\utils\\excel\\xl_create.py 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 def create_payloads () -> None : \"\"\" Generate and save example payload files for each supported sector (landfill, oil and gas, energy). Returns: None Examples: Input : None Output: Sector payloads written to xl_payloads/*.json Notes: - Each payload is saved to `xl_payloads/{schema_version}_payload_01.json`. - If a backup file exists, the new payload is compared against it for consistency. - Uses `create_payload()` to handle serialization and metadata embedding. \"\"\" logger . debug ( f \"create_payloads() called\" ) import arb.utils.excel.xl_hardcoded as xl_hardcoded for template in EXCEL_TEMPLATES : schema_version = template [ \"schema_version\" ] payload_name = template [ \"payload_name\" ] # Dynamically get the payload object from xl_hardcoded payload = getattr ( xl_hardcoded , payload_name ) file_name = PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _payload_01.json\" file_backup = PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _payload_01_backup.json\" create_payload ( payload , file_name , schema_version ) if file_name . is_file () and file_backup . is_file (): compare_json_files ( file_name , file_backup ) create_schemas_and_payloads () Generate all schema, payload, and Excel artifacts for the feedback system. This orchestration function performs the full pipeline Creates the default value types schema. Processes all sector-specific schema files (landfill, oil and gas, energy). Writes default payloads for each schema. Generates test payloads and renders Excel templates. Returns: None \u2013 None Examples: Input : None Output: All schema/payload/template artifacts generated under processed_versions/ Notes Create all required directories under processed_versions . Intended for one-time use during development or deployment setup. Logs each operation and file path for debugging. Source code in arb\\utils\\excel\\xl_create.py 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 def create_schemas_and_payloads () -> None : \"\"\" Generate all schema, payload, and Excel artifacts for the feedback system. This orchestration function performs the full pipeline: - Creates the default value types schema. - Processes all sector-specific schema files (landfill, oil and gas, energy). - Writes default payloads for each schema. - Generates test payloads and renders Excel templates. Returns: None Examples: Input : None Output: All schema/payload/template artifacts generated under processed_versions/ Notes: - Create all required directories under `processed_versions`. - Intended for one-time use during development or deployment setup. - Logs each operation and file path for debugging. \"\"\" logger . debug ( f \"create_schemas_and_payloads() called\" ) ensure_dir_exists ( PROCESSED_VERSIONS / \"xl_schemas\" ) ensure_dir_exists ( PROCESSED_VERSIONS / \"xl_workbooks\" ) ensure_dir_exists ( PROCESSED_VERSIONS / \"xl_payloads\" ) create_default_types_schema ( diagnostics = True ) prep_xl_templates () create_payloads () test_update_xlsx_payloads_01 () prep_xl_templates () Prepare processed Excel templates and payloads for landfill, oil and gas, and energy sectors. This function Copies original schema and Excel files to the processed directory. Converts VBA-generated schema files by injecting type info. Writes upgraded schema and default payload JSON files. Produces Jinja-compatible Excel workbook versions for templating. Returns: None \u2013 None Notes File paths are derived from structured configs for each sector. Overwrites files in the output directory if they already exist. Output directories are created if they don't exist. Source code in arb\\utils\\excel\\xl_create.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 def prep_xl_templates () -> None : \"\"\" Prepare processed Excel templates and payloads for landfill, oil and gas, and energy sectors. This function: - Copies original schema and Excel files to the processed directory. - Converts VBA-generated schema files by injecting type info. - Writes upgraded schema and default payload JSON files. - Produces Jinja-compatible Excel workbook versions for templating. Returns: None Notes: - File paths are derived from structured configs for each sector. - Overwrites files in the output directory if they already exist. - Output directories are created if they don't exist. \"\"\" logger . debug ( f \"prep_xl_templates() called for all templates in TEMPLATES\" ) file_specs = [] input_dir = PROJECT_ROOT / \"feedback_forms/current_versions\" output_dir = PROJECT_ROOT / \"feedback_forms/processed_versions\" ensure_dir_exists ( output_dir / \"xl_schemas\" ) ensure_dir_exists ( output_dir / \"xl_workbooks\" ) ensure_dir_exists ( output_dir / \"xl_payloads\" ) for template in EXCEL_TEMPLATES : schema_version = template [ \"schema_version\" ] prefix = template [ \"prefix\" ] version = template [ \"version\" ] spec = { \"schema_version\" : schema_version , \"input_schema_vba_path\" : input_dir / f \" { schema_version } _vba.json\" , \"input_xl_path\" : input_dir / f \" { prefix } _ { version } .xlsx\" , \"input_xl_jinja_path\" : input_dir / f \" { prefix } _ { version } _jinja_.xlsx\" , \"output_schema_vba_path\" : output_dir / \"xl_schemas\" / f \" { schema_version } _vba.json\" , \"output_schema_path\" : output_dir / \"xl_schemas\" / f \" { schema_version } .json\" , \"output_xl_path\" : output_dir / \"xl_workbooks\" / f \" { prefix } _ { version } .xlsx\" , \"output_xl_jinja_path\" : output_dir / \"xl_workbooks\" / f \" { prefix } _ { version } _jinja_.xlsx\" , \"output_payload_path\" : output_dir / \"xl_payloads\" / f \" { schema_version } _defaults.json\" , } file_specs . append ( spec ) for spec in file_specs : logger . debug ( f \"Processing schema_version { spec [ 'schema_version' ] } \" ) file_map = [ ( spec [ \"input_schema_vba_path\" ], spec [ \"output_schema_vba_path\" ]), ( spec [ \"input_xl_path\" ], spec [ \"output_xl_path\" ]), ( spec [ \"input_xl_jinja_path\" ], spec [ \"output_xl_jinja_path\" ]), ] for file_old , file_new in file_map : logger . debug ( f \"Copying file from: { file_old } to: { file_new } \" ) shutil . copy ( file_old , file_new ) update_vba_schema ( spec [ \"schema_version\" ], file_name_in = spec [ \"output_schema_vba_path\" ], file_name_out = spec [ \"output_schema_path\" ]) schema_to_default_json ( file_name_in = spec [ \"output_schema_path\" ], file_name_out = spec [ \"output_payload_path\" ]) run_diagnostics () Execute a full suite of diagnostic routines to verify Excel templating functionality. This includes Creating default value types schema. Generating upgraded schema files and default payloads. Running test payload injection for Jinja-enabled Excel files. Returns: None \u2013 None Notes Logs each step of the process to the application logger. Catches and logs any exceptions that occur during testing. Intended for developers to verify schema and workbook generation end-to-end. Source code in arb\\utils\\excel\\xl_create.py 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 def run_diagnostics () -> None : \"\"\" Execute a full suite of diagnostic routines to verify Excel templating functionality. This includes: - Creating default value types schema. - Generating upgraded schema files and default payloads. - Running test payload injection for Jinja-enabled Excel files. Returns: None Notes: - Logs each step of the process to the application logger. - Catches and logs any exceptions that occur during testing. - Intended for developers to verify schema and workbook generation end-to-end. \"\"\" logger . info ( f \"Running diagnostics...\" ) try : logger . info ( f \"Step 1: Creating default type schema\" ) create_default_types_schema ( diagnostics = True ) logger . info ( f \"Step 2: Creating and verifying schema files and payloads\" ) prep_xl_templates () create_payloads () logger . info ( f \"Step 3: Performing test Excel generation\" ) test_update_xlsx_payloads_01 () logger . info ( f \"Diagnostics complete. Check output directory and logs for details.\" ) except Exception as e : logger . exception ( f \"Diagnostics failed: { e } \" ) schema_to_default_dict ( schema_file_name ) Generate default values and metadata from an Excel schema JSON file. Parameters: schema_file_name ( Path ) \u2013 Path to the schema JSON file. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: - defaults: Dictionary mapping variable names to default values. * Drop-downs get \"Please Select\". * All other fields get an empty string. - metadata: Metadata dictionary from the schema file. Examples: Input : Path(\"xl_schemas/landfill_v01_00.json\") Output: (defaults dictionary, metadata dictionary) Notes The schema must include an \"is_drop_down\" flag for correct default generation. Useful for pre-populating forms with valid placeholder values. Source code in arb\\utils\\excel\\xl_create.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def schema_to_default_dict ( schema_file_name : Path ) -> tuple [ dict , dict ]: \"\"\" Generate default values and metadata from an Excel schema JSON file. Args: schema_file_name (Path): Path to the schema JSON file. Returns: tuple[dict, dict]: - defaults: Dictionary mapping variable names to default values. * Drop-downs get \"Please Select\". * All other fields get an empty string. - metadata: Metadata dictionary from the schema file. Examples: Input : Path(\"xl_schemas/landfill_v01_00.json\") Output: (defaults dictionary, metadata dictionary) Notes: - The schema must include an \"is_drop_down\" flag for correct default generation. - Useful for pre-populating forms with valid placeholder values. \"\"\" logger . debug ( f \"schema_to_default_dict() called for { schema_file_name =} \" ) data , metadata = json_load_with_meta ( schema_file_name ) logger . debug ( f \" { metadata =} \" ) defaults = { variable : PLEASE_SELECT if sub_schema . get ( \"is_drop_down\" ) else \"\" for variable , sub_schema in data . items () } return defaults , metadata schema_to_default_json ( file_name_in , file_name_out = None ) Save default values extracted from a schema into a JSON file with metadata. Parameters: file_name_in ( Path ) \u2013 Input path to the schema JSON file. file_name_out ( Path , default: None ) \u2013 Output path for the default JSON. If None, defaults to 'xl_payloads/{schema_version}_defaults.json'. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: - defaults: Dictionary of default values derived from the schema. - metadata: Metadata dictionary included in the output JSON. Examples: Input : Path(\"xl_schemas/landfill_v01_00.json\") Output: Tuple of (defaults dict, metadata dict), written to output path Notes Drop-down fields default to \"Please Select\". Other fields default to an empty string. Adds a note to metadata explaining default value behavior. Ensures output directory exists before writing. Source code in arb\\utils\\excel\\xl_create.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def schema_to_default_json ( file_name_in : Path , file_name_out : Path | None = None ) -> tuple [ dict , dict ]: \"\"\" Save default values extracted from a schema into a JSON file with metadata. Args: file_name_in (Path): Input path to the schema JSON file. file_name_out (Path, optional): Output path for the default JSON. If None, defaults to 'xl_payloads/{schema_version}_defaults.json'. Returns: tuple[dict, dict]: - defaults: Dictionary of default values derived from the schema. - metadata: Metadata dictionary included in the output JSON. Examples: Input : Path(\"xl_schemas/landfill_v01_00.json\") Output: Tuple of (defaults dict, metadata dict), written to output path Notes: - Drop-down fields default to \"Please Select\". - Other fields default to an empty string. - Adds a note to metadata explaining default value behavior. - Ensures output directory exists before writing. \"\"\" logger . debug ( f \"schema_to_default_json() called for { file_name_in =} \" ) defaults , metadata = schema_to_default_dict ( file_name_in ) metadata [ 'notes' ] = ( \"Default values are empty strings unless the field is a drop-down cell. \" \"For drop-down cells, the default is 'Please Select'.\" ) if file_name_out is None : file_name_out = f \"xl_payloads/ { metadata [ 'schema_version' ] } _defaults.json\" ensure_parent_dirs ( file_name_out ) json_save_with_meta ( file_name_out , data = defaults , metadata = metadata , json_options = None ) return defaults , metadata schema_to_json_file ( data , schema_version , file_name = None ) Save an Excel schema to a JSON file with metadata and validate the round-trip. Parameters: data ( dict ) \u2013 The Excel schema to be serialized and written to disk. schema_version ( str ) \u2013 Schema version identifier to include in metadata. file_name ( str , default: None ) \u2013 Output file path. Default to \"xl_schemas/{schema_version}.json\". Returns: None \u2013 None Examples: Input : my_schema, schema_version=\"v01_00\" Output: JSON file written to \"xl_schemas/v01_00.json\" (default path) Notes If file_name is not provided, the output will be saved to \"xl_schemas/{schema_version}.json\". Metadata will include the schema version. Performs a round-trip serialization test to verify that the saved data and metadata match the originals. Source code in arb\\utils\\excel\\xl_create.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def schema_to_json_file ( data : dict , schema_version : str , file_name : str | None = None ) -> None : \"\"\" Save an Excel schema to a JSON file with metadata and validate the round-trip. Args: data (dict): The Excel schema to be serialized and written to disk. schema_version (str): Schema version identifier to include in metadata. file_name (str, optional): Output file path. Default to \"xl_schemas/{schema_version}.json\". Returns: None Examples: Input : my_schema, schema_version=\"v01_00\" Output: JSON file written to \"xl_schemas/v01_00.json\" (default path) Notes: - If `file_name` is not provided, the output will be saved to \"xl_schemas/{schema_version}.json\". - Metadata will include the schema version. - Performs a round-trip serialization test to verify that the saved data and metadata match the originals. \"\"\" logger . debug ( f \"schema_to_json_file() called with { schema_version =} , { file_name =} \" ) if file_name is None : file_name = f \"xl_schemas/ { schema_version } .json\" ensure_parent_dirs ( file_name ) metadata = { 'schema_version' : schema_version } logger . debug ( f \"Saving schema to: { file_name } with metadata: { metadata } \" ) json_save_with_meta ( file_name , data = data , metadata = metadata , json_options = None ) # Verify round-trip serialization read_data , read_metadata = json_load_with_meta ( file_name ) if read_data == data and read_metadata == metadata : logger . debug ( f \"SUCCESS: JSON serialization round-trip matches original.\" ) else : logger . warning ( f \"FAILURE: Mismatch in JSON serialization round-trip.\" ) sort_xl_schema ( xl_schema , sort_by = 'variable_name' ) Sort an Excel schema and its sub-schema dictionaries for easier comparison. This function modifies sub-schemas in place and returns a new dictionary with the top-level keys sorted according to the selected strategy. Sub-schemas are reordered so that keys appear in the order: \"label\", \"label_address\", \"value_address\", \"value_type\", then others. Parameters: xl_schema ( dict ) \u2013 Dictionary where keys are variable names and values are sub-schema dicts. sort_by ( str , default: 'variable_name' ) \u2013 Sorting strategy: - \"variable_name\": Sort top-level keys alphabetically (default). - \"label_address\": Sort based on Excel row order of each sub-schema's 'label_address'. Returns: dict ( dict ) \u2013 A new dictionary with reordered sub-schemas and sorted top-level keys. Raises: ValueError \u2013 If an unrecognized sorting strategy is provided. Examples: Input : schema, sort_by=\"label_address\" Output: New dictionary with sorted keys and reordered sub-schemas Source code in arb\\utils\\excel\\xl_create.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def sort_xl_schema ( xl_schema : dict , sort_by : str = \"variable_name\" ) -> dict : \"\"\" Sort an Excel schema and its sub-schema dictionaries for easier comparison. This function modifies sub-schemas in place and returns a new dictionary with the top-level keys sorted according to the selected strategy. Sub-schemas are reordered so that keys appear in the order: \"label\", \"label_address\", \"value_address\", \"value_type\", then others. Args: xl_schema (dict): Dictionary where keys are variable names and values are sub-schema dicts. sort_by (str): Sorting strategy: - \"variable_name\": Sort top-level keys alphabetically (default). - \"label_address\": Sort based on Excel row order of each sub-schema's 'label_address'. Returns: dict: A new dictionary with reordered sub-schemas and sorted top-level keys. Raises: ValueError: If an unrecognized sorting strategy is provided. Examples: Input : schema, sort_by=\"label_address\" Output: New dictionary with sorted keys and reordered sub-schemas \"\"\" logger . debug ( f \"sort_xl_schema() called\" ) # Reorder each sub-schema dict for variable_name , sub_schema in xl_schema . items (): reordered = {} for key in ( \"label\" , \"label_address\" , \"value_address\" , \"value_type\" ): if key in sub_schema : reordered [ key ] = sub_schema . pop ( key ) reordered . update ( sub_schema ) xl_schema [ variable_name ] = reordered # Sort top-level dictionary if sort_by == \"variable_name\" : logger . debug ( f \"Sorting schema by variable_name\" ) sorted_items = dict ( sorted ( xl_schema . items (), key = lambda item : item [ 0 ])) elif sort_by == \"label_address\" : logger . debug ( f \"Sorting schema by label_address\" ) get_xl_row = partial ( xl_address_sort , address_location = \"value\" , sort_by = \"row\" , sub_keys = \"label_address\" ) sorted_items = dict ( sorted ( xl_schema . items (), key = get_xl_row )) else : raise ValueError ( \"sort_by must be 'variable_name' or 'label_address'\" ) return sorted_items test_update_xlsx_payloads_01 () Run test cases that populate Jinja-templated Excel files with known payloads. This test routine helps validate that Excel generation is functioning correctly for all supported sectors (landfill, oil and gas, energy). Returns: None \u2013 None Examples: Input : None Output: Diagnostic Excel files created in xl_workbooks/ Notes Writes populated Excel files to the xl_workbooks directory. Uses both file-based and inline payloads. Intended for development and diagnostic use, not production. Source code in arb\\utils\\excel\\xl_create.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def test_update_xlsx_payloads_01 () -> None : \"\"\" Run test cases that populate Jinja-templated Excel files with known payloads. This test routine helps validate that Excel generation is functioning correctly for all supported sectors (landfill, oil and gas, energy). Returns: None Examples: Input : None Output: Diagnostic Excel files created in xl_workbooks/ Notes: - Writes populated Excel files to the `xl_workbooks` directory. - Uses both file-based and inline payloads. - Intended for development and diagnostic use, not production. \"\"\" logger . debug ( f \"test_update_xlsx_payloads_01() called\" ) for template in EXCEL_TEMPLATES : schema_version = template [ \"schema_version\" ] prefix = template [ \"prefix\" ] version = template [ \"version\" ] # Test with two payloads from file (defaults + payload_01) update_xlsx_payloads ( PROCESSED_VERSIONS / f \"xl_workbooks/ { prefix } _ { version } _jinja_.xlsx\" , PROCESSED_VERSIONS / f \"xl_workbooks/ { prefix } _ { version } _populated_01.xlsx\" , [ PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _defaults.json\" , PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _payload_01.json\" , ] ) # Test with one file payload and one inline dict update_xlsx_payloads ( PROCESSED_VERSIONS / f \"xl_workbooks/ { prefix } _ { version } _jinja_.xlsx\" , PROCESSED_VERSIONS / f \"xl_workbooks/ { prefix } _ { version } _populated_02.xlsx\" , [ PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _payload_01.json\" , { \"id_incidence\" : \"123456\" }, ] ) update_vba_schema ( schema_version , file_name_in = None , file_name_out = None , file_name_default_value_types = None ) Update a VBA-generated Excel schema with value_type info and re-sort it. Parameters: schema_version ( str ) \u2013 Identifier for the schema version. file_name_in ( Path , default: None ) \u2013 Path to the raw VBA schema JSON file. Default to \"processed_versions/xl_schemas/{schema_version}_vba.json\". file_name_out ( Path , default: None ) \u2013 Path to output the upgraded schema JSON. Default to \"processed_versions/xl_schemas/{schema_version}.json\". file_name_default_value_types ( Path , default: None ) \u2013 Path to JSON file defining default value types. Defaults to \"processed_versions/xl_schemas/default_value_types_v01_00.json\". Returns: dict ( dict ) \u2013 The updated and sorted schema dictionary. Examples: Input : \"landfill_v01_00\" Output: Sorted schema dictionary with value types injected Notes This function ensures that all schema entries include a 'value_type'. Applies sort_xl_schema() with sort_by=\"label_address\". Use schema_to_json_file() to write the result to disk. Source code in arb\\utils\\excel\\xl_create.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def update_vba_schema ( schema_version : str , file_name_in : Path | None = None , file_name_out : Path | None = None , file_name_default_value_types : Path | None = None ) -> dict : \"\"\" Update a VBA-generated Excel schema with value_type info and re-sort it. Args: schema_version (str): Identifier for the schema version. file_name_in (Path, optional): Path to the raw VBA schema JSON file. Default to \"processed_versions/xl_schemas/{schema_version}_vba.json\". file_name_out (Path, optional): Path to output the upgraded schema JSON. Default to \"processed_versions/xl_schemas/{schema_version}.json\". file_name_default_value_types (Path, optional): Path to JSON file defining default value types. Defaults to \"processed_versions/xl_schemas/default_value_types_v01_00.json\". Returns: dict: The updated and sorted schema dictionary. Examples: Input : \"landfill_v01_00\" Output: Sorted schema dictionary with value types injected Notes: - This function ensures that all schema entries include a 'value_type'. - Applies `sort_xl_schema()` with sort_by=\"label_address\". - Use `schema_to_json_file()` to write the result to disk. \"\"\" logger . debug ( f \"update_vba_schema() called with { schema_version =} , { file_name_in =} , \" f \" { file_name_out =} , { file_name_default_value_types =} \" ) if file_name_in is None : file_name_in = PROCESSED_VERSIONS / \"xl_schemas\" / f \" { schema_version } _vba.json\" if file_name_out is None : file_name_out = PROCESSED_VERSIONS / \"xl_schemas\" / f \" { schema_version } .json\" if file_name_default_value_types is None : file_name_default_value_types = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00.json\" ensure_parent_dirs ( file_name_in ) ensure_parent_dirs ( file_name_out ) ensure_parent_dirs ( file_name_default_value_types ) default_value_types , _ = json_load_with_meta ( file_name_default_value_types ) schema = json_load ( file_name_in , json_options = None ) ensure_key_value_pair ( schema , default_value_types , \"value_type\" ) schema = sort_xl_schema ( schema , sort_by = \"label_address\" ) schema_to_json_file ( schema , schema_version , file_name = file_name_out ) return schema update_vba_schemas () Batch update of known VBA-generated schemas using update_vba_schema() . This function applies schema upgrades to all templates defined in TEMPLATES. Returns: None \u2013 None Notes Calls update_vba_schema() for each template in TEMPLATES. Output schemas are written to the processed_versions/xl_schemas directory. Source code in arb\\utils\\excel\\xl_create.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def update_vba_schemas () -> None : \"\"\" Batch update of known VBA-generated schemas using `update_vba_schema()`. This function applies schema upgrades to all templates defined in TEMPLATES. Returns: None Notes: - Calls `update_vba_schema()` for each template in TEMPLATES. - Output schemas are written to the processed_versions/xl_schemas directory. \"\"\" logger . debug ( f \"update_vba_schemas() called\" ) for template in EXCEL_TEMPLATES : update_vba_schema ( template [ \"schema_version\" ]) update_xlsx ( file_in , file_out , jinja_dict ) Render a Jinja-templated Excel (.xlsx) file by replacing placeholders with dictionary values. Parameters: file_in ( Path ) \u2013 Path to the input Excel file containing Jinja placeholders. file_out ( Path ) \u2013 Path where the rendered Excel file will be saved. jinja_dict ( dict ) \u2013 Dictionary mapping Jinja template variables to replacement values. Returns: None \u2013 None Examples: Input : template.xlsx, output.xlsx, {\"site_name\": \"Landfill A\"} Output: output.xlsx with rendered values Notes Only modifies 'xl/sharedStrings.xml' within the XLSX zip archive. All other file contents are passed through unchanged. Useful for populating pre-tagged Excel templates with form data. Source code in arb\\utils\\excel\\xl_create.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 def update_xlsx ( file_in : Path , file_out : Path , jinja_dict : dict ) -> None : \"\"\" Render a Jinja-templated Excel (.xlsx) file by replacing placeholders with dictionary values. Args: file_in (Path): Path to the input Excel file containing Jinja placeholders. file_out (Path): Path where the rendered Excel file will be saved. jinja_dict (dict): Dictionary mapping Jinja template variables to replacement values. Returns: None Examples: Input : template.xlsx, output.xlsx, {\"site_name\": \"Landfill A\"} Output: output.xlsx with rendered values Notes: - Only modifies 'xl/sharedStrings.xml' within the XLSX zip archive. - All other file contents are passed through unchanged. - Useful for populating pre-tagged Excel templates with form data. \"\"\" logger . debug ( f \"Rendering Excel from { file_in } to { file_out } using { jinja_dict } \" ) with zipfile . ZipFile ( file_in , 'r' ) as xlsx , zipfile . ZipFile ( file_out , 'w' ) as new_xlsx : for filename in xlsx . namelist (): with xlsx . open ( filename ) as file : contents = file . read () if filename == 'xl/sharedStrings.xml' : contents = jinja2 . Template ( contents . decode ( 'utf-8' )) . render ( jinja_dict ) . encode ( 'utf-8' ) new_xlsx . writestr ( filename , contents ) update_xlsx_payloads ( file_in , file_out , payloads ) Apply multiple payloads to a Jinja-templated Excel file and render the result. Parameters: file_in ( Path ) \u2013 Path to the input Excel file containing Jinja placeholders. file_out ( Path ) \u2013 Path where the populated Excel file will be written. payloads ( list | tuple ) \u2013 Sequence of dictionaries or JSON file paths. - Payloads are merged in order, with later values overriding earlier ones. Returns: None \u2013 None Notes Each payload may be a dictionary or a Path to a JSON file with metadata. Designed to support tiered rendering: default payload + override. Uses json_load_with_meta() for file payloads. Passes final merged dictionary to update_xlsx() . Source code in arb\\utils\\excel\\xl_create.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 def update_xlsx_payloads ( file_in : Path , file_out : Path , payloads : list | tuple ) -> None : \"\"\" Apply multiple payloads to a Jinja-templated Excel file and render the result. Args: file_in (Path): Path to the input Excel file containing Jinja placeholders. file_out (Path): Path where the populated Excel file will be written. payloads (list | tuple): Sequence of dictionaries or JSON file paths. - Payloads are merged in order, with later values overriding earlier ones. Returns: None Notes: - Each payload may be a dictionary or a Path to a JSON file with metadata. - Designed to support tiered rendering: default payload + override. - Uses `json_load_with_meta()` for file payloads. - Passes final merged dictionary to `update_xlsx()`. \"\"\" logger . debug ( f \"update_xlsx_payloads() called with: { file_in =} , { file_out =} , { payloads =} \" ) new_dict = {} for payload in payloads : if isinstance ( payload , dict ): data = payload else : data , _ = json_load_with_meta ( payload ) new_dict . update ( data ) update_xlsx ( file_in , file_out , new_dict )","title":"arb.utils.excel.xl_create"},{"location":"reference/arb/utils/excel/xl_create/#arbutilsexcelxl_create","text":"Module to prepare Excel templates and generate new Excel files using Jinja-rendered payloads. This module performs schema-based templating of Excel spreadsheets for feedback forms, injects metadata, applies default values, and renders Excel files based on structured JSON payloads. Typical Workflow Use a spreadsheet with named ranges and Jinja placeholders. Generate an initial JSON schema using a VBA macro. Refine the schema by injecting typing information. Generate default and test payloads. Populate Excel files using these payloads. Run this file directly to create all schema and payload artifacts for landfill, oil and gas, and energy templates.","title":"arb.utils.excel.xl_create"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.create_default_types_schema","text":"Create a JSON file that maps variable names to their default Python value types. Parameters: diagnostics ( bool , default: False ) \u2013 If True, logs each variable name and its type to the debug logger. Returns: dict ( dict ) \u2013 Dictionary mapping variable names to Python types (e.g., str, int, datetime). Examples: Input : diagnostics=True Output: Dictionary of variable names \u2192 value types, logged if diagnostics enabled Notes Output is saved to 'xl_schemas/default_value_types_v01_00.json'. A backup is compared against the newly generated file if present. Field names and types are sourced from xl_hardcoded.default_value_types_v01_00 . Source code in arb\\utils\\excel\\xl_create.py 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 def create_default_types_schema ( diagnostics : bool = False ) -> dict : \"\"\" Create a JSON file that maps variable names to their default Python value types. Args: diagnostics (bool): If True, logs each variable name and its type to the debug logger. Returns: dict: Dictionary mapping variable names to Python types (e.g., str, int, datetime). Examples: Input : diagnostics=True Output: Dictionary of variable names \u2192 value types, logged if diagnostics enabled Notes: - Output is saved to 'xl_schemas/default_value_types_v01_00.json'. - A backup is compared against the newly generated file if present. - Field names and types are sourced from `xl_hardcoded.default_value_types_v01_00`. \"\"\" from arb.utils.excel.xl_hardcoded import default_value_types_v01_00 logger . debug ( f \"create_default_types_schema() called\" ) file_name = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00.json\" file_backup = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00_backup.json\" field_types = dict ( sorted ( default_value_types_v01_00 . items ())) if diagnostics : for name , typ in field_types . items (): logger . debug ( f \"' { name } ': { typ . __name__ } ,\" ) metadata = { \"schema_version\" : \"default_value_types_v01_00\" } json_save_with_meta ( file_name , field_types , metadata = metadata ) if file_name . is_file () and file_backup . is_file (): compare_json_files ( file_name , file_backup ) return field_types","title":"create_default_types_schema"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.create_payload","text":"Create a JSON payload file with embedded metadata describing the schema version. Parameters: payload ( dict ) \u2013 Dictionary of values to serialize to JSON. file_name ( Path ) \u2013 Path to output the payload JSON file. schema_version ( str ) \u2013 Identifier for the schema the payload conforms to. metadata ( dict , default: None ) \u2013 Additional metadata to embed. If None, a new dict is created. Returns: None \u2013 None Examples: Input : {\"id_case\": \"A42\"}, Path(\"payload.json\"), \"v01_00\" Output: JSON file with metadata saved to payload.json Notes Adds 'schema_version' and a default payload description to metadata. Uses json_save_with_meta() to embed metadata into the JSON file. Logs all key actions and file paths for diagnostics. Source code in arb\\utils\\excel\\xl_create.py 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 def create_payload ( payload : dict , file_name : Path , schema_version : str , metadata : dict | None = None ) -> None : \"\"\" Create a JSON payload file with embedded metadata describing the schema version. Args: payload (dict): Dictionary of values to serialize to JSON. file_name (Path): Path to output the payload JSON file. schema_version (str): Identifier for the schema the payload conforms to. metadata (dict, optional): Additional metadata to embed. If None, a new dict is created. Returns: None Examples: Input : {\"id_case\": \"A42\"}, Path(\"payload.json\"), \"v01_00\" Output: JSON file with metadata saved to payload.json Notes: - Adds 'schema_version' and a default payload description to metadata. - Uses `json_save_with_meta()` to embed metadata into the JSON file. - Logs all key actions and file paths for diagnostics. \"\"\" logger . debug ( f \"create_payload() called\" ) if metadata is None : metadata = {} metadata [ \"schema_version\" ] = schema_version metadata [ \"payload description\" ] = \"Test of Excel jinja templating system\" logger . debug ( f \"Writing payload to { file_name } with metadata: { metadata } \" ) json_save_with_meta ( file_name , data = payload , metadata = metadata )","title":"create_payload"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.create_payloads","text":"Generate and save example payload files for each supported sector (landfill, oil and gas, energy). Returns: None \u2013 None Examples: Input : None Output: Sector payloads written to xl_payloads/*.json Notes Each payload is saved to xl_payloads/{schema_version}_payload_01.json . If a backup file exists, the new payload is compared against it for consistency. Uses create_payload() to handle serialization and metadata embedding. Source code in arb\\utils\\excel\\xl_create.py 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 def create_payloads () -> None : \"\"\" Generate and save example payload files for each supported sector (landfill, oil and gas, energy). Returns: None Examples: Input : None Output: Sector payloads written to xl_payloads/*.json Notes: - Each payload is saved to `xl_payloads/{schema_version}_payload_01.json`. - If a backup file exists, the new payload is compared against it for consistency. - Uses `create_payload()` to handle serialization and metadata embedding. \"\"\" logger . debug ( f \"create_payloads() called\" ) import arb.utils.excel.xl_hardcoded as xl_hardcoded for template in EXCEL_TEMPLATES : schema_version = template [ \"schema_version\" ] payload_name = template [ \"payload_name\" ] # Dynamically get the payload object from xl_hardcoded payload = getattr ( xl_hardcoded , payload_name ) file_name = PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _payload_01.json\" file_backup = PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _payload_01_backup.json\" create_payload ( payload , file_name , schema_version ) if file_name . is_file () and file_backup . is_file (): compare_json_files ( file_name , file_backup )","title":"create_payloads"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.create_schemas_and_payloads","text":"Generate all schema, payload, and Excel artifacts for the feedback system. This orchestration function performs the full pipeline Creates the default value types schema. Processes all sector-specific schema files (landfill, oil and gas, energy). Writes default payloads for each schema. Generates test payloads and renders Excel templates. Returns: None \u2013 None Examples: Input : None Output: All schema/payload/template artifacts generated under processed_versions/ Notes Create all required directories under processed_versions . Intended for one-time use during development or deployment setup. Logs each operation and file path for debugging. Source code in arb\\utils\\excel\\xl_create.py 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 def create_schemas_and_payloads () -> None : \"\"\" Generate all schema, payload, and Excel artifacts for the feedback system. This orchestration function performs the full pipeline: - Creates the default value types schema. - Processes all sector-specific schema files (landfill, oil and gas, energy). - Writes default payloads for each schema. - Generates test payloads and renders Excel templates. Returns: None Examples: Input : None Output: All schema/payload/template artifacts generated under processed_versions/ Notes: - Create all required directories under `processed_versions`. - Intended for one-time use during development or deployment setup. - Logs each operation and file path for debugging. \"\"\" logger . debug ( f \"create_schemas_and_payloads() called\" ) ensure_dir_exists ( PROCESSED_VERSIONS / \"xl_schemas\" ) ensure_dir_exists ( PROCESSED_VERSIONS / \"xl_workbooks\" ) ensure_dir_exists ( PROCESSED_VERSIONS / \"xl_payloads\" ) create_default_types_schema ( diagnostics = True ) prep_xl_templates () create_payloads () test_update_xlsx_payloads_01 ()","title":"create_schemas_and_payloads"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.prep_xl_templates","text":"Prepare processed Excel templates and payloads for landfill, oil and gas, and energy sectors. This function Copies original schema and Excel files to the processed directory. Converts VBA-generated schema files by injecting type info. Writes upgraded schema and default payload JSON files. Produces Jinja-compatible Excel workbook versions for templating. Returns: None \u2013 None Notes File paths are derived from structured configs for each sector. Overwrites files in the output directory if they already exist. Output directories are created if they don't exist. Source code in arb\\utils\\excel\\xl_create.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 def prep_xl_templates () -> None : \"\"\" Prepare processed Excel templates and payloads for landfill, oil and gas, and energy sectors. This function: - Copies original schema and Excel files to the processed directory. - Converts VBA-generated schema files by injecting type info. - Writes upgraded schema and default payload JSON files. - Produces Jinja-compatible Excel workbook versions for templating. Returns: None Notes: - File paths are derived from structured configs for each sector. - Overwrites files in the output directory if they already exist. - Output directories are created if they don't exist. \"\"\" logger . debug ( f \"prep_xl_templates() called for all templates in TEMPLATES\" ) file_specs = [] input_dir = PROJECT_ROOT / \"feedback_forms/current_versions\" output_dir = PROJECT_ROOT / \"feedback_forms/processed_versions\" ensure_dir_exists ( output_dir / \"xl_schemas\" ) ensure_dir_exists ( output_dir / \"xl_workbooks\" ) ensure_dir_exists ( output_dir / \"xl_payloads\" ) for template in EXCEL_TEMPLATES : schema_version = template [ \"schema_version\" ] prefix = template [ \"prefix\" ] version = template [ \"version\" ] spec = { \"schema_version\" : schema_version , \"input_schema_vba_path\" : input_dir / f \" { schema_version } _vba.json\" , \"input_xl_path\" : input_dir / f \" { prefix } _ { version } .xlsx\" , \"input_xl_jinja_path\" : input_dir / f \" { prefix } _ { version } _jinja_.xlsx\" , \"output_schema_vba_path\" : output_dir / \"xl_schemas\" / f \" { schema_version } _vba.json\" , \"output_schema_path\" : output_dir / \"xl_schemas\" / f \" { schema_version } .json\" , \"output_xl_path\" : output_dir / \"xl_workbooks\" / f \" { prefix } _ { version } .xlsx\" , \"output_xl_jinja_path\" : output_dir / \"xl_workbooks\" / f \" { prefix } _ { version } _jinja_.xlsx\" , \"output_payload_path\" : output_dir / \"xl_payloads\" / f \" { schema_version } _defaults.json\" , } file_specs . append ( spec ) for spec in file_specs : logger . debug ( f \"Processing schema_version { spec [ 'schema_version' ] } \" ) file_map = [ ( spec [ \"input_schema_vba_path\" ], spec [ \"output_schema_vba_path\" ]), ( spec [ \"input_xl_path\" ], spec [ \"output_xl_path\" ]), ( spec [ \"input_xl_jinja_path\" ], spec [ \"output_xl_jinja_path\" ]), ] for file_old , file_new in file_map : logger . debug ( f \"Copying file from: { file_old } to: { file_new } \" ) shutil . copy ( file_old , file_new ) update_vba_schema ( spec [ \"schema_version\" ], file_name_in = spec [ \"output_schema_vba_path\" ], file_name_out = spec [ \"output_schema_path\" ]) schema_to_default_json ( file_name_in = spec [ \"output_schema_path\" ], file_name_out = spec [ \"output_payload_path\" ])","title":"prep_xl_templates"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.run_diagnostics","text":"Execute a full suite of diagnostic routines to verify Excel templating functionality. This includes Creating default value types schema. Generating upgraded schema files and default payloads. Running test payload injection for Jinja-enabled Excel files. Returns: None \u2013 None Notes Logs each step of the process to the application logger. Catches and logs any exceptions that occur during testing. Intended for developers to verify schema and workbook generation end-to-end. Source code in arb\\utils\\excel\\xl_create.py 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 def run_diagnostics () -> None : \"\"\" Execute a full suite of diagnostic routines to verify Excel templating functionality. This includes: - Creating default value types schema. - Generating upgraded schema files and default payloads. - Running test payload injection for Jinja-enabled Excel files. Returns: None Notes: - Logs each step of the process to the application logger. - Catches and logs any exceptions that occur during testing. - Intended for developers to verify schema and workbook generation end-to-end. \"\"\" logger . info ( f \"Running diagnostics...\" ) try : logger . info ( f \"Step 1: Creating default type schema\" ) create_default_types_schema ( diagnostics = True ) logger . info ( f \"Step 2: Creating and verifying schema files and payloads\" ) prep_xl_templates () create_payloads () logger . info ( f \"Step 3: Performing test Excel generation\" ) test_update_xlsx_payloads_01 () logger . info ( f \"Diagnostics complete. Check output directory and logs for details.\" ) except Exception as e : logger . exception ( f \"Diagnostics failed: { e } \" )","title":"run_diagnostics"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.schema_to_default_dict","text":"Generate default values and metadata from an Excel schema JSON file. Parameters: schema_file_name ( Path ) \u2013 Path to the schema JSON file. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: - defaults: Dictionary mapping variable names to default values. * Drop-downs get \"Please Select\". * All other fields get an empty string. - metadata: Metadata dictionary from the schema file. Examples: Input : Path(\"xl_schemas/landfill_v01_00.json\") Output: (defaults dictionary, metadata dictionary) Notes The schema must include an \"is_drop_down\" flag for correct default generation. Useful for pre-populating forms with valid placeholder values. Source code in arb\\utils\\excel\\xl_create.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 def schema_to_default_dict ( schema_file_name : Path ) -> tuple [ dict , dict ]: \"\"\" Generate default values and metadata from an Excel schema JSON file. Args: schema_file_name (Path): Path to the schema JSON file. Returns: tuple[dict, dict]: - defaults: Dictionary mapping variable names to default values. * Drop-downs get \"Please Select\". * All other fields get an empty string. - metadata: Metadata dictionary from the schema file. Examples: Input : Path(\"xl_schemas/landfill_v01_00.json\") Output: (defaults dictionary, metadata dictionary) Notes: - The schema must include an \"is_drop_down\" flag for correct default generation. - Useful for pre-populating forms with valid placeholder values. \"\"\" logger . debug ( f \"schema_to_default_dict() called for { schema_file_name =} \" ) data , metadata = json_load_with_meta ( schema_file_name ) logger . debug ( f \" { metadata =} \" ) defaults = { variable : PLEASE_SELECT if sub_schema . get ( \"is_drop_down\" ) else \"\" for variable , sub_schema in data . items () } return defaults , metadata","title":"schema_to_default_dict"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.schema_to_default_json","text":"Save default values extracted from a schema into a JSON file with metadata. Parameters: file_name_in ( Path ) \u2013 Input path to the schema JSON file. file_name_out ( Path , default: None ) \u2013 Output path for the default JSON. If None, defaults to 'xl_payloads/{schema_version}_defaults.json'. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: - defaults: Dictionary of default values derived from the schema. - metadata: Metadata dictionary included in the output JSON. Examples: Input : Path(\"xl_schemas/landfill_v01_00.json\") Output: Tuple of (defaults dict, metadata dict), written to output path Notes Drop-down fields default to \"Please Select\". Other fields default to an empty string. Adds a note to metadata explaining default value behavior. Ensures output directory exists before writing. Source code in arb\\utils\\excel\\xl_create.py 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def schema_to_default_json ( file_name_in : Path , file_name_out : Path | None = None ) -> tuple [ dict , dict ]: \"\"\" Save default values extracted from a schema into a JSON file with metadata. Args: file_name_in (Path): Input path to the schema JSON file. file_name_out (Path, optional): Output path for the default JSON. If None, defaults to 'xl_payloads/{schema_version}_defaults.json'. Returns: tuple[dict, dict]: - defaults: Dictionary of default values derived from the schema. - metadata: Metadata dictionary included in the output JSON. Examples: Input : Path(\"xl_schemas/landfill_v01_00.json\") Output: Tuple of (defaults dict, metadata dict), written to output path Notes: - Drop-down fields default to \"Please Select\". - Other fields default to an empty string. - Adds a note to metadata explaining default value behavior. - Ensures output directory exists before writing. \"\"\" logger . debug ( f \"schema_to_default_json() called for { file_name_in =} \" ) defaults , metadata = schema_to_default_dict ( file_name_in ) metadata [ 'notes' ] = ( \"Default values are empty strings unless the field is a drop-down cell. \" \"For drop-down cells, the default is 'Please Select'.\" ) if file_name_out is None : file_name_out = f \"xl_payloads/ { metadata [ 'schema_version' ] } _defaults.json\" ensure_parent_dirs ( file_name_out ) json_save_with_meta ( file_name_out , data = defaults , metadata = metadata , json_options = None ) return defaults , metadata","title":"schema_to_default_json"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.schema_to_json_file","text":"Save an Excel schema to a JSON file with metadata and validate the round-trip. Parameters: data ( dict ) \u2013 The Excel schema to be serialized and written to disk. schema_version ( str ) \u2013 Schema version identifier to include in metadata. file_name ( str , default: None ) \u2013 Output file path. Default to \"xl_schemas/{schema_version}.json\". Returns: None \u2013 None Examples: Input : my_schema, schema_version=\"v01_00\" Output: JSON file written to \"xl_schemas/v01_00.json\" (default path) Notes If file_name is not provided, the output will be saved to \"xl_schemas/{schema_version}.json\". Metadata will include the schema version. Performs a round-trip serialization test to verify that the saved data and metadata match the originals. Source code in arb\\utils\\excel\\xl_create.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def schema_to_json_file ( data : dict , schema_version : str , file_name : str | None = None ) -> None : \"\"\" Save an Excel schema to a JSON file with metadata and validate the round-trip. Args: data (dict): The Excel schema to be serialized and written to disk. schema_version (str): Schema version identifier to include in metadata. file_name (str, optional): Output file path. Default to \"xl_schemas/{schema_version}.json\". Returns: None Examples: Input : my_schema, schema_version=\"v01_00\" Output: JSON file written to \"xl_schemas/v01_00.json\" (default path) Notes: - If `file_name` is not provided, the output will be saved to \"xl_schemas/{schema_version}.json\". - Metadata will include the schema version. - Performs a round-trip serialization test to verify that the saved data and metadata match the originals. \"\"\" logger . debug ( f \"schema_to_json_file() called with { schema_version =} , { file_name =} \" ) if file_name is None : file_name = f \"xl_schemas/ { schema_version } .json\" ensure_parent_dirs ( file_name ) metadata = { 'schema_version' : schema_version } logger . debug ( f \"Saving schema to: { file_name } with metadata: { metadata } \" ) json_save_with_meta ( file_name , data = data , metadata = metadata , json_options = None ) # Verify round-trip serialization read_data , read_metadata = json_load_with_meta ( file_name ) if read_data == data and read_metadata == metadata : logger . debug ( f \"SUCCESS: JSON serialization round-trip matches original.\" ) else : logger . warning ( f \"FAILURE: Mismatch in JSON serialization round-trip.\" )","title":"schema_to_json_file"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.sort_xl_schema","text":"Sort an Excel schema and its sub-schema dictionaries for easier comparison. This function modifies sub-schemas in place and returns a new dictionary with the top-level keys sorted according to the selected strategy. Sub-schemas are reordered so that keys appear in the order: \"label\", \"label_address\", \"value_address\", \"value_type\", then others. Parameters: xl_schema ( dict ) \u2013 Dictionary where keys are variable names and values are sub-schema dicts. sort_by ( str , default: 'variable_name' ) \u2013 Sorting strategy: - \"variable_name\": Sort top-level keys alphabetically (default). - \"label_address\": Sort based on Excel row order of each sub-schema's 'label_address'. Returns: dict ( dict ) \u2013 A new dictionary with reordered sub-schemas and sorted top-level keys. Raises: ValueError \u2013 If an unrecognized sorting strategy is provided. Examples: Input : schema, sort_by=\"label_address\" Output: New dictionary with sorted keys and reordered sub-schemas Source code in arb\\utils\\excel\\xl_create.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def sort_xl_schema ( xl_schema : dict , sort_by : str = \"variable_name\" ) -> dict : \"\"\" Sort an Excel schema and its sub-schema dictionaries for easier comparison. This function modifies sub-schemas in place and returns a new dictionary with the top-level keys sorted according to the selected strategy. Sub-schemas are reordered so that keys appear in the order: \"label\", \"label_address\", \"value_address\", \"value_type\", then others. Args: xl_schema (dict): Dictionary where keys are variable names and values are sub-schema dicts. sort_by (str): Sorting strategy: - \"variable_name\": Sort top-level keys alphabetically (default). - \"label_address\": Sort based on Excel row order of each sub-schema's 'label_address'. Returns: dict: A new dictionary with reordered sub-schemas and sorted top-level keys. Raises: ValueError: If an unrecognized sorting strategy is provided. Examples: Input : schema, sort_by=\"label_address\" Output: New dictionary with sorted keys and reordered sub-schemas \"\"\" logger . debug ( f \"sort_xl_schema() called\" ) # Reorder each sub-schema dict for variable_name , sub_schema in xl_schema . items (): reordered = {} for key in ( \"label\" , \"label_address\" , \"value_address\" , \"value_type\" ): if key in sub_schema : reordered [ key ] = sub_schema . pop ( key ) reordered . update ( sub_schema ) xl_schema [ variable_name ] = reordered # Sort top-level dictionary if sort_by == \"variable_name\" : logger . debug ( f \"Sorting schema by variable_name\" ) sorted_items = dict ( sorted ( xl_schema . items (), key = lambda item : item [ 0 ])) elif sort_by == \"label_address\" : logger . debug ( f \"Sorting schema by label_address\" ) get_xl_row = partial ( xl_address_sort , address_location = \"value\" , sort_by = \"row\" , sub_keys = \"label_address\" ) sorted_items = dict ( sorted ( xl_schema . items (), key = get_xl_row )) else : raise ValueError ( \"sort_by must be 'variable_name' or 'label_address'\" ) return sorted_items","title":"sort_xl_schema"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.test_update_xlsx_payloads_01","text":"Run test cases that populate Jinja-templated Excel files with known payloads. This test routine helps validate that Excel generation is functioning correctly for all supported sectors (landfill, oil and gas, energy). Returns: None \u2013 None Examples: Input : None Output: Diagnostic Excel files created in xl_workbooks/ Notes Writes populated Excel files to the xl_workbooks directory. Uses both file-based and inline payloads. Intended for development and diagnostic use, not production. Source code in arb\\utils\\excel\\xl_create.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def test_update_xlsx_payloads_01 () -> None : \"\"\" Run test cases that populate Jinja-templated Excel files with known payloads. This test routine helps validate that Excel generation is functioning correctly for all supported sectors (landfill, oil and gas, energy). Returns: None Examples: Input : None Output: Diagnostic Excel files created in xl_workbooks/ Notes: - Writes populated Excel files to the `xl_workbooks` directory. - Uses both file-based and inline payloads. - Intended for development and diagnostic use, not production. \"\"\" logger . debug ( f \"test_update_xlsx_payloads_01() called\" ) for template in EXCEL_TEMPLATES : schema_version = template [ \"schema_version\" ] prefix = template [ \"prefix\" ] version = template [ \"version\" ] # Test with two payloads from file (defaults + payload_01) update_xlsx_payloads ( PROCESSED_VERSIONS / f \"xl_workbooks/ { prefix } _ { version } _jinja_.xlsx\" , PROCESSED_VERSIONS / f \"xl_workbooks/ { prefix } _ { version } _populated_01.xlsx\" , [ PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _defaults.json\" , PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _payload_01.json\" , ] ) # Test with one file payload and one inline dict update_xlsx_payloads ( PROCESSED_VERSIONS / f \"xl_workbooks/ { prefix } _ { version } _jinja_.xlsx\" , PROCESSED_VERSIONS / f \"xl_workbooks/ { prefix } _ { version } _populated_02.xlsx\" , [ PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _payload_01.json\" , { \"id_incidence\" : \"123456\" }, ] )","title":"test_update_xlsx_payloads_01"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.update_vba_schema","text":"Update a VBA-generated Excel schema with value_type info and re-sort it. Parameters: schema_version ( str ) \u2013 Identifier for the schema version. file_name_in ( Path , default: None ) \u2013 Path to the raw VBA schema JSON file. Default to \"processed_versions/xl_schemas/{schema_version}_vba.json\". file_name_out ( Path , default: None ) \u2013 Path to output the upgraded schema JSON. Default to \"processed_versions/xl_schemas/{schema_version}.json\". file_name_default_value_types ( Path , default: None ) \u2013 Path to JSON file defining default value types. Defaults to \"processed_versions/xl_schemas/default_value_types_v01_00.json\". Returns: dict ( dict ) \u2013 The updated and sorted schema dictionary. Examples: Input : \"landfill_v01_00\" Output: Sorted schema dictionary with value types injected Notes This function ensures that all schema entries include a 'value_type'. Applies sort_xl_schema() with sort_by=\"label_address\". Use schema_to_json_file() to write the result to disk. Source code in arb\\utils\\excel\\xl_create.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def update_vba_schema ( schema_version : str , file_name_in : Path | None = None , file_name_out : Path | None = None , file_name_default_value_types : Path | None = None ) -> dict : \"\"\" Update a VBA-generated Excel schema with value_type info and re-sort it. Args: schema_version (str): Identifier for the schema version. file_name_in (Path, optional): Path to the raw VBA schema JSON file. Default to \"processed_versions/xl_schemas/{schema_version}_vba.json\". file_name_out (Path, optional): Path to output the upgraded schema JSON. Default to \"processed_versions/xl_schemas/{schema_version}.json\". file_name_default_value_types (Path, optional): Path to JSON file defining default value types. Defaults to \"processed_versions/xl_schemas/default_value_types_v01_00.json\". Returns: dict: The updated and sorted schema dictionary. Examples: Input : \"landfill_v01_00\" Output: Sorted schema dictionary with value types injected Notes: - This function ensures that all schema entries include a 'value_type'. - Applies `sort_xl_schema()` with sort_by=\"label_address\". - Use `schema_to_json_file()` to write the result to disk. \"\"\" logger . debug ( f \"update_vba_schema() called with { schema_version =} , { file_name_in =} , \" f \" { file_name_out =} , { file_name_default_value_types =} \" ) if file_name_in is None : file_name_in = PROCESSED_VERSIONS / \"xl_schemas\" / f \" { schema_version } _vba.json\" if file_name_out is None : file_name_out = PROCESSED_VERSIONS / \"xl_schemas\" / f \" { schema_version } .json\" if file_name_default_value_types is None : file_name_default_value_types = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00.json\" ensure_parent_dirs ( file_name_in ) ensure_parent_dirs ( file_name_out ) ensure_parent_dirs ( file_name_default_value_types ) default_value_types , _ = json_load_with_meta ( file_name_default_value_types ) schema = json_load ( file_name_in , json_options = None ) ensure_key_value_pair ( schema , default_value_types , \"value_type\" ) schema = sort_xl_schema ( schema , sort_by = \"label_address\" ) schema_to_json_file ( schema , schema_version , file_name = file_name_out ) return schema","title":"update_vba_schema"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.update_vba_schemas","text":"Batch update of known VBA-generated schemas using update_vba_schema() . This function applies schema upgrades to all templates defined in TEMPLATES. Returns: None \u2013 None Notes Calls update_vba_schema() for each template in TEMPLATES. Output schemas are written to the processed_versions/xl_schemas directory. Source code in arb\\utils\\excel\\xl_create.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def update_vba_schemas () -> None : \"\"\" Batch update of known VBA-generated schemas using `update_vba_schema()`. This function applies schema upgrades to all templates defined in TEMPLATES. Returns: None Notes: - Calls `update_vba_schema()` for each template in TEMPLATES. - Output schemas are written to the processed_versions/xl_schemas directory. \"\"\" logger . debug ( f \"update_vba_schemas() called\" ) for template in EXCEL_TEMPLATES : update_vba_schema ( template [ \"schema_version\" ])","title":"update_vba_schemas"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.update_xlsx","text":"Render a Jinja-templated Excel (.xlsx) file by replacing placeholders with dictionary values. Parameters: file_in ( Path ) \u2013 Path to the input Excel file containing Jinja placeholders. file_out ( Path ) \u2013 Path where the rendered Excel file will be saved. jinja_dict ( dict ) \u2013 Dictionary mapping Jinja template variables to replacement values. Returns: None \u2013 None Examples: Input : template.xlsx, output.xlsx, {\"site_name\": \"Landfill A\"} Output: output.xlsx with rendered values Notes Only modifies 'xl/sharedStrings.xml' within the XLSX zip archive. All other file contents are passed through unchanged. Useful for populating pre-tagged Excel templates with form data. Source code in arb\\utils\\excel\\xl_create.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 def update_xlsx ( file_in : Path , file_out : Path , jinja_dict : dict ) -> None : \"\"\" Render a Jinja-templated Excel (.xlsx) file by replacing placeholders with dictionary values. Args: file_in (Path): Path to the input Excel file containing Jinja placeholders. file_out (Path): Path where the rendered Excel file will be saved. jinja_dict (dict): Dictionary mapping Jinja template variables to replacement values. Returns: None Examples: Input : template.xlsx, output.xlsx, {\"site_name\": \"Landfill A\"} Output: output.xlsx with rendered values Notes: - Only modifies 'xl/sharedStrings.xml' within the XLSX zip archive. - All other file contents are passed through unchanged. - Useful for populating pre-tagged Excel templates with form data. \"\"\" logger . debug ( f \"Rendering Excel from { file_in } to { file_out } using { jinja_dict } \" ) with zipfile . ZipFile ( file_in , 'r' ) as xlsx , zipfile . ZipFile ( file_out , 'w' ) as new_xlsx : for filename in xlsx . namelist (): with xlsx . open ( filename ) as file : contents = file . read () if filename == 'xl/sharedStrings.xml' : contents = jinja2 . Template ( contents . decode ( 'utf-8' )) . render ( jinja_dict ) . encode ( 'utf-8' ) new_xlsx . writestr ( filename , contents )","title":"update_xlsx"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.update_xlsx_payloads","text":"Apply multiple payloads to a Jinja-templated Excel file and render the result. Parameters: file_in ( Path ) \u2013 Path to the input Excel file containing Jinja placeholders. file_out ( Path ) \u2013 Path where the populated Excel file will be written. payloads ( list | tuple ) \u2013 Sequence of dictionaries or JSON file paths. - Payloads are merged in order, with later values overriding earlier ones. Returns: None \u2013 None Notes Each payload may be a dictionary or a Path to a JSON file with metadata. Designed to support tiered rendering: default payload + override. Uses json_load_with_meta() for file payloads. Passes final merged dictionary to update_xlsx() . Source code in arb\\utils\\excel\\xl_create.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 def update_xlsx_payloads ( file_in : Path , file_out : Path , payloads : list | tuple ) -> None : \"\"\" Apply multiple payloads to a Jinja-templated Excel file and render the result. Args: file_in (Path): Path to the input Excel file containing Jinja placeholders. file_out (Path): Path where the populated Excel file will be written. payloads (list | tuple): Sequence of dictionaries or JSON file paths. - Payloads are merged in order, with later values overriding earlier ones. Returns: None Notes: - Each payload may be a dictionary or a Path to a JSON file with metadata. - Designed to support tiered rendering: default payload + override. - Uses `json_load_with_meta()` for file payloads. - Passes final merged dictionary to `update_xlsx()`. \"\"\" logger . debug ( f \"update_xlsx_payloads() called with: { file_in =} , { file_out =} , { payloads =} \" ) new_dict = {} for payload in payloads : if isinstance ( payload , dict ): data = payload else : data , _ = json_load_with_meta ( payload ) new_dict . update ( data ) update_xlsx ( file_in , file_out , new_dict )","title":"update_xlsx_payloads"},{"location":"reference/arb/utils/excel/xl_file_structure/","text":"arb.utils.excel.xl_file_structure Module to determine the path to the root of the feedback_portal project in a platform-independent way. This module can be invoked from multiple runtime contexts, including: - The utils.excel directory (e.g., for standalone Excel/VBA payload generation). - The portal Flask app directory. Directory structure reference: /feedback_portal/ <-- Base of project directory tree \u251c\u2500\u2500 feedback_forms/ \u251c\u2500\u2500 current_versions/ <-- Current versions of feedback forms (created in Excel/VBA) \u2514\u2500\u2500 processed_versions/ <-- Updated versions created in Python \u251c\u2500\u2500 xl_payloads/ \u251c\u2500\u2500 xl_schemas/ \u2514\u2500\u2500 xl_workbooks/ \u251c\u2500\u2500 source/ \u2514\u2500\u2500 production/ \u2514\u2500\u2500 arb/ \u251c\u2500\u2500 portal/ <-- Flask app \u2514\u2500\u2500 utils/ \u2514\u2500\u2500 excel/ <-- Excel generation scripts Attributes: PROJECT_ROOT ( Path ) \u2013 Resolved root directory of the project. FEEDBACK_FORMS ( Path ) \u2013 Path to the 'feedback_forms' directory. CURRENT_VERSIONS ( Path ) \u2013 Path to Excel files from current official versions. PROCESSED_VERSIONS ( Path ) \u2013 Path to output files generated via Python processing.","title":"arb.utils.excel.xl_file_structure"},{"location":"reference/arb/utils/excel/xl_file_structure/#arbutilsexcelxl_file_structure","text":"Module to determine the path to the root of the feedback_portal project in a platform-independent way. This module can be invoked from multiple runtime contexts, including: - The utils.excel directory (e.g., for standalone Excel/VBA payload generation). - The portal Flask app directory. Directory structure reference: /feedback_portal/ <-- Base of project directory tree \u251c\u2500\u2500 feedback_forms/ \u251c\u2500\u2500 current_versions/ <-- Current versions of feedback forms (created in Excel/VBA) \u2514\u2500\u2500 processed_versions/ <-- Updated versions created in Python \u251c\u2500\u2500 xl_payloads/ \u251c\u2500\u2500 xl_schemas/ \u2514\u2500\u2500 xl_workbooks/ \u251c\u2500\u2500 source/ \u2514\u2500\u2500 production/ \u2514\u2500\u2500 arb/ \u251c\u2500\u2500 portal/ <-- Flask app \u2514\u2500\u2500 utils/ \u2514\u2500\u2500 excel/ <-- Excel generation scripts Attributes: PROJECT_ROOT ( Path ) \u2013 Resolved root directory of the project. FEEDBACK_FORMS ( Path ) \u2013 Path to the 'feedback_forms' directory. CURRENT_VERSIONS ( Path ) \u2013 Path to Excel files from current official versions. PROCESSED_VERSIONS ( Path ) \u2013 Path to output files generated via Python processing.","title":"arb.utils.excel.xl_file_structure"},{"location":"reference/arb/utils/excel/xl_hardcoded/","text":"arb.utils.excel.xl_hardcoded Hardcoded schema definitions and sample payloads for Excel template processing. The new versioning system uses the naming scheme vxx_yy, where xx represents a major version, and yy represents a minor version (without the 'old' in the prefix). These were manually created by inspecting old_v01 and old_v02 versions of now outdated Excel spreadsheets. Contents default_value_types_v01_00 : field types for v01_00 based on old_v01 and old_v02 schemas Sample payloads for oil & gas and landfill forms jinja_names_set : manually compiled field names used in Jinja templates Diagnostic comparison for field coverage (see __main__ )","title":"arb.utils.excel.xl_hardcoded"},{"location":"reference/arb/utils/excel/xl_hardcoded/#arbutilsexcelxl_hardcoded","text":"Hardcoded schema definitions and sample payloads for Excel template processing. The new versioning system uses the naming scheme vxx_yy, where xx represents a major version, and yy represents a minor version (without the 'old' in the prefix). These were manually created by inspecting old_v01 and old_v02 versions of now outdated Excel spreadsheets. Contents default_value_types_v01_00 : field types for v01_00 based on old_v01 and old_v02 schemas Sample payloads for oil & gas and landfill forms jinja_names_set : manually compiled field names used in Jinja templates Diagnostic comparison for field coverage (see __main__ )","title":"arb.utils.excel.xl_hardcoded"},{"location":"reference/arb/utils/excel/xl_misc/","text":"arb.utils.excel.xl_misc Excel address parsing and sorting utilities. This module provides helper functions for interpreting Excel-style address strings (such as \"$A$1\") and using them for sorting data structures. These utilities are used during schema generation, payload creation, and Excel form manipulation. Functions: Name Description - get_excel_row_column Parses an Excel address into column and row components. - xl_address_sort Extracts a sortable row or column value from an Excel address. - run_diagnostics Test harness for verifying address parsing and sorting behavior. Typical Use Case These functions are primarily invoked when organizing Excel schema dictionaries by their physical layout in the worksheet, either by row or column position. Notes Assumes absolute Excel address formatting (e.g., \"$A$1\"). Designed to be used by other modules like xl_create and xl_file_structure. get_excel_row_column ( xl_address ) Extract the Excel column letters and row number from an address string. Excel absolute references take the form \"$A$1\" or \"$BB$12\", where both the column and row are prefixed with dollar signs. Parameters: xl_address ( str ) \u2013 The Excel address to parse (must be in absolute format like \"$A$1\"). Returns: tuple [ str , int ] \u2013 tuple[str, int]: A tuple of (column letters, row number). Raises: ValueError \u2013 If the format is invalid (e.g., not exactly two-dollar signs, or row not an integer). Examples: Input : \"$Z$9\" Output: ('Z', 9) Input : \"$AA$105\" Output: ('AA', 105) Source code in arb\\utils\\excel\\xl_misc.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_excel_row_column ( xl_address : str ) -> tuple [ str , int ]: \"\"\" Extract the Excel column letters and row number from an address string. Excel absolute references take the form \"$A$1\" or \"$BB$12\", where both the column and row are prefixed with dollar signs. Args: xl_address (str): The Excel address to parse (must be in absolute format like \"$A$1\"). Returns: tuple[str, int]: A tuple of (column letters, row number). Raises: ValueError: If the format is invalid (e.g., not exactly two-dollar signs, or row not an integer). Examples: Input : \"$Z$9\" Output: ('Z', 9) Input : \"$AA$105\" Output: ('AA', 105) \"\"\" if xl_address . count ( '$' ) != 2 : raise ValueError ( f \"Excel address must contain exactly two '$' characters: { xl_address } \" ) first_dollar = xl_address . find ( '$' ) last_dollar = xl_address . rfind ( '$' ) column = xl_address [ first_dollar + 1 : last_dollar ] try : row = int ( xl_address [ last_dollar + 1 :]) except ValueError as e : raise ValueError ( f \"Could not parse row number from Excel address: { xl_address } \" ) from e return column , row run_diagnostics () Run demonstration tests for get_excel_row_column() and xl_address_sort(). This function is only called if this module is run directly. Source code in arb\\utils\\excel\\xl_misc.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def run_diagnostics () -> None : \"\"\" Run demonstration tests for get_excel_row_column() and xl_address_sort(). This function is only called if this module is run directly. \"\"\" # pp = pprint.PrettyPrinter(indent=4, sort_dicts=False) print ( \"=== Testing get_excel_row_column ===\" ) valid_addresses = [ \"$C$42\" , \"$AA$99\" ] for addr in valid_addresses : try : result = get_excel_row_column ( addr ) print ( f \" Address: { addr } => { result } \" ) except Exception as e : print ( f \" ERROR for { addr } : { e } \" ) print ( \" \\n === Testing get_excel_row_column (invalid formats) ===\" ) invalid_addresses = [ \"A$1\" , \"$A1\" , \"$A$1$\" , \"$AB$\" , \"$AB$XYZ\" ] for addr in invalid_addresses : try : result = get_excel_row_column ( addr ) print ( f \" UNEXPECTED SUCCESS: { addr } => { result } \" ) except Exception as e : print ( f \" Expected failure for { addr } : { e } \" ) print ( \" \\n === Testing xl_address_sort ===\" ) test_tuple = ( \"$B$10\" , \"Example\" ) print ( f \" Tuple: { test_tuple } => Row: { xl_address_sort ( test_tuple , 'key' , 'row' ) } \" ) nested = ( \"key\" , { \"nested\" : { \"cell\" : \"$D$20\" }}) print ( f \" Tuple: { nested } => Row: { xl_address_sort ( nested , 'value' , 'row' , sub_keys = [ 'nested' , 'cell' ]) } \" ) xl_address_sort ( xl_tuple , address_location = 'key' , sort_by = 'row' , sub_keys = None ) Extract the Excel row or column value from a tuple of key-value pairs for sorting. This is used when sorting collections of data where either the key or the value contains an Excel-style address string. Supports sorting by either row or column. Parameters: xl_tuple ( tuple ) \u2013 A (key, value) tuple where one element contains a string like \"$A$1\". address_location ( str , default: 'key' ) \u2013 Which element contains the Excel address (\"key\" or \"value\"). sort_by ( str , default: 'row' ) \u2013 Whether to sort by \"row\" (int) or \"column\" (str). sub_keys ( str | list [ str ] | None , default: None ) \u2013 Key(s) to retrieve nested address if inside a dict. Returns: int | str \u2013 int | str: The row (int) or column (str) extracted from the address. Raises: ValueError \u2013 If address_location or sort_by has an invalid value. Examples: Input : (\"$B$3\", \"data\"), address_location=\"key\", sort_by=\"row\" Output: 3 Input : (\"item\", {\"pos\": \"$C$7\"}), address_location=\"value\", sort_by=\"column\", sub_keys=\"pos\" Output: 'C' Source code in arb\\utils\\excel\\xl_misc.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def xl_address_sort ( xl_tuple : tuple , address_location : str = \"key\" , sort_by : str = \"row\" , sub_keys : str | list [ str ] | None = None ) -> int | str : \"\"\" Extract the Excel row or column value from a tuple of key-value pairs for sorting. This is used when sorting collections of data where either the key or the value contains an Excel-style address string. Supports sorting by either row or column. Args: xl_tuple (tuple): A (key, value) tuple where one element contains a string like \"$A$1\". address_location (str): Which element contains the Excel address (\"key\" or \"value\"). sort_by (str): Whether to sort by \"row\" (int) or \"column\" (str). sub_keys (str | list[str] | None): Key(s) to retrieve nested address if inside a dict. Returns: int | str: The row (int) or column (str) extracted from the address. Raises: ValueError: If `address_location` or `sort_by` has an invalid value. Examples: Input : (\"$B$3\", \"data\"), address_location=\"key\", sort_by=\"row\" Output: 3 Input : (\"item\", {\"pos\": \"$C$7\"}), address_location=\"value\", sort_by=\"column\", sub_keys=\"pos\" Output: 'C' \"\"\" if address_location == \"key\" : address = xl_tuple [ 0 ] elif address_location == \"value\" : if sub_keys is None : address = xl_tuple [ 1 ] else : address = get_nested_value ( xl_tuple [ 1 ], sub_keys ) else : raise ValueError ( \"address_location must be 'key' or 'value'\" ) column , row = get_excel_row_column ( address ) if sort_by == \"row\" : return_value = row elif sort_by == \"column\" : return_value = column else : raise ValueError ( \"sort_by must be 'row' or 'column'\" ) return return_value","title":"arb.utils.excel.xl_misc"},{"location":"reference/arb/utils/excel/xl_misc/#arbutilsexcelxl_misc","text":"Excel address parsing and sorting utilities. This module provides helper functions for interpreting Excel-style address strings (such as \"$A$1\") and using them for sorting data structures. These utilities are used during schema generation, payload creation, and Excel form manipulation. Functions: Name Description - get_excel_row_column Parses an Excel address into column and row components. - xl_address_sort Extracts a sortable row or column value from an Excel address. - run_diagnostics Test harness for verifying address parsing and sorting behavior. Typical Use Case These functions are primarily invoked when organizing Excel schema dictionaries by their physical layout in the worksheet, either by row or column position. Notes Assumes absolute Excel address formatting (e.g., \"$A$1\"). Designed to be used by other modules like xl_create and xl_file_structure.","title":"arb.utils.excel.xl_misc"},{"location":"reference/arb/utils/excel/xl_misc/#arb.utils.excel.xl_misc.get_excel_row_column","text":"Extract the Excel column letters and row number from an address string. Excel absolute references take the form \"$A$1\" or \"$BB$12\", where both the column and row are prefixed with dollar signs. Parameters: xl_address ( str ) \u2013 The Excel address to parse (must be in absolute format like \"$A$1\"). Returns: tuple [ str , int ] \u2013 tuple[str, int]: A tuple of (column letters, row number). Raises: ValueError \u2013 If the format is invalid (e.g., not exactly two-dollar signs, or row not an integer). Examples: Input : \"$Z$9\" Output: ('Z', 9) Input : \"$AA$105\" Output: ('AA', 105) Source code in arb\\utils\\excel\\xl_misc.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def get_excel_row_column ( xl_address : str ) -> tuple [ str , int ]: \"\"\" Extract the Excel column letters and row number from an address string. Excel absolute references take the form \"$A$1\" or \"$BB$12\", where both the column and row are prefixed with dollar signs. Args: xl_address (str): The Excel address to parse (must be in absolute format like \"$A$1\"). Returns: tuple[str, int]: A tuple of (column letters, row number). Raises: ValueError: If the format is invalid (e.g., not exactly two-dollar signs, or row not an integer). Examples: Input : \"$Z$9\" Output: ('Z', 9) Input : \"$AA$105\" Output: ('AA', 105) \"\"\" if xl_address . count ( '$' ) != 2 : raise ValueError ( f \"Excel address must contain exactly two '$' characters: { xl_address } \" ) first_dollar = xl_address . find ( '$' ) last_dollar = xl_address . rfind ( '$' ) column = xl_address [ first_dollar + 1 : last_dollar ] try : row = int ( xl_address [ last_dollar + 1 :]) except ValueError as e : raise ValueError ( f \"Could not parse row number from Excel address: { xl_address } \" ) from e return column , row","title":"get_excel_row_column"},{"location":"reference/arb/utils/excel/xl_misc/#arb.utils.excel.xl_misc.run_diagnostics","text":"Run demonstration tests for get_excel_row_column() and xl_address_sort(). This function is only called if this module is run directly. Source code in arb\\utils\\excel\\xl_misc.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def run_diagnostics () -> None : \"\"\" Run demonstration tests for get_excel_row_column() and xl_address_sort(). This function is only called if this module is run directly. \"\"\" # pp = pprint.PrettyPrinter(indent=4, sort_dicts=False) print ( \"=== Testing get_excel_row_column ===\" ) valid_addresses = [ \"$C$42\" , \"$AA$99\" ] for addr in valid_addresses : try : result = get_excel_row_column ( addr ) print ( f \" Address: { addr } => { result } \" ) except Exception as e : print ( f \" ERROR for { addr } : { e } \" ) print ( \" \\n === Testing get_excel_row_column (invalid formats) ===\" ) invalid_addresses = [ \"A$1\" , \"$A1\" , \"$A$1$\" , \"$AB$\" , \"$AB$XYZ\" ] for addr in invalid_addresses : try : result = get_excel_row_column ( addr ) print ( f \" UNEXPECTED SUCCESS: { addr } => { result } \" ) except Exception as e : print ( f \" Expected failure for { addr } : { e } \" ) print ( \" \\n === Testing xl_address_sort ===\" ) test_tuple = ( \"$B$10\" , \"Example\" ) print ( f \" Tuple: { test_tuple } => Row: { xl_address_sort ( test_tuple , 'key' , 'row' ) } \" ) nested = ( \"key\" , { \"nested\" : { \"cell\" : \"$D$20\" }}) print ( f \" Tuple: { nested } => Row: { xl_address_sort ( nested , 'value' , 'row' , sub_keys = [ 'nested' , 'cell' ]) } \" )","title":"run_diagnostics"},{"location":"reference/arb/utils/excel/xl_misc/#arb.utils.excel.xl_misc.xl_address_sort","text":"Extract the Excel row or column value from a tuple of key-value pairs for sorting. This is used when sorting collections of data where either the key or the value contains an Excel-style address string. Supports sorting by either row or column. Parameters: xl_tuple ( tuple ) \u2013 A (key, value) tuple where one element contains a string like \"$A$1\". address_location ( str , default: 'key' ) \u2013 Which element contains the Excel address (\"key\" or \"value\"). sort_by ( str , default: 'row' ) \u2013 Whether to sort by \"row\" (int) or \"column\" (str). sub_keys ( str | list [ str ] | None , default: None ) \u2013 Key(s) to retrieve nested address if inside a dict. Returns: int | str \u2013 int | str: The row (int) or column (str) extracted from the address. Raises: ValueError \u2013 If address_location or sort_by has an invalid value. Examples: Input : (\"$B$3\", \"data\"), address_location=\"key\", sort_by=\"row\" Output: 3 Input : (\"item\", {\"pos\": \"$C$7\"}), address_location=\"value\", sort_by=\"column\", sub_keys=\"pos\" Output: 'C' Source code in arb\\utils\\excel\\xl_misc.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def xl_address_sort ( xl_tuple : tuple , address_location : str = \"key\" , sort_by : str = \"row\" , sub_keys : str | list [ str ] | None = None ) -> int | str : \"\"\" Extract the Excel row or column value from a tuple of key-value pairs for sorting. This is used when sorting collections of data where either the key or the value contains an Excel-style address string. Supports sorting by either row or column. Args: xl_tuple (tuple): A (key, value) tuple where one element contains a string like \"$A$1\". address_location (str): Which element contains the Excel address (\"key\" or \"value\"). sort_by (str): Whether to sort by \"row\" (int) or \"column\" (str). sub_keys (str | list[str] | None): Key(s) to retrieve nested address if inside a dict. Returns: int | str: The row (int) or column (str) extracted from the address. Raises: ValueError: If `address_location` or `sort_by` has an invalid value. Examples: Input : (\"$B$3\", \"data\"), address_location=\"key\", sort_by=\"row\" Output: 3 Input : (\"item\", {\"pos\": \"$C$7\"}), address_location=\"value\", sort_by=\"column\", sub_keys=\"pos\" Output: 'C' \"\"\" if address_location == \"key\" : address = xl_tuple [ 0 ] elif address_location == \"value\" : if sub_keys is None : address = xl_tuple [ 1 ] else : address = get_nested_value ( xl_tuple [ 1 ], sub_keys ) else : raise ValueError ( \"address_location must be 'key' or 'value'\" ) column , row = get_excel_row_column ( address ) if sort_by == \"row\" : return_value = row elif sort_by == \"column\" : return_value = column else : raise ValueError ( \"sort_by must be 'row' or 'column'\" ) return return_value","title":"xl_address_sort"},{"location":"reference/arb/utils/excel/xl_parse/","text":"arb.utils.excel.xl_parse Module to parse and ingest Excel spreadsheet contents. This module provides logic to convert Excel forms into structured dictionary representations, including extraction of tab contents, metadata, and schema references. It is primarily used to support automated feedback template parsing. Notes schema_file_map is a dict where keys are schema names and values are paths to JSON files. schema_map is a dict where keys are schema names and values are: {\"schema\": schema_dict, \"metadata\": metadata_dict} Example Input : xl_schema_map['oil_and_gas_v03']['schema'] Output: Dictionary representing the oil and gas schema convert_upload_to_json ( file_path ) Convert an uploaded Excel or JSON file into a valid JSON payload file. Parameters: file_path ( Path ) \u2013 Path to the uploaded file. Returns: Path | None \u2013 Path | None: - Path to JSON file (either original or newly created), or - None if file type is unsupported or conversion fails. Behavior If the file has a .json extension: \u2192 Assume it is valid JSON and return as-is. If the file has a .xlsx extension: \u2192 Attempt to parse using Excel schema logic. \u2192 Save converted contents as a .json file in the same directory. \u2192 Return the path to that JSON file. If the file is neither .xlsx nor .json : \u2192 Log a warning and return None. Side Effects May write a .json file to disk if an Excel file is successfully parsed. Source code in arb\\utils\\excel\\xl_parse.py 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 def convert_upload_to_json ( file_path : Path ) -> Path | None : \"\"\" Convert an uploaded Excel or JSON file into a valid JSON payload file. Args: file_path (Path): Path to the uploaded file. Returns: Path | None: - Path to JSON file (either original or newly created), or - None if file type is unsupported or conversion fails. Behavior: - If the file has a `.json` extension: \u2192 Assume it is valid JSON and return as-is. - If the file has a `.xlsx` extension: \u2192 Attempt to parse using Excel schema logic. \u2192 Save converted contents as a `.json` file in the same directory. \u2192 Return the path to that JSON file. - If the file is neither `.xlsx` nor `.json`: \u2192 Log a warning and return None. Side Effects: - May write a `.json` file to disk if an Excel file is successfully parsed. \"\"\" extension = file_path . suffix . lower () json_path = None if extension == \".xlsx\" : logger . debug ( f \"Excel upload detected: { file_path } \" ) try : xl_as_dict = parse_xl_file ( file_path ) logger . debug ( f \"Parsed Excel to dict: { xl_as_dict . keys () } \" ) json_path = file_path . with_suffix ( \".json\" ) logger . debug ( f \"Saving Excel-derived JSON as: { json_path } \" ) json_save_with_meta ( json_path , xl_as_dict ) except Exception as e : logger . warning ( f \"Excel parsing failed for { file_path } : { e } \" ) return None elif extension == \".json\" : logger . debug ( f \"JSON upload detected: { file_path } \" ) json_path = file_path else : logger . warning ( f \"Unsupported file type: { file_path } \" ) return json_path create_schema_file_map ( schema_path = None , schema_names = None ) Create a dictionary mapping schema names to their JSON file paths. Parameters: schema_path ( str | Path | None , default: None ) \u2013 Folder containing schema files. Defaults to processed versions dir. schema_names ( list [ str ] | None , default: None ) \u2013 Names of schemas to include. Defaults to schemas from TEMPLATES. Returns: dict [ str , Path ] \u2013 dict[str, Path]: Map from schema name to a schema file path. Source code in arb\\utils\\excel\\xl_parse.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def create_schema_file_map ( schema_path : str | Path | None = None , schema_names : list [ str ] | None = None ) -> dict [ str , Path ]: \"\"\" Create a dictionary mapping schema names to their JSON file paths. Args: schema_path (str | Path | None): Folder containing schema files. Defaults to processed versions dir. schema_names (list[str] | None): Names of schemas to include. Defaults to schemas from TEMPLATES. Returns: dict[str, Path]: Map from schema name to a schema file path. \"\"\" logger . debug ( f \"create_schema_file_map() called with { schema_path =} , { schema_names =} \" ) if isinstance ( schema_path , str ): schema_path = Path ( schema_path ) if schema_path is None : schema_path = PROCESSED_VERSIONS / \"xl_schemas\" if schema_names is None : # Import TEMPLATES from xl_create to ensure consistency from arb.utils.excel.xl_hardcoded import EXCEL_TEMPLATES schema_names = [ template [ \"schema_version\" ] for template in EXCEL_TEMPLATES ] schema_file_map = {} for schema_name in schema_names : schema_file_name = schema_name + \".json\" schema_file_path = schema_path / schema_file_name schema_file_map [ schema_name ] = schema_file_path return schema_file_map ensure_schema ( formatting_schema , schema_map , schema_alias , logger ) Resolves a schema version using the schema map and alias mapping. Logs a warning if an alias is used. Returns the resolved schema version, or None if not found. Parameters: formatting_schema ( str ) \u2013 The schema version to resolve. schema_map ( dict ) \u2013 The mapping of valid schema versions. schema_alias ( dict ) \u2013 The mapping of old schema names to new ones. logger ( Logger ) \u2013 Logger for warnings/errors. Returns: str | None \u2013 str | None: The resolved schema version, or None if not found. Source code in arb\\utils\\excel\\xl_parse.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def ensure_schema ( formatting_schema : str , schema_map : dict , schema_alias : dict , logger : logging . Logger ) -> str | None : \"\"\" Resolves a schema version using the schema map and alias mapping. Logs a warning if an alias is used. Returns the resolved schema version, or None if not found. Args: formatting_schema (str): The schema version to resolve. schema_map (dict): The mapping of valid schema versions. schema_alias (dict): The mapping of old schema names to new ones. logger: Logger for warnings/errors. Returns: str | None: The resolved schema version, or None if not found. \"\"\" if formatting_schema in schema_map : return formatting_schema if formatting_schema in schema_alias : new_schema_version = schema_alias [ formatting_schema ] logger . warning ( f \"Schema ' { formatting_schema } ' not found. Using alias ' { new_schema_version } ' instead.\" ) if new_schema_version in schema_map : return new_schema_version else : logger . error ( f \"Alias ' { new_schema_version } ' not found in schema_map.\" ) return None logger . error ( f \"Schema ' { formatting_schema } ' not found and no alias available.\" ) return None extract_tabs ( wb , schema_map , xl_as_dict ) Extract data from the data tabs that are enumerated in the schema tab. Parameters: wb ( Workbook ) \u2013 OpenPyXL workbook object. schema_map ( dict [ str , dict ] ) \u2013 Schema map with schema definitions. xl_as_dict ( dict ) \u2013 Parsed Excel content, including 'schemas' and 'metadata'. Dictionary with schema tab where keys are the data tab names and values are the formatting_schema to parse the tab. Returns: dict ( dict ) \u2013 Updated xl_as_dict including parsed 'tab_contents'. Source code in arb\\utils\\excel\\xl_parse.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def extract_tabs ( wb : openpyxl . Workbook , schema_map : dict [ str , dict ], xl_as_dict : dict ) -> dict : \"\"\" Extract data from the data tabs that are enumerated in the schema tab. Args: wb (Workbook): OpenPyXL workbook object. schema_map (dict[str, dict]): Schema map with schema definitions. xl_as_dict (dict): Parsed Excel content, including 'schemas' and 'metadata'. Dictionary with schema tab where keys are the data tab names and values are the formatting_schema to parse the tab. Returns: dict: Updated xl_as_dict including parsed 'tab_contents'. \"\"\" skip_please_selects = False result = copy . deepcopy ( xl_as_dict ) for tab_name , formatting_schema in result [ 'schemas' ] . items (): resolved_schema = ensure_schema ( formatting_schema , schema_map , schema_alias , logger ) if not resolved_schema : continue logger . debug ( f \"Extracting data from ' { tab_name } ', using the formatting schema ' { formatting_schema } '\" ) result [ 'tab_contents' ][ tab_name ] = {} ws = wb [ tab_name ] format_dict = schema_map [ resolved_schema ][ 'schema' ] for html_field_name , lookup in format_dict . items (): value_address = lookup [ 'value_address' ] value_type = lookup [ 'value_type' ] is_drop_down = lookup [ 'is_drop_down' ] # works, but you get a lint error because this will break if value_address is not a single cell value = ws [ value_address ] . value # type: ignore[attr-defined] # Strip whitespace for string fields and log if changed if value is not None and value_type == str and isinstance ( value , str ): stripped_value = value . strip () if value != stripped_value : logger . warning ( f \"Whitespace detected for field ' { html_field_name } ' at { value_address } : before strip: { repr ( value ) } , after strip: { repr ( stripped_value ) } \" ) value = stripped_value if skip_please_selects is True : if is_drop_down and value == PLEASE_SELECT : logger . debug ( f \"Skipping { html_field_name } because it is a drop down and is set to { PLEASE_SELECT } \" ) continue # Try to cast the spreadsheet data to the desired type if possible if value is not None : if not isinstance ( value , value_type ): # if it is not supposed to be of type string, but it is a zero-length string, turn it to None if value == \"\" : value = None else : logger . warning ( f \"Warning: < { html_field_name } > value at < { lookup [ 'value_address' ] } > is < { value } > \" f \"and is of type < { type ( value ) } > whereas it should be of type < { value_type } >. \" f \"Attempting to convert the value to the correct type\" ) try : # convert to datetime using a parser if possible # todo - datetime - seems like I could cast to utc here for persistence if value_type == datetime . datetime : local_datetime = excel_str_to_naive_datetime ( value ) if local_datetime and not is_datetime_naive ( local_datetime ): logger . warning ( f \"Date time { value } is not a naive datetime, skipping to avoid data corruption\" ) continue value = local_datetime else : # Use default repr-like conversion if not a datetime value = value_type ( value ) logger . info ( f \"Type conversion successful. value is now < { value } > with type: < { type ( value ) } >\" ) except ( ValueError , TypeError ) as e : logger . warning ( f \"Type conversion failed, resetting value to None\" ) value = None result [ 'tab_contents' ][ tab_name ][ html_field_name ] = value if 'label_address' in lookup and 'label' in lookup : label_address = lookup [ 'label_address' ] # works, but you get a lint error because this will break if value_address is not a single cell label_xl = ws [ label_address ] . value # type: ignore[attr-defined] label_schema = lookup [ 'label' ] if label_xl != label_schema : logger . warning ( f \"Schema data label and spreadsheet data label differ.\" f \" \\n\\t schema label = { label_schema } \\n\\t spreadsheet label ( { label_address } ) = { label_xl } \" ) logger . debug ( f \"Initial spreadsheet extraction of ' { tab_name } ' yields { result [ 'tab_contents' ][ tab_name ] } \" ) # Some cells should be spit into multiple dictionary entries (such as full name, lat/log) split_compound_keys ( result [ 'tab_contents' ][ tab_name ]) logger . debug ( f \"Final corrected spreadsheet extraction of ' { tab_name } ' yields { result [ 'tab_contents' ][ tab_name ] } \" ) return result get_json_file_name_old ( file_name ) Depreciated: use convert_upload_to_json instead Convert a file name (Excel or JSON) into a JSON file name, parsing if needed. Parameters: file_name ( Path ) \u2013 The uploaded file. Returns: Path | None \u2013 Path | None: JSON file path if parsed or detected, otherwise None. Notes: - If the file_name is a JSON file (has .json extension), the file_name is the json_file_name. - If the file is an Excel file (.xlsx extension), try to parse it into a JSON file and return the JSON file name of the parsed contents. - If the file is neither a json file nr a spreadsheet that can be parsed into a json file, return None. - If the file was already a json file, return the file name unaltered. - If the file was an Excel file, return the json file that its data was extracted to. - For all other file types return None. Source code in arb\\utils\\excel\\xl_parse.py 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 def get_json_file_name_old ( file_name : Path ) -> Path | None : \"\"\" Depreciated: use convert_upload_to_json instead Convert a file name (Excel or JSON) into a JSON file name, parsing if needed. Args: file_name (Path): The uploaded file. Returns: Path | None: JSON file path if parsed or detected, otherwise None. Notes: - If the file_name is a JSON file (has .json extension), the file_name is the json_file_name. - If the file is an Excel file (.xlsx extension), try to parse it into a JSON file and return the JSON file name of the parsed contents. - If the file is neither a json file nr a spreadsheet that can be parsed into a json file, return None. - If the file was already a json file, return the file name unaltered. - If the file was an Excel file, return the json file that its data was extracted to. - For all other file types return None. \"\"\" json_file_name = None extension = file_name . suffix # logger.debug(f\"{extension=}\") if extension == \".xlsx\" : logger . debug ( f \"Excel file upload detected. File name: { file_name } \" ) # xl_as_dict = xl_path_to_dict(file_name) xl_as_dict = parse_xl_file ( file_name ) logger . debug ( f \" { xl_as_dict =} \" ) json_file_name = file_name . with_suffix ( '.json' ) logger . debug ( f \"Saving extracted data from Excel as: { json_file_name } \" ) json_save_with_meta ( json_file_name , xl_as_dict ) elif extension == \".json\" : logger . debug ( f \"Json file upload detected. File name: { file_name } \" ) json_file_name = Path ( file_name ) else : logger . debug ( f \"Unknown file type upload detected. File name: { file_name } \" ) return json_file_name get_spreadsheet_key_value_pairs ( wb , tab_name , top_left_cell ) Read key-value pairs from a worksheet starting at a given cell. Parameters: wb ( Workbook ) \u2013 OpenPyXL workbook object. tab_name ( str ) \u2013 Name of the worksheet tab. top_left_cell ( str ) \u2013 Top-left cell of the key/value pair region. Returns: dict [ str , str | None] \u2013 dict[str, str | None]: Parsed key-value pairs. Source code in arb\\utils\\excel\\xl_parse.py 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 def get_spreadsheet_key_value_pairs ( wb : openpyxl . Workbook , tab_name : str , top_left_cell : str ) -> dict [ str , str | None ]: \"\"\" Read key-value pairs from a worksheet starting at a given cell. Args: wb (Workbook): OpenPyXL workbook object. tab_name (str): Name of the worksheet tab. top_left_cell (str): Top-left cell of the key/value pair region. Returns: dict[str, str | None]: Parsed key-value pairs. \"\"\" # logger.debug(f\"{type(wb)=}, \") ws = wb [ tab_name ] # logger.debug(f\"{type(ws)=}, \") return_dict = {} row_offset = 0 while True : key = ws [ top_left_cell ] . offset ( row = row_offset ) . value value = ws [ top_left_cell ] . offset ( row = row_offset , column = 1 ) . value if key not in [ \"\" , None ]: return_dict [ key ] = value row_offset += 1 else : break return return_dict initialize_module () Initialize the module by calling set_globals(). This function loads default schema mappings into global variables. Source code in arb\\utils\\excel\\xl_parse.py 51 52 53 54 55 56 57 58 def initialize_module () -> None : \"\"\" Initialize the module by calling set_globals(). This function loads default schema mappings into global variables. \"\"\" logger . debug ( f \"initialize_module() called\" ) set_globals () load_schema_file_map ( schema_file_map ) Load JSON schema and metadata from a mapping of schema name to a file path. Parameters: schema_file_map ( dict [ str , Path ] ) \u2013 Keys are schema names, values are JSON schema file paths. Returns: dict [ str , dict ] \u2013 dict[str, dict]: Dictionary where keys are schema names and values are dicts with: - \"schema\": The schema dictionary. - \"metadata\": Metadata extracted from the JSON. Source code in arb\\utils\\excel\\xl_parse.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def load_schema_file_map ( schema_file_map : dict [ str , Path ]) -> dict [ str , dict ]: \"\"\" Load JSON schema and metadata from a mapping of schema name to a file path. Args: schema_file_map (dict[str, Path]): Keys are schema names, values are JSON schema file paths. Returns: dict[str, dict]: Dictionary where keys are schema names and values are dicts with: - \"schema\": The schema dictionary. - \"metadata\": Metadata extracted from the JSON. \"\"\" logger . debug ( f \"load_schema_file_map() called with { schema_file_map =} \" ) schema_map = {} for schema_name , json_path in schema_file_map . items (): schema , metadata = json_load_with_meta ( json_path ) schema_map [ schema_name ] = { \"schema\" : schema , \"metadata\" : metadata } return schema_map load_xl_schema ( file_name ) Load schema and metadata from a JSON file. Parameters: file_name ( str | Path ) \u2013 Path to a JSON schema file. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: Tuple of (schema dict, metadata dict). Source code in arb\\utils\\excel\\xl_parse.py 153 154 155 156 157 158 159 160 161 162 163 164 165 def load_xl_schema ( file_name : str | Path ) -> tuple [ dict , dict ]: \"\"\" Load schema and metadata from a JSON file. Args: file_name (str | Path): Path to a JSON schema file. Returns: tuple[dict, dict]: Tuple of (schema dict, metadata dict). \"\"\" logger . debug ( f \"load_xl_schema() called with { file_name =} \" ) schema , metadata = json_load_with_meta ( file_name ) return schema , metadata main () Run all schema and Excel file parsing test functions for diagnostic purposes. Source code in arb\\utils\\excel\\xl_parse.py 538 539 540 541 542 543 544 def main () -> None : \"\"\" Run all schema and Excel file parsing test functions for diagnostic purposes. \"\"\" test_load_xl_schemas () test_load_schema_file_map () test_parse_xl_file () parse_xl_file ( xl_path , schema_map = None ) Parse a spreadsheet and return a dictionary representation using the given schema. Parameters: xl_path ( str | Path ) \u2013 Path to the Excel spreadsheet. schema_map ( dict [ str , dict ] | None , default: None ) \u2013 Map of schema names to their definitions. Returns: dict ( dict [ str , dict ] ) \u2013 Dictionary with extracted metadata, schemas, and tab contents. Notes: - tutorial on openpyxl: https://openpyxl.readthedocs.io/en/stable/tutorial.html Source code in arb\\utils\\excel\\xl_parse.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def parse_xl_file ( xl_path : str | Path , schema_map : dict [ str , dict ] | None = None ) -> dict [ str , dict ]: \"\"\" Parse a spreadsheet and return a dictionary representation using the given schema. Args: xl_path (str | Path): Path to the Excel spreadsheet. schema_map (dict[str, dict] | None): Map of schema names to their definitions. Returns: dict: Dictionary with extracted metadata, schemas, and tab contents. Notes: - tutorial on openpyxl: https://openpyxl.readthedocs.io/en/stable/tutorial.html \"\"\" logger . debug ( f \"parse_xl_with_schema_dict() called with { xl_path =} , { schema_map =} \" ) if schema_map is None : schema_map = xl_schema_map # Dictionary data structure to store Excel contents result = {} result [ 'metadata' ] = {} result [ 'schemas' ] = {} result [ 'tab_contents' ] = {} # Notes on data_only argument. By default, .value returns the 'formula' in the cell. # If data_only=True, then .value returns the last 'value' that was evaluated at the cell. wb = openpyxl . load_workbook ( xl_path , keep_vba = False , data_only = True ) # Extract metadata and schema information from hidden tabs if EXCEL_METADATA_TAB_NAME in wb . sheetnames : logger . debug ( f \"metadata tab detected in Excel file\" ) result [ 'metadata' ] = get_spreadsheet_key_value_pairs ( wb , EXCEL_METADATA_TAB_NAME , EXCEL_TOP_LEFT_KEY_VALUE_CELL ) # todo - maybe want to alias the Sector here? Refinery -> Energy if EXCEL_SCHEMA_TAB_NAME in wb . sheetnames : logger . debug ( f \"Schema tab detected in Excel file\" ) result [ 'schemas' ] = get_spreadsheet_key_value_pairs ( wb , EXCEL_SCHEMA_TAB_NAME , EXCEL_TOP_LEFT_KEY_VALUE_CELL ) # todo - maybe want to alias schemas here before extract tabs else : ValueError ( f 'Spreadsheet must have a { EXCEL_SCHEMA_TAB_NAME } tab' ) # extract data tabs content using specified schemas new_result = extract_tabs ( wb , schema_map , result ) return new_result set_globals ( xl_schema_file_map_ = None ) Set module-level global variables for schema file map and loaded schema map. Parameters: xl_schema_file_map_ ( dict [ str , Path ] | None , default: None ) \u2013 Optional override for the schema file map. If not provided, use a default list of pre-defined schema files based on TEMPLATES. Notes Calls load_schema_file_map() to populate xl_schema_map from JSON files. Uses TEMPLATES structure from xl_create.py for consistency. Source code in arb\\utils\\excel\\xl_parse.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def set_globals ( xl_schema_file_map_ : dict [ str , Path ] | None = None ) -> None : \"\"\" Set module-level global variables for schema file map and loaded schema map. Args: xl_schema_file_map_ (dict[str, Path] | None): Optional override for the schema file map. If not provided, use a default list of pre-defined schema files based on TEMPLATES. Notes: - Calls `load_schema_file_map()` to populate xl_schema_map from JSON files. - Uses TEMPLATES structure from xl_create.py for consistency. \"\"\" global xl_schema_file_map , xl_schema_map logger . debug ( f \"set_globals() called with { xl_schema_file_map_ =} \" ) if xl_schema_file_map_ is None : # Import TEMPLATES from xl_create to ensure consistency from arb.utils.excel.xl_hardcoded import EXCEL_TEMPLATES xl_schema_file_map = {} for template in EXCEL_TEMPLATES : schema_version = template [ \"schema_version\" ] schema_path = PROCESSED_VERSIONS / \"xl_schemas\" / f \" { schema_version } .json\" xl_schema_file_map [ schema_version ] = schema_path else : xl_schema_file_map = xl_schema_file_map_ # load all the schemas if xl_schema_file_map : xl_schema_map = load_schema_file_map ( xl_schema_file_map ) logger . debug ( f \"globals are now: { xl_schema_file_map =} , { xl_schema_map =} \" ) split_compound_keys ( dict_ ) Decompose compound keys into atomic fields. Remove key/value pairs of entries that potentially contain compound keys and replace them with key value pairs that are more atomic. Parameters: dict_ ( dict ) \u2013 Dictionary with potentially compound fields (e.g., lat_and_long). Raises: ValueError \u2013 If 'lat_and_long' is improperly formatted. Source code in arb\\utils\\excel\\xl_parse.py 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 def split_compound_keys ( dict_ : dict ) -> None : \"\"\" Decompose compound keys into atomic fields. Remove key/value pairs of entries that potentially contain compound keys and replace them with key value pairs that are more atomic. Args: dict_ (dict): Dictionary with potentially compound fields (e.g., lat_and_long). Raises: ValueError: If 'lat_and_long' is improperly formatted. \"\"\" for html_field_name in list ( dict_ . keys ()): value = dict_ [ html_field_name ] if html_field_name == 'lat_and_long' : if value : lat_longs = value . split ( ',' ) if len ( lat_longs ) == 2 : dict_ [ 'lat_arb' ] = lat_longs [ 0 ] dict_ [ 'long_arb' ] = lat_longs [ 1 ] else : raise ValueError ( f \"Lat long must be a blank or a comma separated list of lat/long pairs\" ) del dict_ [ html_field_name ] test_load_schema_file_map () Debug test for loading a schema file map and displaying contents. Source code in arb\\utils\\excel\\xl_parse.py 502 503 504 505 506 507 508 509 def test_load_schema_file_map () -> None : \"\"\" Debug test for loading a schema file map and displaying contents. \"\"\" logger . debug ( f \"test_load_schema_file_map() called\" ) schema_map = create_schema_file_map () schemas = load_schema_file_map ( schema_map ) logger . debug ( f \" { schemas =} \" ) test_load_xl_schemas () Debug test for loading default schemas from xl_schema_file_map. Source code in arb\\utils\\excel\\xl_parse.py 512 513 514 515 516 517 518 519 520 521 522 def test_load_xl_schemas () -> None : \"\"\" Debug test for loading default schemas from xl_schema_file_map. \"\"\" from arb.logging.arb_logging import get_pretty_printer _ , pp_log = get_pretty_printer () logger . debug ( f \"Testing load_xl_schemas() with test_load_xl_schemas\" ) schemas = load_schema_file_map ( xl_schema_file_map ) logger . debug ( f \"Testing load_xl_schemas() with test_load_xl_schemas\" ) logging . debug ( f \"schemas = { pp_log ( schemas ) } \" ) test_parse_xl_file () Debug test to parse a known Excel file into dictionary form using schemas. Source code in arb\\utils\\excel\\xl_parse.py 526 527 528 529 530 531 532 533 534 def test_parse_xl_file () -> None : \"\"\" Debug test to parse a known Excel file into dictionary form using schemas. \"\"\" logger . debug ( f \"test_parse_xl_file() called\" ) xl_path = PROCESSED_VERSIONS / \"xl_workbooks\" / \"landfill_operator_feedback_v070_populated_01.xlsx\" print ( f \" { xl_path =} \" ) result = parse_xl_file ( xl_path , xl_schema_map ) logger . debug ( f \" { result =} \" )","title":"arb.utils.excel.xl_parse"},{"location":"reference/arb/utils/excel/xl_parse/#arbutilsexcelxl_parse","text":"Module to parse and ingest Excel spreadsheet contents. This module provides logic to convert Excel forms into structured dictionary representations, including extraction of tab contents, metadata, and schema references. It is primarily used to support automated feedback template parsing. Notes schema_file_map is a dict where keys are schema names and values are paths to JSON files. schema_map is a dict where keys are schema names and values are: {\"schema\": schema_dict, \"metadata\": metadata_dict} Example Input : xl_schema_map['oil_and_gas_v03']['schema'] Output: Dictionary representing the oil and gas schema","title":"arb.utils.excel.xl_parse"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.convert_upload_to_json","text":"Convert an uploaded Excel or JSON file into a valid JSON payload file. Parameters: file_path ( Path ) \u2013 Path to the uploaded file. Returns: Path | None \u2013 Path | None: - Path to JSON file (either original or newly created), or - None if file type is unsupported or conversion fails. Behavior If the file has a .json extension: \u2192 Assume it is valid JSON and return as-is. If the file has a .xlsx extension: \u2192 Attempt to parse using Excel schema logic. \u2192 Save converted contents as a .json file in the same directory. \u2192 Return the path to that JSON file. If the file is neither .xlsx nor .json : \u2192 Log a warning and return None. Side Effects May write a .json file to disk if an Excel file is successfully parsed. Source code in arb\\utils\\excel\\xl_parse.py 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 def convert_upload_to_json ( file_path : Path ) -> Path | None : \"\"\" Convert an uploaded Excel or JSON file into a valid JSON payload file. Args: file_path (Path): Path to the uploaded file. Returns: Path | None: - Path to JSON file (either original or newly created), or - None if file type is unsupported or conversion fails. Behavior: - If the file has a `.json` extension: \u2192 Assume it is valid JSON and return as-is. - If the file has a `.xlsx` extension: \u2192 Attempt to parse using Excel schema logic. \u2192 Save converted contents as a `.json` file in the same directory. \u2192 Return the path to that JSON file. - If the file is neither `.xlsx` nor `.json`: \u2192 Log a warning and return None. Side Effects: - May write a `.json` file to disk if an Excel file is successfully parsed. \"\"\" extension = file_path . suffix . lower () json_path = None if extension == \".xlsx\" : logger . debug ( f \"Excel upload detected: { file_path } \" ) try : xl_as_dict = parse_xl_file ( file_path ) logger . debug ( f \"Parsed Excel to dict: { xl_as_dict . keys () } \" ) json_path = file_path . with_suffix ( \".json\" ) logger . debug ( f \"Saving Excel-derived JSON as: { json_path } \" ) json_save_with_meta ( json_path , xl_as_dict ) except Exception as e : logger . warning ( f \"Excel parsing failed for { file_path } : { e } \" ) return None elif extension == \".json\" : logger . debug ( f \"JSON upload detected: { file_path } \" ) json_path = file_path else : logger . warning ( f \"Unsupported file type: { file_path } \" ) return json_path","title":"convert_upload_to_json"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.create_schema_file_map","text":"Create a dictionary mapping schema names to their JSON file paths. Parameters: schema_path ( str | Path | None , default: None ) \u2013 Folder containing schema files. Defaults to processed versions dir. schema_names ( list [ str ] | None , default: None ) \u2013 Names of schemas to include. Defaults to schemas from TEMPLATES. Returns: dict [ str , Path ] \u2013 dict[str, Path]: Map from schema name to a schema file path. Source code in arb\\utils\\excel\\xl_parse.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def create_schema_file_map ( schema_path : str | Path | None = None , schema_names : list [ str ] | None = None ) -> dict [ str , Path ]: \"\"\" Create a dictionary mapping schema names to their JSON file paths. Args: schema_path (str | Path | None): Folder containing schema files. Defaults to processed versions dir. schema_names (list[str] | None): Names of schemas to include. Defaults to schemas from TEMPLATES. Returns: dict[str, Path]: Map from schema name to a schema file path. \"\"\" logger . debug ( f \"create_schema_file_map() called with { schema_path =} , { schema_names =} \" ) if isinstance ( schema_path , str ): schema_path = Path ( schema_path ) if schema_path is None : schema_path = PROCESSED_VERSIONS / \"xl_schemas\" if schema_names is None : # Import TEMPLATES from xl_create to ensure consistency from arb.utils.excel.xl_hardcoded import EXCEL_TEMPLATES schema_names = [ template [ \"schema_version\" ] for template in EXCEL_TEMPLATES ] schema_file_map = {} for schema_name in schema_names : schema_file_name = schema_name + \".json\" schema_file_path = schema_path / schema_file_name schema_file_map [ schema_name ] = schema_file_path return schema_file_map","title":"create_schema_file_map"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.ensure_schema","text":"Resolves a schema version using the schema map and alias mapping. Logs a warning if an alias is used. Returns the resolved schema version, or None if not found. Parameters: formatting_schema ( str ) \u2013 The schema version to resolve. schema_map ( dict ) \u2013 The mapping of valid schema versions. schema_alias ( dict ) \u2013 The mapping of old schema names to new ones. logger ( Logger ) \u2013 Logger for warnings/errors. Returns: str | None \u2013 str | None: The resolved schema version, or None if not found. Source code in arb\\utils\\excel\\xl_parse.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 def ensure_schema ( formatting_schema : str , schema_map : dict , schema_alias : dict , logger : logging . Logger ) -> str | None : \"\"\" Resolves a schema version using the schema map and alias mapping. Logs a warning if an alias is used. Returns the resolved schema version, or None if not found. Args: formatting_schema (str): The schema version to resolve. schema_map (dict): The mapping of valid schema versions. schema_alias (dict): The mapping of old schema names to new ones. logger: Logger for warnings/errors. Returns: str | None: The resolved schema version, or None if not found. \"\"\" if formatting_schema in schema_map : return formatting_schema if formatting_schema in schema_alias : new_schema_version = schema_alias [ formatting_schema ] logger . warning ( f \"Schema ' { formatting_schema } ' not found. Using alias ' { new_schema_version } ' instead.\" ) if new_schema_version in schema_map : return new_schema_version else : logger . error ( f \"Alias ' { new_schema_version } ' not found in schema_map.\" ) return None logger . error ( f \"Schema ' { formatting_schema } ' not found and no alias available.\" ) return None","title":"ensure_schema"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.extract_tabs","text":"Extract data from the data tabs that are enumerated in the schema tab. Parameters: wb ( Workbook ) \u2013 OpenPyXL workbook object. schema_map ( dict [ str , dict ] ) \u2013 Schema map with schema definitions. xl_as_dict ( dict ) \u2013 Parsed Excel content, including 'schemas' and 'metadata'. Dictionary with schema tab where keys are the data tab names and values are the formatting_schema to parse the tab. Returns: dict ( dict ) \u2013 Updated xl_as_dict including parsed 'tab_contents'. Source code in arb\\utils\\excel\\xl_parse.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def extract_tabs ( wb : openpyxl . Workbook , schema_map : dict [ str , dict ], xl_as_dict : dict ) -> dict : \"\"\" Extract data from the data tabs that are enumerated in the schema tab. Args: wb (Workbook): OpenPyXL workbook object. schema_map (dict[str, dict]): Schema map with schema definitions. xl_as_dict (dict): Parsed Excel content, including 'schemas' and 'metadata'. Dictionary with schema tab where keys are the data tab names and values are the formatting_schema to parse the tab. Returns: dict: Updated xl_as_dict including parsed 'tab_contents'. \"\"\" skip_please_selects = False result = copy . deepcopy ( xl_as_dict ) for tab_name , formatting_schema in result [ 'schemas' ] . items (): resolved_schema = ensure_schema ( formatting_schema , schema_map , schema_alias , logger ) if not resolved_schema : continue logger . debug ( f \"Extracting data from ' { tab_name } ', using the formatting schema ' { formatting_schema } '\" ) result [ 'tab_contents' ][ tab_name ] = {} ws = wb [ tab_name ] format_dict = schema_map [ resolved_schema ][ 'schema' ] for html_field_name , lookup in format_dict . items (): value_address = lookup [ 'value_address' ] value_type = lookup [ 'value_type' ] is_drop_down = lookup [ 'is_drop_down' ] # works, but you get a lint error because this will break if value_address is not a single cell value = ws [ value_address ] . value # type: ignore[attr-defined] # Strip whitespace for string fields and log if changed if value is not None and value_type == str and isinstance ( value , str ): stripped_value = value . strip () if value != stripped_value : logger . warning ( f \"Whitespace detected for field ' { html_field_name } ' at { value_address } : before strip: { repr ( value ) } , after strip: { repr ( stripped_value ) } \" ) value = stripped_value if skip_please_selects is True : if is_drop_down and value == PLEASE_SELECT : logger . debug ( f \"Skipping { html_field_name } because it is a drop down and is set to { PLEASE_SELECT } \" ) continue # Try to cast the spreadsheet data to the desired type if possible if value is not None : if not isinstance ( value , value_type ): # if it is not supposed to be of type string, but it is a zero-length string, turn it to None if value == \"\" : value = None else : logger . warning ( f \"Warning: < { html_field_name } > value at < { lookup [ 'value_address' ] } > is < { value } > \" f \"and is of type < { type ( value ) } > whereas it should be of type < { value_type } >. \" f \"Attempting to convert the value to the correct type\" ) try : # convert to datetime using a parser if possible # todo - datetime - seems like I could cast to utc here for persistence if value_type == datetime . datetime : local_datetime = excel_str_to_naive_datetime ( value ) if local_datetime and not is_datetime_naive ( local_datetime ): logger . warning ( f \"Date time { value } is not a naive datetime, skipping to avoid data corruption\" ) continue value = local_datetime else : # Use default repr-like conversion if not a datetime value = value_type ( value ) logger . info ( f \"Type conversion successful. value is now < { value } > with type: < { type ( value ) } >\" ) except ( ValueError , TypeError ) as e : logger . warning ( f \"Type conversion failed, resetting value to None\" ) value = None result [ 'tab_contents' ][ tab_name ][ html_field_name ] = value if 'label_address' in lookup and 'label' in lookup : label_address = lookup [ 'label_address' ] # works, but you get a lint error because this will break if value_address is not a single cell label_xl = ws [ label_address ] . value # type: ignore[attr-defined] label_schema = lookup [ 'label' ] if label_xl != label_schema : logger . warning ( f \"Schema data label and spreadsheet data label differ.\" f \" \\n\\t schema label = { label_schema } \\n\\t spreadsheet label ( { label_address } ) = { label_xl } \" ) logger . debug ( f \"Initial spreadsheet extraction of ' { tab_name } ' yields { result [ 'tab_contents' ][ tab_name ] } \" ) # Some cells should be spit into multiple dictionary entries (such as full name, lat/log) split_compound_keys ( result [ 'tab_contents' ][ tab_name ]) logger . debug ( f \"Final corrected spreadsheet extraction of ' { tab_name } ' yields { result [ 'tab_contents' ][ tab_name ] } \" ) return result","title":"extract_tabs"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.get_json_file_name_old","text":"Depreciated: use convert_upload_to_json instead Convert a file name (Excel or JSON) into a JSON file name, parsing if needed. Parameters: file_name ( Path ) \u2013 The uploaded file. Returns: Path | None \u2013 Path | None: JSON file path if parsed or detected, otherwise None. Notes: - If the file_name is a JSON file (has .json extension), the file_name is the json_file_name. - If the file is an Excel file (.xlsx extension), try to parse it into a JSON file and return the JSON file name of the parsed contents. - If the file is neither a json file nr a spreadsheet that can be parsed into a json file, return None. - If the file was already a json file, return the file name unaltered. - If the file was an Excel file, return the json file that its data was extracted to. - For all other file types return None. Source code in arb\\utils\\excel\\xl_parse.py 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 def get_json_file_name_old ( file_name : Path ) -> Path | None : \"\"\" Depreciated: use convert_upload_to_json instead Convert a file name (Excel or JSON) into a JSON file name, parsing if needed. Args: file_name (Path): The uploaded file. Returns: Path | None: JSON file path if parsed or detected, otherwise None. Notes: - If the file_name is a JSON file (has .json extension), the file_name is the json_file_name. - If the file is an Excel file (.xlsx extension), try to parse it into a JSON file and return the JSON file name of the parsed contents. - If the file is neither a json file nr a spreadsheet that can be parsed into a json file, return None. - If the file was already a json file, return the file name unaltered. - If the file was an Excel file, return the json file that its data was extracted to. - For all other file types return None. \"\"\" json_file_name = None extension = file_name . suffix # logger.debug(f\"{extension=}\") if extension == \".xlsx\" : logger . debug ( f \"Excel file upload detected. File name: { file_name } \" ) # xl_as_dict = xl_path_to_dict(file_name) xl_as_dict = parse_xl_file ( file_name ) logger . debug ( f \" { xl_as_dict =} \" ) json_file_name = file_name . with_suffix ( '.json' ) logger . debug ( f \"Saving extracted data from Excel as: { json_file_name } \" ) json_save_with_meta ( json_file_name , xl_as_dict ) elif extension == \".json\" : logger . debug ( f \"Json file upload detected. File name: { file_name } \" ) json_file_name = Path ( file_name ) else : logger . debug ( f \"Unknown file type upload detected. File name: { file_name } \" ) return json_file_name","title":"get_json_file_name_old"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.get_spreadsheet_key_value_pairs","text":"Read key-value pairs from a worksheet starting at a given cell. Parameters: wb ( Workbook ) \u2013 OpenPyXL workbook object. tab_name ( str ) \u2013 Name of the worksheet tab. top_left_cell ( str ) \u2013 Top-left cell of the key/value pair region. Returns: dict [ str , str | None] \u2013 dict[str, str | None]: Parsed key-value pairs. Source code in arb\\utils\\excel\\xl_parse.py 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 def get_spreadsheet_key_value_pairs ( wb : openpyxl . Workbook , tab_name : str , top_left_cell : str ) -> dict [ str , str | None ]: \"\"\" Read key-value pairs from a worksheet starting at a given cell. Args: wb (Workbook): OpenPyXL workbook object. tab_name (str): Name of the worksheet tab. top_left_cell (str): Top-left cell of the key/value pair region. Returns: dict[str, str | None]: Parsed key-value pairs. \"\"\" # logger.debug(f\"{type(wb)=}, \") ws = wb [ tab_name ] # logger.debug(f\"{type(ws)=}, \") return_dict = {} row_offset = 0 while True : key = ws [ top_left_cell ] . offset ( row = row_offset ) . value value = ws [ top_left_cell ] . offset ( row = row_offset , column = 1 ) . value if key not in [ \"\" , None ]: return_dict [ key ] = value row_offset += 1 else : break return return_dict","title":"get_spreadsheet_key_value_pairs"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.initialize_module","text":"Initialize the module by calling set_globals(). This function loads default schema mappings into global variables. Source code in arb\\utils\\excel\\xl_parse.py 51 52 53 54 55 56 57 58 def initialize_module () -> None : \"\"\" Initialize the module by calling set_globals(). This function loads default schema mappings into global variables. \"\"\" logger . debug ( f \"initialize_module() called\" ) set_globals ()","title":"initialize_module"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.load_schema_file_map","text":"Load JSON schema and metadata from a mapping of schema name to a file path. Parameters: schema_file_map ( dict [ str , Path ] ) \u2013 Keys are schema names, values are JSON schema file paths. Returns: dict [ str , dict ] \u2013 dict[str, dict]: Dictionary where keys are schema names and values are dicts with: - \"schema\": The schema dictionary. - \"metadata\": Metadata extracted from the JSON. Source code in arb\\utils\\excel\\xl_parse.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def load_schema_file_map ( schema_file_map : dict [ str , Path ]) -> dict [ str , dict ]: \"\"\" Load JSON schema and metadata from a mapping of schema name to a file path. Args: schema_file_map (dict[str, Path]): Keys are schema names, values are JSON schema file paths. Returns: dict[str, dict]: Dictionary where keys are schema names and values are dicts with: - \"schema\": The schema dictionary. - \"metadata\": Metadata extracted from the JSON. \"\"\" logger . debug ( f \"load_schema_file_map() called with { schema_file_map =} \" ) schema_map = {} for schema_name , json_path in schema_file_map . items (): schema , metadata = json_load_with_meta ( json_path ) schema_map [ schema_name ] = { \"schema\" : schema , \"metadata\" : metadata } return schema_map","title":"load_schema_file_map"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.load_xl_schema","text":"Load schema and metadata from a JSON file. Parameters: file_name ( str | Path ) \u2013 Path to a JSON schema file. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: Tuple of (schema dict, metadata dict). Source code in arb\\utils\\excel\\xl_parse.py 153 154 155 156 157 158 159 160 161 162 163 164 165 def load_xl_schema ( file_name : str | Path ) -> tuple [ dict , dict ]: \"\"\" Load schema and metadata from a JSON file. Args: file_name (str | Path): Path to a JSON schema file. Returns: tuple[dict, dict]: Tuple of (schema dict, metadata dict). \"\"\" logger . debug ( f \"load_xl_schema() called with { file_name =} \" ) schema , metadata = json_load_with_meta ( file_name ) return schema , metadata","title":"load_xl_schema"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.main","text":"Run all schema and Excel file parsing test functions for diagnostic purposes. Source code in arb\\utils\\excel\\xl_parse.py 538 539 540 541 542 543 544 def main () -> None : \"\"\" Run all schema and Excel file parsing test functions for diagnostic purposes. \"\"\" test_load_xl_schemas () test_load_schema_file_map () test_parse_xl_file ()","title":"main"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.parse_xl_file","text":"Parse a spreadsheet and return a dictionary representation using the given schema. Parameters: xl_path ( str | Path ) \u2013 Path to the Excel spreadsheet. schema_map ( dict [ str , dict ] | None , default: None ) \u2013 Map of schema names to their definitions. Returns: dict ( dict [ str , dict ] ) \u2013 Dictionary with extracted metadata, schemas, and tab contents. Notes: - tutorial on openpyxl: https://openpyxl.readthedocs.io/en/stable/tutorial.html Source code in arb\\utils\\excel\\xl_parse.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def parse_xl_file ( xl_path : str | Path , schema_map : dict [ str , dict ] | None = None ) -> dict [ str , dict ]: \"\"\" Parse a spreadsheet and return a dictionary representation using the given schema. Args: xl_path (str | Path): Path to the Excel spreadsheet. schema_map (dict[str, dict] | None): Map of schema names to their definitions. Returns: dict: Dictionary with extracted metadata, schemas, and tab contents. Notes: - tutorial on openpyxl: https://openpyxl.readthedocs.io/en/stable/tutorial.html \"\"\" logger . debug ( f \"parse_xl_with_schema_dict() called with { xl_path =} , { schema_map =} \" ) if schema_map is None : schema_map = xl_schema_map # Dictionary data structure to store Excel contents result = {} result [ 'metadata' ] = {} result [ 'schemas' ] = {} result [ 'tab_contents' ] = {} # Notes on data_only argument. By default, .value returns the 'formula' in the cell. # If data_only=True, then .value returns the last 'value' that was evaluated at the cell. wb = openpyxl . load_workbook ( xl_path , keep_vba = False , data_only = True ) # Extract metadata and schema information from hidden tabs if EXCEL_METADATA_TAB_NAME in wb . sheetnames : logger . debug ( f \"metadata tab detected in Excel file\" ) result [ 'metadata' ] = get_spreadsheet_key_value_pairs ( wb , EXCEL_METADATA_TAB_NAME , EXCEL_TOP_LEFT_KEY_VALUE_CELL ) # todo - maybe want to alias the Sector here? Refinery -> Energy if EXCEL_SCHEMA_TAB_NAME in wb . sheetnames : logger . debug ( f \"Schema tab detected in Excel file\" ) result [ 'schemas' ] = get_spreadsheet_key_value_pairs ( wb , EXCEL_SCHEMA_TAB_NAME , EXCEL_TOP_LEFT_KEY_VALUE_CELL ) # todo - maybe want to alias schemas here before extract tabs else : ValueError ( f 'Spreadsheet must have a { EXCEL_SCHEMA_TAB_NAME } tab' ) # extract data tabs content using specified schemas new_result = extract_tabs ( wb , schema_map , result ) return new_result","title":"parse_xl_file"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.set_globals","text":"Set module-level global variables for schema file map and loaded schema map. Parameters: xl_schema_file_map_ ( dict [ str , Path ] | None , default: None ) \u2013 Optional override for the schema file map. If not provided, use a default list of pre-defined schema files based on TEMPLATES. Notes Calls load_schema_file_map() to populate xl_schema_map from JSON files. Uses TEMPLATES structure from xl_create.py for consistency. Source code in arb\\utils\\excel\\xl_parse.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def set_globals ( xl_schema_file_map_ : dict [ str , Path ] | None = None ) -> None : \"\"\" Set module-level global variables for schema file map and loaded schema map. Args: xl_schema_file_map_ (dict[str, Path] | None): Optional override for the schema file map. If not provided, use a default list of pre-defined schema files based on TEMPLATES. Notes: - Calls `load_schema_file_map()` to populate xl_schema_map from JSON files. - Uses TEMPLATES structure from xl_create.py for consistency. \"\"\" global xl_schema_file_map , xl_schema_map logger . debug ( f \"set_globals() called with { xl_schema_file_map_ =} \" ) if xl_schema_file_map_ is None : # Import TEMPLATES from xl_create to ensure consistency from arb.utils.excel.xl_hardcoded import EXCEL_TEMPLATES xl_schema_file_map = {} for template in EXCEL_TEMPLATES : schema_version = template [ \"schema_version\" ] schema_path = PROCESSED_VERSIONS / \"xl_schemas\" / f \" { schema_version } .json\" xl_schema_file_map [ schema_version ] = schema_path else : xl_schema_file_map = xl_schema_file_map_ # load all the schemas if xl_schema_file_map : xl_schema_map = load_schema_file_map ( xl_schema_file_map ) logger . debug ( f \"globals are now: { xl_schema_file_map =} , { xl_schema_map =} \" )","title":"set_globals"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.split_compound_keys","text":"Decompose compound keys into atomic fields. Remove key/value pairs of entries that potentially contain compound keys and replace them with key value pairs that are more atomic. Parameters: dict_ ( dict ) \u2013 Dictionary with potentially compound fields (e.g., lat_and_long). Raises: ValueError \u2013 If 'lat_and_long' is improperly formatted. Source code in arb\\utils\\excel\\xl_parse.py 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 def split_compound_keys ( dict_ : dict ) -> None : \"\"\" Decompose compound keys into atomic fields. Remove key/value pairs of entries that potentially contain compound keys and replace them with key value pairs that are more atomic. Args: dict_ (dict): Dictionary with potentially compound fields (e.g., lat_and_long). Raises: ValueError: If 'lat_and_long' is improperly formatted. \"\"\" for html_field_name in list ( dict_ . keys ()): value = dict_ [ html_field_name ] if html_field_name == 'lat_and_long' : if value : lat_longs = value . split ( ',' ) if len ( lat_longs ) == 2 : dict_ [ 'lat_arb' ] = lat_longs [ 0 ] dict_ [ 'long_arb' ] = lat_longs [ 1 ] else : raise ValueError ( f \"Lat long must be a blank or a comma separated list of lat/long pairs\" ) del dict_ [ html_field_name ]","title":"split_compound_keys"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.test_load_schema_file_map","text":"Debug test for loading a schema file map and displaying contents. Source code in arb\\utils\\excel\\xl_parse.py 502 503 504 505 506 507 508 509 def test_load_schema_file_map () -> None : \"\"\" Debug test for loading a schema file map and displaying contents. \"\"\" logger . debug ( f \"test_load_schema_file_map() called\" ) schema_map = create_schema_file_map () schemas = load_schema_file_map ( schema_map ) logger . debug ( f \" { schemas =} \" )","title":"test_load_schema_file_map"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.test_load_xl_schemas","text":"Debug test for loading default schemas from xl_schema_file_map. Source code in arb\\utils\\excel\\xl_parse.py 512 513 514 515 516 517 518 519 520 521 522 def test_load_xl_schemas () -> None : \"\"\" Debug test for loading default schemas from xl_schema_file_map. \"\"\" from arb.logging.arb_logging import get_pretty_printer _ , pp_log = get_pretty_printer () logger . debug ( f \"Testing load_xl_schemas() with test_load_xl_schemas\" ) schemas = load_schema_file_map ( xl_schema_file_map ) logger . debug ( f \"Testing load_xl_schemas() with test_load_xl_schemas\" ) logging . debug ( f \"schemas = { pp_log ( schemas ) } \" )","title":"test_load_xl_schemas"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.test_parse_xl_file","text":"Debug test to parse a known Excel file into dictionary form using schemas. Source code in arb\\utils\\excel\\xl_parse.py 526 527 528 529 530 531 532 533 534 def test_parse_xl_file () -> None : \"\"\" Debug test to parse a known Excel file into dictionary form using schemas. \"\"\" logger . debug ( f \"test_parse_xl_file() called\" ) xl_path = PROCESSED_VERSIONS / \"xl_workbooks\" / \"landfill_operator_feedback_v070_populated_01.xlsx\" print ( f \" { xl_path =} \" ) result = parse_xl_file ( xl_path , xl_schema_map ) logger . debug ( f \" { result =} \" )","title":"test_parse_xl_file"}]}