{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome &amp; Instructions","text":"<p>Welcome to the backend documentation site for the CalSMP Operator Feedback Portal.</p> <p>This documentation is auto-generated from Python source code and includes:</p> <ul> <li>Flask route handlers and form logic</li> <li>Utility functions and helpers</li> <li>Excel file schema generation and parsing</li> <li>SQLAlchemy integration and metadata support</li> </ul> <p>Use the left sidebar to explore the available modules.</p>"},{"location":"reference/arb/__get_logger/","title":"<code>arb.__get_logger</code>","text":"<p>Centralized logging utility for use across the ARB portal (or any Python project).</p> <p>This module should be imported first in any file that requires logging. It is deliberately named <code>__get_logger.py</code> to ensure it appears first when imports are sorted alphabetically\u2014ensuring logging is configured before any modules emit log messages.</p>"},{"location":"reference/arb/__get_logger/#arb.__get_logger--key-features","title":"Key Features:","text":"<ul> <li>Initializes logging once per Python process to prevent redundant configurations.</li> <li>Automatically creates log files under a <code>logs/</code> directory, named after the entry-point script   (e.g., running <code>wsgi.py</code> results in <code>logs/wsgi.log</code>).</li> <li>Provides a built-in <code>PrettyPrinter</code> helper for human-readable, structured log output.</li> <li>Supports console logging (optional) and custom output paths for log files.</li> </ul>"},{"location":"reference/arb/__get_logger/#arb.__get_logger--usage-examples","title":"Usage Examples:","text":"<p>Import and initialize the logger in any module:</p> <p>from arb import get_logger as get_logger   logger, pp_log = get_logger(__name)</p> <p>logger.debug(\"Basic log message\")   logger.debug(pp_log({\"structured\": \"data\", \"for\": \"inspection\"}))</p> <p>To customize behavior:</p> <p>logger, pp_log = get_logger(     file_stem=name,     log_to_console=True,     force_command_line=False,     file_path=\"custom_logs/\"   )</p>"},{"location":"reference/arb/__get_logger/#arb.__get_logger--configuration-behavior","title":"Configuration Behavior:","text":"<ul> <li>If <code>file_path</code> is provided, logs are written to that directory using the provided <code>file_stem</code>.</li> <li>If <code>file_path</code> is not provided, the logger writes to <code>logs/&lt;stem&gt;.log</code>.</li> <li>If executed from <code>__main__</code> or <code>__init__</code>, the log file defaults to <code>logs/app_logger.log</code>.</li> <li>All log files use UTF-8 encoding and include timestamps with millisecond precision.</li> </ul>"},{"location":"reference/arb/__get_logger/#arb.__get_logger--recommendation","title":"Recommendation:","text":"<p>Place <code>__get_logger.py</code> near the root of your source tree and import it as early as possible in each module to guarantee consistent logging setup.</p>"},{"location":"reference/arb/__get_logger/#arb.__get_logger.get_logger","title":"<code>get_logger(file_stem='app_logger', file_path=None, log_to_console=False, force_command_line=False)</code>","text":"<p>Return a configured logger instance and a structured logging helper.</p> <p>Parameters:</p> Name Type Description Default <code>file_stem</code> <code>str | None</code> <p>Name used for the logger and log filename.</p> <code>'app_logger'</code> <code>file_path</code> <code>str | Path | None</code> <p>Directory path to store logs (default: <code>logs/</code>).</p> <code>None</code> <code>log_to_console</code> <code>bool</code> <p>Whether to also output logs to the console.</p> <code>False</code> <code>force_command_line</code> <code>bool</code> <p>Use command-line filename as logger name.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[Logger, any]</code> <p>tuple[Logger, callable]: A configured logger and a pretty-print function.</p> Notes <ul> <li>Logging setup is only performed once per process.</li> <li>Default log filename: <code>logs/&lt;file_stem&gt;.log</code>.</li> </ul> Source code in <code>arb\\__get_logger.py</code> <pre><code>def get_logger(\n    file_stem: str | None = \"app_logger\",\n    file_path: str | Path | None = None,\n    log_to_console: bool = False,\n    force_command_line: bool = False\n) -&gt; tuple[Logger, any]:\n  \"\"\"\n  Return a configured logger instance and a structured logging helper.\n\n  Args:\n    file_stem (str | None): Name used for the logger and log filename.\n    file_path (str | Path | None): Directory path to store logs (default: `logs/`).\n    log_to_console (bool): Whether to also output logs to the console.\n    force_command_line (bool): Use command-line filename as logger name.\n\n  Returns:\n    tuple[Logger, callable]: A configured logger and a pretty-print function.\n\n  Notes:\n    - Logging setup is only performed once per process.\n    - Default log filename: `logs/&lt;file_stem&gt;.log`.\n  \"\"\"\n  # log_format_old = \"+%(asctime)s.%(msecs)03d | %(levelname)-8s | %(name)s | %(filename)s | %(lineno)d | %(message)s\"\n  log_format_proposed = \"+%(asctime)s.%(msecs)03d | %(levelname)-8s | %(name)s | %(filename)s | %(lineno)d | user:%(user)s | %(message)s\"\n  log_format = \"+%(asctime)s.%(msecs)03d | %(levelname)-8s | %(name)-16s | user:anonymous | %(lineno)-5d | %(filename)-20s | %(message)s\"\n  log_datefmt = \"%Y-%m-%d %H:%M:%S\"\n\n  # Determine file stem based on command-line script if requested\n  if force_command_line:\n    script_path = Path(sys.argv[0])\n    file_stem = script_path.stem\n\n  if file_stem in [None, \"\", \"__init__\", \"__main__\"]:\n    file_stem = \"app_logger\"\n\n  file_stem = file_stem.replace(\".\", \"_\")\n\n  # Default to logs/ directory next to this file\n  if file_path is None:\n    file_path = Path(__file__).parent / \"logs\"\n  else:\n    file_path = Path(file_path)\n\n  file_name = file_path / f\"{file_stem}.log\"\n\n  # Create or retrieve logger\n  logger = logging.getLogger(file_stem)\n  is_logger_already_configured = bool(logging.getLogger().handlers)\n\n  if not is_logger_already_configured:\n    file_name.parent.mkdir(parents=True, exist_ok=True)\n    logging.basicConfig(\n      level=logging.DEBUG,\n      format=log_format,\n      datefmt=log_datefmt,\n      filename=str(file_name),\n      encoding=\"utf-8\",\n    )\n\n  # Optional console logging (only add if one doesn't already exist)\n  if log_to_console:\n    root_logger = logging.getLogger()\n    has_console_handler = any(\n        isinstance(handler, logging.StreamHandler) and not isinstance(handler, logging.FileHandler)\n        for handler in root_logger.handlers\n    )\n    if not has_console_handler:\n      console_handler = logging.StreamHandler()\n      console_handler.setFormatter(logging.Formatter(log_format, datefmt=log_datefmt))\n      root_logger.addHandler(console_handler)\n\n  logger.debug(f\"get_logger() called with {file_stem = }, {file_path =}, {log_to_console =}, {force_command_line =}, {sys.argv = }\")\n  if is_logger_already_configured:\n    logging.debug(\"Logging has already been initialized; configuration will not be changed.\")\n  else:\n    logging.debug(f\"Logging was initialized on first usage. Outputting logs to {file_name}\")\n\n  _, pp_log = get_pretty_printer()\n  return logger, pp_log\n</code></pre>"},{"location":"reference/arb/__get_logger/#arb.__get_logger.get_pretty_printer","title":"<code>get_pretty_printer(**kwargs)</code>","text":"<p>Return a <code>PrettyPrinter</code> instance and a formatting function for structured logging.</p> <p>This is useful for debugging or logging nested data structures like dictionaries or deeply nested lists.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Options passed to <code>pprint.PrettyPrinter</code>, including: - indent (int): Indentation level (default: 4). - sort_dicts (bool): Whether to sort dictionary keys (default: False). - width (int): Max character width per line (default: 120).</p> <code>{}</code> <p>Returns:</p> Type Description <code>tuple[PrettyPrinter, callable]</code> <p>tuple[PrettyPrinter, callable]: Printer object and its .pformat method.</p> Source code in <code>arb\\__get_logger.py</code> <pre><code>def get_pretty_printer(**kwargs: Any) -&gt; tuple[pprint.PrettyPrinter, callable]:\n  \"\"\"\n  Return a `PrettyPrinter` instance and a formatting function for structured logging.\n\n  This is useful for debugging or logging nested data structures like dictionaries\n  or deeply nested lists.\n\n  Args:\n    **kwargs (Any): Options passed to `pprint.PrettyPrinter`, including:\n      - indent (int): Indentation level (default: 4).\n      - sort_dicts (bool): Whether to sort dictionary keys (default: False).\n      - width (int): Max character width per line (default: 120).\n\n  Returns:\n    tuple[PrettyPrinter, callable]: Printer object and its .pformat method.\n  \"\"\"\n  options = {\n    \"indent\": 4,\n    \"sort_dicts\": False,\n    \"width\": 120\n  }\n  options.update(kwargs)\n\n  pp = pprint.PrettyPrinter(**options)\n  return pp, pp.pformat\n</code></pre>"},{"location":"reference/arb/wsgi/","title":"<code>arb.wsgi</code>","text":"<p>WSGI entry point for serving the Flask app.</p> <p>This file enables the application to be run via a WSGI server (e.g., Gunicorn or uWSGI) or directly via <code>flask run</code> or <code>python wsgi.py</code>.</p> <p>It provides detailed notes for various execution contexts, Flask CLI behavior, debugging strategies, and developer workflows including PyCharm integration.</p> <p>Note on running Flask Apps:</p> <p>1) You can run a flask app from the CLI in two ways:</p> <ul> <li>python <ul> <li>run flask app directly from python without the flask CLI</li> <li>Errors shown in terminal; browser only shows generic 500 unless debug=True in source code</li> </ul> <li>flask run <ul> <li>uses the Flask CLI</li> <li>makes use of Flask related environment variables and command line arguments</li> </ul> <p>2) Flask configuration precedence:</p> <pre><code>The effective behavior of your Flask app depends on how it's launched and which\nconfiguration values are set at various levels. The following precedence applies:\n\nPrecedence Order (from strongest to weakest):\n\n  1. Arguments passed directly to `app.run(...)`\n     - These override everything else, including CLI flags and environment variables.\n\n  2. Flask CLI command-line options\n     - e.g., `flask run --port=8000` overrides any FLASK_RUN_PORT setting.\n\n  3. Environment variables\n     - e.g., FLASK_ENV, FLASK_DEBUG, FLASK_RUN_PORT\n</code></pre> <p>2) Environmental variables and running from the Flask CLI</p> <ul> <li>FLASK_APP:       sets the default name for the flask app if not specified.       \"flask run\" is equivalent to \"flask --app FLASK_APP run\"       Likely FLASK_APP=app.py or FLASK_APP=wsgi</li> <li>FLASK_ENV:       can be 'development' or 'production'       development enables debug mode, auto-reload, and detailed error pages,       production disables them.       Likely will allways want FLASK_ENV=development for CARB development</li> <li>FLASK_DEBUG:       1 enables the interactive browser debugger (Werkzeug);       0 disables it.</li> <li>PYTHONPATH:       Adds directories to Python's module resolution path (sometimes needed for imports)</li> </ul> <p>3) Flask CLI arguments     * key options       * --app        * --debug or --no-debug         * determines if the Werkzeug browser debugger is on/off       * --no-reload  &lt;-- faster load time and does not restart app on source code changes <p>4) Source code app arguments:</p> <pre><code>Commonly used arguments for `app.run()`:\n\nArgs:\n  * host (str, optional): The IP address to bind to.\n      Defaults to `'127.0.0.1'`. Use `'0.0.0.0'` to make the app\n      publicly accessible (e.g., on a local network).\n  * port (int, optional): The port number to listen on.\n      Defaults to `5000`.\n  * debug (bool, optional): Enables debug mode, which activates\n      auto-reload and the interactive browser debugger. Defaults to `None`.\n  * use_reloader (bool, optional): Enables the auto-reloader to restart\n      the server on code changes. Defaults to `True` if debug is enabled.\n  * use_debugger (bool, optional): Enables the interactive debugger\n      in the browser when errors occur. Defaults to `True` if debug is enabled.\n  * threaded (bool, optional): Run the server in multithreaded mode.\n      Useful for handling multiple concurrent requests. Defaults to `False`.\n  * processes (int, optional): Number of worker processes for handling requests.\n      Mutually exclusive with `threaded=True`. Defaults to `1`.\n  * load_dotenv (bool, optional): Whether to load environment variables from\n      a `.env` file. Defaults to `True`.\n</code></pre> <p>5) Best practices:   1. use app.run(debug=True) in the wsgi file except for official release to 3rd parties     * will give you access to Browser-based call trace or python debugger depending on other factors   2. Use 'development' over 'production' until product is final.   3. testing web interactions in browser without python debugger     * flask run --app wsgi   4. Debugging with PyCharm (breakpoints + console)     * Use a Run Configuration:         * Script: wsgi.py         * Working Dir: production/arb         * Env vars: FLASK_ENV=development, FLASK_DEBUG=0         * app.run(debug=True) in wsgi.py   5. Combined Workflow (PyCharm + Browser)     * Run wsgi.py in PyCharm with debug=True     * Set FLASK_ENV=development, FLASK_DEBUG=1       * This allows:         * PyCharm to log &amp; capture         * Browser to display detailed error trace         * Breakpoints still work (though sometimes suppressed by Werkzeug internals)</p> <p>6) Root directory notes:   - The project root directory is \"feedback_portal\"   - if the app is run from wsgi.py file with path: feedback_portal/source/production/arb/wsgi.py     - Path(file).resolve().parents[3] \u2192 .../feedback_portal</p>"},{"location":"reference/arb/wsgi/#arb.wsgi--todo-work-this-in-run-with-flask-app-wsgi-run-no-reload","title":"todo - work this in, run with: flask --app wsgi run --no-reload","text":""},{"location":"reference/arb/portal/app/","title":"<code>arb.portal.app</code>","text":"<p>Application factory for the ARB Feedback Portal (Flask app).</p> <p>This module defines the <code>create_app()</code> function, which initializes and configures the Flask application with required extensions, startup behavior, routing, and globals.</p>"},{"location":"reference/arb/portal/app/#arb.portal.app--key-responsibilities","title":"Key Responsibilities:","text":"<ul> <li>Load Flask configuration dynamically using <code>get_config()</code></li> <li>Apply global app settings via <code>configure_flask_app()</code></li> <li>Initialize SQLAlchemy and optionally CSRF protection</li> <li>Reflect and optionally create the application database schema</li> <li>Load dropdowns and type mappings into the app context</li> <li>Register Flask blueprints (e.g., <code>main</code>)</li> </ul>"},{"location":"reference/arb/portal/app/#arb.portal.app--usage","title":"Usage:","text":"<p>Used by WSGI, CLI tools, or testing utilities:</p> <pre><code>from arb.portal.app import create_app\napp = create_app()\n</code></pre>"},{"location":"reference/arb/portal/app/#arb.portal.app.create_app","title":"<code>create_app()</code>","text":"<p>Create and configure the ARB Feedback Portal Flask application.</p> <p>Follows the Flask application factory pattern. This function loads configuration, initializes extensions, binds SQLAlchemy to the app, and registers route blueprints and global utilities.</p> <p>Returns:</p> Name Type Description <code>Flask</code> <code>Flask</code> <p>A fully initialized Flask application instance with: - App context globals (dropdowns, types) - SQLAlchemy base metadata (<code>app.base</code>) - Registered routes via blueprints</p> Source code in <code>arb\\portal\\app.py</code> <pre><code>def create_app() -&gt; Flask:\n  \"\"\"\n  Create and configure the ARB Feedback Portal Flask application.\n\n  Follows the Flask application factory pattern. This function loads configuration,\n  initializes extensions, binds SQLAlchemy to the app, and registers route blueprints\n  and global utilities.\n\n  Returns:\n    Flask: A fully initialized Flask application instance with:\n      - App context globals (dropdowns, types)\n      - SQLAlchemy base metadata (`app.base`)\n      - Registered routes via blueprints\n  \"\"\"\n  app: Flask = Flask(__name__)\n\n  # Load configuration from config/settings.py\n  app.config.from_object(get_config())\n\n  # Setup Jinja2, logging, and app-wide config\n  configure_flask_app(app)\n\n  # Initialize Flask extensions\n  db.init_app(app)\n  # GPT recommends this, but I'm commenting it out for now\n  # csrf.init_app(app)\n\n  # Database initialization and reflection (within app context)\n  with app.app_context():\n    db_initialize_and_create()\n    reflect_database()\n\n    # Load dropdowns, mappings, and other global data\n    base: AutomapBase = get_reflected_base(db)  # reuse db.metadata without hitting DB again\n    app.base = base  # \u2705 Attach automap base to app object\n\n    Globals.load_type_mapping(app, db, base)\n    Globals.load_drop_downs(app, db)\n\n  # Register route blueprints\n  app.register_blueprint(main)\n\n  return app\n</code></pre>"},{"location":"reference/arb/portal/app_util/","title":"<code>arb.portal.app_util</code>","text":"<p>Application-specific utility functions for the ARB Feedback Portal.</p> <p>This module provides helpers for resolving sector data, handling file uploads, preparing database rows, and integrating WTForms with SQLAlchemy models.</p>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util--key-capabilities","title":"Key Capabilities:","text":"<ul> <li>Resolve sector and sector_type for an incidence</li> <li>Insert or update rows from Excel/JSON payloads</li> <li>Reflect and verify database schema and rows</li> <li>Track uploaded files via the UploadedFile table</li> <li>Apply filter logic to the portal_updates log view</li> <li>Generate context and form logic for feedback pages</li> </ul>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util--typical-usage","title":"Typical Usage:","text":"<ul> <li>File ingestion and incidence row creation</li> <li>Dynamic form loading from model rows</li> <li>Sector/type resolution from related tables</li> <li>Upload tracking and file diagnostics</li> </ul>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.add_file_to_upload_table","title":"<code>add_file_to_upload_table(db, file_name, status=None, description=None)</code>","text":"<p>Insert a record into the <code>UploadedFile</code> table for audit and diagnostics.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy database instance.</p> required <code>file_name</code> <code>str | Path</code> <p>File path or name to be recorded.</p> required <code>status</code> <code>str | None</code> <p>Optional upload status label.</p> <code>None</code> <code>description</code> <code>str | None</code> <p>Optional notes for the upload event.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>arb\\portal\\app_util.py</code> <pre><code>def add_file_to_upload_table(db, file_name: str | Path,\n                             status=None,\n                             description=None) -&gt; None:\n  \"\"\"\n  Insert a record into the `UploadedFile` table for audit and diagnostics.\n\n  Args:\n    db (SQLAlchemy): SQLAlchemy database instance.\n    file_name (str | Path): File path or name to be recorded.\n    status (str | None): Optional upload status label.\n    description (str | None): Optional notes for the upload event.\n\n  Returns:\n    None\n  \"\"\"\n\n  # todo (consider) to wrap commit in log?\n  from arb.portal.sqla_models import UploadedFile\n\n  logger.debug(f\"Adding uploaded file to upload table: {file_name=}\")\n  model_uploaded_file = UploadedFile(\n    path=str(file_name),\n    status=status,\n    description=description,\n  )\n  db.session.add(model_uploaded_file)\n  db.session.commit()\n  logger.debug(f\"{model_uploaded_file=}\")\n</code></pre>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.apply_portal_update_filters","title":"<code>apply_portal_update_filters(query, PortalUpdate, args)</code>","text":"<p>Apply user-defined filters to a <code>PortalUpdate</code> SQLAlchemy query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>SQLAlchemy Query</code> <p>Query to be filtered.</p> required <code>PortalUpdate</code> <code>Base</code> <p>ORM model class for the portal_updates table.</p> required <code>args</code> <code>dict</code> <p>Typically from <code>request.args</code>, containing filter values.</p> required Supported filters <ul> <li>Substring matches on key, user, comments</li> <li>ID exact match or range parsing (e.g. \"100-200, 250\")</li> <li>Date range filtering via <code>start_date</code> and <code>end_date</code></li> </ul>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.apply_portal_update_filters--supported-id-formats-via-filter_id_incidence","title":"Supported ID formats (via filter_id_incidence):","text":"<ul> <li>\"123\"                  \u2192 Matches ID 123 exactly</li> <li>\"100-200\"              \u2192 Matches IDs from 100 to 200 inclusive</li> <li>\"-250\"                 \u2192 Matches all IDs \u2264 250</li> <li>\"300-\"                 \u2192 Matches all IDs \u2265 300</li> <li>\"123,150-200,250-\"     \u2192 Mixed exacts and ranges</li> <li>\"abc, 100-xyz, 222\"    \u2192 Invalid parts are ignored</li> </ul> <p>Returns (SQLAlchemy Query):   SQLAlchemy query: Modified query with filters applied.</p> Source code in <code>arb\\portal\\app_util.py</code> <pre><code>def apply_portal_update_filters(query, PortalUpdate, args: dict):\n  \"\"\"\n  Apply user-defined filters to a `PortalUpdate` SQLAlchemy query.\n\n  Args:\n    query (SQLAlchemy Query): Query to be filtered.\n    PortalUpdate (Base): ORM model class for the portal_updates table.\n    args (dict): Typically from `request.args`, containing filter values.\n\n  Supported filters:\n    - Substring matches on key, user, comments\n    - ID exact match or range parsing (e.g. \"100-200, 250\")\n    - Date range filtering via `start_date` and `end_date`\n\n  Supported ID formats (via filter_id_incidence):\n  ------------------------------------------------\n  - \"123\"                  \u2192 Matches ID 123 exactly\n  - \"100-200\"              \u2192 Matches IDs from 100 to 200 inclusive\n  - \"-250\"                 \u2192 Matches all IDs \u2264 250\n  - \"300-\"                 \u2192 Matches all IDs \u2265 300\n  - \"123,150-200,250-\"     \u2192 Mixed exacts and ranges\n  - \"abc, 100-xyz, 222\"    \u2192 Invalid parts are ignored\n\n  Returns (SQLAlchemy Query):\n    SQLAlchemy query: Modified query with filters applied.\n  \"\"\"\n  filter_key = args.get(\"filter_key\", \"\").strip()\n  filter_user = args.get(\"filter_user\", \"\").strip()\n  filter_comments = args.get(\"filter_comments\", \"\").strip()\n  filter_id_incidence = args.get(\"filter_id_incidence\", \"\").strip()\n  start_date_str = args.get(\"start_date\", \"\").strip()\n  end_date_str = args.get(\"end_date\", \"\").strip()\n\n  if filter_key:\n    query = query.filter(PortalUpdate.key.ilike(f\"%{filter_key}%\"))\n  if filter_user:\n    query = query.filter(PortalUpdate.user.ilike(f\"%{filter_user}%\"))\n  if filter_comments:\n    query = query.filter(PortalUpdate.comments.ilike(f\"%{filter_comments}%\"))\n\n  if filter_id_incidence:\n    id_exact = set()\n    id_range_clauses = []\n\n    for part in filter_id_incidence.split(\",\"):\n      part = part.strip()\n      if not part:\n        continue\n      if \"-\" in part:\n        try:\n          start, end = part.split(\"-\")\n          start = start.strip()\n          end = end.strip()\n          if start and end:\n            start_val = int(start)\n            end_val = int(end)\n            if start_val &lt;= end_val:\n              id_range_clauses.append(PortalUpdate.id_incidence.between(start_val, end_val))\n          elif start:\n            start_val = int(start)\n            id_range_clauses.append(PortalUpdate.id_incidence &gt;= start_val)\n          elif end:\n            end_val = int(end)\n            id_range_clauses.append(PortalUpdate.id_incidence &lt;= end_val)\n        except ValueError:\n          continue  # Ignore malformed part\n      elif part.isdigit():\n        id_exact.add(int(part))\n\n    clause_list = []\n    if id_exact:\n      clause_list.append(PortalUpdate.id_incidence.in_(sorted(id_exact)))\n    clause_list.extend(id_range_clauses)\n\n    if clause_list:\n      query = query.filter(or_(*clause_list))\n\n  try:\n    if start_date_str:\n      start_dt = datetime.strptime(start_date_str, \"%Y-%m-%d\")\n      query = query.filter(PortalUpdate.timestamp &gt;= start_dt)\n    if end_date_str:\n      end_dt = datetime.strptime(end_date_str, \"%Y-%m-%d\")\n      end_dt = end_dt.replace(hour=23, minute=59, second=59)\n      query = query.filter(PortalUpdate.timestamp &lt;= end_dt)\n  except ValueError:\n    pass  # Silently ignore invalid date inputs\n\n  return query\n</code></pre>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.dict_to_database","title":"<code>dict_to_database(db, base, data_dict, table_name='incidences', primary_key='id_incidence', json_field='misc_json')</code>","text":"<p>Insert or update a row in the specified table using a dictionary payload.</p> <p>The payload is merged into a model instance and committed to the database.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy database instance.</p> required <code>base</code> <code>AutomapBase</code> <p>Reflected SQLAlchemy base metadata.</p> required <code>data_dict</code> <code>dict</code> <p>Dictionary containing payload data.</p> required <code>table_name</code> <code>str</code> <p>Table name to modify. Defaults to 'incidences'.</p> <code>'incidences'</code> <code>primary_key</code> <code>str</code> <p>Name of the primary key field. Defaults to 'id_incidence'.</p> <code>'id_incidence'</code> <code>json_field</code> <code>str</code> <p>Name of the JSON field to update. Defaults to 'misc_json'.</p> <code>'misc_json'</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Final value of the primary key for the affected row.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If data_dict is empty.</p> <code>AttributeError</code> <p>If the resulting model does not expose the primary key.</p> Source code in <code>arb\\portal\\app_util.py</code> <pre><code>def dict_to_database(db,\n                     base,\n                     data_dict: dict,\n                     table_name=\"incidences\",\n                     primary_key=\"id_incidence\",\n                     json_field=\"misc_json\") -&gt; int:\n  \"\"\"\n  Insert or update a row in the specified table using a dictionary payload.\n\n  The payload is merged into a model instance and committed to the database.\n\n  Args:\n    db (SQLAlchemy): SQLAlchemy database instance.\n    base (AutomapBase): Reflected SQLAlchemy base metadata.\n    data_dict (dict): Dictionary containing payload data.\n    table_name (str): Table name to modify. Defaults to 'incidences'.\n    primary_key (str): Name of the primary key field. Defaults to 'id_incidence'.\n    json_field (str): Name of the JSON field to update. Defaults to 'misc_json'.\n\n  Returns:\n    int: Final value of the primary key for the affected row.\n\n  Raises:\n    ValueError: If data_dict is empty.\n    AttributeError: If the resulting model does not expose the primary key.\n  \"\"\"\n\n  from arb.utils.wtf_forms_util import update_model_with_payload\n\n  if not data_dict:\n    msg = \"Attempt to add empty entry to database\"\n    logger.warning(msg)\n    raise ValueError(msg)\n\n  id_ = data_dict.get(primary_key)\n\n  model, id_, is_new_row = get_ensured_row(\n    db=db,\n    base=base,\n    table_name=table_name,\n    primary_key_name=primary_key,\n    id_=id_\n  )\n\n  # Backfill generated primary key into payload if it was not supplied\n  if is_new_row:\n    logger.debug(f\"Backfilling {primary_key} = {id_} into payload\")\n    data_dict[primary_key] = id_\n\n  update_model_with_payload(model, data_dict, json_field=json_field)\n\n  session = db.session\n  session.add(model)\n  session.commit()\n\n  # Final safety: extract final PK from model\n  try:\n    return getattr(model, primary_key)\n  except AttributeError as e:\n    logger.error(f\"Model has no attribute '{primary_key}': {e}\")\n    raise\n</code></pre>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.get_ensured_row","title":"<code>get_ensured_row(db, base, table_name='incidences', primary_key_name='id_incidence', id_=None)</code>","text":"<p>Retrieve or create a row in the specified table using a primary key.</p> <p>If the row exists, it is returned. Otherwise, a new row is created and committed.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy database instance.</p> required <code>base</code> <code>AutomapBase</code> <p>Reflected SQLAlchemy base metadata.</p> required <code>table_name</code> <code>str</code> <p>Table name to operate on. Defaults to 'incidences'.</p> <code>'incidences'</code> <code>primary_key_name</code> <code>str</code> <p>Name of the primary key column. Defaults to 'id_incidence'.</p> <code>'id_incidence'</code> <code>id_</code> <code>int | None</code> <p>Primary key value. If None, a new row is created.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>(model, id_, is_new_row) - model: SQLAlchemy ORM instance - id_: Primary key value - is_new_row: Whether a new row was created (True/False)</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the model class lacks the specified primary key.</p> Source code in <code>arb\\portal\\app_util.py</code> <pre><code>def get_ensured_row(db, base, table_name=\"incidences\", primary_key_name=\"id_incidence\", id_=None) -&gt; tuple:\n  \"\"\"\n  Retrieve or create a row in the specified table using a primary key.\n\n  If the row exists, it is returned. Otherwise, a new row is created and committed.\n\n  Args:\n    db (SQLAlchemy): SQLAlchemy database instance.\n    base (AutomapBase): Reflected SQLAlchemy base metadata.\n    table_name (str): Table name to operate on. Defaults to 'incidences'.\n    primary_key_name (str): Name of the primary key column. Defaults to 'id_incidence'.\n    id_ (int | None): Primary key value. If None, a new row is created.\n\n  Returns:\n    tuple: (model, id_, is_new_row)\n      - model: SQLAlchemy ORM instance\n      - id_: Primary key value\n      - is_new_row: Whether a new row was created (True/False)\n\n  Raises:\n    AttributeError: If the model class lacks the specified primary key.\n  \"\"\"\n\n  is_new_row = False\n\n  session = db.session\n  table = get_class_from_table_name(base, table_name)\n\n  if id_ is not None:\n    logger.debug(f\"Retrieving {table_name} row with {primary_key_name}={id_}\")\n    model = session.get(table, id_)\n    if model is None:\n      is_new_row = True\n      logger.debug(f\"No existing row found; creating new {table_name} row with {primary_key_name}={id_}\")\n      model = table(**{primary_key_name: id_})\n  else:\n    is_new_row = True\n    logger.debug(f\"Creating new {table_name} row with auto-generated {primary_key_name}\")\n    model = table(**{primary_key_name: None})\n    session.add(model)\n    session.commit()\n    id_ = getattr(model, primary_key_name)\n    logger.debug(f\"{table_name} row created with {primary_key_name}={id_}\")\n\n  return model, id_, is_new_row\n</code></pre>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.get_sector_info","title":"<code>get_sector_info(db, base, id_)</code>","text":"<p>Resolve the sector and sector_type for a given incidence ID.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy database instance.</p> required <code>base</code> <code>AutomapBase</code> <p>SQLAlchemy automapped declarative base.</p> required <code>id_</code> <code>int</code> <p>ID of the row in the <code>incidences</code> table.</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]: (sector, sector_type)</p> Source code in <code>arb\\portal\\app_util.py</code> <pre><code>def get_sector_info(db: SQLAlchemy,\n                    base: AutomapBase,\n                    id_: int) -&gt; tuple[str, str]:\n  \"\"\"\n  Resolve the sector and sector_type for a given incidence ID.\n\n  Args:\n    db (SQLAlchemy): SQLAlchemy database instance.\n    base (AutomapBase): SQLAlchemy automapped declarative base.\n    id_ (int): ID of the row in the `incidences` table.\n\n  Returns:\n    tuple[str, str]: (sector, sector_type)\n  \"\"\"\n  logger.debug(f\"get_sector_info() called to determine sector &amp; sector type for {id_=}\")\n  primary_table_name = \"incidences\"\n  json_column = \"misc_json\"\n  sector = None\n  sector_type = None\n\n  # Find the sector from the foreign table if incidence was created by plume tracker.\n  sector_by_foreign_key = get_foreign_value(\n    db, base,\n    primary_table_name=primary_table_name,\n    foreign_table_name=\"sources\",\n    primary_table_fk_name=\"source_id\",\n    foreign_table_column_name=\"sector\",\n    primary_table_pk_value=id_,\n  )\n\n  # Get the row and misc_json field from the incidence table\n  row, misc_json = get_table_row_and_column(\n    db, base,\n    table_name=primary_table_name,\n    column_name=json_column,\n    id_=id_,\n  )\n\n  if misc_json is None:\n    misc_json = {}\n\n  sector = resolve_sector(sector_by_foreign_key, row, misc_json)\n  sector_type = get_sector_type(sector)\n\n  logger.debug(f\"get_sector_info() returning {sector=} {sector_type=}\")\n  return sector, sector_type\n</code></pre>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.get_sector_type","title":"<code>get_sector_type(sector)</code>","text":"<p>Map a sector name to its broad classification.</p> <p>Parameters:</p> Name Type Description Default <code>sector</code> <code>str</code> <p>Input sector label.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>One of \"Oil &amp; Gas\" or \"Landfill\".</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>On unknown sector input.</p> Source code in <code>arb\\portal\\app_util.py</code> <pre><code>def get_sector_type(sector: str) -&gt; str:\n  \"\"\"\n  Map a sector name to its broad classification.\n\n  Args:\n    sector (str): Input sector label.\n\n  Returns:\n    str: One of \"Oil &amp; Gas\" or \"Landfill\".\n\n  Raises:\n    ValueError: On unknown sector input.\n  \"\"\"\n\n  if sector in OIL_AND_GAS_SECTORS:\n    return \"Oil &amp; Gas\"\n  elif sector in LANDFILL_SECTORS:\n    return \"Landfill\"\n  else:\n    raise ValueError(f\"Unknown sector type: '{sector}'.\")\n</code></pre>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.incidence_prep","title":"<code>incidence_prep(model_row, crud_type, sector_type, default_dropdown)</code>","text":"<p>Generate the context and render the HTML template for a feedback record.</p> <p>Populates WTForms fields from the model and applies validation logic depending on the request method (GET/POST). Integrates conditional dropdown resets, CSRF-less validation, and feedback record persistence.</p> <p>Parameters:</p> Name Type Description Default <code>model_row</code> <code>DeclarativeMeta</code> <p>SQLAlchemy model row for the feedback entry.</p> required <code>crud_type</code> <code>str</code> <p>'create' or 'update'.</p> required <code>sector_type</code> <code>str</code> <p>'Oil &amp; Gas' or 'Landfill'.</p> required <code>default_dropdown</code> <code>str</code> <p>Value used to fill in blank selects.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Rendered HTML from the appropriate feedback template.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sector type is invalid.</p> Source code in <code>arb\\portal\\app_util.py</code> <pre><code>def incidence_prep(model_row: DeclarativeMeta,\n                   crud_type: str,\n                   sector_type: str,\n                   default_dropdown: str) -&gt; str:\n  \"\"\"\n  Generate the context and render the HTML template for a feedback record.\n\n  Populates WTForms fields from the model and applies validation logic\n  depending on the request method (GET/POST). Integrates conditional\n  dropdown resets, CSRF-less validation, and feedback record persistence.\n\n  Args:\n    model_row (DeclarativeMeta): SQLAlchemy model row for the feedback entry.\n    crud_type (str): 'create' or 'update'.\n    sector_type (str): 'Oil &amp; Gas' or 'Landfill'.\n    default_dropdown (str): Value used to fill in blank selects.\n\n  Returns:\n    str: Rendered HTML from the appropriate feedback template.\n\n  Raises:\n    ValueError: If the sector type is invalid.\n  \"\"\"\n  # imports below can't be moved to top of file because they require Globals to be initialized\n  # prior to first use (Globals.load_drop_downs(app, db)).\n  from arb.portal.wtf_landfill import LandfillFeedback\n  from arb.portal.wtf_oil_and_gas import OGFeedback\n\n  logger.debug(f\"incidence_prep() called with {crud_type=}, {sector_type=}\")\n  sa_model_diagnostics(model_row)\n\n  if default_dropdown is None:\n    default_dropdown = PLEASE_SELECT\n\n  if sector_type == \"Oil &amp; Gas\":\n    logger.debug(f\"({sector_type=}) will use an Oil &amp; Gas Feedback Form\")\n    wtf_form = OGFeedback()\n    template_file = 'feedback_oil_and_gas.html'\n  elif sector_type == \"Landfill\":\n    logger.debug(f\"({sector_type=}) will use a Landfill Feedback Form\")\n    wtf_form = LandfillFeedback()\n    template_file = 'feedback_landfill.html'\n  else:\n    raise ValueError(f\"Unknown sector type: '{sector_type}'.\")\n\n  if request.method == 'GET':\n    # Populate wtform from model data\n    model_to_wtform(model_row, wtf_form)\n    # todo - maybe put update contingencies here?\n    # obj_diagnostics(wtf_form, message=\"wtf_form in incidence_prep() after model_to_wtform\")\n\n    # For GET requests for row creation, don't validate and error_count_dict will be all zeros\n    # For GET requests for row update, validate (except for the csrf token that is only present for a POST)\n    if crud_type == 'update':\n      validate_no_csrf(wtf_form, extra_validators=None)\n\n  # todo - trying to make sure invalid drop-downs become \"Please Select\"\n  #        may want to look into using validate_no_csrf or initialize_drop_downs (or combo)\n\n  # Set all select elements that are a default value (None) to \"Please Select\" value\n  initialize_drop_downs(wtf_form, default=default_dropdown)\n  # logger.debug(f\"\\n\\t{wtf_form.data=}\")\n\n  if request.method == 'POST':\n    # Validate and count errors\n    wtf_form.validate()\n    error_count_dict = wtf_count_errors(wtf_form, log_errors=True)\n\n    # Diagnostics of model before updating with wtform values\n    # Likely can comment out model_before and add_commit_and_log_model\n    # if you want less diagnostics and redundant commits\n    model_before = sa_model_to_dict(model_row)\n    wtform_to_model(model_row, wtf_form, ignore_fields=[\"id_incidence\"])\n    add_commit_and_log_model(db,\n                             model_row,\n                             comment='call to wtform_to_model()',\n                             model_before=model_before)\n\n    # Determine course of action for successful database update based on which button was submitted\n    button = request.form.get('submit_button')\n\n    # todo - change the button name to save?\n    if button == 'validate_and_submit':\n      logger.debug(f\"validate_and_submit was pressed\")\n      if wtf_form.validate():\n        return redirect(url_for('main.index'))\n\n  error_count_dict = wtf_count_errors(wtf_form, log_errors=True)\n\n  logger.debug(f\"incidence_prep() about to render get template\")\n\n  return render_template(template_file,\n                         wtf_form=wtf_form,\n                         crud_type=crud_type,\n                         error_count_dict=error_count_dict,\n                         id_incidence=model_row.id_incidence,\n                         )\n</code></pre>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.json_file_to_db","title":"<code>json_file_to_db(db, file_name, base)</code>","text":"<p>Load a JSON file and insert its contents into the <code>incidences</code> table.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy session used to commit the new row.</p> required <code>file_name</code> <code>str | Path</code> <p>Path to the JSON file on disk.</p> required <code>base</code> <code>AutomapBase</code> <p>SQLAlchemy automapped metadata base.</p> required <p>Returns:</p> Type Description <code>tuple[int, str]</code> <p>tuple[int, str]: The (id_incidence, sector) extracted from the inserted row.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified file path does not exist.</p> <code>JSONDecodeError</code> <p>If the file is not valid JSON.</p> Source code in <code>arb\\portal\\app_util.py</code> <pre><code>def json_file_to_db(db: SQLAlchemy,\n                    file_name: str | Path,\n                    base: AutomapBase\n                    ) -&gt; tuple[int, str]:\n  \"\"\"\n  Load a JSON file and insert its contents into the `incidences` table.\n\n  Args:\n    db (SQLAlchemy): SQLAlchemy session used to commit the new row.\n    file_name (str | Path): Path to the JSON file on disk.\n    base (AutomapBase): SQLAlchemy automapped metadata base.\n\n  Returns:\n    tuple[int, str]: The (id_incidence, sector) extracted from the inserted row.\n\n  Raises:\n    FileNotFoundError: If the specified file path does not exist.\n    json.JSONDecodeError: If the file is not valid JSON.\n  \"\"\"\n\n  json_as_dict, metadata = json_load_with_meta(file_name)\n  return xl_dict_to_database(db, base, json_as_dict)\n</code></pre>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.resolve_sector","title":"<code>resolve_sector(sector_by_foreign_key, row, misc_json)</code>","text":"<p>Determine the appropriate sector from FK and JSON sources.</p> <p>Parameters:</p> Name Type Description Default <code>sector_by_foreign_key</code> <code>str | None</code> <p>Sector from <code>sources</code> table.</p> required <code>row</code> <code>DeclarativeMeta</code> <p>Row from <code>incidences</code> table (SQLAlchemy result).</p> required <code>misc_json</code> <code>dict</code> <p>Parsed <code>misc_json</code> content.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Sector string.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If values are missing or conflict.</p> Source code in <code>arb\\portal\\app_util.py</code> <pre><code>def resolve_sector(sector_by_foreign_key: str | None,\n                   row: DeclarativeMeta,\n                   misc_json: dict) -&gt; str:\n  \"\"\"\n  Determine the appropriate sector from FK and JSON sources.\n\n  Args:\n    sector_by_foreign_key (str | None): Sector from `sources` table.\n    row (DeclarativeMeta): Row from `incidences` table (SQLAlchemy result).\n    misc_json (dict): Parsed `misc_json` content.\n\n  Returns:\n    str: Sector string.\n\n  Raises:\n    ValueError: If values are missing or conflict.\n  \"\"\"\n  logger.debug(f\"resolve_sector() called with {sector_by_foreign_key=}, {row=}, {misc_json=}\")\n  sector = None\n  sector_by_json = misc_json.get(\"sector\")\n\n  if sector_by_foreign_key is None:\n    logger.warning(\"sector column value in sources table is None.\")\n\n  if sector_by_json is None:\n    logger.warning(\"'sector' not in misc_json\")\n\n  if sector_by_foreign_key is None and sector_by_json is None:\n    logger.error(\"Can't determine incidence sector\")\n    raise ValueError(\"Can't determine incidence sector\")\n\n  if sector_by_foreign_key is not None and sector_by_json is not None:\n    if sector_by_foreign_key != sector_by_json:\n      logger.error(f\"Sector mismatch: {sector_by_foreign_key=}, {sector_by_json=}\")\n      raise ValueError(\"Can't determine incidence sector\")\n\n  sector = sector_by_foreign_key or sector_by_json\n\n  logger.debug(f\"resolve_sector() returning {sector=}\")\n  return sector\n</code></pre>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.upload_and_update_db","title":"<code>upload_and_update_db(db, upload_dir, request_file, base)</code>","text":"<p>Save uploaded file, parse contents, and insert or update DB rows.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>Database instance.</p> required <code>upload_dir</code> <code>str | Path</code> <p>Directory where file will be saved.</p> required <code>request_file</code> <code>FileStorage</code> <p>Flask <code>request.files[...]</code> object.</p> required <code>base</code> <code>AutomapBase</code> <p>Automapped schema metadata.</p> required <p>Returns:</p> Type Description <code>tuple[str, int | None, str | None]</code> <p>tuple[str, int | None, str | None]: Filename, id_incidence, sector.</p> Source code in <code>arb\\portal\\app_util.py</code> <pre><code>def upload_and_update_db(db: SQLAlchemy,\n                         upload_dir: str | Path,\n                         request_file: FileStorage,\n                         base: AutomapBase\n                         ) -&gt; tuple[str, int | None, str | None]:\n  \"\"\"\n  Save uploaded file, parse contents, and insert or update DB rows.\n\n  Args:\n    db (SQLAlchemy): Database instance.\n    upload_dir (str | Path): Directory where file will be saved.\n    request_file (FileStorage): Flask `request.files[...]` object.\n    base (AutomapBase): Automapped schema metadata.\n\n  Returns:\n    tuple[str, int | None, str | None]: Filename, id_incidence, sector.\n  \"\"\"\n  logger.debug(f\"upload_and_update_db() called with {request_file=}\")\n  id_ = None\n  sector = None\n\n  file_name = upload_single_file(upload_dir, request_file)\n  add_file_to_upload_table(db, file_name, status=\"File Added\", description=None)\n\n  # if file is xl and can be converted to json,\n  # save a json version of the file and return the filename\n  json_file_name = get_json_file_name(file_name)\n  if json_file_name:\n    id_, sector = json_file_to_db(db, json_file_name, base)\n\n  return file_name, id_, sector\n</code></pre>"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.xl_dict_to_database","title":"<code>xl_dict_to_database(db, base, xl_dict, tab_name)</code>","text":"<p>Insert or update a row from an Excel-parsed JSON dictionary into the database.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy database instance.</p> required <code>base</code> <code>AutomapBase</code> <p>Reflected SQLAlchemy base metadata.</p> required <code>xl_dict</code> <code>dict</code> <p>Parsed Excel document with 'metadata' and 'tab_contents'.</p> required <code>tab_name</code> <code>str</code> <p>Name of the worksheet tab to extract.</p> required <p>Returns:</p> Type Description <code>tuple[int, str]</code> <p>tuple[int, str]: Tuple of (id_incidence, sector) after row insertion.</p> Source code in <code>arb\\portal\\app_util.py</code> <pre><code>def xl_dict_to_database(db, base, xl_dict: dict, tab_name: str) -&gt; tuple[int, str]:\n  \"\"\"\n  Insert or update a row from an Excel-parsed JSON dictionary into the database.\n\n  Args:\n    db (SQLAlchemy): SQLAlchemy database instance.\n    base (AutomapBase): Reflected SQLAlchemy base metadata.\n    xl_dict (dict): Parsed Excel document with 'metadata' and 'tab_contents'.\n    tab_name (str): Name of the worksheet tab to extract.\n\n  Returns:\n    tuple[int, str]: Tuple of (id_incidence, sector) after row insertion.\n  \"\"\"\n  logger.debug(f\"xl_dict_to_database() called with {xl_dict=}\")\n  metadata = xl_dict[\"metadata\"]\n  sector = metadata[\"sector\"]\n  tab_data = xl_dict[\"tab_contents\"][tab_name]\n  tab_data[\"sector\"] = sector\n\n  id_ = dict_to_database(db, base, tab_data)\n  return id_, sector\n</code></pre>"},{"location":"reference/arb/portal/constants/","title":"<code>arb.portal.constants</code>","text":"<p>Shared application-wide constants for the ARB Methane Feedback Portal.</p> <p>These constants are designed to be immutable and centrally maintained. They support consistent behavior and validation across:</p> <ul> <li>Web form defaults and placeholders</li> <li>Geospatial input validation</li> <li>Spreadsheet cell parsing</li> <li>Time zone-aware datetime formatting</li> <li>Filename-safe timestamp generation</li> </ul> Structure <ul> <li>UI Constants</li> <li>Geographic Boundaries (California-specific)</li> <li>Time Zones and Datetime Formats</li> <li>Module Self-Test</li> </ul> Notes <ul> <li>Constants should always be imported from this module instead of redefined.</li> <li>Time zone constants use <code>zoneinfo.ZoneInfo</code> and are safe for use with   timezone-aware datetime objects.</li> </ul>"},{"location":"reference/arb/portal/constants/#arb.portal.constants.CA_TIME_ZONE","title":"<code>CA_TIME_ZONE = ZoneInfo('America/Los_Angeles')</code>  <code>module-attribute</code>","text":"<p>ZoneInfo: Timezone objects used for datetime conversion and formatting.</p>"},{"location":"reference/arb/portal/constants/#arb.portal.constants.DATETIME_WITH_SECONDS","title":"<code>DATETIME_WITH_SECONDS = '%Y_%m_%d_%H_%M_%S'</code>  <code>module-attribute</code>","text":"<p>str: Filename-safe datetime string format (includes seconds).</p>"},{"location":"reference/arb/portal/constants/#arb.portal.constants.GPS_RESOLUTION","title":"<code>GPS_RESOLUTION = 5</code>  <code>module-attribute</code>","text":"<p>int: Desired decimal precision for GPS values.</p>"},{"location":"reference/arb/portal/constants/#arb.portal.constants.HTML_LOCAL_TIME_FORMAT","title":"<code>HTML_LOCAL_TIME_FORMAT = '%Y-%m-%dT%H:%M'</code>  <code>module-attribute</code>","text":"<p>str: HTML5-compatible format string for .</p>"},{"location":"reference/arb/portal/constants/#arb.portal.constants.LATITUDE_VALIDATION","title":"<code>LATITUDE_VALIDATION = {'min': MIN_LATITUDE, 'max': MAX_LATITUDE, 'message': f'Latitudes must be blank or valid California number between {MIN_LATITUDE} and {MAX_LATITUDE}.'}</code>  <code>module-attribute</code>","text":"<p>dict: Latitude validation schema for WTForms or other validators.</p>"},{"location":"reference/arb/portal/constants/#arb.portal.constants.LONGITUDE_VALIDATION","title":"<code>LONGITUDE_VALIDATION = {'min': MIN_LONGITUDE, 'max': MAX_LONGITUDE, 'message': f'Longitudes must be blank or valid California number between {MIN_LONGITUDE} and {MAX_LONGITUDE}.'}</code>  <code>module-attribute</code>","text":"<p>dict: Longitude validation schema for WTForms or other validators.</p>"},{"location":"reference/arb/portal/constants/#arb.portal.constants.MAX_LATITUDE","title":"<code>MAX_LATITUDE = 42.0</code>  <code>module-attribute</code>","text":"<p>float: Maximum possible CA latitude.</p>"},{"location":"reference/arb/portal/constants/#arb.portal.constants.MAX_LONGITUDE","title":"<code>MAX_LONGITUDE = -114.0</code>  <code>module-attribute</code>","text":"<p>float: Maximum possible CA longitude.</p>"},{"location":"reference/arb/portal/constants/#arb.portal.constants.MIN_LATITUDE","title":"<code>MIN_LATITUDE = 32.0</code>  <code>module-attribute</code>","text":"<p>float: Minimum possible CA latitude.</p>"},{"location":"reference/arb/portal/constants/#arb.portal.constants.MIN_LONGITUDE","title":"<code>MIN_LONGITUDE = -125.0</code>  <code>module-attribute</code>","text":"<p>float: Minimum possible CA longitude.</p>"},{"location":"reference/arb/portal/constants/#arb.portal.constants.PLEASE_SELECT","title":"<code>PLEASE_SELECT = 'Please Select'</code>  <code>module-attribute</code>","text":"<p>str: Placeholder value used for dropdowns where no selection is made.</p>"},{"location":"reference/arb/portal/constants/#arb.portal.constants.UTC_TIME_ZONE","title":"<code>UTC_TIME_ZONE = ZoneInfo('UTC')</code>  <code>module-attribute</code>","text":"<p>ZoneInfo: UTC (Zulu) timezone.</p>"},{"location":"reference/arb/portal/db_hardcoded/","title":"<code>arb.portal.db_hardcoded</code>","text":"<p>Hardcoded testing data and dropdown lookup values for the ARB Methane Feedback Portal.</p> This module provides <ul> <li>Dummy incidence records for Oil &amp; Gas and Landfill sectors</li> <li>Lookup values for HTML dropdowns (independent and contingent)</li> <li>Shared test values for local debugging or spreadsheet seeding</li> </ul> Notes <ul> <li>Intended for use during development and offline diagnostics</li> <li>Not suitable for production database seeding</li> </ul>"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.add_og_dummy_data","title":"<code>add_og_dummy_data(db, base, table_name)</code>","text":"<p>(Depreciated) Populate the database with synthetic Oil &amp; Gas incidence rows for diagnostics.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>Active SQLAlchemy session bound to the database.</p> required <code>base</code> <code>AutomapBase</code> <p>SQLAlchemy automap base for resolving table classes.</p> required <code>table_name</code> <code>str</code> <p>Target table name (e.g., 'incidences').</p> required Notes <ul> <li>This routine is likely outdated and is kept only as a template.</li> <li>It is valid, but not necessary to specify 'Please Select' in dummy data.</li> <li>Uses an offset in <code>id_incidence</code> to avoid primary key conflicts.</li> <li>Inserts 9 rows with dummy <code>misc_json</code> fields.</li> </ul> Source code in <code>arb\\portal\\db_hardcoded.py</code> <pre><code>def add_og_dummy_data(db, base, table_name) -&gt; None:\n  \"\"\"\n  (Depreciated) Populate the database with synthetic Oil &amp; Gas incidence rows for diagnostics.\n\n  Args:\n    db (SQLAlchemy): Active SQLAlchemy session bound to the database.\n    base (AutomapBase): SQLAlchemy automap base for resolving table classes.\n    table_name (str): Target table name (e.g., 'incidences').\n\n  Notes:\n    - This routine is likely outdated and is kept only as a template.\n    - It is valid, but not necessary to specify 'Please Select' in dummy data.\n    - Uses an offset in `id_incidence` to avoid primary key conflicts.\n    - Inserts 9 rows with dummy `misc_json` fields.\n  \"\"\"\n\n  from arb.utils.sql_alchemy import get_class_from_table_name\n  logger.debug(\"Adding dummy oil and gas data to populate the database\")\n  table = get_class_from_table_name(base, table_name)\n  col_name = \"misc_json\"\n\n  offset = 2000000  # adjust so that you don't have a unique constraint issue\n\n  for i in range(1, 10):\n    id_incidence = i + offset\n    id_plume = i + 100\n    lat_arb = i + 50.\n    long_arb = i + 75.\n    observation_timestamp = datetime.datetime.now().replace(second=0, microsecond=0)\n\n    facility_name = f\"facility_{i}\"\n    contact_name = f\"contact_name_{i}\"\n    contact_phone = f\"(555) 555-5555x{i}\"\n    contact_email = f\"my_email_{i}@server.com\"\n    sector = \"Oil &amp; Gas\"\n    sector_type = \"Oil &amp; Gas\"\n\n    json_data = {\"id_incidence\": id_incidence,\n                 \"id_plume\": id_plume,\n                 \"lat_arb\": lat_arb,\n                 \"long_arb\": long_arb,\n                 # \"observation_timestamp\": observation_timestamp.strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n                 \"observation_timestamp\": observation_timestamp,\n                 \"facility_name\": facility_name,\n                 \"contact_name\": contact_name,\n                 \"contact_phone\": contact_phone,\n                 \"contact_email\": contact_email,\n                 \"sector\": sector,\n                 \"sector_type\": sector_type,\n                 }\n\n    model = table(description=\"Dummy data created by add_og_dummy_data\", **{col_name: json_data})\n\n    logger.debug(f\"Adding incidence with json dummy data: {json_data=}\")\n\n    db.session.add(model)\n  db.session.commit()\n</code></pre>"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_excel_dropdown_data","title":"<code>get_excel_dropdown_data()</code>","text":"<p>Return dropdown lookup values used in Excel and HTML form rendering.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[dict[str, list[str]], dict[str, dict[str, list[str]]]]</code> <ul> <li>dict[str, list[str]]: Independent dropdowns keyed by HTML field name.</li> <li>dict[str, dict[str, list[str]]]: Contingent dropdowns dependent on parent field values.</li> </ul> Notes <ul> <li>Dropdown values mirror those found in Excel templates.</li> <li>Each list element is a selectable value; <code>\"Please Select\"</code> is prepended externally.</li> <li>Contingent keys follow the format: <code>field2_contingent_on_field1</code>.</li> <li>Each tuple is 2 or 3 items in length with the format:   (select value, select text, and an optional dictionary of additional html formatting)</li> </ul>"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_excel_dropdown_data--todo-the-new-drop-downs-are-not-context-dependent-like-they-are-in-excel-and-the","title":"todo - The new drop-downs are not context dependent like they are in excel and the","text":"<pre><code>     validate logic needs to be updated.\n</code></pre> Source code in <code>arb\\portal\\db_hardcoded.py</code> <pre><code>def get_excel_dropdown_data() -&gt; tuple[dict[str, list[str]], dict[str, dict[str, list[str]]]]:\n  \"\"\"\n  Return dropdown lookup values used in Excel and HTML form rendering.\n\n  Returns:\n    tuple:\n      - dict[str, list[str]]: Independent dropdowns keyed by HTML field name.\n      - dict[str, dict[str, list[str]]]: Contingent dropdowns dependent on parent field values.\n\n  Notes:\n    - Dropdown values mirror those found in Excel templates.\n    - Each list element is a selectable value; `\"Please Select\"` is prepended externally.\n    - Contingent keys follow the format: `field2_contingent_on_field1`.\n    - Each tuple is 2 or 3 items in length with the format:\n      (select value, select text, and an optional dictionary of additional html formatting)\n\n  # todo - The new drop-downs are not context dependent like they are in excel and the\n           validate logic needs to be updated.\n  \"\"\"\n\n  # Oil &amp; Gas\n  drop_downs = {\n    \"venting_exclusion\": [\n      \"Yes\",\n      \"No\",\n    ],\n    \"ogi_performed\": [\n      \"Yes\",\n      \"No\",\n    ],\n    \"ogi_result\": [\n      \"Not applicable as OGI was not performed\",\n      \"No source found\",\n      \"Unintentional-leak\",\n      \"Unintentional-non-component\",\n      \"Venting-construction/maintenance\",\n      \"Venting-routine\",\n    ],\n    \"method21_performed\": [\n      \"Yes\",\n      \"No\",\n    ],\n    \"method21_result\": [\n      \"Not applicable as Method 21 was not performed\",\n      \"No source found\",\n      \"Unintentional-below leak threshold\",\n      \"Unintentional-leak\",\n      \"Unintentional-non-component\",\n      \"Venting-construction/maintenance\",\n      \"Venting-routine\",\n    ],\n    \"equipment_at_source\": [\n      \"Centrifugal Natural Gas Compressor\",\n      \"Continuous High Bleed Natural Gas-actuated Pneumatic Device\",\n      \"Continuous Low Bleed Natural Gas-actuated Pneumatic Device\",\n      \"Intermittent Bleed Natural Gas-actuated Pneumatic Device\",\n      \"Natural Gas-actuated Pneumatic Pump\",\n      \"Pressure Separator\",\n      \"Reciprocating Natural Gas Compressor\",\n      \"Separator\",\n      \"Tank\",\n      \"Open Well Casing Vent\",\n      \"Piping\",\n      \"Well\",\n      \"Other\",\n    ],\n    \"component_at_source\": [\n      \"Valve\",\n      \"Connector\",\n      \"Flange\",\n      \"Fitting - pressure meter/gauge\",\n      \"Fitting - not pressure meter/gauge\",\n      \"Open-ended line\",\n      \"Plug\",\n      \"Pressure relief device\",\n      \"Stuffing box\",\n      \"Other\",\n    ],\n\n    # Landfill\n    \"emission_identified_flag_fk\": [\n      \"Operator was aware of the leak prior to receiving the CARB plume notification\",\n      \"Operator detected a leak during follow-up monitoring after receipt of the CARB plume notification\",\n      \"No leak was detected\",\n    ],\n    \"emission_type_fk\": [\n      \"Not applicable as no leak was detected\",\n      \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\",\n      \"An unintentional leak  (i.e., the operator was not aware of, and could be repaired if discovered)\",\n      \"An intentional or allowable vent (i.e., the operator was aware of, and/or would not repair)\",\n      \"Due to a temporary activity (i.e., would be resolved without corrective action when the activity is complete)\",\n    ],\n    \"emission_location\": [\n      \"Not applicable as no leak was detected\",\n      \"Gas Collection System Component (e.g., blower, well, valve, port)\",\n      \"Gas Control Device/Control System Component\",\n      \"Landfill Surface: Daily Cover\",\n      \"Landfill Surface: Final Cover\",\n      \"Landfill Surface: Intermediate Cover\",\n      \"Leachate Management System\",\n      \"Working Face (area where active filling was being conducted at the time of detection)\",\n    ],\n    \"emission_cause\": [\n      \"Not applicable as no leak was detected\",\n      \"Collection system downtime\",\n      \"Construction - New Well Installation\",\n      \"Construction - Well Raising or Horizontal Extension\",\n      \"Cover integrity\",\n      \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\",\n      \"Cracked/Broken Seal\",\n      \"Damaged component\",\n      \"Insufficient vacuum\",\n      \"Offline Gas Collection Well(s)\",\n      \"Other\",\n      \"Uncontrolled Area (no gas collection infrastructure)\",\n    ],\n    \"emission_cause_secondary\": [\n      \"Not applicable as no leak was detected\",\n      \"Not applicable as no additional leak cause suspected\",\n      \"Collection system downtime\",\n      \"Construction - New Well Installation\",\n      \"Construction - Well Raising or Horizontal Extension\",\n      \"Cover integrity\",\n      \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\",\n      \"Cracked/Broken Seal\",\n      \"Damaged component\",\n      \"Insufficient vacuum\",\n      \"Offline Gas Collection Well(s)\",\n      \"Other\",\n      \"Uncontrolled Area (no gas collection infrastructure)\",\n    ],\n    \"emission_cause_tertiary\": [\n      \"Not applicable as no leak was detected\",\n      \"Not applicable as no additional leak cause suspected\",\n      \"Collection system downtime\",\n      \"Construction - New Well Installation\",\n      \"Construction - Well Raising or Horizontal Extension\",\n      \"Cover integrity\",\n      \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\",\n      \"Cracked/Broken Seal\",\n      \"Damaged component\",\n      \"Insufficient vacuum\",\n      \"Offline Gas Collection Well(s)\",\n      \"Other\",\n      \"Uncontrolled Area (no gas collection infrastructure)\",\n    ],\n\n    \"included_in_last_lmr\": [\n      \"Yes\",\n      \"No\",\n    ],\n    \"planned_for_next_lmr\": [\n      \"Yes\",\n      \"No\",\n    ],\n\n  }\n\n  # keys to the contingent dropdowns follow the patter html_selector2_contingent_on_html_selector1\n  # for instance, emission_cause_contingent_on_emission_location means that the choices for\n  # emission_cause are based on a lookup of the value of emission_location\n\n  drop_downs_contingent = {\n    \"emission_cause_contingent_on_emission_location\": {\n      \"Gas Collection System Component (e.g., blower, well, valve, port)\": [\n        \"Construction - New Well Installation\",\n        \"Construction - Well Raising or Horizontal Extension\",\n        \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\",\n        \"Damaged component\",\n        \"Insufficient vacuum\",\n        \"Offline Gas Collection Well(s)\",\n        \"Other\",\n      ],\n      \"Gas Control Device/Control System Component\": [\n        \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\",\n        \"Damaged component\",\n        \"Other\",\n      ],\n      \"Landfill Surface: Daily Cover\": [\n        \"Collection system downtime\",\n        \"Construction - New Well Installation\",\n        \"Construction - Well Raising or Horizontal Extension\",\n        \"Cover integrity\",\n        \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\",\n        \"Cracked/Broken Seal\",\n        \"Damaged component\",\n        \"Insufficient vacuum\",\n        \"Offline Gas Collection Well(s)\",\n        \"Other\",\n        \"Uncontrolled Area (no gas collection infrastructure)\",\n      ],\n      \"Landfill Surface: Intermediate Cover\": [\n        \"Collection system downtime\",\n        \"Construction - New Well Installation\",\n        \"Cover integrity\",\n        \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\",\n        \"Cracked/Broken Seal\",\n        \"Damaged component\",\n        \"Insufficient vacuum\",\n        \"Offline Gas Collection Well(s)\",\n        \"Other\",\n        \"Uncontrolled Area (no gas collection infrastructure)\",\n      ],\n      \"Landfill Surface: Final Cover\": [\n        \"Collection system downtime\",\n        \"Construction - New Well Installation\",\n        \"Construction - Well Raising or Horizontal Extension\",\n        \"Cover integrity\",\n        \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\",\n        \"Cracked/Broken Seal\",\n        \"Damaged component\",\n        \"Insufficient vacuum\",\n        \"Offline Gas Collection Well(s)\",\n        \"Other\",\n        \"Uncontrolled Area (no gas collection infrastructure)\",\n      ],\n      \"Leachate Management System\": [\n        \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\",\n        \"Damaged component\",\n        \"Offline Gas Collection Well(s)\",\n        \"Other\",\n      ],\n      \"Working Face (area where active filling was being conducted at the time of detection)\": [\n        \"Construction - New Well Installation\",\n        \"Construction - Well Raising or Horizontal Extension\",\n        \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\",\n        \"Offline Gas Collection Well(s)\",\n        \"Other\",\n        \"Uncontrolled Area (no gas collection infrastructure)\",\n      ],\n    },\n  }\n\n  # Note, the drop_downs get \"Please Select\" prepended, but the drop_down_contingent content is not modified\n  drop_downs = update_selector_dict(drop_downs)\n  return drop_downs, drop_downs_contingent\n</code></pre>"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_landfill_dummy_data","title":"<code>get_landfill_dummy_data()</code>","text":"<p>Generate dummy Landfill form data as a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Pre-filled key/value pairs used to populate a feedback form.</p> <p>Notes: - It is valid, but not necessary to specify 'Please Select' in dummy data.</p> Source code in <code>arb\\portal\\db_hardcoded.py</code> <pre><code>def get_landfill_dummy_data() -&gt; dict:\n  \"\"\"\n  Generate dummy Landfill form data as a dictionary.\n\n  Returns:\n    dict: Pre-filled key/value pairs used to populate a feedback form.\n\n  Notes:\n  - It is valid, but not necessary to specify 'Please Select' in dummy data.\n  \"\"\"\n  logger.debug(f\"in landfill_dummy_data()\")\n\n  json_data = {\n    \"additional_activities\": \"additional_activities\",\n    \"additional_notes\": \"additional_notes\",\n    \"contact_email\": \"my_email@email.com\",\n    \"contact_name\": \"contact_name\",\n    \"contact_phone\": f\"(555) 555-5555\",\n    # \"emission_cause\": PLEASE_SELECT,\n    \"emission_cause_notes\": \"emission_cause_notes\",\n    # \"emission_cause_secondary\": PLEASE_SELECT,\n    # \"emission_cause_tertiary\": PLEASE_SELECT,\n    # \"emission_identified_flag_fk\": PLEASE_SELECT,\n    # \"emission_location\": PLEASE_SELECT,\n    \"emission_location_notes\": \"emission_location_notes\",\n    # \"emission_type_fk\": PLEASE_SELECT,\n    \"facility_name\": \"facility_name\",\n    \"id_arb_swis\": \"id_arb_swis\",\n    \"id_incidence\": 2002,\n    \"id_message\": \"id_message\",\n    \"id_plume\": 1002,\n    # \"included_in_last_lmr\": PLEASE_SELECT,\n    \"included_in_last_lmr_description\": \"included_in_last_lmr_description\",\n    \"initial_leak_concentration\": 1002.5,\n    \"inspection_timestamp\": datetime.datetime.now().replace(second=0, microsecond=0),\n    \"instrument\": \"instrument\",\n    \"last_component_leak_monitoring_timestamp\": datetime.datetime.now().replace(second=0, microsecond=0),\n    \"last_surface_monitoring_timestamp\": datetime.datetime.now().replace(second=0, microsecond=0),\n    \"lat_carb\": 102.5,\n    \"lat_revised\": 103.5,\n    \"long_carb\": 104.5,\n    \"long_revised\": 105.5,\n    \"mitigation_actions\": \"mitigation_actions\",\n    \"mitigation_timestamp\": datetime.datetime.now().replace(second=0, microsecond=0),\n    \"observation_timestamp\": datetime.datetime.now().replace(second=0, microsecond=0),\n    # \"planned_for_next_lmr\": PLEASE_SELECT,\n    \"planned_for_next_lmr_description\": \"planned_for_next_lmr_description\",\n    \"re_monitored_concentration\": 1002.5,\n    \"re_monitored_timestamp\": datetime.datetime.now().replace(second=0, microsecond=0),\n\n    \"sector\": \"Landfill\",\n    \"sector_type\": \"Landfill\",\n  }\n  return json_data\n</code></pre>"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_og_dummy_data","title":"<code>get_og_dummy_data()</code>","text":"<p>Generate dummy Oil &amp; Gas form data as a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Pre-filled key/value pairs used to populate a feedback form.</p> <p>Notes: - It is valid, but not necessary to specify 'Please Select' in dummy data.</p> Source code in <code>arb\\portal\\db_hardcoded.py</code> <pre><code>def get_og_dummy_data() -&gt; dict:\n  \"\"\"\n  Generate dummy Oil &amp; Gas form data as a dictionary.\n\n  Returns:\n    dict: Pre-filled key/value pairs used to populate a feedback form.\n\n  Notes:\n  - It is valid, but not necessary to specify 'Please Select' in dummy data.\n  \"\"\"\n\n  json_data = {\n    \"id_incidence\": 2001,\n    \"id_plume\": 1001,\n    \"observation_timestamp\": datetime.datetime.now().replace(second=0, microsecond=0),\n    \"lat_carb\": 100.05,\n    \"long_carb\": 100.06,\n    \"id_message\": \"id_message response\",\n    \"facility_name\": \"facility_name response\",\n    \"id_arb_eggrt\": \"1001\",\n    \"contact_name\": \"contact_name response\",\n    \"contact_phone\": f\"(555) 555-5555\",\n    \"contact_email\": \"my_email@email.com\",\n    # \"venting_exclusion\": PLEASE_SELECT,\n    \"venting_description_1\": \"venting_description_1 response\",\n    # \"ogi_performed\": PLEASE_SELECT,\n    \"ogi_date\": datetime.datetime.now().replace(second=0, microsecond=0),\n    # \"ogi_result\": PLEASE_SELECT,\n    # \"method21_performed\": PLEASE_SELECT,\n    \"method21_date\": datetime.datetime.now().replace(second=0, microsecond=0),\n    # \"method21_result\": PLEASE_SELECT,\n    \"initial_leak_concentration\": 1004,\n    \"venting_description_2\": \"venting_description_2 response\",\n    \"initial_mitigation_plan\": \"initial_mitigation_plan response\",\n    # \"equipment_at_source\": PLEASE_SELECT,\n    \"equipment_other_description\": \"equipment_other_description response\",\n    # \"component_at_source\": PLEASE_SELECT,\n    \"component_other_description\": \"component_other_description response\",\n    \"repair_timestamp\": datetime.datetime.now().replace(second=0, microsecond=0),\n    \"final_repair_concentration\": 101.05,\n    \"repair_description\": \"repair_description response\",\n    \"additional_notes\": \"additional_notes response\",\n\n    \"sector\": \"Oil &amp; Gas\",\n    \"sector_type\": \"Oil &amp; Gas\",\n  }\n\n  return json_data\n</code></pre>"},{"location":"reference/arb/portal/extensions/","title":"<code>arb.portal.extensions</code>","text":"<p>Centralized definition of Flask extension instances used throughout the portal.</p> <p>This module avoids circular imports by creating extension objects (e.g., <code>db</code>, <code>csrf</code>) at the top level, without initializing them until <code>app.init_app()</code> is called elsewhere.</p> Extensions Defined <ul> <li>db (SQLAlchemy): SQLAlchemy instance shared across all models and routes.</li> <li>csrf (CSRFProtect): CSRF protection used for form validation.</li> </ul> Notes <ul> <li><code>geoalchemy2.Geometry</code> must be imported for spatial field introspection,   even if not directly referenced in code.</li> <li>Use <code>with app.app_context():</code> when accessing <code>db</code> outside a Flask route.</li> </ul> Example <p>from arb.portal.extensions import db with app.app_context(): ...     db.create_all()</p>"},{"location":"reference/arb/portal/extensions/#arb.portal.extensions.csrf","title":"<code>csrf = CSRFProtect()</code>  <code>module-attribute</code>","text":"<p>CSRFProtect: Flask-WTF extension for CSRF form protection.</p>"},{"location":"reference/arb/portal/extensions/#arb.portal.extensions.db","title":"<code>db = SQLAlchemy()</code>  <code>module-attribute</code>","text":"<p>SQLAlchemy: Flask SQLAlchemy instance for managing ORM and schema.</p>"},{"location":"reference/arb/portal/globals/","title":"<code>arb.portal.globals</code>","text":"<p>Global variables and dropdown selector loading for Flask/SQLAlchemy applications.</p> <p>This module provides the <code>Globals</code> class for holding runtime-initialized data structures such as dropdown selectors and database column type mappings.</p> Primary Uses <ul> <li>Prevent circular imports in SQLAlchemy/Flask environments</li> <li>Store shared type and dropdown definitions used throughout the app</li> <li>Enable lazy initialization of values dependent on app context</li> </ul> Notes <ul> <li>Globals are not intended to be mutable after initialization.</li> <li>Centralizes dropdown and type mapping logic for app-wide reuse.</li> <li>Static values that do not require runtime context should live in <code>constants.py</code>.</li> </ul>"},{"location":"reference/arb/portal/globals/#arb.portal.globals.Globals","title":"<code>Globals</code>","text":"<p>Central class for holding runtime-global mappings used in the Flask app.</p> <p>Attributes:</p> Name Type Description <code>db_column_types</code> <code>dict[str, dict[str, dict[str, Any]]]</code> <p>Mapping of table.column to SQLAlchemy type metadata (includes <code>db_type</code>, <code>sa_type</code>, <code>py_type</code>).</p> <code>drop_downs</code> <code>dict[str, list[str]]</code> <p>Field name to independent dropdown options.</p> <code>drop_downs_contingent</code> <code>dict[str, dict[str, list[str]]]</code> <p>Parent-dependent options for contingent dropdowns (e.g., county \u2192 list of sub-counties).</p> Source code in <code>arb\\portal\\globals.py</code> <pre><code>class Globals:\n  \"\"\"\n  Central class for holding runtime-global mappings used in the Flask app.\n\n  Attributes:\n    db_column_types (dict[str, dict[str, dict[str, Any]]]): Mapping of table.column\n      to SQLAlchemy type metadata (includes `db_type`, `sa_type`, `py_type`).\n    drop_downs (dict[str, list[str]]): Field name to independent dropdown options.\n    drop_downs_contingent (dict[str, dict[str, list[str]]]): Parent-dependent options\n      for contingent dropdowns (e.g., county \u2192 list of sub-counties).\n  \"\"\"\n\n  db_column_types = {}\n  drop_downs = {}\n  drop_downs_contingent = {}\n\n  @classmethod\n  def load_drop_downs(cls, flask_app: Flask, db: SQLAlchemy) -&gt; None:\n    \"\"\"\n    Load dropdown data from hardcoded configuration and cache it globally.\n\n    Args:\n      flask_app (Flask): The active Flask app instance.\n      db (SQLAlchemy): SQLAlchemy instance (not used in this function but passed for consistency).\n\n    Returns:\n      None\n\n    Notes:\n      - Uses `get_excel_dropdown_data()` from `db_hardcoded` to populate form options.\n      - Populates both `Globals.drop_downs` and `Globals.drop_downs_contingent`.\n      - Should be called once after app startup or reflection.\n    \"\"\"\n\n    from arb.portal.db_hardcoded import get_excel_dropdown_data\n\n    logger.debug(\"In load_drop_downs()\")\n\n    Globals.drop_downs, Globals.drop_downs_contingent = get_excel_dropdown_data()\n\n    logger.debug(f\"Globals.drop_downs={Globals.drop_downs}\")\n    logger.debug(f\"Globals.drop_downs_contingent={Globals.drop_downs_contingent}\")\n\n  @classmethod\n  def load_type_mapping(cls, flask_app: Flask, db: SQLAlchemy, base) -&gt; None:\n    \"\"\"\n    Populate column type metadata for all reflected tables in the SQLAlchemy base.\n\n    Args:\n      flask_app (Flask): The current Flask application (used for context scoping).\n      db (SQLAlchemy): SQLAlchemy instance, already bound to a live database engine.\n      base (AutomapBase): Reflected SQLAlchemy metadata containing all mapped models.\n\n    Returns:\n      None\n\n    Example:\n      &gt;&gt;&gt; Globals.load_type_mapping(app, db, base)\n      &gt;&gt;&gt; Globals.db_column_types['incidences']['id_plume']\n      {`db_type`: `INTEGER`, `sa_type`: Integer, `py_type`: &lt;class 'int'&gt;}\n\n    Notes:\n      - Uses `arb.utils.sql_alchemy.get_sa_automap_types()` for reflection.\n      - The resulting `Globals.db_column_types` is used in form pre-population and validation.\n    \"\"\"\n\n    from arb.utils.sql_alchemy import get_sa_automap_types\n\n    with flask_app.app_context():\n      engine = db.engine\n      Globals.db_column_types = get_sa_automap_types(engine, base)\n\n    logger.debug(f\"Database type mapping: Globals.db_column_types={Globals.db_column_types}\")\n</code></pre>"},{"location":"reference/arb/portal/globals/#arb.portal.globals.Globals.load_drop_downs","title":"<code>load_drop_downs(flask_app, db)</code>  <code>classmethod</code>","text":"<p>Load dropdown data from hardcoded configuration and cache it globally.</p> <p>Parameters:</p> Name Type Description Default <code>flask_app</code> <code>Flask</code> <p>The active Flask app instance.</p> required <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy instance (not used in this function but passed for consistency).</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <ul> <li>Uses <code>get_excel_dropdown_data()</code> from <code>db_hardcoded</code> to populate form options.</li> <li>Populates both <code>Globals.drop_downs</code> and <code>Globals.drop_downs_contingent</code>.</li> <li>Should be called once after app startup or reflection.</li> </ul> Source code in <code>arb\\portal\\globals.py</code> <pre><code>@classmethod\ndef load_drop_downs(cls, flask_app: Flask, db: SQLAlchemy) -&gt; None:\n  \"\"\"\n  Load dropdown data from hardcoded configuration and cache it globally.\n\n  Args:\n    flask_app (Flask): The active Flask app instance.\n    db (SQLAlchemy): SQLAlchemy instance (not used in this function but passed for consistency).\n\n  Returns:\n    None\n\n  Notes:\n    - Uses `get_excel_dropdown_data()` from `db_hardcoded` to populate form options.\n    - Populates both `Globals.drop_downs` and `Globals.drop_downs_contingent`.\n    - Should be called once after app startup or reflection.\n  \"\"\"\n\n  from arb.portal.db_hardcoded import get_excel_dropdown_data\n\n  logger.debug(\"In load_drop_downs()\")\n\n  Globals.drop_downs, Globals.drop_downs_contingent = get_excel_dropdown_data()\n\n  logger.debug(f\"Globals.drop_downs={Globals.drop_downs}\")\n  logger.debug(f\"Globals.drop_downs_contingent={Globals.drop_downs_contingent}\")\n</code></pre>"},{"location":"reference/arb/portal/globals/#arb.portal.globals.Globals.load_type_mapping","title":"<code>load_type_mapping(flask_app, db, base)</code>  <code>classmethod</code>","text":"<p>Populate column type metadata for all reflected tables in the SQLAlchemy base.</p> <p>Parameters:</p> Name Type Description Default <code>flask_app</code> <code>Flask</code> <p>The current Flask application (used for context scoping).</p> required <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy instance, already bound to a live database engine.</p> required <code>base</code> <code>AutomapBase</code> <p>Reflected SQLAlchemy metadata containing all mapped models.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>Globals.load_type_mapping(app, db, base) Globals.db_column_types['incidences']['id_plume'] {<code>db_type</code>: <code>INTEGER</code>, <code>sa_type</code>: Integer, <code>py_type</code>: } Notes <ul> <li>Uses <code>arb.utils.sql_alchemy.get_sa_automap_types()</code> for reflection.</li> <li>The resulting <code>Globals.db_column_types</code> is used in form pre-population and validation.</li> </ul> Source code in <code>arb\\portal\\globals.py</code> <pre><code>@classmethod\ndef load_type_mapping(cls, flask_app: Flask, db: SQLAlchemy, base) -&gt; None:\n  \"\"\"\n  Populate column type metadata for all reflected tables in the SQLAlchemy base.\n\n  Args:\n    flask_app (Flask): The current Flask application (used for context scoping).\n    db (SQLAlchemy): SQLAlchemy instance, already bound to a live database engine.\n    base (AutomapBase): Reflected SQLAlchemy metadata containing all mapped models.\n\n  Returns:\n    None\n\n  Example:\n    &gt;&gt;&gt; Globals.load_type_mapping(app, db, base)\n    &gt;&gt;&gt; Globals.db_column_types['incidences']['id_plume']\n    {`db_type`: `INTEGER`, `sa_type`: Integer, `py_type`: &lt;class 'int'&gt;}\n\n  Notes:\n    - Uses `arb.utils.sql_alchemy.get_sa_automap_types()` for reflection.\n    - The resulting `Globals.db_column_types` is used in form pre-population and validation.\n  \"\"\"\n\n  from arb.utils.sql_alchemy import get_sa_automap_types\n\n  with flask_app.app_context():\n    engine = db.engine\n    Globals.db_column_types = get_sa_automap_types(engine, base)\n\n  logger.debug(f\"Database type mapping: Globals.db_column_types={Globals.db_column_types}\")\n</code></pre>"},{"location":"reference/arb/portal/json_update_util/","title":"<code>arb.portal.json_update_util</code>","text":"<p>Utility functions to apply updates to a SQLAlchemy model's JSON field and log each change to the portal_updates table for auditing purposes.</p> Features <ul> <li>Compares current vs. new values in a model's JSON field</li> <li>Logs only meaningful changes to a structured audit table</li> <li>Excludes no-op or default placeholders (e.g., None, \"\")</li> </ul> Typical Use <p>Called when a form submission modifies a feedback record, with changes applied to the model and written to the database via SQLAlchemy.</p>"},{"location":"reference/arb/portal/json_update_util/#arb.portal.json_update_util.apply_json_patch_and_log","title":"<code>apply_json_patch_and_log(model, updates, json_field='misc_json', user='anonymous', comments='')</code>","text":"<p>Apply updates to a model's JSON field and log each change in portal_updates.</p> <p>This function performs a key-by-key comparison between the current JSON field (<code>model.misc_json</code> by default) and the proposed <code>updates</code>. For each key where the value has changed:   - The field is updated   - The change is logged to <code>portal_updates</code> with a timestamp and user info   - Redundant or placeholder updates are skipped (e.g., None \u2192 None)</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SQLAlchemy model</code> <p>A SQLAlchemy ORM instance with a JSON column.</p> required <code>updates</code> <code>dict</code> <p>Dictionary of key-value updates to apply.</p> required <code>json_field</code> <code>str</code> <p>Name of the JSON field (default: 'misc_json').</p> <code>'misc_json'</code> <code>user</code> <code>str</code> <p>Identifier of the user performing the change (default: 'anonymous').</p> <code>'anonymous'</code> <code>comments</code> <code>str</code> <p>Optional comment for the log entry.</p> <code>''</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If the specified JSON field does not exist on the model.</p> Example <p>apply_json_patch_and_log(incidence, {\"status\": \"Resolved\"}, user=\"admin\")</p> Source code in <code>arb\\portal\\json_update_util.py</code> <pre><code>def apply_json_patch_and_log(model,\n                             updates: dict,\n                             json_field: str = \"misc_json\",\n                             user: str = \"anonymous\",\n                             comments: str = \"\") -&gt; None:\n  \"\"\"\n  Apply updates to a model's JSON field and log each change in portal_updates.\n\n  This function performs a key-by-key comparison between the current JSON field\n  (`model.misc_json` by default) and the proposed `updates`. For each key where\n  the value has changed:\n    - The field is updated\n    - The change is logged to `portal_updates` with a timestamp and user info\n    - Redundant or placeholder updates are skipped (e.g., None \u2192 None)\n\n  Args:\n    model (SQLAlchemy model): A SQLAlchemy ORM instance with a JSON column.\n    updates (dict): Dictionary of key-value updates to apply.\n    json_field (str): Name of the JSON field (default: 'misc_json').\n    user (str): Identifier of the user performing the change (default: 'anonymous').\n    comments (str): Optional comment for the log entry.\n\n  Returns:\n    None\n\n  Raises:\n    AttributeError: If the specified JSON field does not exist on the model.\n\n  Example:\n    &gt;&gt;&gt; apply_json_patch_and_log(incidence, {\"status\": \"Resolved\"}, user=\"admin\")\n  \"\"\"\n\n  # In the future, may want to handle new rows differently\n  json_data = getattr(model, json_field)\n  if json_data is None:\n    json_data = {}\n    is_new_row = True\n  else:\n    is_new_row = False\n\n  # Consistency check\n  if \"id_incidence\" in json_data and json_data[\"id_incidence\"] != model.id_incidence:\n    logger.warning(f\"[apply_json_patch_and_log] MISMATCH: model.id_incidence={model.id_incidence} \"\n                   f\"!= misc_json['id_incidence']={json_data['id_incidence']}\")\n\n  # Remove id_incidence from updates to avoid contaminating misc_json\n  if \"id_incidence\" in updates:\n    if updates[\"id_incidence\"] != model.id_incidence:\n      logger.warning(f\"[json_update] Removing conflicting id_incidence from updates: \"\n                     f\"{updates['id_incidence']}\")\n      del updates[\"id_incidence\"]\n\n  for key, new_value in updates.items():\n\n    old_value = json_data.get(key)\n    json_data[key] = new_value\n\n    # Filter out non-useful updates\n    if old_value is None and new_value is None:\n      continue\n    if old_value is None and new_value == \"\":\n      continue\n    # Note, on the rare situation that \"Please Select\" is a valid entry in a string field, it will be filtered out\n    if old_value is None and new_value == PLEASE_SELECT:\n      continue\n\n    if old_value != new_value:\n      log_entry = PortalUpdate(\n        timestamp=datetime.datetime.now(datetime.UTC),\n        key=key,\n        old_value=str(old_value),\n        new_value=str(new_value),\n        user=user,\n        comments=comments or \"\",\n        id_incidence=model.id_incidence,\n      )\n      db.session.add(log_entry)\n\n  setattr(model, json_field, json_data)\n  flag_modified(model, json_field)\n  db.session.commit()\n</code></pre>"},{"location":"reference/arb/portal/routes/","title":"<code>arb.portal.routes</code>","text":"<p>Blueprint-based route definitions for the ARB Feedback Portal.</p> <p>This module defines all Flask routes originally found in <code>app.py</code>, now organized under the <code>main</code> Blueprint for modularity.</p> Routes cover <ul> <li>Incidence form creation, editing, and deletion</li> <li>File upload and viewing</li> <li>Portal update log display and export</li> <li>Diagnostics and developer views</li> </ul> Notes <ul> <li>All routes assume that <code>create_app()</code> registers the <code>main</code> Blueprint.</li> <li>Developer diagnostics are inlined near the end of the module.</li> </ul>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.diagnostics","title":"<code>diagnostics()</code>","text":"<p>Run diagnostics on the 'incidences' table and show next ID.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Rendered HTML showing auto-increment ID diagnostic.</p> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.route('/diagnostics')\ndef diagnostics() -&gt; str:\n  \"\"\"\n  Run diagnostics on the 'incidences' table and show next ID.\n\n  Returns:\n    str: Rendered HTML showing auto-increment ID diagnostic.\n  \"\"\"\n\n  logger.info(f\"diagnostics() called\")\n\n  result = find_auto_increment_value(db, \"incidences\", \"id_incidence\")\n\n  html_content = f\"&lt;p&gt;&lt;strong&gt;Diagnostic Results=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result}&lt;/p&gt;\"\n  return render_template('diagnostics.html',\n                         header=\"Auto-Increment Check\",\n                         subheader=\"Next available ID value in the 'incidences' table.\",\n                         html_content=html_content,\n                         modal_title=\"Success\",\n                         modal_message=\"Diagnostics completed successfully.\",\n                         )\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.export_portal_updates","title":"<code>export_portal_updates()</code>","text":"<p>Export filtered portal update logs as a downloadable CSV file.</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>CSV content as an attachment.</p> Notes <ul> <li>Respects filters set in the <code>/portal_updates</code> page.</li> <li>Uses standard CSV headers and UTF-8 encoding.</li> </ul> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.route(\"/portal_updates/export\")\ndef export_portal_updates() -&gt; Response:\n  \"\"\"\n  Export filtered portal update logs as a downloadable CSV file.\n\n  Returns:\n    Response: CSV content as an attachment.\n\n  Notes:\n    - Respects filters set in the `/portal_updates` page.\n    - Uses standard CSV headers and UTF-8 encoding.\n  \"\"\"\n  from arb.portal.sqla_models import PortalUpdate\n  from flask import request, Response\n  from io import StringIO\n  import csv\n\n  query = db.session.query(PortalUpdate)\n  query = apply_portal_update_filters(query, PortalUpdate, request.args)\n\n  updates = query.order_by(PortalUpdate.timestamp.desc()).all()\n\n  si = StringIO()\n  writer = csv.writer(si)\n  writer.writerow([\"timestamp\", \"key\", \"old_value\", \"new_value\", \"user\", \"comments\", \"id_incidence\"])\n\n  for u in updates:\n    writer.writerow([\n      u.timestamp,\n      u.key,\n      u.old_value,\n      u.new_value,\n      u.user,\n      u.comments,\n      u.id_incidence or \"\"\n    ])\n\n  return Response(\n    si.getvalue(),\n    mimetype=\"text/csv\",\n    headers={\"Content-Disposition\": \"attachment; filename=portal_updates_export.csv\"}\n  )\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.incidence_delete","title":"<code>incidence_delete(id_)</code>","text":"<p>Delete a specified incidence from the database.</p> <p>Parameters:</p> Name Type Description Default <code>id_</code> <code>int</code> <p>Primary key of the incidence to delete.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>Redirect to the homepage after deletion.</p> Notes <ul> <li>Future: consider adding authorization (e.g., CARB password) to restrict access.</li> </ul> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.post('/incidence_delete/&lt;int:id_&gt;/')\ndef incidence_delete(id_) -&gt; Response:\n  \"\"\"\n  Delete a specified incidence from the database.\n\n  Args:\n    id_ (int): Primary key of the incidence to delete.\n\n  Returns:\n    Response: Redirect to the homepage after deletion.\n\n  Notes:\n    - Future: consider adding authorization (e.g., CARB password) to restrict access.\n  \"\"\"\n\n  logger.debug(f\"Updating database with route incidence_delete for id= {id_}:\")\n  base: DeclarativeMeta = current_app.base  # type: ignore[attr-defined]\n\n  table_name = 'incidences'\n  table = get_class_from_table_name(base, table_name)\n  model_row = db.session.query(table).get_or_404(id_)\n\n  # todo - ensure portal changes are properly updated\n  arb.utils.sql_alchemy.delete_commit_and_log_model(db,\n                                                    model_row,\n                                                    comment=f'Deleting incidence row {id_}')\n  return redirect(url_for('main.index'))\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.incidence_update","title":"<code>incidence_update(id_)</code>","text":"<p>Display and edit a specific incidence record by ID.</p> <p>Parameters:</p> Name Type Description Default <code>id_</code> <code>int</code> <p>Primary key of the incidence to edit.</p> required <p>Returns:</p> Type Description <code>str | Response</code> <p>str|Response: Rendered HTML of the feedback form for the selected incidence,    or a redirect to the upload page if the ID is missing.</p> <p>Raises:</p> Type Description <code>500 Internal Server Error</code> <p>If multiple records are found for the same ID.</p> Notes <ul> <li>Redirects if the ID is not found in the database.</li> <li>Assumes each incidence ID is unique.</li> </ul> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.route('/incidence_update/&lt;int:id_&gt;/', methods=('GET', 'POST'))\ndef incidence_update(id_) -&gt; str | Response:\n  \"\"\"\n  Display and edit a specific incidence record by ID.\n\n  Args:\n    id_ (int): Primary key of the incidence to edit.\n\n  Returns:\n    str|Response: Rendered HTML of the feedback form for the selected incidence,\n         or a redirect to the upload page if the ID is missing.\n\n  Raises:\n    500 Internal Server Error: If multiple records are found for the same ID.\n\n  Notes:\n    - Redirects if the ID is not found in the database.\n    - Assumes each incidence ID is unique.\n  \"\"\"\n\n  logger.debug(f\"incidence_update called with id= {id_}.\")\n  base: DeclarativeMeta = current_app.base  # type: ignore[attr-defined]\n  table_name = 'incidences'\n  table = get_class_from_table_name(base, table_name)\n\n  # get_or_404 uses the tables primary key\n  # model_row = db.session.query(table).get_or_404(id_)\n  # todo turn this into a get and if it is null, then redirect? to the spreadsheet upload\n  # todo consider turning into one_or_none and have error handling\n  rows = db.session.query(table).filter_by(id_incidence=id_).all()\n  if not rows:\n    message = f\"A request was made to edit a non-existent id_incidence ({id_}).  Consider uploading the incidence by importing a spreadsheet.\"\n    return redirect(url_for('main.upload_file', message=message))\n  if len(rows) &gt; 1:\n    abort(500, description=f\"Multiple rows found for id={id_}\")\n  model_row = rows[0]\n\n  sector, sector_type = get_sector_info(db, base, id_)\n\n  logger.debug(f\"calling incidence_prep()\")\n  return incidence_prep(model_row,\n                        crud_type='update',\n                        sector_type=sector_type,\n                        default_dropdown=PLEASE_SELECT)\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.index","title":"<code>index()</code>","text":"<p>Display the homepage with a list of all existing incidence records.</p> <p>Queries the 'incidences' table in descending order of ID and renders the results in a summary table on the landing page.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Rendered HTML for the homepage with incidence records.</p> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.route('/')\ndef index() -&gt; str:\n  \"\"\"\n  Display the homepage with a list of all existing incidence records.\n\n  Queries the 'incidences' table in descending order of ID and renders\n  the results in a summary table on the landing page.\n\n  Returns:\n    str: Rendered HTML for the homepage with incidence records.\n  \"\"\"\n\n  base: DeclarativeMeta = current_app.base  # type: ignore[attr-defined]\n  table_name = 'incidences'\n  colum_name_pk = 'id_incidence'\n  rows = get_rows_by_table_name(db, base, table_name, colum_name_pk, ascending=False)\n\n  return render_template('index.html', model_rows=rows)\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.landfill_incidence_create","title":"<code>landfill_incidence_create()</code>","text":"<p>Create a new dummy Landfill incidence and redirect to its edit form.</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>Redirect to the <code>incidence_update</code> page for the newly created ID.</p> Notes <ul> <li>Dummy data is loaded from <code>db_hardcoded.get_landfill_dummy_data()</code>.</li> </ul> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.route('/landfill_incidence_create/', methods=('GET', 'POST'))\ndef landfill_incidence_create() -&gt; Response:\n  \"\"\"\n  Create a new dummy Landfill incidence and redirect to its edit form.\n\n  Returns:\n    Response: Redirect to the `incidence_update` page for the newly created ID.\n\n  Notes:\n    - Dummy data is loaded from `db_hardcoded.get_landfill_dummy_data()`.\n  \"\"\"\n\n  logger.debug(f\"landfill_incidence_create called.\")\n  base: DeclarativeMeta = current_app.base  # type: ignore[attr-defined]\n  table_name = 'incidences'\n  col_name = 'misc_json'\n\n  data_dict = arb.portal.db_hardcoded.get_landfill_dummy_data()\n\n  id_ = dict_to_database(db,\n                         base,\n                         data_dict,\n                         table_name=table_name,\n                         json_field=col_name,\n                         )\n\n  logger.debug(f\"landfill_incidence_create() - leaving.\")\n  return redirect(url_for('main.incidence_update', id_=id_))\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.list_uploads","title":"<code>list_uploads()</code>","text":"<p>List all files in the upload directory.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Rendered HTML showing all uploaded Excel files available on disk.</p> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.route('/list_uploads')\ndef list_uploads() -&gt; str:\n  \"\"\"\n  List all files in the upload directory.\n\n  Returns:\n    str: Rendered HTML showing all uploaded Excel files available on disk.\n  \"\"\"\n\n  logger.debug(f\"in list_uploads\")\n  upload_folder = current_app.config[\"UPLOAD_FOLDER\"]\n  # up_dir = Path(\"portal/static/uploads\")\n  # print(f\"{type(up_dir)=}: {up_dir=}\")\n  files = [x.name for x in upload_folder.iterdir() if x.is_file()]\n  logger.debug(f\"{files=}\")\n\n  return render_template('uploads_list.html', files=files)\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.og_incidence_create","title":"<code>og_incidence_create()</code>","text":"<p>Create a new dummy Oil &amp; Gas incidence and redirect to its edit form.</p> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>Redirect to the <code>incidence_update</code> page for the newly created ID.</p> Notes <ul> <li>Dummy data is loaded from <code>db_hardcoded.get_og_dummy_data()</code>.</li> </ul> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.route('/og_incidence_create/', methods=('GET', 'POST'))\ndef og_incidence_create() -&gt; Response:\n  \"\"\"\n  Create a new dummy Oil &amp; Gas incidence and redirect to its edit form.\n\n  Returns:\n    Response: Redirect to the `incidence_update` page for the newly created ID.\n\n  Notes:\n    - Dummy data is loaded from `db_hardcoded.get_og_dummy_data()`.\n  \"\"\"\n  logger.debug(f\"og_incidence_create() - beginning.\")\n  base: DeclarativeMeta = current_app.base  # type: ignore[attr-defined]\n  table_name = 'incidences'\n  col_name = 'misc_json'\n\n  data_dict = arb.portal.db_hardcoded.get_og_dummy_data()\n\n  id_ = dict_to_database(db,\n                         base,\n                         data_dict,\n                         table_name=table_name,\n                         json_field=col_name,\n                         )\n\n  logger.debug(f\"og_incidence_create() - leaving.\")\n  return redirect(url_for('main.incidence_update', id_=id_))\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.search","title":"<code>search()</code>","text":"<p>Search route triggered by the navigation bar (stub for future use).</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Renders a search results page showing the query string.</p> Notes <ul> <li>Currently echoes the user-submitted query string.</li> </ul> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.route('/search/', methods=('GET', 'POST'))\ndef search() -&gt; str:\n  \"\"\"\n  Search route triggered by the navigation bar (stub for future use).\n\n  Returns:\n    str: Renders a search results page showing the query string.\n\n  Notes:\n    - Currently echoes the user-submitted query string.\n  \"\"\"\n  logger.debug(f\"In search route:\")\n  logger.debug(f\"{request.form=}\")\n  search_string = request.form.get('navbar_search')\n  logger.debug(f\"{search_string=}\")\n\n  return render_template('search.html',\n                         search_string=search_string,\n                         )\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.serve_file","title":"<code>serve_file(filename)</code>","text":"<p>Serve a file from the server\u2019s upload directory.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the file to serve.</p> required <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>Sends file content to the browser or triggers download.</p> <p>Raises:</p> Type Description <code>404 Not Found</code> <p>If the file does not exist on disk.</p> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.route(\"/serve_file/&lt;path:filename&gt;\")\ndef serve_file(filename) -&gt; Response:\n  \"\"\"\n  Serve a file from the server\u2019s upload directory.\n\n  Args:\n    filename (str): Name of the file to serve.\n\n  Returns:\n    Response: Sends file content to the browser or triggers download.\n\n  Raises:\n    404 Not Found: If the file does not exist on disk.\n  \"\"\"\n\n  upload_folder = current_app.config[\"UPLOAD_FOLDER\"]\n  file_path = os.path.join(upload_folder, filename)\n\n  if not os.path.exists(file_path):\n    abort(404)\n\n  return send_from_directory(upload_folder, filename)\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_database_structure","title":"<code>show_database_structure()</code>","text":"<p>Show structure of all reflected database columns.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Rendered HTML with column type information for all tables.</p> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.route('/show_database_structure')\ndef show_database_structure() -&gt; str:\n  \"\"\"\n  Show structure of all reflected database columns.\n\n  Returns:\n    str: Rendered HTML with column type information for all tables.\n  \"\"\"\n\n  logger.info(f\"Displaying database structure\")\n  result = obj_to_html(Globals.db_column_types)\n  result = f\"&lt;p&gt;&lt;strong&gt;Postgres Database Structure=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result}&lt;/p&gt;\"\n  return render_template('diagnostics.html',\n                         header=\"Database Structure Overview\",\n                         subheader=\"Reflecting SQLAlchemy model metadata.\",\n                         html_content=result,\n                         )\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_dropdown_dict","title":"<code>show_dropdown_dict()</code>","text":"<p>Display current dropdown and contingent dropdown values.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Rendered HTML table of dropdown structures.</p> Notes <ul> <li>Useful for verifying dropdown contents used in WTForms.</li> </ul> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.route('/show_dropdown_dict')\ndef show_dropdown_dict() -&gt; str:\n  \"\"\"\n  Display current dropdown and contingent dropdown values.\n\n  Returns:\n    str: Rendered HTML table of dropdown structures.\n\n  Notes:\n    - Useful for verifying dropdown contents used in WTForms.\n  \"\"\"\n\n  logger.info(f\"Determining dropdown dict\")\n  # update drop-down tables\n  Globals.load_drop_downs(current_app, db)\n  result1 = obj_to_html(Globals.drop_downs)\n  result2 = obj_to_html(Globals.drop_downs_contingent)\n  result = (f\"&lt;p&gt;&lt;strong&gt;Globals.drop_downs=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result1}&lt;/p&gt;\"\n            f\"&lt;p&gt;&lt;strong&gt;Globals.drop_downs_contingent=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result2}&lt;/p&gt;\")\n  return render_template('diagnostics.html',\n                         header=\"Dropdown Dictionaries\",\n                         subheader=\"Loaded dropdown values and contingent mappings.\",\n                         html_content=result,\n                         )\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_feedback_form_structure","title":"<code>show_feedback_form_structure()</code>","text":"<p>Inspect and display WTForms structure for feedback forms.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Rendered HTML showing field names/types for OG and Landfill forms.</p> Notes <ul> <li>Uses <code>get_wtforms_fields()</code> utility to introspect each form.</li> </ul> Source code in <code>arb\\portal\\routes.py</code> <pre><code>@main.route('/show_feedback_form_structure')\ndef show_feedback_form_structure() -&gt; str:\n  \"\"\"\n  Inspect and display WTForms structure for feedback forms.\n\n  Returns:\n    str: Rendered HTML showing field names/types for OG and Landfill forms.\n\n  Notes:\n    - Uses `get_wtforms_fields()` utility to introspect each form.\n  \"\"\"\n\n  from arb.portal.wtf_landfill import LandfillFeedback\n  from arb.portal.wtf_oil_and_gas import OGFeedback\n  logger.info(f\"Displaying wtforms structure as a diagnostic\")\n\n  form1 = OGFeedback()\n  fields1 = get_wtforms_fields(form1)\n  result1 = obj_to_html(fields1)\n\n  form2 = LandfillFeedback()\n  fields2 = get_wtforms_fields(form2)\n  result2 = obj_to_html(fields2)\n\n  result = (f\"&lt;p&gt;&lt;strong&gt;WTF OGFeedback Form Structure=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result1}&lt;/p&gt;\"\n            f\"&lt;p&gt;&lt;strong&gt;WTF LandfillFeedback Form Structure=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;{result2}&lt;/p&gt;\")\n\n  return render_template('diagnostics.html',\n                         header=\"WTForms Feedback Form Structure\",\n                         subheader=\"Inspecting field mappings in Oil &amp; Gas and Landfill feedback forms.\",\n                         html_content=result,\n                         )\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_log_file","title":"<code>show_log_file()</code>","text":"<p>Display the contents of the server's current log file.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Rendered HTML with the full log file shown inside a <pre> block.\n              \n            \n          \n      \n    \n\n\n\n  Notes\n  <ul>\n<li>Useful for debugging in development or staging.</li>\n</ul>\n\n\n            \n              Source code in <code>arb\\portal\\routes.py</code>\n              <pre><code>@main.route('/show_log_file')\ndef show_log_file() -&gt; str:\n  \"\"\"\n  Display the contents of the server's current log file.\n\n  Returns:\n    str: Rendered HTML with the full log file shown inside a &lt;pre&gt; block.\n\n  Notes:\n    - Useful for debugging in development or staging.\n  \"\"\"\n\n  logger.info(f\"Displaying the log file as a diagnostic\")\n  with open(LOG_FILE, 'r') as file:\n    file_content = file.read()\n\n  # result = obj_to_html(file_content)\n  result = f\"&lt;p&gt;&lt;strong&gt;Logger file content=&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;pre&gt;{file_content}&lt;/pre&gt;&lt;/p&gt;\"\n  return render_template('diagnostics.html',\n                         header=\"Log File Contents\",\n                         # subheader=\"Full log output from the running server instance.\",\n                         html_content=result,\n                         )\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.upload_file","title":"<code>upload_file(message=None)</code>","text":"<p>Upload an Excel file and process its contents.</p>\n\n\n<p>Parameters:</p>\n    \n      \n        \n          Name\n          Type\n          Description\n          Default\n        \n      \n      \n          \n            \n                <code>message</code>\n            \n            \n                  <code>str | None</code>\n            \n            \n              \n                <p>Optional error/info message passed via redirect.</p>\n              \n            \n            \n                  <code>None</code>\n            \n          \n      \n    \n\n\n    <p>Returns:</p>\n    \n      \n        \n          Type\n          Description\n        \n      \n      \n          \n            \n                  <code>str | Response</code>\n            \n            \n              \n                <p>str | Response: Renders the upload form or redirects to incidence update.</p>\n              \n            \n          \n      \n    \n\n\n\n  Notes\n  <ul>\n<li>Supports drag-and-drop Excel upload.</li>\n<li>Catches and logs exceptions during upload and parsing.</li>\n</ul>\n\n\n            \n              Source code in <code>arb\\portal\\routes.py</code>\n              <pre><code>@main.route('/upload', methods=['GET', 'POST'])\n@main.route('/upload/&lt;message&gt;', methods=['GET', 'POST'])\ndef upload_file(message=None) -&gt; str | Response:\n  \"\"\"\n  Upload an Excel file and process its contents.\n\n  Args:\n    message (str | None): Optional error/info message passed via redirect.\n\n  Returns:\n    str | Response: Renders the upload form or redirects to incidence update.\n\n  Notes:\n    - Supports drag-and-drop Excel upload.\n    - Catches and logs exceptions during upload and parsing.\n  \"\"\"\n\n  logger.debug(\"upload_file route called.\")\n  base: DeclarativeMeta = current_app.base  # type: ignore[attr-defined]\n  form = UploadForm()\n\n  # Handle optional URL message\n  if message:\n    message = unquote(message)\n    logger.debug(f\"upload_file called with message: {message}\")\n\n  upload_dir = current_app.config['UPLOAD_FOLDER']\n  logger.debug(f\"Upload request with: {request.files=}, upload_dir={upload_dir}\")\n\n  if request.method == 'POST':\n    try:\n      if 'file' not in request.files or not request.files['file'].filename:\n        logger.warning(\"No file selected in POST request.\")\n        return render_template('upload.html', upload_message=\"No file selected. Please choose a file.\")\n\n      request_file = request.files['file']\n      logger.debug(f\"Received uploaded file: {request_file.filename}\")\n\n      if request_file:\n        # todo - little confusing how the update logic works cascading from xl to json, etc\n        #        consider making the steps and function names a little clearer to help the\n        #        update to change logging\n        file_name, id_, sector = upload_and_update_db(db, upload_dir, request_file, base)\n        logger.debug(f\"{sector=}\")\n\n        if id_:\n          logger.debug(f\"Upload successful: redirecting to incidence update for id={id_}\")\n          return redirect(url_for('main.incidence_update', id_=id_))\n        else:\n          logger.debug(f\"Upload did not match expected format: {file_name=}\")\n          return render_template('upload.html', form=form, upload_message=f\"Uploaded file: {file_name.name} \u2014 format not recognized.\")\n\n    except Exception as e:\n      logger.exception(\"Error occurred during file upload.\")\n      return render_template(\n        'upload.html',\n        upload_message=\"Error: Could not process the uploaded file. \"\n                       \"Make sure it is closed and try again.\"\n      )\n\n  # GET request\n  return render_template('upload.html', form=form, upload_message=message)\n</code></pre>"},{"location":"reference/arb/portal/routes/#arb.portal.routes.view_portal_updates","title":"<code>view_portal_updates()</code>","text":"<p>Display a table of all updates recorded in <code>portal_updates</code>.</p>\n\n\n    <p>Returns:</p>\n    \n      \n        \nName          Type\n          Description\n        \n      \n      \n          \n<code>str</code>            \n                  <code>str</code>\n            \n            \n              \n                <p>Rendered HTML with sortable and filterable update logs.</p>\n              \n            \n          \n      \n    \n\n\n\n  Notes\n  <ul>\n<li>Supports pagination, filtering, and sorting via query parameters.</li>\n<li>Default sort is descending by timestamp.</li>\n</ul>\n\n\n            \n              Source code in <code>arb\\portal\\routes.py</code>\n              <pre><code>@main.route(\"/portal_updates\")\ndef view_portal_updates() -&gt; str:\n  \"\"\"\n  Display a table of all updates recorded in `portal_updates`.\n\n  Returns:\n    str: Rendered HTML with sortable and filterable update logs.\n\n  Notes:\n    - Supports pagination, filtering, and sorting via query parameters.\n    - Default sort is descending by timestamp.\n  \"\"\"\n  from arb.portal.sqla_models import PortalUpdate\n  from flask import request, render_template\n\n  sort_by = request.args.get(\"sort_by\", \"timestamp\")\n  direction = request.args.get(\"direction\", \"desc\")\n  page = int(request.args.get(\"page\", 1))\n  per_page = int(request.args.get(\"per_page\", 100))\n\n  query = db.session.query(PortalUpdate)\n  query = apply_portal_update_filters(query, PortalUpdate, request.args)\n\n  updates = query.order_by(PortalUpdate.timestamp.desc()).all()\n\n  return render_template(\n    \"portal_updates.html\",\n    updates=updates,\n    sort_by=sort_by,\n    direction=direction,\n    page=page,\n    per_page=per_page,\n    total_pages=1,\n    filter_key=request.args.get(\"filter_key\", \"\").strip(),\n    filter_user=request.args.get(\"filter_user\", \"\").strip(),\n    filter_comments=request.args.get(\"filter_comments\", \"\").strip(),\n    filter_id_incidence=request.args.get(\"filter_id_incidence\", \"\").strip(),\n    start_date=request.args.get(\"start_date\", \"\").strip(),\n    end_date=request.args.get(\"end_date\", \"\").strip(),\n  )\n</code></pre>"},{"location":"reference/arb/portal/sqla_models/","title":"<code>arb.portal.sqla_models</code>","text":"<p>SQLAlchemy model definitions for the ARB Feedback Portal.</p> <p>This module defines ORM classes that map to key tables in the database, including uploaded file metadata and portal JSON update logs.</p> Notes <ul> <li>Only models explicitly defined here will be created by SQLAlchemy via <code>db.create_all()</code>.</li> <li>Most schema inspection and data access for <code>incidences</code> is handled dynamically via reflection.</li> <li>Timezone-aware UTC timestamps are used on all tracked models.</li> <li>All models inherit from <code>db.Model</code>, and can be directly queried with SQLAlchemy syntax.</li> </ul> Example <p>file = UploadedFile(path=\"uploads/report.xlsx\", status=\"pending\") db.session.add(file) db.session.commit()</p>"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.PortalUpdate","title":"<code>PortalUpdate</code>","text":"<p>               Bases: <code>Model</code></p> <p>SQLAlchemy model tracking updates to the misc_json field on incidence records.</p> <p>Used for auditing key/value changes made through the portal interface. Each row represents a single change to a single field on a specific incidence.</p> Table Name <p>portal_updates</p> Columns <p>id (int): Primary key. timestamp (datetime): UTC time when the change was logged. key (str): JSON key that was modified. old_value (str | None): Previous value (nullable). new_value (str): New value. user (str): Username or identifier of the user making the change. comments (str): Optional explanatory comment. id_incidence (int | None): Foreign key to the modified incidence (nullable).</p> Notes <ul> <li>Automatically populated by <code>apply_json_patch_and_log()</code>.</li> <li>Used for rendering the <code>portal_updates.html</code> table.</li> </ul> Source code in <code>arb\\portal\\sqla_models.py</code> <pre><code>class PortalUpdate(db.Model):\n  \"\"\"\n  SQLAlchemy model tracking updates to the misc_json field on incidence records.\n\n  Used for auditing key/value changes made through the portal interface. Each row\n  represents a single change to a single field on a specific incidence.\n\n  Table Name:\n    portal_updates\n\n  Columns:\n    id (int): Primary key.\n    timestamp (datetime): UTC time when the change was logged.\n    key (str): JSON key that was modified.\n    old_value (str | None): Previous value (nullable).\n    new_value (str): New value.\n    user (str): Username or identifier of the user making the change.\n    comments (str): Optional explanatory comment.\n    id_incidence (int | None): Foreign key to the modified incidence (nullable).\n\n  Notes:\n    - Automatically populated by `apply_json_patch_and_log()`.\n    - Used for rendering the `portal_updates.html` table.\n  \"\"\"\n\n  __tablename__ = \"portal_updates\"\n\n  id = Column(Integer, primary_key=True)\n  timestamp = Column(DateTime(timezone=True), nullable=False, server_default=func.now())\n\n  key = Column(String(255), nullable=False)\n  old_value = Column(Text, nullable=True)\n  new_value = Column(Text, nullable=False)\n  user = Column(String(255), nullable=False, default=\"anonymous\")\n  comments = Column(Text, nullable=False, default=\"\")\n  id_incidence = Column(Integer, nullable=True)\n\n  def __repr__(self):\n    return (\n      f\"&lt;PortalUpdate id={self.id} key={self.key!r} old={self.old_value!r} \"\n      f\"new={self.new_value!r} user={self.user!r} at={self.timestamp}&gt;\"\n    )\n</code></pre>"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.UploadedFile","title":"<code>UploadedFile</code>","text":"<p>               Bases: <code>Model</code></p> <p>SQLAlchemy model representing a user-uploaded file.</p> <p>Stores metadata for files uploaded via the portal interface, including the file path, status, and optional description. Automatically tracks creation and last modification timestamps.</p> Table Name <p>uploaded_files</p> Columns <p>id_ (int): Primary key. path (str): Filesystem path to the uploaded file. description (str | None): Optional human-friendly explanation. status (str | None): Upload status, e.g., 'pending', 'processed', or 'error'. created_timestamp (datetime): UTC timestamp of initial creation. modified_timestamp (datetime): UTC timestamp of last update.</p> Example <p>file = UploadedFile(path=\"uploads/test.xlsx\", status=\"pending\") db.session.add(file) db.session.commit()</p> Notes <ul> <li>Timestamps use UTC and are timezone-aware.</li> <li>This table is managed by SQLAlchemy directly (not introspected).</li> </ul> Source code in <code>arb\\portal\\sqla_models.py</code> <pre><code>class UploadedFile(db.Model):\n  \"\"\"\n    SQLAlchemy model representing a user-uploaded file.\n\n    Stores metadata for files uploaded via the portal interface, including\n    the file path, status, and optional description. Automatically tracks\n    creation and last modification timestamps.\n\n    Table Name:\n      uploaded_files\n\n    Columns:\n      id_ (int): Primary key.\n      path (str): Filesystem path to the uploaded file.\n      description (str | None): Optional human-friendly explanation.\n      status (str | None): Upload status, e.g., 'pending', 'processed', or 'error'.\n      created_timestamp (datetime): UTC timestamp of initial creation.\n      modified_timestamp (datetime): UTC timestamp of last update.\n\n    Example:\n      &gt;&gt;&gt; file = UploadedFile(path=\"uploads/test.xlsx\", status=\"pending\")\n      &gt;&gt;&gt; db.session.add(file)\n      &gt;&gt;&gt; db.session.commit()\n\n    Notes:\n      - Timestamps use UTC and are timezone-aware.\n      - This table is managed by SQLAlchemy directly (not introspected).\n    \"\"\"\n\n  __tablename__ = \"uploaded_files\"\n\n  id_ = db.Column(db.Integer, primary_key=True)\n  path = db.Column(db.Text, nullable=False)\n  description = db.Column(db.Text, nullable=True)\n  status = db.Column(db.Text, nullable=True)\n  created_timestamp = db.Column(\n    db.DateTime(timezone=True),\n    server_default=func.now()\n  )\n  modified_timestamp = db.Column(\n    db.DateTime(timezone=True),\n    server_default=func.now()\n  )\n\n  def __repr__(self) -&gt; str:\n    \"\"\"\n    Return a human-readable string representation of the uploaded file record.\n\n    Returns:\n        str: Summary string showing the ID, path, description, and status.\n\n    Example:\n        &gt;&gt;&gt; repr(UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\"))\n        '&lt;Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done&gt;'\n    \"\"\"\n    return (\n      f'&lt;Uploaded File: {self.id_}, Path: {self.path}, '\n      f'Description: {self.description}, Status: {self.status}&gt;'\n    )\n</code></pre>"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.UploadedFile.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a human-readable string representation of the uploaded file record.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Summary string showing the ID, path, description, and status.</p> Example <p>repr(UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\")) '' Source code in <code>arb\\portal\\sqla_models.py</code> <pre><code>def __repr__(self) -&gt; str:\n  \"\"\"\n  Return a human-readable string representation of the uploaded file record.\n\n  Returns:\n      str: Summary string showing the ID, path, description, and status.\n\n  Example:\n      &gt;&gt;&gt; repr(UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\"))\n      '&lt;Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done&gt;'\n  \"\"\"\n  return (\n    f'&lt;Uploaded File: {self.id_}, Path: {self.path}, '\n    f'Description: {self.description}, Status: {self.status}&gt;'\n  )\n</code></pre>"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.run_diagnostics","title":"<code>run_diagnostics()</code>","text":"<p>Run a test transaction to validate UploadedFile model functionality.</p> <p>This utility performs an insert, fetch, and rollback on the UploadedFile model to verify that the ORM mapping and database connection are working.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If database access or fetch fails.</p> Notes <ul> <li>Meant for developer use in test environments only.</li> <li>This function leaves no data in the database due to rollback.</li> <li>Logs diagnostic information using the project logger.</li> </ul> Source code in <code>arb\\portal\\sqla_models.py</code> <pre><code>def run_diagnostics() -&gt; None:\n  \"\"\"\n  Run a test transaction to validate UploadedFile model functionality.\n\n  This utility performs an insert, fetch, and rollback on the UploadedFile\n  model to verify that the ORM mapping and database connection are working.\n\n  Returns:\n    None\n\n  Raises:\n    RuntimeError: If database access or fetch fails.\n\n  Notes:\n    - Meant for developer use in test environments only.\n    - This function leaves no data in the database due to rollback.\n    - Logs diagnostic information using the project logger.\n  \"\"\"\n\n  logger.info(\"Running UploadedFile diagnostics...\")\n\n  try:\n    logger.debug(\"Beginning diagnostic transaction...\")\n    test_file = UploadedFile(\n      path=\"uploads/test_file.xlsx\",\n      description=\"Diagnostic test file\",\n      status=\"testing\"\n    )\n    db.session.add(test_file)\n    db.session.flush()  # Ensures test_file.id_ is populated\n\n    logger.info(f\"Inserted test file with ID: {test_file.id_}\")\n    fetched = UploadedFile.query.get(test_file.id_)\n\n    if fetched is None:\n      raise RuntimeError(\"Failed to retrieve inserted UploadedFile instance.\")\n\n    logger.info(f\"Fetched file: {fetched}\")\n    logger.debug(f\"repr: {repr(fetched)}\")\n    logger.debug(f\"created_timestamp: {fetched.created_timestamp}\")\n\n  except SQLAlchemyError as e:\n    logger.exception(\"SQLAlchemy error during diagnostics.\")\n    raise RuntimeError(\"Database error during UploadedFile diagnostics.\") from e\n\n  finally:\n    logger.debug(\"Rolling back diagnostic transaction.\")\n    db.session.rollback()\n    logger.info(\"Diagnostics completed and transaction rolled back.\")\n</code></pre>"},{"location":"reference/arb/portal/wtf_landfill/","title":"<code>arb.portal.wtf_landfill</code>","text":"<p>Landfill feedback form definition for the ARB Feedback Portal (WTForms).</p> <p>This module defines the <code>LandfillFeedback</code> class, a comprehensive WTForms-based HTML form for collecting information on methane emission inspections and responses at landfill sites. The form is organized into multiple logical sections and includes dynamic dropdown behavior, conditional validation, and cross-field logic.</p>"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill--key-features","title":"Key Features:","text":"<ul> <li>Uses <code>FlaskForm</code> as a base and is rendered using Bootstrap-compatible templates.</li> <li>Dropdowns support conditional dependencies using <code>Globals.drop_downs_contingent</code>.</li> <li>Validators are programmatically adjusted to enforce or relax constraints based on user input.</li> <li>Supports optional \"Other\" fields that are only required when triggered.</li> <li>Final validation is managed via a custom <code>validate()</code> override.</li> </ul>"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill--example-usage","title":"Example Usage:","text":"<p>form = LandfillFeedback()   form.process(request.form)</p> <p>if form.validate_on_submit():     # Process and store form data     save_landfill_feedback(form.data)</p>"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill--notes","title":"Notes:","text":"<ul> <li>The <code>update_contingent_selectors()</code> method updates selector/contingent choices.</li> <li>The <code>determine_contingent_fields()</code> method enforces dynamic field-level validation.</li> <li>Intended for use with the <code>landfill_incidence_update</code> route and similar flows.</li> <li>General-purpose WTForms utilities are located in:     arb.utils.wtf_forms_util.py</li> </ul>"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback","title":"<code>LandfillFeedback</code>","text":"<p>               Bases: <code>FlaskForm</code></p> <p>WTForms form class for collecting landfill feedback data.</p> <p>Captures user-submitted information about methane emissions, inspections, corrective actions, and contact details related to landfill facility operations.</p> Notes <ul> <li>Some fields are conditionally validated depending on selections.</li> <li>The form dynamically updates contingent dropdowns using   <code>update_contingent_selectors()</code>.</li> <li>Final validation is enforced in the <code>validate()</code> method.</li> </ul> Source code in <code>arb\\portal\\wtf_landfill.py</code> <pre><code>class LandfillFeedback(FlaskForm):\n  \"\"\"\n  WTForms form class for collecting landfill feedback data.\n\n  Captures user-submitted information about methane emissions,\n  inspections, corrective actions, and contact details related to\n  landfill facility operations.\n\n  Notes:\n    - Some fields are conditionally validated depending on selections.\n    - The form dynamically updates contingent dropdowns using\n      `update_contingent_selectors()`.\n    - Final validation is enforced in the `validate()` method.\n  \"\"\"\n\n  # Section 2\n  # todo - likely have to change these to InputRequired(), Optional(), blank and removed\n  # label = \"1.  Incidence/Emission ID\"\n  id_incidence = IntegerField(\n    \"Incidence/Emission ID\",\n    validators=[Optional()],\n    render_kw={\"readonly\": True}\n  )\n\n  label = \"2.  Plume ID(s)\"\n  id_plume = IntegerField(\n    label=label,\n    validators=[InputRequired(), NumberRange(min=1, message=\"Plume ID must be a positive integer\")],\n  )  # REFERENCES plumes (id_plume)\n\n  label = \"3.  Plume Observation Date\"\n  observation_timestamp = DateTimeLocalField(\n    label=label,\n    validators=[InputRequired()],\n    format=HTML_LOCAL_TIME_FORMAT,\n  )\n\n  label = \"4.  Plume Origin CARB Estimated Latitude\"\n  # I think lat/longs are failing because they were renamed ...\n\n  lat_carb = DecimalField(\n    label=label,\n    places=GPS_RESOLUTION,\n    # validators=[Optional(), NumberRange(**LATITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)],\n    validators=[Optional(), NumberRange(**LATITUDE_VALIDATION)],\n  )\n\n  label = \"5.  Plume Origin CARB Estimated Longitude\"\n  long_carb = DecimalField(\n    label=label,\n    places=GPS_RESOLUTION,\n    # validators=[Optional(), NumberRange(**LONGITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)],\n    validators=[Optional(), NumberRange(**LONGITUDE_VALIDATION)],\n  )\n\n  label = \"6.  CARB Message ID\"\n  id_message = StringField(\n    label=label,\n    validators=[Optional()],\n  )\n\n  # Section 3\n  label = \"Q1.  Facility Name\"\n  facility_name = StringField(\n    label=label,\n    validators=[InputRequired()],\n  )\n\n  label = \"Q2.  Facility SWIS ID\"\n  id_arb_swis = StringField(\n    label=label,\n    validators=[Optional()],\n  )\n\n  label = \"Q3.  Contact Name\"\n  contact_name = StringField(\n    label=label,\n    validators=[InputRequired()],\n  )\n  # contact_phone = StringField(label=\"Contact Phone\", validators=[InputRequired()])\n  label = \"Q4.  Contact Phone\"\n  message = \"Invalid phone number. Phone number must be in format '(123) 456-7890' or '(123) 456-7890 x1234567'.\"\n  contact_phone = StringField(\n    label=label,\n    validators=[InputRequired(),\n                Regexp(regex=r\"^\\(\\d{3}\\) \\d{3}-\\d{4}( x\\d{1,7})?$\", message=message)\n                ],\n  )\n\n  label = \"Q5.  Contact Email\"\n  contact_email = EmailField(\n    label=label,\n    validators=[InputRequired(), Email()])\n\n  # Section 4\n  label = \"Q6.  Date of owner/operator\u2019s follow-up ground monitoring.\"\n  inspection_timestamp = DateTimeLocalField(\n    label=label,\n    validators=[InputRequired(), ],\n    format=HTML_LOCAL_TIME_FORMAT,\n  )\n\n  label = \"Q7.  Instrument used to locate the leak (e.g., Fisher Scientific TVA2020; RKI Multigas Analyzer Eagle 2; TDL).\"\n  instrument = StringField(\n    label=label,\n    validators=[InputRequired()])\n\n  label = \"Q8.  Was a leak identified through prior knowledge or by follow-up monitoring after receipt of a CARB plume notice?\"\n  emission_identified_flag_fk = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"emission_identified_flag_fk\"],\n    validators=[InputRequired(), ],\n  )\n\n  label = (f\"Q9.  If no leaks were found, please describe any events or activities that may have \"\n           f\"contributed to the plume observed on the date provided in Section 2.\")\n  additional_activities = TextAreaField(\n    label=label,\n    validators=[Optional()],\n  )\n\n  # Section 5\n  label = \"Q10:  Maximum concentration of methane leak (in ppmv).\"\n  initial_leak_concentration = DecimalField(\n    label=label,\n    validators=[InputRequired()],\n  )\n\n  label = \"Q11.  Please provide a revised latitude if the leak location differs from CARB's estimate in Section 2.\"\n  lat_revised = DecimalField(\n    label=label,\n    places=GPS_RESOLUTION,\n    # validators=[Optional(), NumberRange(**LATITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)],\n    validators=[Optional(), NumberRange(**LATITUDE_VALIDATION)],\n  )\n\n  label = \"Q12.  Please provide a revised longitude if the leak location differs from CARB's estimate in Section 2.\"\n  long_revised = DecimalField(\n    label=label,\n    places=GPS_RESOLUTION,\n    # validators=[Optional(), NumberRange(**LONGITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)],\n    validators=[Optional(), NumberRange(**LONGITUDE_VALIDATION)],\n  )\n\n  label = \"Q13:  Please select from the drop-down menu which option best matches the description of the leak.\"\n  emission_type_fk = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"emission_type_fk\"],\n    validators=[InputRequired(), ],\n  )\n\n  label = \"Q14.  Please select from the drop-down menu which option best describes the location of the leak.\"\n  emission_location = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"emission_location\"],\n    validators=[InputRequired(), ],\n  )\n\n  label = (f\"Q15.  Please provide a more detailed description of the leak location, \"\n           f\"including grid ID number or component name, if applicable.\")\n  emission_location_notes = TextAreaField(\n    label=label,\n    validators=[],\n  )\n\n  label = \"Q16.  Please select the most likely cause of the leak.\"\n  emission_cause = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"emission_cause\"],\n    validators=[InputRequired(), ],\n  )\n\n  label = (f\"Q17 (Optional).  Please select an alternative cause (only if suspected).  \"\n           f\"This should not be the same as your Q16 response.\")\n  emission_cause_secondary = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"emission_cause_secondary\"],\n    validators=[Optional()],\n  )\n\n  label = (f\"Q18 (Optional).  Please select an alternative cause (only if suspected).  \"\n           f\"This should not be the same as your Q16 or Q17 responses.\")\n  emission_cause_tertiary = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"emission_cause_tertiary\"],\n    validators=[Optional()],\n  )\n\n  label = (f\"Q19.  Please provide a more detailed description of the cause(s), \"\n           f\"including the reason for and duration of any construction activity or downtime.\")\n  emission_cause_notes = TextAreaField(\n    label=label,\n    validators=[InputRequired()],\n  )\n\n  label = \"Q20.  Describe any corrective actions taken.\"\n  mitigation_actions = TextAreaField(\n    label=label,\n    validators=[InputRequired()],\n  )\n\n  label = \"Q21.  Repair date.\"\n  mitigation_timestamp = DateTimeLocalField(\n    label=label,\n    validators=[InputRequired()],\n    format=HTML_LOCAL_TIME_FORMAT\n  )\n\n  label = \"Q22.  Re-monitored date.\"\n  re_monitored_timestamp = DateTimeLocalField(\n    label=label,\n    validators=[Optional()],\n    format=HTML_LOCAL_TIME_FORMAT\n  )\n\n  label = \"Q23.  Re-monitored methane concentration after repair (ppmv).\"\n  re_monitored_concentration = DecimalField(\n    label=label,\n    validators=[InputRequired()],\n  )\n\n  label = (f\"Q24.  Was the leak location monitored in the most recent \"\n           f\"prior quarterly/annual surface emissions or quarterly component leak monitoring event?\")\n  included_in_last_lmr = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"included_in_last_lmr\"],\n    validators=[InputRequired(), ],\n  )\n\n  label = \"Q25.  If 'No' to Q24, please explain why the area was excluded from monitoring.\"\n  included_in_last_lmr_description = TextAreaField(\n    label=label,\n    validators=[InputRequired()])\n\n  label = \"Q26.  Is this grid/component planned for inclusion in the next quarterly/annual leak monitoring?\"\n  planned_for_next_lmr = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"planned_for_next_lmr\"],\n    validators=[InputRequired(), ],\n  )\n\n  label = \"Q27.  If 'No' to Q26, please state why the area will not be monitored.\"\n  planned_for_next_lmr_description = TextAreaField(\n    label=label,\n    validators=[InputRequired()])\n\n  label = \"Q28.  Date of most recent surface emissions monitoring event (prior to this notification).\"\n  last_component_leak_monitoring_timestamp = DateTimeLocalField(\n    label=label,\n    validators=[InputRequired()],\n    format=HTML_LOCAL_TIME_FORMAT\n  )\n\n  label = \"Q29.  Date of most recent component leak monitoring event (prior to this notification).\"\n  last_surface_monitoring_timestamp = DateTimeLocalField(\n    label=label,\n    validators=[InputRequired()],\n    format=HTML_LOCAL_TIME_FORMAT\n  )\n\n  label = \"Q30. Additional notes or comments.\"\n  additional_notes = TextAreaField(\n    label=label,\n    validators=[],\n  )\n\n  label = \"1. CARB internal notes\"\n  carb_notes = TextAreaField(\n    label=label,\n    validators=[],\n  )\n\n  def update_contingent_selectors(self) -&gt; None:\n    \"\"\"\n    Update contingent dropdown field choices based on current field selections.\n\n    This method looks up selector/contingent relationships defined in\n    `Globals.drop_downs_contingent` and dynamically modifies the `choices`\n    for child fields when a selector field has a known dependency.\n\n    This method dynamically updates the primary, secondary, and tertiary\n    emission cause fields based on the value of `self.emission_location`. It\n    ensures valid dropdown options and clears invalid selections.\n\n    Assumes:\n      - `self.emission_location`, `self.emission_cause`,\n        `self.emission_cause_secondary`, and `self.emission_cause_tertiary`\n        are all `SelectField` instances.\n      - `Globals.drop_downs_contingent` contains a nested dictionary of\n        location-contingent dropdown options.\n\n    Returns:\n      None\n    \"\"\"\n    # todo - update contingent dropdowns?\n\n    logger.debug(\"Running update_contingent_selectors()\")\n\n    emission_location = self.emission_location.data\n    logger.debug(f\"Selected emission_location: {emission_location!r}\")\n\n    emission_cause_dict = Globals.drop_downs_contingent.get(\n      \"emission_cause_contingent_on_emission_location\", {}\n    )\n    choices_raw = emission_cause_dict.get(emission_location, [])\n    logger.debug(f\"Available contingent causes: {choices_raw!r}\")\n\n    # Define headers\n    primary_header = [\n      (PLEASE_SELECT, PLEASE_SELECT, {\"disabled\": True}),\n      (\"Not applicable as no leak was detected\",\n       \"Not applicable as no leak was detected\", {}),\n    ]\n    secondary_tertiary_header = primary_header + [\n      (\"Not applicable as no additional leak cause suspected\",\n       \"Not applicable as no additional leak cause suspected\", {}),\n    ]\n\n    # Build full choices\n    primary_choices = build_choices(primary_header, choices_raw)\n    secondary_tertiary_choices = build_choices(secondary_tertiary_header, choices_raw)\n\n    # Update each field's choices\n    self.emission_cause.choices = primary_choices\n    self.emission_cause_secondary.choices = secondary_tertiary_choices\n    self.emission_cause_tertiary.choices = secondary_tertiary_choices\n\n  def validate(self, extra_validators=None) -&gt; bool:\n    \"\"\"\n    Override WTForms default validation with custom cross-field logic.\n\n    Ensures required fields are conditionally enforced based on upstream\n    values, including:\n      - Facility activity selections imply required contingent selections\n      - If \"Other\" is chosen, corresponding text input must be filled\n      - If leak is confirmed, additional emission details are required\n\n    Returns:\n      bool: True if form is valid, False otherwise.\n\n    Notes:\n      - Calls `determine_contingent_fields()` before validation to\n        ensure field validators are correct.\n      - Uses built-in `super().validate()` after adjusting validators.\n    \"\"\"\n\n    logger.debug(f\"validate() called.\")\n    form_fields = get_wtforms_fields(self)\n\n    # Dictionary to replace standard WTForm messages with an alternative message\n    error_message_replacement_dict = {\"Not a valid float value.\": \"Not a valid numeric value.\"}\n\n    ###################################################################################################\n    # Add, Remove, or Modify validation at a field level here before the super is called (for example)\n    ###################################################################################################\n    self.determine_contingent_fields()\n    self.update_contingent_selectors()\n\n    ###################################################################################################\n    # Set selectors with values not in their choices list to \"Please Select\"\n    ###################################################################################################\n    for field_name in form_fields:\n      field = getattr(self, field_name)\n      logger.debug(f\"field_name: {field_name}, {type(field.data)=}, {field.data=}, {type(field.raw_data)=}\")\n      if isinstance(field, SelectField):\n        ensure_field_choice(field_name, field)\n\n    ###################################################################################################\n    # call the super to perform each field's individual validation (which saves to form.errors)\n    # This will create the form.errors dictionary.  If there are form_errors they will be in the None key.\n    # The form_errors will not affect if validate returns True/False, only the fields are considered.\n    ###################################################################################################\n    # logger.debug(\"in the validator before super\")\n    obj_diagnostics(self, message=\"in the validator before super\")\n\n    super_return = super().validate(extra_validators=extra_validators)\n\n    ###################################################################################################\n    # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data\n    ###################################################################################################\n    validate_selectors(self, PLEASE_SELECT)\n\n    ###################################################################################################\n    # Perform any field level validation where one field is cross-referenced to another\n    # The error will be associated with one of the fields\n    ###################################################################################################\n    # todo - move field level validation to separate function\n\n    if self.emission_identified_flag_fk.data == \"No leak was detected\":\n      valid_options = [PLEASE_SELECT,\n                       \"Not applicable as no leak was detected\",\n                       \"Not applicable as no additional leak cause suspected\",\n                       ]\n      if self.emission_type_fk.data not in valid_options:\n        self.emission_type_fk.errors.append(f\"Q8 and Q13 appear to be inconsistent\")\n      if self.emission_location.data not in valid_options:\n        self.emission_location.errors.append(f\"Q8 and Q14 appear to be inconsistent\")\n      if self.emission_cause.data not in valid_options:\n        self.emission_cause.errors.append(f\"Q8 and Q16 appear to be inconsistent\")\n      if self.emission_cause_secondary.data not in valid_options:\n        self.emission_cause_secondary.errors.append(f\"Q8 and Q17 appear to be inconsistent\")\n      if self.emission_cause_tertiary.data not in valid_options:\n        self.emission_cause.errors.append(f\"Q8 and Q18 appear to be inconsistent\")\n\n    # Q8 and Q13 should be coupled to Operator-aware response\n    elif self.emission_identified_flag_fk.data == \"Operator was aware of the leak prior to receiving the CARB plume notification\":\n      valid_options = [PLEASE_SELECT,\n                       \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\", ]\n      if self.emission_type_fk.data not in valid_options:\n        self.emission_type_fk.errors.append(f\"Q8 and Q13 appear to be inconsistent\")\n\n    if self.emission_identified_flag_fk.data != \"No leak was detected\":\n      invalid_options = [\"Not applicable as no leak was detected\", ]\n      if self.emission_type_fk.data in invalid_options:\n        self.emission_type_fk.errors.append(f\"Q8 and Q13 appear to be inconsistent\")\n      if self.emission_location.data in invalid_options:\n        self.emission_location.errors.append(f\"Q8 and Q14 appear to be inconsistent\")\n      if self.emission_cause.data in invalid_options:\n        self.emission_cause.errors.append(f\"Q8 and Q16 appear to be inconsistent\")\n      if self.emission_cause_secondary.data in invalid_options:\n        self.emission_cause_secondary.errors.append(f\"Q8 and Q17 appear to be inconsistent\")\n      if self.emission_cause_tertiary.data in invalid_options:\n        self.emission_cause_tertiary.errors.append(f\"Q8 and Q18 appear to be inconsistent\")\n\n    if self.inspection_timestamp.data and self.mitigation_timestamp.data:\n      if self.mitigation_timestamp.data &lt; self.inspection_timestamp.data:\n        self.mitigation_timestamp.errors.append(\n          \"Date of mitigation cannot be prior to initial site inspection.\")\n\n    # todo - add that 2nd and 3rd can't be repeats\n    ignore_repeats = [PLEASE_SELECT,\n                      \"Not applicable as no leak was detected\",\n                      \"Not applicable as no additional leak cause suspected\",\n                      ]\n\n    if (self.emission_cause_secondary.data not in ignore_repeats and\n        self.emission_cause_secondary.data in [self.emission_cause.data]):\n      self.emission_cause_secondary.errors.append(f\"Q17 appears to be a repeat\")\n\n    if (self.emission_cause_tertiary.data not in ignore_repeats and\n        self.emission_cause_tertiary.data in [self.emission_cause.data, self.emission_cause_secondary.data]):\n      self.emission_cause_secondary.errors.append(f\"Q18 appears to be a repeat\")\n\n    # not sure if this test makes sense since they may have know about it prior to the plume (going to comment out)\n    # if self.observation_timestamp.data and self.inspection_timestamp.data:\n    #   if self.inspection_timestamp.data &lt; self.observation_timestamp.data:\n    #     self.inspection_timestamp.errors.append(\n    #       \"Date of inspection cannot be prior to date of initial plume observation.\")\n\n    ###################################################################################################\n    # perform any form level validation and append it to the form_errors property\n    # This may not be useful, but if you want to have form level errors appear at the top of the error\n    # header, put the logic here.\n    ###################################################################################################\n    # self.form_errors.append(\"I'm a form level error #1\")\n    # self.form_errors.append(\"I'm a form level error #2\")\n\n    ###################################################################################################\n    # Search and replace the error messages associated with input fields to a custom message\n    # For instance, the default 'float' error is changed because a typical user will not know what a\n    # float value is (they will be more comfortable with the word 'numeric')\n    ###################################################################################################\n    for field in form_fields:\n      field_errors = getattr(self, field).errors\n      replace_list_occurrences(field_errors, error_message_replacement_dict)\n\n    ###################################################################################################\n    # Current logic to determine if form is valid the error dict must be empty.\n    # #Consider other approaches\n    ###################################################################################################\n    form_valid = not bool(self.errors)\n\n    return form_valid\n\n  def determine_contingent_fields(self):\n    \"\"\"\n    Add or remove field validators depending on contingent dropdown selections.\n\n    Some dropdown options imply that no further input is needed (e.g.,\n    selecting \"No leak was detected\" disables required validation on\n    follow-up questions). This function clears or restores validators\n    accordingly.\n\n    These fields toggle between required and optional depending on related\n    field values (e.g., dropdowns that are set to \"Other\", or location-dependent fields).\n    Some validation rules involve mutually exclusive or fallback logic.\n\n    Notes:\n      - This function should be called before validation to sync requirements.\n      - May need to re-order exclusions (e.g., venting) for edge cases.\n    \"\"\"\n    # If a venting exclusion is claimed, then a venting description is required and many fields become optional\n    required_if_emission_identified = [\n      \"additional_activities\",\n      \"initial_leak_concentration\",\n      # \"lat_revised\",\n      # \"long_revised\",\n      \"emission_type_fk\",\n      \"emission_location\",\n      # \"emission_location_notes\",\n      \"emission_cause\",\n      # \"emission_cause_secondary\",\n      # \"emission_cause_tertiary\",\n      \"emission_cause_notes\",\n      \"mitigation_actions\",\n      \"mitigation_timestamp\",\n      \"re_monitored_timestamp\",\n      \"re_monitored_concentration\",\n      \"included_in_last_lmr\",\n      \"included_in_last_lmr_description\",\n      \"planned_for_next_lmr\",\n      \"planned_for_next_lmr_description\",\n      \"last_surface_monitoring_timestamp\",\n      \"last_component_leak_monitoring_timestamp\",\n      \"additional_notes\",\n    ]\n    # todo - update logic for new selectors\n    emission_identified_test = self.emission_identified_flag_fk.data != \"No leak was detected\"\n    # print(f\"{emission_identified_test=}\")\n    change_validators_on_test(self, emission_identified_test, required_if_emission_identified)\n\n    if emission_identified_test:\n      lmr_included_test = self.included_in_last_lmr.data == \"No\"\n      logger.debug(f\"{lmr_included_test=}\")\n      change_validators_on_test(self, lmr_included_test, [\"included_in_last_lmr_description\"])\n\n      lmr_planned_test = self.planned_for_next_lmr.data == \"No\"\n      logger.debug(\n        f\"{lmr_planned_test=}\")\n      change_validators_on_test(self, lmr_planned_test, [\"planned_for_next_lmr_description\"])\n</code></pre>"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback.determine_contingent_fields","title":"<code>determine_contingent_fields()</code>","text":"<p>Add or remove field validators depending on contingent dropdown selections.</p> <p>Some dropdown options imply that no further input is needed (e.g., selecting \"No leak was detected\" disables required validation on follow-up questions). This function clears or restores validators accordingly.</p> <p>These fields toggle between required and optional depending on related field values (e.g., dropdowns that are set to \"Other\", or location-dependent fields). Some validation rules involve mutually exclusive or fallback logic.</p> Notes <ul> <li>This function should be called before validation to sync requirements.</li> <li>May need to re-order exclusions (e.g., venting) for edge cases.</li> </ul> Source code in <code>arb\\portal\\wtf_landfill.py</code> <pre><code>def determine_contingent_fields(self):\n  \"\"\"\n  Add or remove field validators depending on contingent dropdown selections.\n\n  Some dropdown options imply that no further input is needed (e.g.,\n  selecting \"No leak was detected\" disables required validation on\n  follow-up questions). This function clears or restores validators\n  accordingly.\n\n  These fields toggle between required and optional depending on related\n  field values (e.g., dropdowns that are set to \"Other\", or location-dependent fields).\n  Some validation rules involve mutually exclusive or fallback logic.\n\n  Notes:\n    - This function should be called before validation to sync requirements.\n    - May need to re-order exclusions (e.g., venting) for edge cases.\n  \"\"\"\n  # If a venting exclusion is claimed, then a venting description is required and many fields become optional\n  required_if_emission_identified = [\n    \"additional_activities\",\n    \"initial_leak_concentration\",\n    # \"lat_revised\",\n    # \"long_revised\",\n    \"emission_type_fk\",\n    \"emission_location\",\n    # \"emission_location_notes\",\n    \"emission_cause\",\n    # \"emission_cause_secondary\",\n    # \"emission_cause_tertiary\",\n    \"emission_cause_notes\",\n    \"mitigation_actions\",\n    \"mitigation_timestamp\",\n    \"re_monitored_timestamp\",\n    \"re_monitored_concentration\",\n    \"included_in_last_lmr\",\n    \"included_in_last_lmr_description\",\n    \"planned_for_next_lmr\",\n    \"planned_for_next_lmr_description\",\n    \"last_surface_monitoring_timestamp\",\n    \"last_component_leak_monitoring_timestamp\",\n    \"additional_notes\",\n  ]\n  # todo - update logic for new selectors\n  emission_identified_test = self.emission_identified_flag_fk.data != \"No leak was detected\"\n  # print(f\"{emission_identified_test=}\")\n  change_validators_on_test(self, emission_identified_test, required_if_emission_identified)\n\n  if emission_identified_test:\n    lmr_included_test = self.included_in_last_lmr.data == \"No\"\n    logger.debug(f\"{lmr_included_test=}\")\n    change_validators_on_test(self, lmr_included_test, [\"included_in_last_lmr_description\"])\n\n    lmr_planned_test = self.planned_for_next_lmr.data == \"No\"\n    logger.debug(\n      f\"{lmr_planned_test=}\")\n    change_validators_on_test(self, lmr_planned_test, [\"planned_for_next_lmr_description\"])\n</code></pre>"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback.update_contingent_selectors","title":"<code>update_contingent_selectors()</code>","text":"<p>Update contingent dropdown field choices based on current field selections.</p> <p>This method looks up selector/contingent relationships defined in <code>Globals.drop_downs_contingent</code> and dynamically modifies the <code>choices</code> for child fields when a selector field has a known dependency.</p> <p>This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of <code>self.emission_location</code>. It ensures valid dropdown options and clears invalid selections.</p> Assumes <ul> <li><code>self.emission_location</code>, <code>self.emission_cause</code>,   <code>self.emission_cause_secondary</code>, and <code>self.emission_cause_tertiary</code>   are all <code>SelectField</code> instances.</li> <li><code>Globals.drop_downs_contingent</code> contains a nested dictionary of   location-contingent dropdown options.</li> </ul> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>arb\\portal\\wtf_landfill.py</code> <pre><code>def update_contingent_selectors(self) -&gt; None:\n  \"\"\"\n  Update contingent dropdown field choices based on current field selections.\n\n  This method looks up selector/contingent relationships defined in\n  `Globals.drop_downs_contingent` and dynamically modifies the `choices`\n  for child fields when a selector field has a known dependency.\n\n  This method dynamically updates the primary, secondary, and tertiary\n  emission cause fields based on the value of `self.emission_location`. It\n  ensures valid dropdown options and clears invalid selections.\n\n  Assumes:\n    - `self.emission_location`, `self.emission_cause`,\n      `self.emission_cause_secondary`, and `self.emission_cause_tertiary`\n      are all `SelectField` instances.\n    - `Globals.drop_downs_contingent` contains a nested dictionary of\n      location-contingent dropdown options.\n\n  Returns:\n    None\n  \"\"\"\n  # todo - update contingent dropdowns?\n\n  logger.debug(\"Running update_contingent_selectors()\")\n\n  emission_location = self.emission_location.data\n  logger.debug(f\"Selected emission_location: {emission_location!r}\")\n\n  emission_cause_dict = Globals.drop_downs_contingent.get(\n    \"emission_cause_contingent_on_emission_location\", {}\n  )\n  choices_raw = emission_cause_dict.get(emission_location, [])\n  logger.debug(f\"Available contingent causes: {choices_raw!r}\")\n\n  # Define headers\n  primary_header = [\n    (PLEASE_SELECT, PLEASE_SELECT, {\"disabled\": True}),\n    (\"Not applicable as no leak was detected\",\n     \"Not applicable as no leak was detected\", {}),\n  ]\n  secondary_tertiary_header = primary_header + [\n    (\"Not applicable as no additional leak cause suspected\",\n     \"Not applicable as no additional leak cause suspected\", {}),\n  ]\n\n  # Build full choices\n  primary_choices = build_choices(primary_header, choices_raw)\n  secondary_tertiary_choices = build_choices(secondary_tertiary_header, choices_raw)\n\n  # Update each field's choices\n  self.emission_cause.choices = primary_choices\n  self.emission_cause_secondary.choices = secondary_tertiary_choices\n  self.emission_cause_tertiary.choices = secondary_tertiary_choices\n</code></pre>"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback.validate","title":"<code>validate(extra_validators=None)</code>","text":"<p>Override WTForms default validation with custom cross-field logic.</p> <p>Ensures required fields are conditionally enforced based on upstream values, including:   - Facility activity selections imply required contingent selections   - If \"Other\" is chosen, corresponding text input must be filled   - If leak is confirmed, additional emission details are required</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if form is valid, False otherwise.</p> Notes <ul> <li>Calls <code>determine_contingent_fields()</code> before validation to   ensure field validators are correct.</li> <li>Uses built-in <code>super().validate()</code> after adjusting validators.</li> </ul> Source code in <code>arb\\portal\\wtf_landfill.py</code> <pre><code>def validate(self, extra_validators=None) -&gt; bool:\n  \"\"\"\n  Override WTForms default validation with custom cross-field logic.\n\n  Ensures required fields are conditionally enforced based on upstream\n  values, including:\n    - Facility activity selections imply required contingent selections\n    - If \"Other\" is chosen, corresponding text input must be filled\n    - If leak is confirmed, additional emission details are required\n\n  Returns:\n    bool: True if form is valid, False otherwise.\n\n  Notes:\n    - Calls `determine_contingent_fields()` before validation to\n      ensure field validators are correct.\n    - Uses built-in `super().validate()` after adjusting validators.\n  \"\"\"\n\n  logger.debug(f\"validate() called.\")\n  form_fields = get_wtforms_fields(self)\n\n  # Dictionary to replace standard WTForm messages with an alternative message\n  error_message_replacement_dict = {\"Not a valid float value.\": \"Not a valid numeric value.\"}\n\n  ###################################################################################################\n  # Add, Remove, or Modify validation at a field level here before the super is called (for example)\n  ###################################################################################################\n  self.determine_contingent_fields()\n  self.update_contingent_selectors()\n\n  ###################################################################################################\n  # Set selectors with values not in their choices list to \"Please Select\"\n  ###################################################################################################\n  for field_name in form_fields:\n    field = getattr(self, field_name)\n    logger.debug(f\"field_name: {field_name}, {type(field.data)=}, {field.data=}, {type(field.raw_data)=}\")\n    if isinstance(field, SelectField):\n      ensure_field_choice(field_name, field)\n\n  ###################################################################################################\n  # call the super to perform each field's individual validation (which saves to form.errors)\n  # This will create the form.errors dictionary.  If there are form_errors they will be in the None key.\n  # The form_errors will not affect if validate returns True/False, only the fields are considered.\n  ###################################################################################################\n  # logger.debug(\"in the validator before super\")\n  obj_diagnostics(self, message=\"in the validator before super\")\n\n  super_return = super().validate(extra_validators=extra_validators)\n\n  ###################################################################################################\n  # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data\n  ###################################################################################################\n  validate_selectors(self, PLEASE_SELECT)\n\n  ###################################################################################################\n  # Perform any field level validation where one field is cross-referenced to another\n  # The error will be associated with one of the fields\n  ###################################################################################################\n  # todo - move field level validation to separate function\n\n  if self.emission_identified_flag_fk.data == \"No leak was detected\":\n    valid_options = [PLEASE_SELECT,\n                     \"Not applicable as no leak was detected\",\n                     \"Not applicable as no additional leak cause suspected\",\n                     ]\n    if self.emission_type_fk.data not in valid_options:\n      self.emission_type_fk.errors.append(f\"Q8 and Q13 appear to be inconsistent\")\n    if self.emission_location.data not in valid_options:\n      self.emission_location.errors.append(f\"Q8 and Q14 appear to be inconsistent\")\n    if self.emission_cause.data not in valid_options:\n      self.emission_cause.errors.append(f\"Q8 and Q16 appear to be inconsistent\")\n    if self.emission_cause_secondary.data not in valid_options:\n      self.emission_cause_secondary.errors.append(f\"Q8 and Q17 appear to be inconsistent\")\n    if self.emission_cause_tertiary.data not in valid_options:\n      self.emission_cause.errors.append(f\"Q8 and Q18 appear to be inconsistent\")\n\n  # Q8 and Q13 should be coupled to Operator-aware response\n  elif self.emission_identified_flag_fk.data == \"Operator was aware of the leak prior to receiving the CARB plume notification\":\n    valid_options = [PLEASE_SELECT,\n                     \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\", ]\n    if self.emission_type_fk.data not in valid_options:\n      self.emission_type_fk.errors.append(f\"Q8 and Q13 appear to be inconsistent\")\n\n  if self.emission_identified_flag_fk.data != \"No leak was detected\":\n    invalid_options = [\"Not applicable as no leak was detected\", ]\n    if self.emission_type_fk.data in invalid_options:\n      self.emission_type_fk.errors.append(f\"Q8 and Q13 appear to be inconsistent\")\n    if self.emission_location.data in invalid_options:\n      self.emission_location.errors.append(f\"Q8 and Q14 appear to be inconsistent\")\n    if self.emission_cause.data in invalid_options:\n      self.emission_cause.errors.append(f\"Q8 and Q16 appear to be inconsistent\")\n    if self.emission_cause_secondary.data in invalid_options:\n      self.emission_cause_secondary.errors.append(f\"Q8 and Q17 appear to be inconsistent\")\n    if self.emission_cause_tertiary.data in invalid_options:\n      self.emission_cause_tertiary.errors.append(f\"Q8 and Q18 appear to be inconsistent\")\n\n  if self.inspection_timestamp.data and self.mitigation_timestamp.data:\n    if self.mitigation_timestamp.data &lt; self.inspection_timestamp.data:\n      self.mitigation_timestamp.errors.append(\n        \"Date of mitigation cannot be prior to initial site inspection.\")\n\n  # todo - add that 2nd and 3rd can't be repeats\n  ignore_repeats = [PLEASE_SELECT,\n                    \"Not applicable as no leak was detected\",\n                    \"Not applicable as no additional leak cause suspected\",\n                    ]\n\n  if (self.emission_cause_secondary.data not in ignore_repeats and\n      self.emission_cause_secondary.data in [self.emission_cause.data]):\n    self.emission_cause_secondary.errors.append(f\"Q17 appears to be a repeat\")\n\n  if (self.emission_cause_tertiary.data not in ignore_repeats and\n      self.emission_cause_tertiary.data in [self.emission_cause.data, self.emission_cause_secondary.data]):\n    self.emission_cause_secondary.errors.append(f\"Q18 appears to be a repeat\")\n\n  # not sure if this test makes sense since they may have know about it prior to the plume (going to comment out)\n  # if self.observation_timestamp.data and self.inspection_timestamp.data:\n  #   if self.inspection_timestamp.data &lt; self.observation_timestamp.data:\n  #     self.inspection_timestamp.errors.append(\n  #       \"Date of inspection cannot be prior to date of initial plume observation.\")\n\n  ###################################################################################################\n  # perform any form level validation and append it to the form_errors property\n  # This may not be useful, but if you want to have form level errors appear at the top of the error\n  # header, put the logic here.\n  ###################################################################################################\n  # self.form_errors.append(\"I'm a form level error #1\")\n  # self.form_errors.append(\"I'm a form level error #2\")\n\n  ###################################################################################################\n  # Search and replace the error messages associated with input fields to a custom message\n  # For instance, the default 'float' error is changed because a typical user will not know what a\n  # float value is (they will be more comfortable with the word 'numeric')\n  ###################################################################################################\n  for field in form_fields:\n    field_errors = getattr(self, field).errors\n    replace_list_occurrences(field_errors, error_message_replacement_dict)\n\n  ###################################################################################################\n  # Current logic to determine if form is valid the error dict must be empty.\n  # #Consider other approaches\n  ###################################################################################################\n  form_valid = not bool(self.errors)\n\n  return form_valid\n</code></pre>"},{"location":"reference/arb/portal/wtf_oil_and_gas/","title":"<code>arb.portal.wtf_oil_and_gas</code>","text":"<p>Oil &amp; Gas Feedback Form (WTForms) for the ARB Feedback Portal.</p> <p>Defines the <code>OGFeedback</code> class, a complex feedback form used for collecting structured data about methane emission incidents in the oil and gas sector. The form logic mirrors the official O&amp;G spreadsheet and includes conditional field validation, dynamic dropdown dependencies, and timestamp-based consistency checks.</p>"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas--key-features","title":"Key Features:","text":"<ul> <li>Enforces correct response flows based on regulatory logic (e.g., 95669.1(b)(1) exclusions).</li> <li>Includes geospatial validation and timestamp sequencing checks.</li> <li>Cross-field validation logic implemented in <code>validate()</code>.</li> <li>Supports conditional validation with custom helpers like <code>change_validators_on_test()</code>.</li> </ul>"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas--usage","title":"Usage:","text":"<p>form = OGFeedback()   form.process(request.form)</p> <p>if form.validate_on_submit():     process_feedback_data(form.data)</p>"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas--notes","title":"Notes:","text":"<ul> <li>Fields such as <code>id_incidence</code> are read-only and display-only.</li> <li>Contingent dropdowns are updated via <code>update_contingent_selectors()</code>.</li> <li>Cross-dependencies (e.g., OGI required if no venting exclusion) are enforced dynamically.</li> </ul>"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback","title":"<code>OGFeedback</code>","text":"<p>               Bases: <code>FlaskForm</code></p> <p>WTForms class for collecting feedback on Oil &amp; Gas methane emissions.</p> <p>This form models the structure of the O&amp;G feedback spreadsheet and enforces regulatory logic outlined in California methane rules (e.g., 95669.1). Sections include metadata, inspection information, emissions details, mitigation actions, and contact data.</p> Core Features <ul> <li>Uses standard WTForms field types, with conditionally required fields.</li> <li>Dropdowns update dynamically based on user selections.</li> <li>Includes geospatial coordinates and timestamp logic.</li> <li>Implements cross-field validation for inspection results and mitigation status.</li> </ul> Used by <ul> <li>The web-based feedback form in the ARB Feedback Portal.</li> <li>Routes such as <code>og_incidence_create</code> and <code>incidence_update</code>.</li> </ul> Notes <ul> <li>Sector-specific contingent dropdowns are handled via Globals.</li> <li>Validators are adjusted at runtime depending on the selected conditions.</li> </ul> Source code in <code>arb\\portal\\wtf_oil_and_gas.py</code> <pre><code>class OGFeedback(FlaskForm):\n  \"\"\"\n  WTForms class for collecting feedback on Oil &amp; Gas methane emissions.\n\n  This form models the structure of the O&amp;G feedback spreadsheet and enforces\n  regulatory logic outlined in California methane rules (e.g., 95669.1).\n  Sections include metadata, inspection information, emissions details,\n  mitigation actions, and contact data.\n\n  Core Features:\n    - Uses standard WTForms field types, with conditionally required fields.\n    - Dropdowns update dynamically based on user selections.\n    - Includes geospatial coordinates and timestamp logic.\n    - Implements cross-field validation for inspection results and mitigation status.\n\n  Used by:\n    - The web-based feedback form in the ARB Feedback Portal.\n    - Routes such as `og_incidence_create` and `incidence_update`.\n\n  Notes:\n    - Sector-specific contingent dropdowns are handled via Globals.\n    - Validators are adjusted at runtime depending on the selected conditions.\n  \"\"\"\n\n  # venting through inspection (not through the 95669.1(b)(1) exclusion)\n  venting_responses = [\n    \"Venting-construction/maintenance\",\n    \"Venting-routine\",\n  ]\n\n  # These are considered leaks that require mitigation\n  unintentional_leak = [\n    \"Unintentional-leak\",\n    \"Unintentional-non-component\",\n  ]\n\n  # Section 3\n  # This field is read-only and displayed for context only. It should not be edited or submitted.\n  label = \"1.  Incidence/Emission ID\"\n  id_incidence = IntegerField(\n    label,\n    validators=[Optional()],\n    render_kw={\"readonly\": True}\n  )\n\n  label = \"2.  Plume ID(s)\"\n  id_plume = IntegerField(\n    label=label,\n    validators=[InputRequired(), NumberRange(min=1, message=\"Plume ID must be a positive integer\")],\n  )  # REFERENCES plumes (id_plume)\n\n  label = \"3.  Plume Observation Timestamp(s)\"\n  observation_timestamp = DateTimeLocalField(\n    label=label,\n    validators=[InputRequired()],\n    format=HTML_LOCAL_TIME_FORMAT,\n  )\n\n  label = \"4.  Plume CARB Estimated Latitude\"\n  lat_carb = DecimalField(\n    label=label,\n    places=GPS_RESOLUTION,\n    validators=[InputRequired(), NumberRange(**LATITUDE_VALIDATION)],\n  )\n\n  label = \"5.  Plume CARB Estimated Longitude\"\n  long_carb = DecimalField(\n    label=label,\n    places=GPS_RESOLUTION,\n    validators=[InputRequired(), NumberRange(**LONGITUDE_VALIDATION)],\n  )\n\n  label = \"6.  CARB Message ID\"\n  id_message = StringField(\n    label=label,\n    validators=[Optional()],\n  )\n\n  # Section 4\n  label = \"Q1.  Facility Name\"\n  facility_name = StringField(\n    label=label,\n    validators=[InputRequired()],\n  )\n\n  label = \"Q2.  Facility's Cal e-GGRT ARB ID (if known)\"\n  id_arb_eggrt = StringField(\n    label=label,\n    validators=[Optional()],\n  )\n\n  label = \"Q3.  Contact Name\"\n  contact_name = StringField(\n    label=label,\n    validators=[InputRequired()],\n  )\n  # contact_phone = StringField(label=\"Contact Phone\", validators=[InputRequired()])\n  label = \"Q4.  Contact Phone Number\"\n  message = \"Invalid phone number. Phone number must be in format '(123) 456-7890' or '(123) 456-7890 x1234567'.\"\n  contact_phone = StringField(\n    label=label,\n    validators=[InputRequired(),\n                Regexp(regex=r\"^\\(\\d{3}\\) \\d{3}-\\d{4}( x\\d{1,7})?$\", message=message)\n                ],\n  )\n\n  label = \"Q5.  Contact Email Address\"\n  contact_email = EmailField(\n    label=label,\n    validators=[InputRequired(), Email()],\n  )\n\n  # Section 5\n  label = (f\"Q6.  Was the plume a result of activity-based venting that is being reported \"\n           f\"per section 95669.1(b)(1) of the Oil and Gas Methane Regulation?\")\n  venting_exclusion = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"venting_exclusion\"],\n    validators=[InputRequired()],\n  )\n\n  label = (f\"Q7.  If you answered 'Yes' to Q6, please provide a brief summary of the source of the venting \"\n           f\"defined by Regulation 95669.1(b)(1) and why the venting occurred.\")\n  message = \"If provided, a description must be at least 30 characters.\"\n  venting_description_1 = TextAreaField(\n    label=label,\n    validators=[InputRequired(),\n                Length(min=30, message=message)],\n  )\n\n  # Section 6\n  label = \"Q8. Was an OGI inspection performed?\"\n  ogi_performed = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"ogi_performed\"],\n    validators=[InputRequired()],\n  )\n\n  label = \"Q9.  If you answered 'Yes' to Q8, what date and time was the OGI inspection performed?\"\n  ogi_date = DateTimeLocalField(\n    label=label,\n    validators=[InputRequired()],\n    format=HTML_LOCAL_TIME_FORMAT,\n  )\n\n  label = \"Q10. If you answered 'Yes' to Q8, what type of source was found using OGI?\"\n  ogi_result = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"ogi_result\"],\n    validators=[InputRequired()],\n  )\n\n  label = \"Q11.  Was a Method 21 inspection performed?\"\n  method21_performed = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"method21_performed\"],\n    validators=[InputRequired()],\n  )\n\n  label = \"Q12.  If you answered 'Yes' to Q11, what date and time was the Method 21 inspection performed?\"\n  method21_date = DateTimeLocalField(\n    label=label,\n    validators=[InputRequired()],\n    format=HTML_LOCAL_TIME_FORMAT,\n  )\n\n  label = \"Q13. If you answered 'Yes' to Q11, what type of source was found using Method 21?\"\n  method21_result = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"method21_result\"],\n    validators=[InputRequired()],\n  )\n\n  label = \"Q14. If you answered 'Yes' to Q11, what was the initial leak concentration in ppmv (if applicable)?\"\n  initial_leak_concentration = FloatField(\n    label=label,\n    validators=[InputRequired()],\n  )\n\n  label = (f\"Q15.  If you answered 'Venting' to Q10 or Q13, please provide a brief summary of the source \"\n           f\"of the venting discovered during the ground inspection and why the venting occurred.\")\n  venting_description_2 = TextAreaField(\n    label=label,\n    validators=[InputRequired()],\n  )\n\n  label = (f\"Q16.  If you answered a 'Unintentional-leak' or 'Unintentional-non-component' to Q10 or Q13, \"\n           f\"please provide a description of your initial mitigation plan.\")\n  initial_mitigation_plan = TextAreaField(\n    label=label,\n    validators=[InputRequired()],\n  )\n\n  # Section 7\n  label = f\"Q17.  What type of equipment is at the source of the emissions?\"\n  equipment_at_source = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"equipment_at_source\"],\n    validators=[InputRequired(), ],\n  )\n\n  label = \"Q18.  If you answered 'Other' for Q17, please provide an additional description of the equipment.\"\n  equipment_other_description = TextAreaField(\n    label=label,\n    validators=[InputRequired()],\n  )\n\n  label = f\"Q19.  If your source is a component, what type of component is at the source of the emissions?\"\n  component_at_source = SelectField(\n    label=label,\n    choices=Globals.drop_downs[\"component_at_source\"],\n    validators=[],\n  )\n\n  label = \"Q20.  If you answered 'Other' for Q19, please provide an additional description of the component.\"\n  component_other_description = TextAreaField(\n    label=label, validators=[InputRequired()],\n  )\n\n  label = f\"Q21.  Repair/mitigation completion date &amp; time (if applicable).\"\n  repair_timestamp = DateTimeLocalField(\n    label=label,\n    validators=[InputRequired()],\n    format=HTML_LOCAL_TIME_FORMAT,\n  )\n\n  label = f\"Q22.  Final repair concentration in ppmv (if applicable).\"\n  final_repair_concentration = FloatField(\n    label=label,\n    validators=[InputRequired()],\n  )\n\n  label = f\"Q23.  Repair/Mitigation actions taken (if applicable).\"\n  repair_description = StringField(\n    label=label,\n    validators=[InputRequired()],\n  )\n\n  # Section 8\n  label = f\"Q24. Additional notes or comments.\"\n  additional_notes = TextAreaField(\n    label=label,\n    validators=[],\n  )\n\n  label = \"1. CARB internal notes\"\n  carb_notes = TextAreaField(\n    label=label,\n    validators=[],\n  )\n\n  def update_contingent_selectors(self) -&gt; None:\n    \"\"\"\n    Update dropdown field options based on dependent selector fields.\n\n    Dynamically replaces `.choices` for contingent fields depending on\n    parent selections. Uses the `Globals.drop_downs_contingent` structure\n    for Oil &amp; Gas to determine appropriate mappings.\n\n    Examples:\n      - not implemented yet\n\n    Returns:\n      None\n    \"\"\"\n  pass\n\n  def validate(self, extra_validators=None) -&gt; bool:\n    \"\"\"\n      Override the default WTForms validation logic with cross-field rules\n      specific to Oil &amp; Gas reporting.\n\n      Invokes:\n        - `determine_contingent_fields()` to update validators before validation.\n        - `super().validate()` to apply all field and form-level validations.\n\n      Custom checks include:\n        - Required fields based on mitigation status or inspection outcomes.\n        - Logical enforcement of conditional relationships between fields.\n\n      Args:\n        extra_validators (dict, optional): Additional validators provided at runtime.\n\n      Returns:\n        bool: True if form passes all validation rules, otherwise False.\n      \"\"\"\n    logger.debug(f\"validate() called.\")\n    form_fields = get_wtforms_fields(self)\n\n    # Dictionary to replace standard WTForm messages with alternative message\n    error_message_replacement_dict = {\"Not a valid float value.\": \"Not a valid numeric value.\"}\n\n    ###################################################################################################\n    # Add, Remove, or Modify validation at a field level here before the super is called (for example)\n    ###################################################################################################\n    self.determine_contingent_fields()\n\n    ###################################################################################################\n    # Set selectors with values not in their choices list to \"Please Select\"\n    ###################################################################################################\n    for field_name in form_fields:\n      field = getattr(self, field_name)\n      logger.debug(f\"field_name: {field_name}, {type(field.data)=}, {field.data=}, {type(field.raw_data)=}\")\n      if isinstance(field, SelectField):\n        ensure_field_choice(field_name, field)\n\n    ###################################################################################################\n    # call the super to perform each fields individual validation (which saves to form.errors)\n    # This will create the form.errors dictionary.  If there are form_errors they will be in the None key.\n    # The form_errors will not affect if validate returns True/False, only the fields are considered.\n    ###################################################################################################\n    # logger.debug(\"in the validator before super\")\n    super_return = super().validate(extra_validators=extra_validators)\n\n    ###################################################################################################\n    # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data\n    ###################################################################################################\n    validate_selectors(self, PLEASE_SELECT)\n\n    ###################################################################################################\n    # Perform any field level validation where one field is cross-referenced to another\n    # The error will be associated with one of the fields\n    ###################################################################################################\n    if self.observation_timestamp.data and self.ogi_date.data:\n      if self.observation_timestamp.data &gt; self.ogi_date.data:\n        self.ogi_date.errors.append(\n          \"Initial OGI timestamp must be after the plume observation timestamp\")\n\n    if self.observation_timestamp.data and self.method21_date.data:\n      if self.observation_timestamp.data &gt; self.method21_date.data:\n        self.method21_date.errors.append(\n          \"Initial Method 21 timestamp must be after the plume observation timestamp\")\n\n    if self.observation_timestamp.data and self.repair_timestamp.data:\n      if self.observation_timestamp.data &gt; self.repair_timestamp.data:\n        self.method21_date.errors.append(\n          \"Repair timestamp must be after the plume observation timestamp\")\n\n    if self.venting_exclusion and self.ogi_result.data:\n      if self.venting_exclusion.data == \"Yes\":\n        if self.ogi_result.data in [\"Unintentional-leak\"]:\n          self.ogi_result.errors.append(\"If you claim a venting exclusion, you can't also have a leak detected with OGI.\")\n\n    if self.venting_exclusion and self.method21_result.data:\n      if self.venting_exclusion.data == \"Yes\":\n        if self.method21_result.data in [\"Unintentional-leak\"]:\n          self.method21_result.errors.append(\"If you claim a venting exclusion, you can't also have a leak detected with Method 21.\")\n\n    if self.ogi_result.data in self.unintentional_leak:\n      if self.method21_performed.data != \"Yes\":\n        self.method21_performed.errors.append(\"If a leak was detected via OGI, Method 21 must be performed.\")\n\n    if self.ogi_performed.data == \"No\":\n      if self.ogi_date.data:\n        self.ogi_date.errors.append(\"Can't have an OGI inspection date if OGI was not performed\")\n      # print(f\"{self.ogi_result.data=}\")\n      if self.ogi_result.data != PLEASE_SELECT:\n        if self.ogi_result.data != \"Not applicable as OGI was not performed\":\n          self.ogi_result.errors.append(\"Can't have an OGI result if OGI was not performed\")\n\n    if self.method21_performed.data == \"No\":\n      if self.method21_date.data:\n        self.method21_date.errors.append(\"Can't have an Method 21 inspection date if Method 21 was not performed\")\n      if self.initial_leak_concentration.data:\n        self.initial_leak_concentration.errors.append(\"Can't have an Method 21 concentration if Method 21 was not performed\")\n      # print(f\"{self.method21_result.data=}\")\n      if self.method21_result.data != PLEASE_SELECT:\n        if self.method21_result.data != \"Not applicable as Method 21 was not performed\":\n          self.method21_result.errors.append(\"Can't have an Method 21 result if Method 21 was not performed\")\n\n    if self.venting_exclusion.data == \"No\" and self.ogi_performed.data == \"No\" and self.method21_performed.data == \"No\":\n      self.method21_performed.errors.append(\"If you do not claim a venting exclusion, Method 21 or OGI must be performed.\")\n\n    # todo (consider) - you could also remove the option for not applicable rather than the following two tests\n    if self.ogi_performed.data == \"Yes\":\n      if self.ogi_result.data == \"Not applicable as OGI was not performed\":\n        self.ogi_result.errors.append(\"Invalid response given your Q8 answer\")\n\n    if self.method21_performed.data == \"Yes\":\n      if self.method21_result.data == \"Not applicable as Method 21 was not performed\":\n        self.method21_result.errors.append(\"Invalid response given your Q11 answer\")\n\n    ###################################################################################################\n    # perform any form level validation and append it to the form_errors property\n    # This may not be useful, but if you want to have form level errors appear at the top of the error\n    # header, put the logic here.\n    ###################################################################################################\n    # self.form_errors.append(\"I'm a form level error #1\")\n    # self.form_errors.append(\"I'm a form level error #2\")\n\n    ###################################################################################################\n    # Search and replace the error messages associated with input fields to a custom message\n    # For instance, the default 'float' error is changed because a typical user will not know what a\n    # float value is (they will be more comfortable with the word 'numeric')\n    ###################################################################################################\n    for field in form_fields:\n      field_errors = getattr(self, field).errors\n      replace_list_occurrences(field_errors, error_message_replacement_dict)\n\n    ###################################################################################################\n    # Current logic to determine if form is valid the error dict must be empty.\n    # #Consider other approaches\n    ###################################################################################################\n    form_valid = not bool(self.errors)\n\n    logger.debug(f\"after validate(): {self.errors=}\")\n    return form_valid\n\n  def determine_contingent_fields(self) -&gt; None:\n    \"\"\"\n    Adjust validators based on user selections that imply exclusions or\n    optional behavior.\n\n    Affects validation logic such as:\n      - 95669.1(b)(1) exclusions where OGI inspection is not required.\n      - Skipping downstream fields when \"No leak was detected\" is selected.\n      - Making \"Other\" explanations required only if \"Other\" is selected.\n\n    Notes:\n      - Should be called before validation to sync rules with input state.\n      - Venting-related exclusions may need careful ordering to preserve business logic.\n\n    Returns:\n      None.\n    \"\"\"\n\n    # logger.debug(f\"In determine_contingent_fields()\")\n\n    # If a venting exclusion is claimed, then a venting description is required and many fields become optional\n    required_if_venting_exclusion = [\"venting_description_1\", ]\n    optional_if_venting_exclusion = [\n      \"ogi_performed\",\n      \"ogi_date\",\n      \"ogi_result\",\n      \"method21_performed\",\n      \"method21_date\",\n      \"method21_result\",\n      \"initial_leak_concentration\",\n      \"venting_description_2\",\n      \"initial_mitigation_plan\",\n      \"equipment_at_source\",\n      \"equipment_other_description\",\n      \"component_at_source\",\n      \"component_other_description\",\n      \"repair_timestamp\",\n      \"final_repair_concentration\",\n      \"repair_description\",\n      \"additional_notes\",\n    ]\n    venting_exclusion_test = self.venting_exclusion.data == \"Yes\"\n    # logger.debug(f\"\\n\\t{venting_exclusion_test=}, {self.venting_exclusion_test.data=}\")\n    change_validators_on_test(self, venting_exclusion_test, required_if_venting_exclusion, optional_if_venting_exclusion)\n\n    required_if_ogi_performed = [\n      \"ogi_date\",\n      \"ogi_result\",\n    ]\n    ogi_test = self.ogi_performed.data == \"Yes\"\n    change_validators_on_test(self, ogi_test, required_if_ogi_performed)\n\n    required_if_method21_performed = [\n      \"method21_date\",\n      \"method21_result\",\n      \"initial_leak_concentration\",\n    ]\n    method21_test = self.method21_performed.data == \"Yes\"\n    change_validators_on_test(self, method21_test, required_if_method21_performed)\n\n    required_if_venting_on_inspection = [\n      \"venting_description_2\",\n    ]\n    venting2_test = False\n    if self.ogi_result.data in self.venting_responses or self.method21_result.data in self.venting_responses:\n      venting2_test = True\n    change_validators_on_test(self, venting2_test, required_if_venting_on_inspection)\n\n    required_if_unintentional = [\n      \"initial_mitigation_plan\",\n      \"equipment_at_source\",\n      \"repair_timestamp\",\n      \"final_repair_concentration\",\n      \"repair_description\",\n    ]\n    unintentional_test = False\n    if self.ogi_result.data in self.unintentional_leak or self.method21_result.data in self.unintentional_leak:\n      unintentional_test = True\n    change_validators_on_test(self, unintentional_test, required_if_unintentional)\n\n    required_if_equipment_other = [\n      \"equipment_other_description\",\n    ]\n    equipment_other_test = self.equipment_at_source.data == \"Other\"\n    change_validators_on_test(self, equipment_other_test, required_if_equipment_other)\n\n    required_if_component_other = [\n      \"component_other_description\",\n    ]\n    component_other_test = self.component_at_source.data == \"Other\"\n    change_validators_on_test(self, component_other_test, required_if_component_other)\n</code></pre>"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback.determine_contingent_fields","title":"<code>determine_contingent_fields()</code>","text":"<p>Adjust validators based on user selections that imply exclusions or optional behavior.</p> Affects validation logic such as <ul> <li>95669.1(b)(1) exclusions where OGI inspection is not required.</li> <li>Skipping downstream fields when \"No leak was detected\" is selected.</li> <li>Making \"Other\" explanations required only if \"Other\" is selected.</li> </ul> Notes <ul> <li>Should be called before validation to sync rules with input state.</li> <li>Venting-related exclusions may need careful ordering to preserve business logic.</li> </ul> <p>Returns:</p> Type Description <code>None</code> <p>None.</p> Source code in <code>arb\\portal\\wtf_oil_and_gas.py</code> <pre><code>def determine_contingent_fields(self) -&gt; None:\n  \"\"\"\n  Adjust validators based on user selections that imply exclusions or\n  optional behavior.\n\n  Affects validation logic such as:\n    - 95669.1(b)(1) exclusions where OGI inspection is not required.\n    - Skipping downstream fields when \"No leak was detected\" is selected.\n    - Making \"Other\" explanations required only if \"Other\" is selected.\n\n  Notes:\n    - Should be called before validation to sync rules with input state.\n    - Venting-related exclusions may need careful ordering to preserve business logic.\n\n  Returns:\n    None.\n  \"\"\"\n\n  # logger.debug(f\"In determine_contingent_fields()\")\n\n  # If a venting exclusion is claimed, then a venting description is required and many fields become optional\n  required_if_venting_exclusion = [\"venting_description_1\", ]\n  optional_if_venting_exclusion = [\n    \"ogi_performed\",\n    \"ogi_date\",\n    \"ogi_result\",\n    \"method21_performed\",\n    \"method21_date\",\n    \"method21_result\",\n    \"initial_leak_concentration\",\n    \"venting_description_2\",\n    \"initial_mitigation_plan\",\n    \"equipment_at_source\",\n    \"equipment_other_description\",\n    \"component_at_source\",\n    \"component_other_description\",\n    \"repair_timestamp\",\n    \"final_repair_concentration\",\n    \"repair_description\",\n    \"additional_notes\",\n  ]\n  venting_exclusion_test = self.venting_exclusion.data == \"Yes\"\n  # logger.debug(f\"\\n\\t{venting_exclusion_test=}, {self.venting_exclusion_test.data=}\")\n  change_validators_on_test(self, venting_exclusion_test, required_if_venting_exclusion, optional_if_venting_exclusion)\n\n  required_if_ogi_performed = [\n    \"ogi_date\",\n    \"ogi_result\",\n  ]\n  ogi_test = self.ogi_performed.data == \"Yes\"\n  change_validators_on_test(self, ogi_test, required_if_ogi_performed)\n\n  required_if_method21_performed = [\n    \"method21_date\",\n    \"method21_result\",\n    \"initial_leak_concentration\",\n  ]\n  method21_test = self.method21_performed.data == \"Yes\"\n  change_validators_on_test(self, method21_test, required_if_method21_performed)\n\n  required_if_venting_on_inspection = [\n    \"venting_description_2\",\n  ]\n  venting2_test = False\n  if self.ogi_result.data in self.venting_responses or self.method21_result.data in self.venting_responses:\n    venting2_test = True\n  change_validators_on_test(self, venting2_test, required_if_venting_on_inspection)\n\n  required_if_unintentional = [\n    \"initial_mitigation_plan\",\n    \"equipment_at_source\",\n    \"repair_timestamp\",\n    \"final_repair_concentration\",\n    \"repair_description\",\n  ]\n  unintentional_test = False\n  if self.ogi_result.data in self.unintentional_leak or self.method21_result.data in self.unintentional_leak:\n    unintentional_test = True\n  change_validators_on_test(self, unintentional_test, required_if_unintentional)\n\n  required_if_equipment_other = [\n    \"equipment_other_description\",\n  ]\n  equipment_other_test = self.equipment_at_source.data == \"Other\"\n  change_validators_on_test(self, equipment_other_test, required_if_equipment_other)\n\n  required_if_component_other = [\n    \"component_other_description\",\n  ]\n  component_other_test = self.component_at_source.data == \"Other\"\n  change_validators_on_test(self, component_other_test, required_if_component_other)\n</code></pre>"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback.update_contingent_selectors","title":"<code>update_contingent_selectors()</code>","text":"<p>Update dropdown field options based on dependent selector fields.</p> <p>Dynamically replaces <code>.choices</code> for contingent fields depending on parent selections. Uses the <code>Globals.drop_downs_contingent</code> structure for Oil &amp; Gas to determine appropriate mappings.</p> <p>Examples:</p> <ul> <li>not implemented yet</li> </ul> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>arb\\portal\\wtf_oil_and_gas.py</code> <pre><code>def update_contingent_selectors(self) -&gt; None:\n  \"\"\"\n  Update dropdown field options based on dependent selector fields.\n\n  Dynamically replaces `.choices` for contingent fields depending on\n  parent selections. Uses the `Globals.drop_downs_contingent` structure\n  for Oil &amp; Gas to determine appropriate mappings.\n\n  Examples:\n    - not implemented yet\n\n  Returns:\n    None\n  \"\"\"\n</code></pre>"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback.validate","title":"<code>validate(extra_validators=None)</code>","text":"<p>Override the default WTForms validation logic with cross-field rules specific to Oil &amp; Gas reporting.</p> Invokes <ul> <li><code>determine_contingent_fields()</code> to update validators before validation.</li> <li><code>super().validate()</code> to apply all field and form-level validations.</li> </ul> Custom checks include <ul> <li>Required fields based on mitigation status or inspection outcomes.</li> <li>Logical enforcement of conditional relationships between fields.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>extra_validators</code> <code>dict</code> <p>Additional validators provided at runtime.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if form passes all validation rules, otherwise False.</p> Source code in <code>arb\\portal\\wtf_oil_and_gas.py</code> <pre><code>def validate(self, extra_validators=None) -&gt; bool:\n  \"\"\"\n    Override the default WTForms validation logic with cross-field rules\n    specific to Oil &amp; Gas reporting.\n\n    Invokes:\n      - `determine_contingent_fields()` to update validators before validation.\n      - `super().validate()` to apply all field and form-level validations.\n\n    Custom checks include:\n      - Required fields based on mitigation status or inspection outcomes.\n      - Logical enforcement of conditional relationships between fields.\n\n    Args:\n      extra_validators (dict, optional): Additional validators provided at runtime.\n\n    Returns:\n      bool: True if form passes all validation rules, otherwise False.\n    \"\"\"\n  logger.debug(f\"validate() called.\")\n  form_fields = get_wtforms_fields(self)\n\n  # Dictionary to replace standard WTForm messages with alternative message\n  error_message_replacement_dict = {\"Not a valid float value.\": \"Not a valid numeric value.\"}\n\n  ###################################################################################################\n  # Add, Remove, or Modify validation at a field level here before the super is called (for example)\n  ###################################################################################################\n  self.determine_contingent_fields()\n\n  ###################################################################################################\n  # Set selectors with values not in their choices list to \"Please Select\"\n  ###################################################################################################\n  for field_name in form_fields:\n    field = getattr(self, field_name)\n    logger.debug(f\"field_name: {field_name}, {type(field.data)=}, {field.data=}, {type(field.raw_data)=}\")\n    if isinstance(field, SelectField):\n      ensure_field_choice(field_name, field)\n\n  ###################################################################################################\n  # call the super to perform each fields individual validation (which saves to form.errors)\n  # This will create the form.errors dictionary.  If there are form_errors they will be in the None key.\n  # The form_errors will not affect if validate returns True/False, only the fields are considered.\n  ###################################################################################################\n  # logger.debug(\"in the validator before super\")\n  super_return = super().validate(extra_validators=extra_validators)\n\n  ###################################################################################################\n  # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data\n  ###################################################################################################\n  validate_selectors(self, PLEASE_SELECT)\n\n  ###################################################################################################\n  # Perform any field level validation where one field is cross-referenced to another\n  # The error will be associated with one of the fields\n  ###################################################################################################\n  if self.observation_timestamp.data and self.ogi_date.data:\n    if self.observation_timestamp.data &gt; self.ogi_date.data:\n      self.ogi_date.errors.append(\n        \"Initial OGI timestamp must be after the plume observation timestamp\")\n\n  if self.observation_timestamp.data and self.method21_date.data:\n    if self.observation_timestamp.data &gt; self.method21_date.data:\n      self.method21_date.errors.append(\n        \"Initial Method 21 timestamp must be after the plume observation timestamp\")\n\n  if self.observation_timestamp.data and self.repair_timestamp.data:\n    if self.observation_timestamp.data &gt; self.repair_timestamp.data:\n      self.method21_date.errors.append(\n        \"Repair timestamp must be after the plume observation timestamp\")\n\n  if self.venting_exclusion and self.ogi_result.data:\n    if self.venting_exclusion.data == \"Yes\":\n      if self.ogi_result.data in [\"Unintentional-leak\"]:\n        self.ogi_result.errors.append(\"If you claim a venting exclusion, you can't also have a leak detected with OGI.\")\n\n  if self.venting_exclusion and self.method21_result.data:\n    if self.venting_exclusion.data == \"Yes\":\n      if self.method21_result.data in [\"Unintentional-leak\"]:\n        self.method21_result.errors.append(\"If you claim a venting exclusion, you can't also have a leak detected with Method 21.\")\n\n  if self.ogi_result.data in self.unintentional_leak:\n    if self.method21_performed.data != \"Yes\":\n      self.method21_performed.errors.append(\"If a leak was detected via OGI, Method 21 must be performed.\")\n\n  if self.ogi_performed.data == \"No\":\n    if self.ogi_date.data:\n      self.ogi_date.errors.append(\"Can't have an OGI inspection date if OGI was not performed\")\n    # print(f\"{self.ogi_result.data=}\")\n    if self.ogi_result.data != PLEASE_SELECT:\n      if self.ogi_result.data != \"Not applicable as OGI was not performed\":\n        self.ogi_result.errors.append(\"Can't have an OGI result if OGI was not performed\")\n\n  if self.method21_performed.data == \"No\":\n    if self.method21_date.data:\n      self.method21_date.errors.append(\"Can't have an Method 21 inspection date if Method 21 was not performed\")\n    if self.initial_leak_concentration.data:\n      self.initial_leak_concentration.errors.append(\"Can't have an Method 21 concentration if Method 21 was not performed\")\n    # print(f\"{self.method21_result.data=}\")\n    if self.method21_result.data != PLEASE_SELECT:\n      if self.method21_result.data != \"Not applicable as Method 21 was not performed\":\n        self.method21_result.errors.append(\"Can't have an Method 21 result if Method 21 was not performed\")\n\n  if self.venting_exclusion.data == \"No\" and self.ogi_performed.data == \"No\" and self.method21_performed.data == \"No\":\n    self.method21_performed.errors.append(\"If you do not claim a venting exclusion, Method 21 or OGI must be performed.\")\n\n  # todo (consider) - you could also remove the option for not applicable rather than the following two tests\n  if self.ogi_performed.data == \"Yes\":\n    if self.ogi_result.data == \"Not applicable as OGI was not performed\":\n      self.ogi_result.errors.append(\"Invalid response given your Q8 answer\")\n\n  if self.method21_performed.data == \"Yes\":\n    if self.method21_result.data == \"Not applicable as Method 21 was not performed\":\n      self.method21_result.errors.append(\"Invalid response given your Q11 answer\")\n\n  ###################################################################################################\n  # perform any form level validation and append it to the form_errors property\n  # This may not be useful, but if you want to have form level errors appear at the top of the error\n  # header, put the logic here.\n  ###################################################################################################\n  # self.form_errors.append(\"I'm a form level error #1\")\n  # self.form_errors.append(\"I'm a form level error #2\")\n\n  ###################################################################################################\n  # Search and replace the error messages associated with input fields to a custom message\n  # For instance, the default 'float' error is changed because a typical user will not know what a\n  # float value is (they will be more comfortable with the word 'numeric')\n  ###################################################################################################\n  for field in form_fields:\n    field_errors = getattr(self, field).errors\n    replace_list_occurrences(field_errors, error_message_replacement_dict)\n\n  ###################################################################################################\n  # Current logic to determine if form is valid the error dict must be empty.\n  # #Consider other approaches\n  ###################################################################################################\n  form_valid = not bool(self.errors)\n\n  logger.debug(f\"after validate(): {self.errors=}\")\n  return form_valid\n</code></pre>"},{"location":"reference/arb/portal/wtf_upload/","title":"<code>arb.portal.wtf_upload</code>","text":"<p>WTForms-based upload form for the ARB Feedback Portal.</p> <p>Defines a minimal form used to upload Excel files via the web interface. Typically used in the <code>/upload</code> route.</p>"},{"location":"reference/arb/portal/wtf_upload/#arb.portal.wtf_upload--fields","title":"Fields:","text":"<ul> <li>file: Accepts <code>.xls</code> or <code>.xlsx</code> files only.</li> <li>submit: Triggers form submission.</li> </ul>"},{"location":"reference/arb/portal/wtf_upload/#arb.portal.wtf_upload--notes","title":"Notes:","text":"<ul> <li>Leverages Flask-WTF integration with Bootstrap-compatible rendering.</li> <li>Additional validation for file size or filename may be added externally.</li> </ul>"},{"location":"reference/arb/portal/wtf_upload/#arb.portal.wtf_upload.UploadForm","title":"<code>UploadForm</code>","text":"<p>               Bases: <code>FlaskForm</code></p> <p>WTForm for uploading Excel or JSON files via the ARB Feedback Portal.</p> Fields <p>file (FileField): Upload field for selecting a <code>.xls</code> or <code>.xlsx</code> file. submit (SubmitField): Form button to initiate upload.</p> Notes <ul> <li>Uses Flask-WTF and integrates with Bootstrap templates.</li> <li>File extension restrictions enforced via <code>FileAllowed</code>.</li> <li>Form is rendered in the Upload UI at <code>/upload</code>.</li> </ul> Source code in <code>arb\\portal\\wtf_upload.py</code> <pre><code>class UploadForm(FlaskForm):\n  \"\"\"\n  WTForm for uploading Excel or JSON files via the ARB Feedback Portal.\n\n  Fields:\n    file (FileField): Upload field for selecting a `.xls` or `.xlsx` file.\n    submit (SubmitField): Form button to initiate upload.\n\n  Notes:\n    - Uses Flask-WTF and integrates with Bootstrap templates.\n    - File extension restrictions enforced via `FileAllowed`.\n    - Form is rendered in the Upload UI at `/upload`.\n  \"\"\"\n\n  file = FileField(\n    \"Choose Excel File\",\n    validators=[DataRequired(), FileAllowed(['xls', 'xlsx'], 'Excel files only!')]\n  )\n  submit = SubmitField(\"Upload\")\n</code></pre>"},{"location":"reference/arb/portal/config/settings/","title":"<code>arb.portal.config.settings</code>","text":"<p>Environment-specific configuration classes for the Flask application.</p> <p>Defines base and derived configuration classes used by the ARB portal. Each config class inherits from <code>BaseConfig</code> and may override environment-specific values.</p> Usage <p>from config.settings import DevelopmentConfig, ProductionConfig, TestingConfig</p> Notes <ul> <li>Static and environment-derived values belong here.</li> <li>Runtime-dependent settings (platform, CLI, etc.) should go in <code>startup/runtime_info.py</code>.</li> </ul>"},{"location":"reference/arb/portal/config/settings/#arb.portal.config.settings.BaseConfig","title":"<code>BaseConfig</code>","text":"<p>Base configuration shared across all environments.</p> <p>Attributes:</p> Name Type Description <code>POSTGRES_DB_URI</code> <code>str</code> <p>Default PostgreSQL URI if DATABASE_URI is unset.</p> <code>SQLALCHEMY_ENGINE_OPTIONS</code> <code>dict</code> <p>Connection settings for SQLAlchemy.</p> <code>SECRET_KEY</code> <code>str</code> <p>Flask session key.</p> <code>SQLALCHEMY_DATABASE_URI</code> <code>str</code> <p>Final URI used by the app.</p> <code>SQLALCHEMY_TRACK_MODIFICATIONS</code> <code>bool</code> <p>SQLAlchemy event system flag.</p> <code>EXPLAIN_TEMPLATE_LOADING</code> <code>bool</code> <p>Whether to trace template resolution errors.</p> <code>WTF_CSRF_ENABLED</code> <code>bool</code> <p>Cross-site request forgery protection toggle.</p> <code>LOG_LEVEL</code> <code>str</code> <p>Default logging level.</p> <code>TIMEZONE</code> <code>str</code> <p>Target timezone for timestamp formatting.</p> <code>FAST_LOAD</code> <code>bool</code> <p>Enables performance optimizations at startup.</p> Source code in <code>arb\\portal\\config\\settings.py</code> <pre><code>class BaseConfig:\n  \"\"\"\n  Base configuration shared across all environments.\n\n  Attributes:\n    POSTGRES_DB_URI (str): Default PostgreSQL URI if DATABASE_URI is unset.\n    SQLALCHEMY_ENGINE_OPTIONS (dict): Connection settings for SQLAlchemy.\n    SECRET_KEY (str): Flask session key.\n    SQLALCHEMY_DATABASE_URI (str): Final URI used by the app.\n    SQLALCHEMY_TRACK_MODIFICATIONS (bool): SQLAlchemy event system flag.\n    EXPLAIN_TEMPLATE_LOADING (bool): Whether to trace template resolution errors.\n    WTF_CSRF_ENABLED (bool): Cross-site request forgery protection toggle.\n    LOG_LEVEL (str): Default logging level.\n    TIMEZONE (str): Target timezone for timestamp formatting.\n    FAST_LOAD (bool): Enables performance optimizations at startup.\n  \"\"\"\n  POSTGRES_DB_URI = (\n    'postgresql+psycopg2://methane:methaneCH4@prj-bus-methane-aurora-postgresql-instance-1'\n    '.cdae8kkz3fpi.us-west-2.rds.amazonaws.com/plumetracker'\n  )\n\n  SQLALCHEMY_ENGINE_OPTIONS = {'connect_args': {\n    # 'options': '-c search_path=satellite_tracker_demo1,public -c timezone=UTC'  # practice schema\n    'options': '-c search_path=satellite_tracker_new,public -c timezone=UTC'  # dan's live schema\n  }\n  }\n\n  SECRET_KEY = os.environ.get('SECRET_KEY') or 'secret-key-goes-here'\n  SQLALCHEMY_DATABASE_URI = os.environ.get('DATABASE_URI') or POSTGRES_DB_URI\n  SQLALCHEMY_TRACK_MODIFICATIONS = False\n  # When enabled, Flask will log detailed information about templating files\n  # consider setting to True if you're getting TemplateNotFound errors.\n  EXPLAIN_TEMPLATE_LOADING = False  # Recommended setting for most use cases.\n\n  WTF_CSRF_ENABLED = True\n  LOG_LEVEL = \"INFO\"\n  TIMEZONE = \"America/Los_Angeles\"\n\n  # ---------------------------------------------------------------------\n  # Get other relevant environmental variables here and commandline flags here\n  # for example: set FAST_LOAD=true\n  # ---------------------------------------------------------------------\n  FAST_LOAD = False\n  # flask does not allow for custom arguments so the next block is commented out\n  # if \"--fast-load\" in sys.argv:\n  #   print(f\"--fast-load detected in CLI arguments\")\n  #   FAST_LOAD = True\n  if os.getenv(\"FAST_LOAD\") == \"true\":\n    logger.info(f\"FAST_LOAD detected in CLI arguments\")\n    FAST_LOAD = True\n  logger.info(f\"{FAST_LOAD = }\")\n</code></pre>"},{"location":"reference/arb/portal/config/settings/#arb.portal.config.settings.DevelopmentConfig","title":"<code>DevelopmentConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for local development.</p> <p>Attributes:</p> Name Type Description <code>DEBUG</code> <code>bool</code> <p>Enables debug mode.</p> <code>FLASK_ENV</code> <code>str</code> <p>Flask environment indicator.</p> <code>LOG_LEVEL</code> <code>str</code> <p>Logging level (default: \"DEBUG\").</p> Source code in <code>arb\\portal\\config\\settings.py</code> <pre><code>class DevelopmentConfig(BaseConfig):\n  \"\"\"\n  Configuration for local development.\n\n  Attributes:\n    DEBUG (bool): Enables debug mode.\n    FLASK_ENV (str): Flask environment indicator.\n    LOG_LEVEL (str): Logging level (default: \"DEBUG\").\n  \"\"\"\n  DEBUG = True\n  FLASK_ENV = \"development\"\n  # EXPLAIN_TEMPLATE_LOADING = True\n  LOG_LEVEL = \"DEBUG\"\n</code></pre>"},{"location":"reference/arb/portal/config/settings/#arb.portal.config.settings.ProductionConfig","title":"<code>ProductionConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for deployed production environments.</p> <p>Attributes:</p> Name Type Description <code>DEBUG</code> <code>bool</code> <p>Disables debug features.</p> <code>FLASK_ENV</code> <code>str</code> <p>Environment label for Flask runtime.</p> <code>WTF_CSRF_ENABLED</code> <code>bool</code> <p>Enables CSRF protection.</p> <code>LOG_LEVEL</code> <code>str</code> <p>Logging level (default: \"INFO\").</p> Source code in <code>arb\\portal\\config\\settings.py</code> <pre><code>class ProductionConfig(BaseConfig):\n  \"\"\"\n  Configuration for deployed production environments.\n\n  Attributes:\n    DEBUG (bool): Disables debug features.\n    FLASK_ENV (str): Environment label for Flask runtime.\n    WTF_CSRF_ENABLED (bool): Enables CSRF protection.\n    LOG_LEVEL (str): Logging level (default: \"INFO\").\n  \"\"\"\n  DEBUG = False\n  FLASK_ENV = \"production\"\n  WTF_CSRF_ENABLED = True\n  LOG_LEVEL = \"INFO\"\n</code></pre>"},{"location":"reference/arb/portal/config/settings/#arb.portal.config.settings.TestingConfig","title":"<code>TestingConfig</code>","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Configuration for isolated testing environments.</p> <p>Attributes:</p> Name Type Description <code>TESTING</code> <code>bool</code> <p>Enables Flask test mode.</p> <code>DEBUG</code> <code>bool</code> <p>Enables debug logging.</p> <code>FLASK_ENV</code> <code>str</code> <p>Flask environment label.</p> <code>WTF_CSRF_ENABLED</code> <code>bool</code> <p>Disables CSRF for test convenience.</p> <code>LOG_LEVEL</code> <code>str</code> <p>Logging level (default: \"WARNING\").</p> Source code in <code>arb\\portal\\config\\settings.py</code> <pre><code>class TestingConfig(BaseConfig):\n  \"\"\"\n  Configuration for isolated testing environments.\n\n  Attributes:\n    TESTING (bool): Enables Flask test mode.\n    DEBUG (bool): Enables debug logging.\n    FLASK_ENV (str): Flask environment label.\n    WTF_CSRF_ENABLED (bool): Disables CSRF for test convenience.\n    LOG_LEVEL (str): Logging level (default: \"WARNING\").\n  \"\"\"\n  TESTING = True\n  DEBUG = True\n  FLASK_ENV = \"testing\"\n  WTF_CSRF_ENABLED = False\n  LOG_LEVEL = \"WARNING\"\n</code></pre>"},{"location":"reference/arb/portal/startup/db/","title":"<code>arb.portal.startup.db</code>","text":"<p>Database initialization and reflection routines for the ARB Feedback Portal.</p> <p>These functions are intended to be called during Flask app startup (from <code>create_app()</code>) to configure SQLAlchemy metadata, initialize models, and create missing tables.</p> Usage <p>from startup.db import reflect_database, db_initialize_and_create</p> Notes <ul> <li>SQLAlchemy models must be explicitly imported to register before table creation.</li> <li>Logging is enabled throughout to trace database state and startup flow.</li> </ul>"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.db_create","title":"<code>db_create()</code>","text":"<p>Create all tables defined in SQLAlchemy metadata if they don\u2019t exist.</p> <p>Skips creation if <code>FAST_LOAD=True</code> is set in the app config.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Logs <ul> <li>Warn: If creation is skipped due to FAST_LOAD</li> <li>Info: When table creation begins</li> <li>Debug: After schema creation completes</li> </ul> Source code in <code>arb\\portal\\startup\\db.py</code> <pre><code>def db_create() -&gt; None:\n  \"\"\"\n  Create all tables defined in SQLAlchemy metadata if they don\u2019t exist.\n\n  Skips creation if `FAST_LOAD=True` is set in the app config.\n\n  Returns:\n    None\n\n  Logs:\n    - Warn: If creation is skipped due to FAST_LOAD\n    - Info: When table creation begins\n    - Debug: After schema creation completes\n  \"\"\"\n  if current_app.config.get(\"FAST_LOAD\", False) is True:\n    logger.warning(\"Skipping table creation for FAST_LOAD=True.\")\n    return\n\n  logger.info(\"Creating all missing tables.\")\n  db.create_all()\n  logger.debug(\"Database schema created.\")\n</code></pre>"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.db_initialize","title":"<code>db_initialize()</code>","text":"<p>Import and register SQLAlchemy ORM models.</p> <p>This ensures model classes are registered before calling <code>db.create_all()</code>.</p> Notes <ul> <li>Import must be executed (even if unused) to register models.</li> </ul> Example <p>import arb.portal.sqla_models as models</p> Source code in <code>arb\\portal\\startup\\db.py</code> <pre><code>def db_initialize() -&gt; None:\n  \"\"\"\n  Import and register SQLAlchemy ORM models.\n\n  This ensures model classes are registered before calling `db.create_all()`.\n\n  Notes:\n    - Import must be executed (even if unused) to register models.\n\n  Example:\n    import arb.portal.sqla_models as models\n  \"\"\"\n  logger.info(\"Initializing database models.\")\n  # Add model registration below\n\n  # noinspection PyUnresolvedReferences\n  import arb.portal.sqla_models as models\n</code></pre>"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.db_initialize_and_create","title":"<code>db_initialize_and_create()</code>","text":"<p>Register models and create missing tables in one call.</p> <p>Combines <code>db_initialize()</code> and <code>db_create()</code> for convenience.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Logs <ul> <li>Info: Upon successful database initialization</li> </ul> Source code in <code>arb\\portal\\startup\\db.py</code> <pre><code>def db_initialize_and_create() -&gt; None:\n  \"\"\"\n  Register models and create missing tables in one call.\n\n  Combines `db_initialize()` and `db_create()` for convenience.\n\n  Returns:\n    None\n\n  Logs:\n    - Info: Upon successful database initialization\n  \"\"\"\n  db_initialize()\n  db_create()\n  logger.info(\"Database initialized and tables ensured.\")\n</code></pre>"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.reflect_database","title":"<code>reflect_database()</code>","text":"<p>Reflect the existing database into SQLAlchemy metadata.</p> <p>This enables access to existing tables even without defined ORM models.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Logs <ul> <li>Info: Start of reflection</li> <li>Debug: Completion of reflection</li> </ul> Source code in <code>arb\\portal\\startup\\db.py</code> <pre><code>def reflect_database() -&gt; None:\n  \"\"\"\n  Reflect the existing database into SQLAlchemy metadata.\n\n  This enables access to existing tables even without defined ORM models.\n\n  Returns:\n    None\n\n  Logs:\n    - Info: Start of reflection\n    - Debug: Completion of reflection\n  \"\"\"\n  logger.info(\"Reflecting database metadata.\")\n  db.metadata.reflect(bind=db.engine)\n  logger.debug(\"Reflection complete.\")\n</code></pre>"},{"location":"reference/arb/portal/startup/flask/","title":"<code>arb.portal.startup.flask</code>","text":"<p>Flask-specific application setup utilities for the ARB Feedback Portal.</p> <p>This module configures Flask app behavior, including:   - Jinja2 environment customization   - Upload limits and paths   - Flask logger settings   - Custom template filters and globals</p> <p>Should be invoked during application factory setup:</p> Example <p>from startup.flask import configure_flask_app app = Flask(name) configure_flask_app(app)</p>"},{"location":"reference/arb/portal/startup/flask/#arb.portal.startup.flask.configure_flask_app","title":"<code>configure_flask_app(app)</code>","text":"<p>Apply global configuration to the Flask app instance.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>Flask</code> <p>The Flask application to configure.</p> required Configures <ul> <li>Jinja2 environment:<ul> <li>Enables strict mode for undefined variables</li> <li>Trims and left-strips whitespace blocks</li> <li>Registers custom filters and timezone globals</li> </ul> </li> <li>Upload settings:<ul> <li>Sets <code>UPLOAD_FOLDER</code> to the shared upload path</li> <li>Limits <code>MAX_CONTENT_LENGTH</code> to 16MB</li> </ul> </li> <li>Logger:<ul> <li>Applies <code>LOG_LEVEL</code> from app config</li> <li>Disables Werkzeug color log markup</li> </ul> </li> </ul> Source code in <code>arb\\portal\\startup\\flask.py</code> <pre><code>def configure_flask_app(app: Flask) -&gt; None:\n  \"\"\"\n  Apply global configuration to the Flask app instance.\n\n  Args:\n    app (Flask): The Flask application to configure.\n\n  Configures:\n    - Jinja2 environment:\n        * Enables strict mode for undefined variables\n        * Trims and left-strips whitespace blocks\n        * Registers custom filters and timezone globals\n    - Upload settings:\n        * Sets `UPLOAD_FOLDER` to the shared upload path\n        * Limits `MAX_CONTENT_LENGTH` to 16MB\n    - Logger:\n        * Applies `LOG_LEVEL` from app config\n        * Disables Werkzeug color log markup\n  \"\"\"\n  logger.debug(\"configure_flask_app() called\")\n\n  app.jinja_env.globals[\"app_name\"] = \"CARB Feedback Portal\"\n\n  # -------------------------------------------------------------------------\n  # Logging Configuration\n  # -------------------------------------------------------------------------\n  logger.setLevel(app.config.get(\"LOG_LEVEL\", \"INFO\"))\n  # Logging: Turn off color coding (avoids special terminal characters in log file)\n  werkzeug.serving._log_add_style = False\n\n  # -------------------------------------------------------------------------\n  # Upload Configuration\n  # -------------------------------------------------------------------------\n  app.config['UPLOAD_FOLDER'] = UPLOAD_PATH\n  app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max upload\n\n  # -------------------------------------------------------------------------\n  # Jinja Configuration\n  # -------------------------------------------------------------------------\n  app.jinja_env.undefined = StrictUndefined\n\n  # Jinja: Trim whitespace before/after {{ }} text injection\n  app.jinja_env.trim_blocks = True\n  app.jinja_env.lstrip_blocks = True\n\n  # Jinja: custom filters for debugging and string manipulation\n  app.jinja_env.filters['debug'] = diag_recursive\n  # todo - make sure these datetime filters work in light of the use of native and UTC timestamps\n  app.jinja_env.filters['date_to_string'] = date_to_string\n  app.jinja_env.filters['repr_datetime_to_string'] = repr_datetime_to_string\n  app.jinja_env.filters['args_to_string'] = args_to_string\n\n  # Jinja: expose Python ZoneInfo class to templates for local time conversion\n  app.jinja_env.globals[\"california_tz\"] = ZoneInfo(\"America/Los_Angeles\")\n  logger.debug(\"Flask Jinja2 globals and logging initialized.\")\n</code></pre>"},{"location":"reference/arb/portal/startup/runtime_info/","title":"<code>arb.portal.startup.runtime_info</code>","text":"<p>Provides runtime metadata and dynamic paths for the application.</p> This module defines <ul> <li>Project root and key directories (uploads, logs, static)</li> <li>Operating system detection (Windows, Linux, macOS)</li> <li>Platform-level info useful for conditional behavior</li> <li>Diagnostic tools for runtime environment inspection</li> </ul> Example <p>from startup.runtime_info import (   PROJECT_ROOT, UPLOAD_PATH, LOG_DIR,   IS_WINDOWS, IS_LINUX, IS_MAC,   print_runtime_diagnostics )</p> Notes <ul> <li>The project root directory is assumed to be named \"feedback_portal\".</li> <li>If the app is run from:     feedback_portal/source/production/arb/wsgi.py   then directory resolution is:     Path(file).resolve().parents[0] \u2192 .../arb     Path(file).resolve().parents[1] \u2192 .../production     Path(file).resolve().parents[2] \u2192 .../source     Path(file).resolve().parents[3] \u2192 .../feedback_portal</li> </ul>"},{"location":"reference/arb/portal/startup/runtime_info/#arb.portal.startup.runtime_info.print_runtime_diagnostics","title":"<code>print_runtime_diagnostics()</code>","text":"<p>Print and log detected runtime paths and platform flags for debugging.</p> Outputs <ul> <li>Platform name and OS flags</li> <li>Resolved project root path</li> <li>Paths for uploads, logs, and static assets</li> </ul> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>arb\\portal\\startup\\runtime_info.py</code> <pre><code>def print_runtime_diagnostics() -&gt; None:\n  \"\"\"\n  Print and log detected runtime paths and platform flags for debugging.\n\n  Outputs:\n    - Platform name and OS flags\n    - Resolved project root path\n    - Paths for uploads, logs, and static assets\n\n  Returns:\n    None\n  \"\"\"\n  logger.info(f\"{'PLATFORM':&lt;20} = {PLATFORM}\")\n  logger.info(f\"{'IS_WINDOWS':&lt;20} = {IS_WINDOWS}\")\n  logger.info(f\"{'IS_LINUX':&lt;20} = {IS_LINUX}\")\n  logger.info(f\"{'IS_MAC':&lt;20} = {IS_MAC}\")\n  logger.info(f\"{'PROJECT_ROOT':&lt;20} = {PROJECT_ROOT}\")\n  logger.info(f\"{'UPLOAD_PATH':&lt;20} = {UPLOAD_PATH}\")\n  logger.info(f\"{'LOG_DIR':&lt;20} = {LOG_DIR}\")\n  logger.info(f\"{'STATIC_DIR':&lt;20} = {STATIC_DIR}\")\n</code></pre>"},{"location":"reference/arb/utils/constants/","title":"<code>arb.utils.constants</code>","text":"<p>Shared constants for general utility modules.</p> <p>These constants are designed to remain immutable and serve as application-wide placeholders or configuration defaults.</p>"},{"location":"reference/arb/utils/constants/#arb.utils.constants.PLEASE_SELECT","title":"<code>PLEASE_SELECT = 'Please Select'</code>  <code>module-attribute</code>","text":"<p>str: Placeholder value used in dropdown selectors to indicate a required user selection.</p>"},{"location":"reference/arb/utils/database/","title":"<code>arb.utils.database</code>","text":"<p>Miscellaneous database utilities.</p> <p>Includes helpers for dropping tables, executing SQL scripts, auto-reflecting base metadata, and bulk cleansing of JSON fields across database rows.</p> <p>Intended for use in migrations, diagnostics, and administrative scripts.</p> Requires <ul> <li>SQLAlchemy (for model and metadata operations)</li> <li>A <code>db</code> object and an automapped or declarative <code>base</code> from the Flask app</li> </ul> <p>Functions:</p> Name Description <code>- db_drop_all</code> <p>Drop all database tables</p> <code>- execute_sql_script</code> <p>Run external SQL script files</p> <code>- get_reflected_base</code> <p>Return a SQLAlchemy automap base</p> <code>- cleanse_misc_json</code> <p>Strip \"Please Select\" values from misc_json fields</p>"},{"location":"reference/arb/utils/database/#arb.utils.database.cleanse_misc_json","title":"<code>cleanse_misc_json(db, base, table_name, json_column_name='misc_json', remove_value='Please Select', dry_run=False)</code>","text":"<p>Remove key/value pairs in a JSON column where value == <code>remove_value</code>.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy instance.</p> required <code>base</code> <code>AutomapBase</code> <p>Declarative or automap base.</p> required <code>table_name</code> <code>str</code> <p>Table name to target (e.g., 'incidences').</p> required <code>json_column_name</code> <code>str</code> <p>Column name to scan (default: \"misc_json\").</p> <code>'misc_json'</code> <code>remove_value</code> <code>str</code> <p>Value to match for deletion (default: \"Please Select\").</p> <code>'Please Select'</code> <code>dry_run</code> <code>bool</code> <p>If True, logs changes but rolls back.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If table or column cannot be found or mapped.</p> <code>RuntimeError</code> <p>On failure to commit or query.</p> Source code in <code>arb\\utils\\database.py</code> <pre><code>def cleanse_misc_json(db: SQLAlchemy,\n                      base: AutomapBase,\n                      table_name: str,\n                      json_column_name: str = \"misc_json\",\n                      remove_value: str = \"Please Select\",\n                      dry_run: bool = False) -&gt; None:\n  \"\"\"\n  Remove key/value pairs in a JSON column where value == `remove_value`.\n\n  Args:\n    db (SQLAlchemy): SQLAlchemy instance.\n    base (AutomapBase): Declarative or automap base.\n    table_name (str): Table name to target (e.g., 'incidences').\n    json_column_name (str): Column name to scan (default: \"misc_json\").\n    remove_value (str): Value to match for deletion (default: \"Please Select\").\n    dry_run (bool): If True, logs changes but rolls back.\n\n  Raises:\n    ValueError: If table or column cannot be found or mapped.\n    RuntimeError: On failure to commit or query.\n  \"\"\"\n\n  from arb.utils.sql_alchemy import get_class_from_table_name\n\n  model_cls = get_class_from_table_name(base, table_name)\n  if model_cls is None:\n    raise ValueError(f\"Table '{table_name}' not found or not mapped.\")\n\n  if not hasattr(model_cls, json_column_name):\n    raise ValueError(f\"Column '{json_column_name}' not found on model for table '{table_name}'.\")\n\n  try:\n    rows = db.session.query(model_cls).all()\n    count_total = len(rows)\n    count_modified = 0\n\n    for row in rows:\n      json_data = getattr(row, json_column_name) or {}\n      if not isinstance(json_data, dict):\n        continue\n\n      filtered = {k: v for k, v in json_data.items() if v != remove_value}\n      if filtered != json_data:\n        setattr(row, json_column_name, filtered)\n        from sqlalchemy.orm.attributes import flag_modified\n        flag_modified(row, json_column_name)\n        count_modified += 1\n\n    if dry_run:\n      logger.info(f\"[Dry Run] {count_modified} of {count_total} rows would be modified.\")\n      db.session.rollback()\n    else:\n      db.session.commit()\n      logger.info(f\"[Committed] {count_modified} of {count_total} rows modified.\")\n\n  except Exception as e:\n    db.session.rollback()\n    raise RuntimeError(f\"Error during cleansing: {e}\")\n</code></pre>"},{"location":"reference/arb/utils/database/#arb.utils.database.db_drop_all","title":"<code>db_drop_all(flask_app, db)</code>","text":"<p>Drop all database tables from the configured database.</p> <p>Parameters:</p> Name Type Description Default <code>flask_app</code> <code>Flask</code> <p>The Flask application object.</p> required <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy instance bound to the Flask app.</p> required Warning <p>This is irreversible \u2014 all tables will be deleted.</p> Source code in <code>arb\\utils\\database.py</code> <pre><code>def db_drop_all(flask_app: Flask, db: SQLAlchemy) -&gt; None:\n  \"\"\"\n  Drop all database tables from the configured database.\n\n  Args:\n    flask_app (Flask): The Flask application object.\n    db (SQLAlchemy): SQLAlchemy instance bound to the Flask app.\n\n  Warning:\n    This is irreversible \u2014 all tables will be deleted.\n  \"\"\"\n\n  logger.debug(\"dropping all database tables\")\n</code></pre>"},{"location":"reference/arb/utils/database/#arb.utils.database.execute_sql_script","title":"<code>execute_sql_script(script_path=None, connection=None)</code>","text":"<p>Execute a SQL script using a provided or default SQLite connection.</p> <p>Parameters:</p> Name Type Description Default <code>script_path</code> <code>str | Path | None</code> <p>Path to the <code>.sql</code> script. Defaults to <code>../sql_scripts/script_01.sql</code>.</p> <code>None</code> <code>connection</code> <code>Connection | None</code> <p>SQLite connection (defaults to <code>sqlite3.connect('app.db')</code> if None).</p> <code>None</code> Source code in <code>arb\\utils\\database.py</code> <pre><code>def execute_sql_script(script_path: str | Path = None,\n                       connection: sqlite3.Connection | None = None) -&gt; None:\n  \"\"\"\n  Execute a SQL script using a provided or default SQLite connection.\n\n  Args:\n    script_path (str | Path | None): Path to the `.sql` script. Defaults to `../sql_scripts/script_01.sql`.\n    connection (sqlite3.Connection | None): SQLite connection (defaults to `sqlite3.connect('app.db')` if None).\n  \"\"\"\n  logger.debug(f\"execute_sql_script() called with {script_path=}, {connection=}\")\n\n  if script_path is None:\n    script_path = '../sql_scripts/script_01.sql'\n  if connection is None:\n    connection = sqlite3.connect('app.db')\n\n  with open(script_path) as f:\n    connection.executescript(f.read())\n\n  connection.commit()\n  connection.close()\n</code></pre>"},{"location":"reference/arb/utils/database/#arb.utils.database.get_reflected_base","title":"<code>get_reflected_base(db)</code>","text":"<p>Return a SQLAlchemy automap base using the existing metadata (no re-reflection).</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy instance with metadata.</p> required <p>Returns:</p> Name Type Description <code>AutomapBase</code> <code>AutomapBase</code> <p>Reflected base class.</p> Source code in <code>arb\\utils\\database.py</code> <pre><code>def get_reflected_base(db: SQLAlchemy) -&gt; AutomapBase:\n  \"\"\"\n  Return a SQLAlchemy automap base using the existing metadata (no re-reflection).\n\n  Args:\n    db (SQLAlchemy): SQLAlchemy instance with metadata.\n\n  Returns:\n    AutomapBase: Reflected base class.\n  \"\"\"\n  Base = automap_base(metadata=db.metadata)  # reuse metadata!\n  Base.prepare(db.engine, reflect=False)  # no extra reflection\n  return Base\n</code></pre>"},{"location":"reference/arb/utils/date_and_time/","title":"<code>arb.utils.date_and_time</code>","text":"<p>Datetime parsing and timezone utilities for ISO 8601, UTC/Pacific conversion, repr-format recovery, and recursive datetime transformation in nested structures.</p> <p>Features: - ISO 8601 validation and parsing (via <code>dateutil</code>) - Conversion between UTC and naive Pacific time (Los Angeles) - Safe handling of repr-formatted datetime strings (e.g., \"datetime.datetime(...)\") - Recursive datetime transformations within nested dicts/lists/sets/tuples</p> <p>Timezone policy: - <code>UTC_TZ</code> and <code>PACIFIC_TZ</code> are globally defined using <code>zoneinfo.ZoneInfo</code> - Naive timestamps are only assumed to be UTC if explicitly configured via arguments</p>"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.ca_naive_to_utc_datetime","title":"<code>ca_naive_to_utc_datetime(dt)</code>","text":"<p>Convert a naive Pacific Time datetime to a UTC-aware datetime.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>datetime</code> <p>A naive datetime.</p> required <p>Returns:</p> Name Type Description <code>datetime</code> <code>datetime</code> <p>A UTC-aware datetime.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input is not naive (i.e., has timezone info).</p> Source code in <code>arb\\utils\\date_and_time.py</code> <pre><code>def ca_naive_to_utc_datetime(dt: datetime) -&gt; datetime:\n  \"\"\"\n  Convert a naive Pacific Time datetime to a UTC-aware datetime.\n\n  Args:\n      dt (datetime): A naive datetime.\n\n  Returns:\n      datetime: A UTC-aware datetime.\n\n  Raises:\n      ValueError: If the input is not naive (i.e., has timezone info).\n  \"\"\"\n  if dt.tzinfo is not None:\n    raise ValueError(f\"Expected naive datetime, got {dt!r}\")\n  return dt.replace(tzinfo=PACIFIC_TZ).astimezone(UTC_TZ)\n</code></pre>"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.convert_ca_naive_datetimes_to_utc","title":"<code>convert_ca_naive_datetimes_to_utc(data)</code>","text":"<p>Recursively convert all naive Pacific Time datetimes in a nested structure to UTC-aware datetimes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>object</code> <p>A nested structure (e.g., dict, list, tuple).</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>The same structure with datetime values converted to UTC-aware format.</p> Source code in <code>arb\\utils\\date_and_time.py</code> <pre><code>def convert_ca_naive_datetimes_to_utc(data: object) -&gt; object:\n  \"\"\"\n  Recursively convert all naive Pacific Time datetimes in a nested structure to UTC-aware datetimes.\n\n  Args:\n      data (object): A nested structure (e.g., dict, list, tuple).\n\n  Returns:\n      object: The same structure with datetime values converted to UTC-aware format.\n  \"\"\"\n  if isinstance(data, datetime):\n    return ca_naive_to_utc_datetime(data)\n  elif isinstance(data, Mapping):\n    return {convert_ca_naive_datetimes_to_utc(k): convert_ca_naive_datetimes_to_utc(v) for k, v in data.items()}\n  elif isinstance(data, list):\n    return [convert_ca_naive_datetimes_to_utc(i) for i in data]\n  elif isinstance(data, tuple):\n    return tuple(convert_ca_naive_datetimes_to_utc(i) for i in data)\n  elif isinstance(data, set):\n    return {convert_ca_naive_datetimes_to_utc(i) for i in data}\n  return data\n</code></pre>"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.convert_datetimes_to_ca_naive","title":"<code>convert_datetimes_to_ca_naive(data, assume_naive_is_utc=False, utc_strict=True)</code>","text":"<p>Recursively convert all datetime objects in a nested structure to naive Pacific Time.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>object</code> <p>A structure that may include datetime values (dict, list, etc.).</p> required <code>assume_naive_is_utc</code> <code>bool</code> <p>Whether to treat naive datetimes as UTC.</p> <code>False</code> <code>utc_strict</code> <code>bool</code> <p>Whether to enforce that input datetimes are explicitly UTC.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>A structure of the same shape, with datetime values converted to naive Pacific.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; from zoneinfo import ZoneInfo\n&gt;&gt;&gt; nested = {\n...     datetime(2025, 4, 23, 15, 0, tzinfo=ZoneInfo(\"UTC\")): [\n...         {\"created\": datetime(2025, 4, 23, 18, 0)},\n...         (datetime(2025, 4, 23, 20, 0, tzinfo=ZoneInfo(\"UTC\")),)\n...     ]\n&gt;&gt;&gt; convert_datetimes_to_ca_naive(nested)\n{\n    datetime.datetime(2025, 4, 23, 8, 0): [\n        {\"created\": datetime.datetime(2025, 4, 23, 11, 0)},\n        (datetime.datetime(2025, 4, 23, 13, 0),)\n    ]\n}\n</code></pre> Source code in <code>arb\\utils\\date_and_time.py</code> <pre><code>def convert_datetimes_to_ca_naive(data: object, assume_naive_is_utc: bool = False, utc_strict: bool = True) -&gt; object:\n  \"\"\"\n  Recursively convert all datetime objects in a nested structure to naive Pacific Time.\n\n  Args:\n      data (object): A structure that may include datetime values (dict, list, etc.).\n      assume_naive_is_utc (bool): Whether to treat naive datetimes as UTC.\n      utc_strict (bool): Whether to enforce that input datetimes are explicitly UTC.\n\n  Returns:\n      object: A structure of the same shape, with datetime values converted to naive Pacific.\n\n  Examples:\n      &gt;&gt;&gt; from datetime import datetime\n      &gt;&gt;&gt; from zoneinfo import ZoneInfo\n      &gt;&gt;&gt; nested = {\n      ...     datetime(2025, 4, 23, 15, 0, tzinfo=ZoneInfo(\"UTC\")): [\n      ...         {\"created\": datetime(2025, 4, 23, 18, 0)},\n      ...         (datetime(2025, 4, 23, 20, 0, tzinfo=ZoneInfo(\"UTC\")),)\n      ...     ]\n      &gt;&gt;&gt; convert_datetimes_to_ca_naive(nested)\n      {\n          datetime.datetime(2025, 4, 23, 8, 0): [\n              {\"created\": datetime.datetime(2025, 4, 23, 11, 0)},\n              (datetime.datetime(2025, 4, 23, 13, 0),)\n          ]\n      }\n  \"\"\"\n  if isinstance(data, datetime):\n    return datetime_to_ca_naive(data, assume_naive_is_utc, utc_strict)\n  elif isinstance(data, Mapping):\n    return {\n      convert_datetimes_to_ca_naive(k, assume_naive_is_utc, utc_strict):\n        convert_datetimes_to_ca_naive(v, assume_naive_is_utc, utc_strict)\n      for k, v in data.items()\n    }\n  elif isinstance(data, list):\n    return [convert_datetimes_to_ca_naive(i, assume_naive_is_utc, utc_strict) for i in data]\n  elif isinstance(data, tuple):\n    return tuple(convert_datetimes_to_ca_naive(i, assume_naive_is_utc, utc_strict) for i in data)\n  elif isinstance(data, set):\n    return {convert_datetimes_to_ca_naive(i, assume_naive_is_utc, utc_strict) for i in data}\n  return data\n</code></pre>"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.date_to_string","title":"<code>date_to_string(dt, str_format='%Y-%m-%dT%H:%M')</code>","text":"<p>Convert a datetime object to a formatted string.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>datetime | None</code> <p>A datetime object or None.</p> required <code>str_format</code> <code>str</code> <p>Format string used with <code>strftime</code>.</p> <code>'%Y-%m-%dT%H:%M'</code> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: The formatted string if input is not None; otherwise, None.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>x</code> is not a datetime object.</p> Source code in <code>arb\\utils\\date_and_time.py</code> <pre><code>def date_to_string(dt: datetime | None, str_format: str = \"%Y-%m-%dT%H:%M\") -&gt; str | None:\n  \"\"\"\n  Convert a datetime object to a formatted string.\n\n  Args:\n      dt (datetime | None): A datetime object or None.\n      str_format (str): Format string used with `strftime`.\n\n  Returns:\n      str | None: The formatted string if input is not None; otherwise, None.\n\n  Raises:\n      TypeError: If `x` is not a datetime object.\n  \"\"\"\n\n  if dt is None:\n    return None\n  if not isinstance(dt, datetime):\n    raise TypeError(f'x must be datetime.datetime or None. {dt=}')\n  return dt.strftime(str_format)\n</code></pre>"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.datetime_to_ca_naive","title":"<code>datetime_to_ca_naive(dt, assume_naive_is_utc=False, utc_strict=True)</code>","text":"<p>Convert a datetime (UTC or naive) to naive Pacific Time.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>datetime</code> <p>The datetime to convert.</p> required <code>assume_naive_is_utc</code> <code>bool</code> <p>If True, interpret naive datetime as UTC.</p> <code>False</code> <code>utc_strict</code> <code>bool</code> <p>If True, raise an error unless datetime is explicitly UTC.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>datetime</code> <code>datetime</code> <p>A naive Pacific Time datetime.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If datetime is naive and <code>assume_naive_is_utc</code> is False,         or if UTC check fails under <code>utc_strict</code>.</p> Source code in <code>arb\\utils\\date_and_time.py</code> <pre><code>def datetime_to_ca_naive(dt: datetime,\n                         assume_naive_is_utc: bool = False,\n                         utc_strict: bool = True) -&gt; datetime:\n  \"\"\"\n  Convert a datetime (UTC or naive) to naive Pacific Time.\n\n  Args:\n      dt (datetime): The datetime to convert.\n      assume_naive_is_utc (bool): If True, interpret naive datetime as UTC.\n      utc_strict (bool): If True, raise an error unless datetime is explicitly UTC.\n\n  Returns:\n      datetime: A naive Pacific Time datetime.\n\n  Raises:\n      ValueError: If datetime is naive and `assume_naive_is_utc` is False,\n                  or if UTC check fails under `utc_strict`.\n  \"\"\"\n  if dt.tzinfo is None:\n    if assume_naive_is_utc:\n      logger.warning(\"Assuming UTC for naive datetime.\")\n      dt = dt.replace(tzinfo=UTC_TZ)\n    else:\n      raise ValueError(\"Naive datetime provided without assume_naive_is_utc=True\")\n  elif utc_strict and dt.tzinfo != UTC_TZ:\n    raise ValueError(\"datetime must be UTC when utc_strict=True\")\n\n  return dt.astimezone(PACIFIC_TZ).replace(tzinfo=None)\n</code></pre>"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.is_datetime_naive","title":"<code>is_datetime_naive(dt)</code>","text":"<p>Check whether a datetime object is naive (lacks timezone info).</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>datetime</code> <p>A datetime instance.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the datetime is naive (tzinfo is None or utcoffset is None); False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_datetime_naive(datetime.now())\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; from zoneinfo import ZoneInfo\n&gt;&gt;&gt; is_datetime_naive(datetime.now(tz=ZoneInfo(\"UTC\")))\nFalse\n</code></pre> Source code in <code>arb\\utils\\date_and_time.py</code> <pre><code>def is_datetime_naive(dt: datetime) -&gt; bool:\n  \"\"\"\n  Check whether a datetime object is naive (lacks timezone info).\n\n  Args:\n      dt (datetime): A datetime instance.\n\n  Returns:\n      bool: True if the datetime is naive (tzinfo is None or utcoffset is None); False otherwise.\n\n  Examples:\n      &gt;&gt;&gt; is_datetime_naive(datetime.now())\n      True\n\n      &gt;&gt;&gt; from zoneinfo import ZoneInfo\n      &gt;&gt;&gt; is_datetime_naive(datetime.now(tz=ZoneInfo(\"UTC\")))\n      False\n  \"\"\"\n  return dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None\n</code></pre>"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.is_iso8601","title":"<code>is_iso8601(string)</code>","text":"<p>Determine whether a string is a valid ISO 8601 datetime.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>The string to validate.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the string conforms to ISO 8601 datetime format, False otherwise.</p> Notes <ul> <li>Uses <code>dateutil.parser.isoparse</code> for strict ISO 8601 parsing.</li> <li>ISO 8601 requires a full date (YYYY-MM-DD) at minimum. Time-only strings are invalid.</li> <li>This does not validate durations, ordinal dates, or week dates.</li> </ul> <p>Examples:</p> <p>Valid cases (returns True):     - \"2024-12-31\"     - \"2024-12-31T23:59:59\"     - \"2024-12-31T23:59:59Z\"             # UTC/Zulu time     - \"2024-12-31T23:59:59+00:00\"        # Timezone offset     - \"2024-12-31T23:59:59.123456\"       # Microsecond precision     - \"2024-12-31T23:59:59.123+02:00\"    # Fractional seconds with TZ</p> <p>Invalid cases (returns False):     - \"12/31/2024\"            # US-style date format     - \"2024-31-12\"            # Invalid format (day and month swapped)     - \"2024\"                  # Year only, not a complete date or datetime     - \"P1Y2M\"                 # ISO 8601 duration, not a datetime     - \"Just some text\"        # Clearly not a date     - \"2024-12-31 23:59:59\"   # Space instead of 'T' (technically invalid ISO 8601)</p> <pre><code># Time-only values \u2014 invalid because ISO 8601 requires at least a full date\n- \"23:59\"                 # Just time (no date)\n- \"23:59:59\"              # Time only, still invalid\n- \"T23:59:59Z\"            # Prefixed with 'T' but still no date\n- \"23:59:59+00:00\"        # Time with timezone, but missing a date\n- \"T23:59:59.123456\"      # Microsecond time, still not valid without date\n</code></pre> Notes <ul> <li>ISO 8601 datetimes require at least a full date (YYYY-MM-DD). Time-only strings are not valid by themselves.</li> <li>This does not validate ISO 8601 durations, ordinal dates, or week dates.</li> <li>This routine strictly uses <code>dateutil.parser.isoparse</code>, which rejects incomplete formats like time-only values.</li> </ul> Source code in <code>arb\\utils\\date_and_time.py</code> <pre><code>def is_iso8601(string: str) -&gt; bool:\n  \"\"\"\n  Determine whether a string is a valid ISO 8601 datetime.\n\n  Args:\n      string (str): The string to validate.\n\n  Returns:\n      bool: True if the string conforms to ISO 8601 datetime format, False otherwise.\n\n  Notes:\n      - Uses `dateutil.parser.isoparse` for strict ISO 8601 parsing.\n      - ISO 8601 requires a full date (YYYY-MM-DD) at minimum. Time-only strings are invalid.\n      - This does not validate durations, ordinal dates, or week dates.\n\n  Examples:\n      Valid cases (returns True):\n          - \"2024-12-31\"\n          - \"2024-12-31T23:59:59\"\n          - \"2024-12-31T23:59:59Z\"             # UTC/Zulu time\n          - \"2024-12-31T23:59:59+00:00\"        # Timezone offset\n          - \"2024-12-31T23:59:59.123456\"       # Microsecond precision\n          - \"2024-12-31T23:59:59.123+02:00\"    # Fractional seconds with TZ\n\n      Invalid cases (returns False):\n          - \"12/31/2024\"            # US-style date format\n          - \"2024-31-12\"            # Invalid format (day and month swapped)\n          - \"2024\"                  # Year only, not a complete date or datetime\n          - \"P1Y2M\"                 # ISO 8601 duration, not a datetime\n          - \"Just some text\"        # Clearly not a date\n          - \"2024-12-31 23:59:59\"   # Space instead of 'T' (technically invalid ISO 8601)\n\n          # Time-only values \u2014 invalid because ISO 8601 requires at least a full date\n          - \"23:59\"                 # Just time (no date)\n          - \"23:59:59\"              # Time only, still invalid\n          - \"T23:59:59Z\"            # Prefixed with 'T' but still no date\n          - \"23:59:59+00:00\"        # Time with timezone, but missing a date\n          - \"T23:59:59.123456\"      # Microsecond time, still not valid without date\n\n  Notes:\n      - ISO 8601 datetimes require at least a full date (YYYY-MM-DD). Time-only strings are not valid by themselves.\n      - This does not validate ISO 8601 durations, ordinal dates, or week dates.\n      - This routine strictly uses `dateutil.parser.isoparse`, which rejects incomplete formats like time-only values.\n  \"\"\"\n  try:\n    parser.isoparse(string)\n    return True\n  except ValueError:\n    return False\n</code></pre>"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.iso8601_to_utc_dt","title":"<code>iso8601_to_utc_dt(iso_string, error_on_missing_tz=True)</code>","text":"<p>Parse an ISO 8601 string and return a timezone-aware datetime object in UTC.</p> <p>This function uses <code>dateutil.parser.isoparse()</code> for robust ISO 8601 parsing, including support for variations like 'Z' (UTC shorthand) and missing separators.</p> <p>Parameters:</p> Name Type Description Default <code>iso_string</code> <code>str</code> <p>An ISO 8601-formatted datetime string.</p> required <code>error_on_missing_tz</code> <code>bool</code> <p>If True, raises a ValueError when the string has no timezone info.</p> <code>True</code> <p>Returns:</p> Type Description <code>datetime</code> <p>A timezone-aware datetime object in UTC.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input string is invalid or lacks timezone info and         <code>error_on_missing_tz</code> is True.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; iso8601_to_utc_dt(\"2025-04-20T14:30:00+00:00\")\ndatetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\"))\n</code></pre> <pre><code>&gt;&gt;&gt; iso8601_to_utc_dt(\"2025-04-20T14:30:00Z\")\ndatetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\"))\n</code></pre> <pre><code>&gt;&gt;&gt; iso8601_to_utc_dt(\"2025-04-20 14:30:00\", error_on_missing_tz=False)\ndatetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\"))\n&gt;&gt;&gt; iso8601_to_utc_dt(\"2025-04-20T14:30:00Z\")\ndatetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo('UTC'))\n</code></pre> Source code in <code>arb\\utils\\date_and_time.py</code> <pre><code>def iso8601_to_utc_dt(iso_string: str, error_on_missing_tz: bool = True) -&gt; datetime:\n  \"\"\"\n  Parse an ISO 8601 string and return a timezone-aware datetime object in UTC.\n\n  This function uses `dateutil.parser.isoparse()` for robust ISO 8601 parsing,\n  including support for variations like 'Z' (UTC shorthand) and missing separators.\n\n  Args:\n      iso_string: An ISO 8601-formatted datetime string.\n      error_on_missing_tz: If True, raises a ValueError when the string has no timezone info.\n\n  Returns:\n      A timezone-aware datetime object in UTC.\n\n  Raises:\n      ValueError: If the input string is invalid or lacks timezone info and\n                  `error_on_missing_tz` is True.\n\n  Examples:\n      &gt;&gt;&gt; iso8601_to_utc_dt(\"2025-04-20T14:30:00+00:00\")\n      datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\"))\n\n      &gt;&gt;&gt; iso8601_to_utc_dt(\"2025-04-20T14:30:00Z\")\n      datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\"))\n\n      &gt;&gt;&gt; iso8601_to_utc_dt(\"2025-04-20 14:30:00\", error_on_missing_tz=False)\n      datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\"))\n      &gt;&gt;&gt; iso8601_to_utc_dt(\"2025-04-20T14:30:00Z\")\n      datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo('UTC'))\n  \"\"\"\n  try:\n    dt = parser.isoparse(iso_string)\n  except (ValueError, TypeError) as e:\n    raise ValueError(f\"Invalid ISO 8601 datetime string: '{iso_string}'\") from e\n\n  if dt.tzinfo is None:\n    if error_on_missing_tz:\n      raise ValueError(\"Missing timezone info in ISO 8601 string.\")\n    logger.warning(f\"Assuming UTC for naive ISO string: {iso_string}\")\n    dt = dt.replace(tzinfo=UTC_TZ)\n\n  return dt.astimezone(UTC_TZ)\n</code></pre>"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.parse_unknown_datetime","title":"<code>parse_unknown_datetime(date_str)</code>","text":"<p>Attempt to parse an arbitrary string into a datetime using <code>dateutil.parser.parse()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>date_str</code> <code>str</code> <p>A date or datetime string in an unknown format.</p> required <p>Returns:</p> Type Description <code>datetime | None</code> <p>datetime | None: Parsed datetime object if successful; otherwise, None.</p> <p>Raises:</p> Type Description <code>None</code> <p>Gracefully handles errors internally.</p> Notes <ul> <li>This method is lenient and accepts a wide range of human-readable formats.</li> <li>Returns None if the string is empty, not a string, or unparseable.</li> </ul> Source code in <code>arb\\utils\\date_and_time.py</code> <pre><code>def parse_unknown_datetime(date_str: str) -&gt; datetime | None:\n  \"\"\"\n  Attempt to parse an arbitrary string into a datetime using `dateutil.parser.parse()`.\n\n  Args:\n      date_str (str): A date or datetime string in an unknown format.\n\n  Returns:\n      datetime | None: Parsed datetime object if successful; otherwise, None.\n\n  Raises:\n      None: Gracefully handles errors internally.\n\n  Notes:\n      - This method is lenient and accepts a wide range of human-readable formats.\n      - Returns None if the string is empty, not a string, or unparseable.\n  \"\"\"\n\n  if not isinstance(date_str, str) or not date_str:\n    return None\n  try:\n    return parser.parse(date_str)\n  except (ValueError, TypeError):\n    return None\n</code></pre>"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.repr_datetime_to_string","title":"<code>repr_datetime_to_string(datetime_string)</code>","text":"<p>Convert a repr-formatted datetime string into an ISO 8601 string.</p> <p>Parameters:</p> Name Type Description Default <code>datetime_string</code> <code>str | None</code> <p>A string in the format \"datetime.datetime(...)\"</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: ISO 8601 string or None if parsing fails or input is None.</p> Source code in <code>arb\\utils\\date_and_time.py</code> <pre><code>def repr_datetime_to_string(datetime_string: str | None) -&gt; str | None:\n  \"\"\"\n  Convert a repr-formatted datetime string into an ISO 8601 string.\n\n  Args:\n      datetime_string (str | None): A string in the format \"datetime.datetime(...)\"\n\n  Returns:\n      str | None: ISO 8601 string or None if parsing fails or input is None.\n  \"\"\"\n  if datetime_string is None:\n    return None\n  dt = str_to_datetime(datetime_string)\n  return dt.isoformat() if dt else None\n</code></pre>"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.run_diagnostics","title":"<code>run_diagnostics()</code>","text":"<p>Run a series of diagnostic operations to verify correctness of datetime utilities.</p> Demonstrates <ul> <li>ISO 8601 parsing to UTC</li> <li>Conversion to naive Pacific time</li> <li>Round-trip UTC conversion</li> <li>Recursive conversion in nested data structures</li> </ul> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>arb\\utils\\date_and_time.py</code> <pre><code>def run_diagnostics() -&gt; None:\n  \"\"\"\n  Run a series of diagnostic operations to verify correctness of datetime utilities.\n\n  Demonstrates:\n      - ISO 8601 parsing to UTC\n      - Conversion to naive Pacific time\n      - Round-trip UTC conversion\n      - Recursive conversion in nested data structures\n\n  Returns:\n      None\n  \"\"\"\n  from pprint import pprint\n  print(\"Running diagnostics on datetime utilities...\\n\")\n\n  sample_iso = \"2025-05-05T10:00:00Z\"\n  dt = iso8601_to_utc_dt(sample_iso)\n  print(\"Parsed ISO 8601 to UTC:\", dt)\n\n  naive_pacific = datetime_to_ca_naive(dt)\n  print(\"Converted to naive Pacific:\", naive_pacific)\n\n  round_trip = ca_naive_to_utc_datetime(naive_pacific)\n  print(\"Round-trip back to UTC:\", round_trip)\n\n  nested_data = {\n    dt: [\n      {\"created\": dt},\n      (dt,)\n    ]\n  }\n\n  pacific_data = convert_datetimes_to_ca_naive(nested_data)\n  print(\"\\nConverted nested UTC datetimes to Pacific:\")\n  pprint(pacific_data)\n\n  restored = convert_ca_naive_datetimes_to_utc(pacific_data)\n  print(\"\\nRestored nested Pacific datetimes to UTC:\")\n  pprint(restored)\n</code></pre>"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.str_to_datetime","title":"<code>str_to_datetime(datetime_str)</code>","text":"<p>Convert a <code>repr()</code>-formatted string into a datetime object.</p> <p>Parameters:</p> Name Type Description Default <code>datetime_str</code> <code>str</code> <p>Example: \"datetime.datetime(2024, 11, 15, 14, 30, 45)\"</p> required <p>Returns:</p> Type Description <code>datetime | None</code> <p>datetime | None: Parsed datetime object if matched; otherwise, None.</p> Notes <p>This reverses <code>repr(datetime)</code> stringification for debugging recovery.</p> Source code in <code>arb\\utils\\date_and_time.py</code> <pre><code>def str_to_datetime(datetime_str: str) -&gt; datetime | None:\n  \"\"\"\n  Convert a `repr()`-formatted string into a datetime object.\n\n  Args:\n      datetime_str (str): Example: \"datetime.datetime(2024, 11, 15, 14, 30, 45)\"\n\n  Returns:\n      datetime | None: Parsed datetime object if matched; otherwise, None.\n\n  Notes:\n      This reverses `repr(datetime)` stringification for debugging recovery.\n  \"\"\"\n  match = re.search(r\"datetime\\.datetime\\(([^)]+)\\)\", datetime_str)\n  if match:\n    date_parts = list(map(int, match.group(1).split(\", \")))\n    return datetime(*date_parts)\n  return None\n</code></pre>"},{"location":"reference/arb/utils/diagnostics/","title":"<code>arb.utils.diagnostics</code>","text":"<p>Diagnostic utilities for inspecting and logging Python objects.</p> <p>Provides: - Object introspection for development/debugging - Attribute/value logging (including hidden and callable members) - Dictionary comparisons and formatting - Recursive HTML-safe rendering of complex data structures - Integration with Flask for developer-oriented diagnostics</p> <p>Intended primarily for use in debug environments, template rendering, or ad-hoc inspection of application state during development.</p>"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.compare_dicts","title":"<code>compare_dicts(dict1, dict2, dict1_name=None, dict2_name=None)</code>","text":"<p>Compare two dictionaries and log differences in keys and values.</p> <p>Parameters:</p> Name Type Description Default <code>dict1</code> <code>dict</code> <p>First dictionary.</p> required <code>dict2</code> <code>dict</code> <p>Second dictionary.</p> required <code>dict1_name</code> <code>str | None</code> <p>Optional label for first dictionary in logs.</p> <code>None</code> <code>dict2_name</code> <code>str | None</code> <p>Optional label for second dictionary in logs.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if dictionaries are equivalent; False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dict1 = {\"a\": 1, \"b\": 2, \"c\": 3}\n&gt;&gt;&gt; dict2 = {\"a\": 1, \"b\": 4, \"d\": 5}\n&gt;&gt;&gt; compare_dicts(dict1, dict2)\nFalse\n</code></pre> Source code in <code>arb\\utils\\diagnostics.py</code> <pre><code>def compare_dicts(dict1: dict,\n                  dict2: dict,\n                  dict1_name: str | None = None,\n                  dict2_name: str | None = None) -&gt; bool:\n  \"\"\"\n  Compare two dictionaries and log differences in keys and values.\n\n  Args:\n      dict1 (dict): First dictionary.\n      dict2 (dict): Second dictionary.\n      dict1_name (str | None): Optional label for first dictionary in logs.\n      dict2_name (str | None): Optional label for second dictionary in logs.\n\n  Returns:\n      bool: True if dictionaries are equivalent; False otherwise.\n\n  Examples:\n      &gt;&gt;&gt; dict1 = {\"a\": 1, \"b\": 2, \"c\": 3}\n      &gt;&gt;&gt; dict2 = {\"a\": 1, \"b\": 4, \"d\": 5}\n      &gt;&gt;&gt; compare_dicts(dict1, dict2)\n      False\n  \"\"\"\n  dict1_name = dict1_name or \"dict_1\"\n  dict2_name = dict2_name or \"dict_2\"\n  logger.debug(f\"compare_dicts called to compare {dict1_name} with {dict2_name}\")\n\n  keys_in_dict1_not_in_dict2 = set(dict1) - set(dict2)\n  keys_in_dict2_not_in_dict1 = set(dict2) - set(dict1)\n\n  differing_values = {\n    key: (dict1[key], dict2[key])\n    for key in dict1.keys() &amp; dict2.keys()\n    if dict1[key] != dict2[key]\n  }\n\n  if keys_in_dict1_not_in_dict2 or keys_in_dict2_not_in_dict1 or differing_values:\n    logger.debug(\"Key differences:\")\n    if keys_in_dict1_not_in_dict2:\n      logger.debug(f\"- In {dict1_name} but not in {dict2_name}: {sorted(keys_in_dict1_not_in_dict2)}\")\n    if keys_in_dict2_not_in_dict1:\n      logger.debug(f\"- In {dict2_name} but not in {dict1_name}: {sorted(keys_in_dict2_not_in_dict1)}\")\n\n    if differing_values:\n      logger.debug(\"Value differences:\")\n      for key, (v1, v2) in dict(sorted(differing_values.items())).items():\n        logger.debug(f\"- Key: '{key}', {dict1_name}: {v1}, {dict2_name}: {v2}\")\n\n    return False\n\n  return True\n</code></pre>"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.diag_recursive","title":"<code>diag_recursive(x, depth=0, index=0)</code>","text":"<p>Recursively log the structure and values of a nested iterable.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>object</code> <p>Input object to inspect.</p> required <code>depth</code> <code>int</code> <p>Current recursion depth.</p> <code>0</code> <code>index</code> <code>int</code> <p>Index at the current level (if applicable).</p> <code>0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <p>Strings are treated as non-iterables.</p> Source code in <code>arb\\utils\\diagnostics.py</code> <pre><code>def diag_recursive(x: object, depth: int = 0, index: int = 0) -&gt; None:\n  \"\"\"\n  Recursively log the structure and values of a nested iterable.\n\n  Args:\n      x (object): Input object to inspect.\n      depth (int): Current recursion depth.\n      index (int): Index at the current level (if applicable).\n\n  Returns:\n      None\n\n  Notes:\n      Strings are treated as non-iterables.\n  \"\"\"\n  indent = ' ' * 3 * depth\n  if depth == 0:\n    logger.debug(f\"diag_recursive diagnostics called\\n{'-' * 120}\")\n    logger.debug(f\"Type: {type(x)}, Value: {x}\")\n  else:\n    logger.debug(f\"{indent} Depth: {depth}, Index: {index}, Type: {type(x)}, Value: {x}\")\n\n  if not isinstance(x, str):\n    try:\n      for i, y in enumerate(x):\n        diag_recursive(y, depth + 1, index=i)\n    except TypeError:\n      pass\n</code></pre>"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.dict_to_str","title":"<code>dict_to_str(x, depth=0)</code>","text":"<p>Convert a dictionary to a pretty-printed multiline string.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>dict</code> <p>Dictionary to convert.</p> required <code>depth</code> <code>int</code> <p>Current indentation depth for nested dictionaries.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of dictionary with indentation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {\"a\": 1, \"b\": {\"c\": 2}}\n&gt;&gt;&gt; print(dict_to_str(d))\na:\n   1\nb:\n   c:\n      2\n</code></pre> Source code in <code>arb\\utils\\diagnostics.py</code> <pre><code>def dict_to_str(x: dict, depth: int = 0) -&gt; str:\n  \"\"\"\n  Convert a dictionary to a pretty-printed multiline string.\n\n  Args:\n      x (dict): Dictionary to convert.\n      depth (int): Current indentation depth for nested dictionaries.\n\n  Returns:\n      str: String representation of dictionary with indentation.\n\n  Examples:\n      &gt;&gt;&gt; d = {\"a\": 1, \"b\": {\"c\": 2}}\n      &gt;&gt;&gt; print(dict_to_str(d))\n      a:\n         1\n      b:\n         c:\n            2\n  \"\"\"\n  msg = \"\"\n  indent = ' ' * 3 * depth\n  for k, v in x.items():\n    msg += f\"{indent}{k}:\\n\"\n    if isinstance(v, dict):\n      msg += dict_to_str(v, depth=depth + 1)\n    else:\n      msg += f\"   {indent}{v}\\n\"\n  return msg\n</code></pre>"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.get_changed_fields","title":"<code>get_changed_fields(new_dict, old_dict)</code>","text":"<p>Extract fields from new_dict that differ from old_dict.</p> <p>Parameters:</p> Name Type Description Default <code>new_dict</code> <code>dict</code> <p>New/updated values (e.g., from a form).</p> required <code>old_dict</code> <code>dict</code> <p>Old/reference values (e.g., from model JSON).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Keys with values that have changed.</p> <p>Notes: - Only keys present in new_dict are considered. This prevents unrelated fields   from being overwritten when merging partial form data into a larger stored structure.</p> Source code in <code>arb\\utils\\diagnostics.py</code> <pre><code>def get_changed_fields(new_dict: dict, old_dict: dict) -&gt; dict:\n  \"\"\"\n  Extract fields from new_dict that differ from old_dict.\n\n  Args:\n      new_dict (dict): New/updated values (e.g., from a form).\n      old_dict (dict): Old/reference values (e.g., from model JSON).\n\n  Returns:\n      dict: Keys with values that have changed.\n\n  Notes:\n  - Only keys present in new_dict are considered. This prevents unrelated fields\n    from being overwritten when merging partial form data into a larger stored structure.\n  \"\"\"\n\n  changes = {}\n  for key in new_dict:\n    if new_dict[key] != old_dict.get(key):\n      changes[key] = new_dict[key]\n  return changes\n</code></pre>"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.list_differences","title":"<code>list_differences(iterable_01, iterable_02, iterable_01_name='List 1', iterable_02_name='List 2', print_warning=False)</code>","text":"<p>Identify differences between two iterable objects (list or dict).</p> <p>Parameters:</p> Name Type Description Default <code>iterable_01</code> <code>list | dict</code> <p>First iterable object to compare.</p> required <code>iterable_02</code> <code>list | dict</code> <p>Second iterable object to compare.</p> required <code>iterable_01_name</code> <code>str</code> <p>Label for the first iterable in log messages.</p> <code>'List 1'</code> <code>iterable_02_name</code> <code>str</code> <p>Label for the second iterable in log messages.</p> <code>'List 2'</code> <code>print_warning</code> <code>bool</code> <p>If True, logs warnings for non-overlapping items.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[list, list]</code> <p>tuple[list, list]: - Items in <code>iterable_01</code> but not in <code>iterable_02</code> - Items in <code>iterable_02</code> but not in <code>iterable_01</code></p> <p>Examples:     &gt;&gt;&gt; list_differences([\"a\", \"b\"], [\"b\", \"c\"])     ([\"a\"], [\"c\"])</p> Source code in <code>arb\\utils\\diagnostics.py</code> <pre><code>def list_differences(iterable_01: list | dict,\n                     iterable_02: list | dict,\n                     iterable_01_name: str = \"List 1\",\n                     iterable_02_name: str = \"List 2\",\n                     print_warning: bool = False) -&gt; tuple[list, list]:\n  \"\"\"\n  Identify differences between two iterable objects (list or dict).\n\n  Args:\n      iterable_01 (list | dict): First iterable object to compare.\n      iterable_02 (list | dict): Second iterable object to compare.\n      iterable_01_name (str): Label for the first iterable in log messages.\n      iterable_02_name (str): Label for the second iterable in log messages.\n      print_warning (bool): If True, logs warnings for non-overlapping items.\n\n  Returns:\n      tuple[list, list]:\n          - Items in `iterable_01` but not in `iterable_02`\n          - Items in `iterable_02` but not in `iterable_01`\n  Examples:\n      &gt;&gt;&gt; list_differences([\"a\", \"b\"], [\"b\", \"c\"])\n      ([\"a\"], [\"c\"])\n  \"\"\"\n  in_iterable_1_only = [x for x in iterable_01 if x not in iterable_02]\n  in_iterable_2_only = [x for x in iterable_02 if x not in iterable_01]\n\n  if print_warning:\n    if in_iterable_1_only:\n      logger.warning(\n        f\"Warning: {iterable_01_name} has {len(in_iterable_1_only)} item(s) not in {iterable_02_name}:\\t{in_iterable_1_only}\")\n    if in_iterable_2_only:\n      logger.warning(\n        f\"Warning: {iterable_02_name} has {len(in_iterable_2_only)} item(s) not in {iterable_01_name}:\\t{in_iterable_2_only}\")\n\n  return in_iterable_1_only, in_iterable_2_only\n</code></pre>"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.obj_diagnostics","title":"<code>obj_diagnostics(obj, include_hidden=False, include_functions=False, message=None)</code>","text":"<p>Log detailed diagnostics about an object's attributes and values.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The object to inspect.</p> required <code>include_hidden</code> <code>bool</code> <p>Whether to include private attributes (starting with <code>_</code>).</p> <code>False</code> <code>include_functions</code> <code>bool</code> <p>Whether to include methods or callable attributes.</p> <code>False</code> <code>message</code> <code>str | None</code> <p>Optional prefix message to label the diagnostic output.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>obj_diagnostics(my_object, include_hidden=True, include_functions=True)</p> Source code in <code>arb\\utils\\diagnostics.py</code> <pre><code>def obj_diagnostics(obj: object,\n                    include_hidden: bool = False,\n                    include_functions: bool = False,\n                    message: str | None = None) -&gt; None:\n  \"\"\"\n  Log detailed diagnostics about an object's attributes and values.\n\n  Args:\n      obj (object): The object to inspect.\n      include_hidden (bool): Whether to include private attributes (starting with `_`).\n      include_functions (bool): Whether to include methods or callable attributes.\n      message (str | None): Optional prefix message to label the diagnostic output.\n\n  Returns:\n      None\n\n  Example:\n      &gt;&gt;&gt; obj_diagnostics(my_object, include_hidden=True, include_functions=True)\n  \"\"\"\n  logger.debug(f\"Diagnostics for: {obj}\")\n  if message:\n    logger.debug(f\"{message=}\")\n\n  for attr_name in dir(obj):\n    attr_value = getattr(obj, attr_name)\n    if not attr_name.startswith('_') or include_hidden:\n      if callable(attr_value):\n        if include_functions:\n          logger.debug(f\"{attr_name}(): is function\")\n      else:\n        logger.debug(f\"{attr_name} {type(attr_value)}:\\t {attr_value}\")\n</code></pre>"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.obj_to_html","title":"<code>obj_to_html(obj)</code>","text":"<p>Convert any Python object to a formatted HTML string for Jinja rendering.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>A Python object suitable for <code>pprint</code>.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>HTML string wrapped in <pre> tags that is safe for use with <code>|safe</code> in templates.\n              \n            \n          \n      \n    \n\n\n\n  Notes\n  <p>The HTML content must be marked <code>|safe</code> in the template to avoid escaping.</p>\n        <p>Example (in Jinja):\n    {% if result is defined %}\n      {{ result|safe }}\n    {% endif %}</p>\n\n\n            \n              Source code in <code>arb\\utils\\diagnostics.py</code>\n              <pre><code>def obj_to_html(obj: object) -&gt; str:\n  \"\"\"\n  Convert any Python object to a formatted HTML string for Jinja rendering.\n\n  Args:\n      obj (object): A Python object suitable for `pprint`.\n\n  Returns:\n      str: HTML string wrapped in &lt;pre&gt; tags that is safe for use with `|safe` in templates.\n\n  Notes:\n      The HTML content must be marked `|safe` in the template to avoid escaping.\n\n  Example (in Jinja):\n      {% if result is defined %}\n        {{ result|safe }}\n      {% endif %}\n  \"\"\"\n  pp = pprint.PrettyPrinter(indent=4, width=200)\n  formatted_data = pp.pformat(obj)\n  soup = BeautifulSoup(\"&lt;pre&gt;&lt;/pre&gt;\", \"html.parser\")\n  soup.pre.string = formatted_data\n  return soup.prettify()\n</code></pre>"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.run_diagnostics","title":"<code>run_diagnostics()</code>","text":"<p>Run example-based tests for diagnostic functions in this module.</p>\n<p>Logs example output for each function.</p>\n\n\n            \n              Source code in <code>arb\\utils\\diagnostics.py</code>\n              <pre><code>def run_diagnostics() -&gt; None:\n  \"\"\"\n  Run example-based tests for diagnostic functions in this module.\n\n  Logs example output for each function.\n  \"\"\"\n  logger.debug(\"Running diagnostics on arb.utils.diagnostics module\")\n\n  # obj_diagnostics\n  class TestClass:\n    def __init__(self):\n      self.x = 42\n      self.y = \"hello\"\n\n    def greet(self):\n      return \"hi\"\n\n  obj = TestClass()\n  obj_diagnostics(obj, include_hidden=False, include_functions=True)\n\n  # list_differences\n  a = [\"x\", \"y\", \"z\"]\n  b = [\"x\", \"z\", \"w\"]\n  list_differences(a, b, \"List A\", \"List B\", print_warning=True)\n\n  # diag_recursive\n  diag_recursive([[1, 2], [3, [4, 5]]])\n\n  # dict_to_str\n  nested_dict = {\"a\": 1, \"b\": {\"c\": 2, \"d\": {\"e\": 3}}}\n  logger.debug(\"dict_to_str output:\\t\" + dict_to_str(nested_dict))\n\n  # obj_to_html\n  html_result = obj_to_html(nested_dict)\n  logger.debug(\"HTML representation of object (truncated):\\t\" + html_result[:300])\n\n  # compare_dicts\n  d1 = {\"x\": 1, \"y\": 2, \"z\": 3}\n  d2 = {\"x\": 1, \"y\": 99, \"w\": 0}\n  compare_dicts(d1, d2, \"First Dict\", \"Second Dict\")\n</code></pre>"},{"location":"reference/arb/utils/file_io/","title":"<code>arb.utils.file_io</code>","text":"<p>Utility functions for file and path handling, including directory creation, secure file name generation, and project root resolution.</p> <p>Features: - Ensures parent and target directories exist - Generates secure, timestamped file names using UTC - Dynamically resolves the project root based on directory structure - Includes diagnostics for local testing and validation</p> <p>Notes: - Uses <code>werkzeug.utils.secure_filename</code> to sanitize input filenames - Timestamps are formatted in UTC using the DATETIME_WITH_SECONDS pattern</p> <p>Potential Future Upgrades: - Add support for Windows-specific path edge cases, if needed. - Expand run_diagnostics to perform write/delete tests in a sandbox directory.</p>"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.ProjectRootNotFoundError","title":"<code>ProjectRootNotFoundError</code>","text":"<p>               Bases: <code>ValueError</code></p> <p>Raised when no matching project root can be determined from the provided file path and candidate folder sequences.</p> Source code in <code>arb\\utils\\file_io.py</code> <pre><code>class ProjectRootNotFoundError(ValueError):\n  \"\"\"\n  Raised when no matching project root can be determined from the provided file path\n  and candidate folder sequences.\n  \"\"\"\n  pass\n</code></pre>"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.ensure_dir_exists","title":"<code>ensure_dir_exists(dir_path)</code>","text":"<p>Ensure that the specified directory exists, creating it if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>str | Path</code> <p>Path to the directory.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the path exists but is not a directory.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>ensure_dir_exists(\"logs/output\")</p> Source code in <code>arb\\utils\\file_io.py</code> <pre><code>def ensure_dir_exists(dir_path: str | Path) -&gt; None:\n  \"\"\"\n  Ensure that the specified directory exists, creating it if necessary.\n\n  Args:\n      dir_path (str | Path): Path to the directory.\n\n  Raises:\n      ValueError: If the path exists but is not a directory.\n\n  Returns:\n      None\n\n  Example:\n      &gt;&gt;&gt; ensure_dir_exists(\"logs/output\")\n  \"\"\"\n  logger.debug(f\"ensure_dir_exists() called for: {dir_path=}\")\n  dir_path = Path(dir_path)\n\n  if dir_path.exists() and not dir_path.is_dir():\n    raise ValueError(f\"The path {dir_path} exists and is not a directory.\")\n\n  dir_path.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.ensure_parent_dirs","title":"<code>ensure_parent_dirs(file_name)</code>","text":"<p>Ensure that the parent directories for a given file path exist.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str | Path</code> <p>The full path to a file. Parent folders will be created if needed.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Example:     &gt;&gt;&gt; ensure_parent_dirs(\"/tmp/some/deep/file.txt\")     &gt;&gt;&gt; ensure_parent_dirs(\"local_file.txt\")</p> Source code in <code>arb\\utils\\file_io.py</code> <pre><code>def ensure_parent_dirs(file_name: str | Path) -&gt; None:\n  \"\"\"\n  Ensure that the parent directories for a given file path exist.\n\n  Args:\n      file_name (str | Path): The full path to a file. Parent folders will be created if needed.\n\n  Returns:\n      None\n  Example:\n      &gt;&gt;&gt; ensure_parent_dirs(\"/tmp/some/deep/file.txt\")\n      &gt;&gt;&gt; ensure_parent_dirs(\"local_file.txt\")\n  \"\"\"\n  logger.debug(f\"ensure_parent_dirs() called for: {file_name=}\")\n  file_path = Path(file_name)\n  file_path.parent.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.get_project_root_dir","title":"<code>get_project_root_dir(file, match_parts)</code>","text":"<p>Traverse up the directory tree from a file path to locate the root of a known structure.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | Path</code> <p>The starting file path, typically <code>__file__</code>.</p> required <code>match_parts</code> <code>list[str]</code> <p>Folder names expected in the path, ordered from root to leaf.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to the top of the matched folder chain.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no matching structure is found in the parent hierarchy.</p> Passing Example <p>If <code>file = \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\"</code> and <code>match_parts = [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"]</code>, then:   \u2192 match found at /Users/tony/dev/feedback_portal/source/production/arb/portal   \u2192 returns: Path(\"/Users/tony/dev/feedback_portal\")</p> Failing Example <p>If the file path is unrelated (e.g., \"/tmp/random_file.py\"), the function will raise a ValueError.</p> Source code in <code>arb\\utils\\file_io.py</code> <pre><code>def get_project_root_dir(file: str | Path, match_parts: list[str]) -&gt; Path:\n  \"\"\"\n  Traverse up the directory tree from a file path to locate the root of a known structure.\n\n  Args:\n      file (str | Path): The starting file path, typically `__file__`.\n      match_parts (list[str]): Folder names expected in the path, ordered from root to leaf.\n\n  Returns:\n      Path: Path to the top of the matched folder chain.\n\n  Raises:\n      ValueError: If no matching structure is found in the parent hierarchy.\n\n  Passing Example:\n    If `file = \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\"`\n    and `match_parts = [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"]`,\n    then:\n      \u2192 match found at /Users/tony/dev/**feedback_portal**/source/production/arb/portal\n      \u2192 returns: Path(\"/Users/tony/dev/feedback_portal\")\n\n  Failing Example:\n    If the file path is unrelated (e.g., \"/tmp/random_file.py\"),\n    the function will raise a ValueError.\n  \"\"\"\n  path = Path(file).resolve()\n  match_len = len(match_parts)\n\n  current = path\n  while current != current.parent:\n    if list(current.parts[-match_len:]) == match_parts:\n      return Path(*current.parts[:len(current.parts) - match_len + 1])\n    current = current.parent\n\n  raise ValueError(f\"Could not locate project root using match_parts={match_parts} from path={path}\")\n</code></pre>"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.get_secure_timestamped_file_name","title":"<code>get_secure_timestamped_file_name(directory, file_name)</code>","text":"<p>Generate a sanitized file name in the given directory, appending a UTC timestamp.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str | Path</code> <p>Target directory where the file will be saved.</p> required <code>file_name</code> <code>str</code> <p>Proposed name for the file, possibly unsafe.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The full secure, timestamped file path.</p> Example <p>get_secure_timestamped_file_name(\"/tmp\", \"user report.xlsx\") Path(\"/home/user/tmp/user_report_ts_2025-05-05T12-30-00Z.xlsx\")</p> Source code in <code>arb\\utils\\file_io.py</code> <pre><code>def get_secure_timestamped_file_name(directory: str | Path, file_name: str) -&gt; Path:\n  \"\"\"\n  Generate a sanitized file name in the given directory, appending a UTC timestamp.\n\n  Args:\n      directory (str | Path): Target directory where the file will be saved.\n      file_name (str): Proposed name for the file, possibly unsafe.\n\n  Returns:\n      Path: The full secure, timestamped file path.\n\n  Example:\n      &gt;&gt;&gt; get_secure_timestamped_file_name(\"/tmp\", \"user report.xlsx\")\n      Path(\"/home/user/tmp/user_report_ts_2025-05-05T12-30-00Z.xlsx\")\n  \"\"\"\n  file_name_clean = secure_filename(file_name)\n  full_path = Path.home() / directory / file_name_clean\n\n  timestamp = datetime.now(ZoneInfo(\"UTC\")).strftime(DATETIME_WITH_SECONDS)\n  new_name = f\"{full_path.stem}_ts_{timestamp}{full_path.suffix}\"\n\n  return full_path.with_name(new_name)\n</code></pre>"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.resolve_project_root","title":"<code>resolve_project_root(file_path, candidate_structures=None)</code>","text":"<p>Attempt to locate the project root directory using known folder sequences.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>The file path to begin traversal from (typically <code>__file__</code>).</p> required <code>candidate_structures</code> <code>list[list[str]] | None</code> <p>List of folder name sequences to match.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path to the root of the matched folder chain.</p> <p>Raises:</p> Type Description <code>ProjectRootNotFoundError</code> <p>If no matching sequence is found.</p> Example <p>resolve_project_root(file) Path(\"/Users/tony/dev/feedback_portal\")</p> Source code in <code>arb\\utils\\file_io.py</code> <pre><code>def resolve_project_root(\n    file_path: str | Path,\n    candidate_structures: list[list[str]] | None = None\n) -&gt; Path:\n  \"\"\"\n  Attempt to locate the project root directory using known folder sequences.\n\n  Args:\n      file_path (str | Path): The file path to begin traversal from (typically `__file__`).\n      candidate_structures (list[list[str]] | None): List of folder name sequences to match.\n\n  Returns:\n      Path: Path to the root of the matched folder chain.\n\n  Raises:\n      ProjectRootNotFoundError: If no matching sequence is found.\n\n  Example:\n      &gt;&gt;&gt; resolve_project_root(__file__)\n      Path(\"/Users/tony/dev/feedback_portal\")\n  \"\"\"\n  if candidate_structures is None:\n    candidate_structures = [\n      ['feedback_portal', 'source', 'production', 'arb', 'utils', 'excel'],\n      ['feedback_portal', 'source', 'production', 'arb', 'portal'],\n    ]\n\n  errors = []\n  for structure in candidate_structures:\n    try:\n      root = get_project_root_dir(file_path, structure)\n      logger.debug(f\"{root =}, based on structure {structure =}\")\n      return root\n    except ValueError as e:\n      errors.append(f\"{structure}: {e}\")\n\n  raise ProjectRootNotFoundError(\n    \"Unable to determine project root. Tried the following structures:\\n\" +\n    \"\\n\".join(errors)\n  )\n</code></pre>"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.run_diagnostics","title":"<code>run_diagnostics()</code>","text":"<p>Run a series of checks to validate directory creation, secure filename generation, and project root resolution logic.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>arb\\utils\\file_io.py</code> <pre><code>def run_diagnostics() -&gt; None:\n  \"\"\"\n  Run a series of checks to validate directory creation, secure filename generation,\n  and project root resolution logic.\n\n  Returns:\n      None\n  \"\"\"\n  import tempfile\n\n  print(\"Running diagnostics...\")\n\n  # Test ensure_dir_exists\n  test_dir = Path(tempfile.gettempdir()) / \"arb_test_nested/subdir\"\n  ensure_dir_exists(test_dir)\n  assert test_dir.exists() and test_dir.is_dir()\n\n  # Test ensure_parent_dirs\n  test_file = test_dir / \"test_file.txt\"\n  ensure_parent_dirs(test_file)\n  assert test_file.parent.exists()\n\n  # Test secure file name generation\n  secured_path = get_secure_timestamped_file_name(test_dir, \"My Unsafe Report.xlsx\")\n  print(f\"Generated secure file: {secured_path}\")\n  assert secured_path.name.startswith(\"My_Unsafe_Report_ts_\")\n\n  # Test project root resolution\n  try:\n    project_root = resolve_project_root(__file__)\n    print(f\"Resolved project root: {project_root}\")\n    assert project_root.exists()\n  except ProjectRootNotFoundError as e:\n    print(\"WARNING: Could not resolve project root. Skipping root validation.\")\n\n  print(\"All diagnostics completed successfully.\")\n</code></pre>"},{"location":"reference/arb/utils/json/","title":"<code>arb.utils.json</code>","text":"<p>Module for JSON-related utility functions and classes.</p> Includes <ul> <li>Custom serialization for datetime and decimal objects</li> <li>Metadata support for enhanced JSON files</li> <li>File-based diagnostics and JSON comparison utilities</li> <li>WTForms integration for form data extraction and casting</li> </ul> Version <p>1.0.0</p> Notes <ul> <li>Designed for structured JSON handling across ARB portal utilities.</li> <li>Emphasizes ISO 8601 datetime formats and consistent value type casting.</li> <li>Supports \"Pacific Time naive\" conversion via ZoneInfo-aware logic.</li> </ul>"},{"location":"reference/arb/utils/json/#arb.utils.json.add_metadata_to_json","title":"<code>add_metadata_to_json(file_name_in, file_name_out=None)</code>","text":"<p>Add metadata to an existing JSON file or overwrite it in-place.</p> <p>Parameters:</p> Name Type Description Default <code>file_name_in</code> <code>str | Path</code> <p>Input JSON file path.</p> required <code>file_name_out</code> <code>str | Path | None</code> <p>Output file path. If None, overwrites input.</p> <code>None</code> <p>Example:     &gt;&gt;&gt; add_metadata_to_json(\"schema.json\")</p> Source code in <code>arb\\utils\\json.py</code> <pre><code>def add_metadata_to_json(\n    file_name_in: str | pathlib.Path,\n    file_name_out: str | pathlib.Path | None = None\n) -&gt; None:\n  \"\"\"\n  Add metadata to an existing JSON file or overwrite it in-place.\n\n  Args:\n      file_name_in (str | Path): Input JSON file path.\n      file_name_out (str | Path | None): Output file path. If None, overwrites input.\n  Example:\n      &gt;&gt;&gt; add_metadata_to_json(\"schema.json\")\n  \"\"\"\n  logger.debug(f\"add_metadata_to_json() called with {file_name_in=}, {file_name_out=}\")\n\n  if file_name_out is None:\n    file_name_out = file_name_in\n\n  data = json_load(file_name_in)\n  json_save_with_meta(file_name_out, data=data)\n</code></pre>"},{"location":"reference/arb/utils/json/#arb.utils.json.cast_model_value","title":"<code>cast_model_value(value, value_type, convert_time_to_ca=False)</code>","text":"<p>Cast a stringified JSON value into a Python object of the expected type.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Input value to cast.</p> required <code>value_type</code> <code>type</code> <p>Python type to cast to.</p> required <code>convert_time_to_ca</code> <code>bool</code> <p>If True, convert UTC to California naive datetime.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>Value converted to the target Python type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value cannot be cast to the given type.</p> Source code in <code>arb\\utils\\json.py</code> <pre><code>def cast_model_value(\n    value: str,\n    value_type: type,\n    convert_time_to_ca: bool = False\n) -&gt; object:\n  \"\"\"\n  Cast a stringified JSON value into a Python object of the expected type.\n\n  Args:\n      value (str): Input value to cast.\n      value_type (type): Python type to cast to.\n      convert_time_to_ca (bool): If True, convert UTC to California naive datetime.\n\n  Returns:\n      object: Value converted to the target Python type.\n\n  Raises:\n      ValueError: If the value cannot be cast to the given type.\n  \"\"\"\n  try:\n    if value_type == str:\n      # No need to cast a string\n      return value\n    elif value_type in [bool, int, float]:\n      return value_type(value)\n    elif value_type == datetime.datetime:\n      dt = iso8601_to_utc_dt(value)\n      return datetime_to_ca_naive(dt) if convert_time_to_ca else dt\n    elif value_type == decimal.Decimal:\n      return decimal.Decimal(value)\n    else:\n      raise ValueError(f\"Unsupported type for casting: {value_type}\")\n  except Exception as e:\n    raise ValueError(f\"Failed to cast {value!r} to {value_type}: {e}\")\n</code></pre>"},{"location":"reference/arb/utils/json/#arb.utils.json.compare_json_files","title":"<code>compare_json_files(file_name_1, file_name_2)</code>","text":"<p>Compare the contents of two JSON files including metadata and values.</p> <p>Parameters:</p> Name Type Description Default <code>file_name_1</code> <code>str | Path</code> <p>Path to the first file.</p> required <code>file_name_2</code> <code>str | Path</code> <p>Path to the second file.</p> required Logs <p>Differences or matches are logged at debug level.</p> Example <p>compare_json_files(\"old.json\", \"new.json\")</p> Source code in <code>arb\\utils\\json.py</code> <pre><code>def compare_json_files(\n    file_name_1: str | pathlib.Path,\n    file_name_2: str | pathlib.Path\n) -&gt; None:\n  \"\"\"\n  Compare the contents of two JSON files including metadata and values.\n\n  Args:\n      file_name_1 (str | Path): Path to the first file.\n      file_name_2 (str | Path): Path to the second file.\n\n  Logs:\n      Differences or matches are logged at debug level.\n\n  Example:\n      &gt;&gt;&gt; compare_json_files(\"old.json\", \"new.json\")\n  \"\"\"\n  logger.debug(f\"compare_json_files() comparing {file_name_1} and {file_name_2}\")\n\n  data_1, meta_1 = json_load_with_meta(file_name_1)\n  data_2, meta_2 = json_load_with_meta(file_name_2)\n\n  logger.debug(\"Comparing metadata\")\n  if compare_dicts(meta_1, meta_2, \"metadata_01\", \"metadata_02\") is True:\n    logger.debug(\"Metadata are equivalent\")\n  else:\n    logger.debug(\"Metadata differ\")\n\n  logger.debug(\"Comparing data\")\n  if compare_dicts(data_1, data_2, \"data_01\", \"data_02\") is True:\n    logger.debug(\"Data are equivalent\")\n  else:\n    logger.debug(\"Data differ\")\n</code></pre>"},{"location":"reference/arb/utils/json/#arb.utils.json.deserialize_dict","title":"<code>deserialize_dict(input_dict, type_map, convert_time_to_ca=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>dict</code> <p>Dictionary of raw values.</p> required <code>type_map</code> <code>dict[str, type]</code> <p>Field-to-type mapping for deserialization.</p> required <code>convert_time_to_ca</code> <code>bool</code> <p>If True, converts datetime to CA time.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Fully deserialized dictionary.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any key is not a string.</p> <code>ValueError</code> <p>If value casting fails for a key.</p> Source code in <code>arb\\utils\\json.py</code> <pre><code>def deserialize_dict(\n    input_dict: dict,\n    type_map: dict[str, type],\n    convert_time_to_ca: bool = False\n) -&gt; dict:\n  \"\"\"\n  Args:\n      input_dict (dict): Dictionary of raw values.\n      type_map (dict[str, type]): Field-to-type mapping for deserialization.\n      convert_time_to_ca (bool): If True, converts datetime to CA time.\n\n  Returns:\n      dict: Fully deserialized dictionary.\n\n  Raises:\n      TypeError: If any key is not a string.\n      ValueError: If value casting fails for a key.\n  \"\"\"\n  result = {}\n\n  for key, value in input_dict.items():\n    if not isinstance(key, str):\n      raise TypeError(f\"All keys must be strings. Invalid key: {key} ({type(key)})\")\n\n    if key in type_map and value is not None:\n      result[key] = cast_model_value(value, type_map[key], convert_time_to_ca)\n    else:\n      result[key] = value\n\n  return result\n</code></pre>"},{"location":"reference/arb/utils/json/#arb.utils.json.json_deserializer","title":"<code>json_deserializer(obj)</code>","text":"<p>Custom JSON deserializer for class/type representations created by <code>json_serializer</code>.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>dict</code> <p>Dictionary object from JSON with special tags for known types.</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>Reconstructed Python object.</p> Example <p>json.loads(json_string, object_hook=json_deserializer)</p> Source code in <code>arb\\utils\\json.py</code> <pre><code>def json_deserializer(obj: dict) -&gt; object:\n  \"\"\"\n  Custom JSON deserializer for class/type representations created by `json_serializer`.\n\n  Args:\n      obj (dict): Dictionary object from JSON with special tags for known types.\n\n  Returns:\n      object: Reconstructed Python object.\n\n  Example:\n      &gt;&gt;&gt; json.loads(json_string, object_hook=json_deserializer)\n  \"\"\"\n  new_obj = obj\n\n  if \"__class__\" in obj:\n    # logger.debug(f\"{obj['__class__']=} detected in object deserializer.\")\n    if obj[\"__class__\"] == \"str\":\n      new_obj = str\n    elif obj[\"__class__\"] == \"int\":\n      new_obj = int\n    elif obj[\"__class__\"] == \"float\":\n      new_obj = float\n    elif obj[\"__class__\"] == \"bool\":\n      new_obj = bool\n    elif obj[\"__class__\"] == \"datetime\":\n      new_obj = datetime.datetime\n    else:\n      raise TypeError(f\"Object of type {type(obj).__name__} is not JSON deserializable\")\n\n  elif \"__type__\" in obj:\n    type_tag = obj[\"__type__\"]\n    if type_tag == \"datetime.datetime\":\n      new_obj = datetime.datetime.fromisoformat(obj[\"value\"])\n    elif type_tag == \"decimal.Decimal\":\n      new_obj = decimal.Decimal(obj[\"value\"])\n    else:\n      logger.debug(f\"No known conversion type for type {obj['__type__']}\")\n\n  # logger.debug(f\"deserializer() returning type= {type(new_obj)}, new_obj= {new_obj}\")\n\n  return new_obj\n</code></pre>"},{"location":"reference/arb/utils/json/#arb.utils.json.json_load","title":"<code>json_load(file_path, json_options=None)</code>","text":"<p>Load and deserialize data from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to the JSON file.</p> required <code>json_options</code> <code>dict | None</code> <p>Optional options passed to <code>json.load</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>Deserialized Python object.</p> <p>Example:     &gt;&gt;&gt; json_load(\"data.json\")</p> Notes <ul> <li>encoding=\"utf-8-sig\" will remove (if present) [BOM (Byte Order Mark)] for UTF-8 (\ufeff)    if it appears as a special marker at the beginning of some UTF-8 encoded files.    This marker can cause JSON decoding errors if not handled properly.</li> </ul> Source code in <code>arb\\utils\\json.py</code> <pre><code>def json_load(\n    file_path: str | pathlib.Path,\n    json_options: dict | None = None\n) -&gt; object:\n  \"\"\"\n  Load and deserialize data from a JSON file.\n\n  Args:\n      file_path (str | Path): Path to the JSON file.\n      json_options (dict | None): Optional options passed to `json.load`.\n\n  Returns:\n      object: Deserialized Python object.\n  Example:\n      &gt;&gt;&gt; json_load(\"data.json\")\n\n  Notes:\n    -  encoding=\"utf-8-sig\" will remove (if present) [BOM (Byte Order Mark)] for UTF-8 (\\uFEFF)\n       if it appears as a special marker at the beginning of some UTF-8 encoded files.\n       This marker can cause JSON decoding errors if not handled properly.\n  \"\"\"\n  logger.debug(f\"json_load() called with {file_path=}, {json_options=}\")\n\n  if json_options is None:\n    json_options = {\"object_hook\": json_deserializer}\n\n  with open(file_path, \"r\", encoding=\"utf-8-sig\") as f:\n    return json.load(f, **json_options)\n</code></pre>"},{"location":"reference/arb/utils/json/#arb.utils.json.json_load_with_meta","title":"<code>json_load_with_meta(file_path, json_options=None)</code>","text":"<p>Load a JSON file and return both data and metadata if present.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to the JSON file.</p> required <code>json_options</code> <code>dict | None</code> <p>Optional options passed to <code>json.load</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[object, dict]</code> <ul> <li>object: Deserialized data in \"data\" (or full file if not present).</li> <li>dict: Deserialized Metadata in \"metadata\" (or empty if not present).</li> </ul> Example <p>data, meta = json_load_with_meta(\"example.json\")</p> Notes <p>If the json file is a dictionary with a key data, then the data and metadata (if possible) will  be extracted.  Otherwise, the data is assumed to be the whole JSON file with no metadata.</p> Source code in <code>arb\\utils\\json.py</code> <pre><code>def json_load_with_meta(file_path: str | pathlib.Path,\n                        json_options: dict | None = None) -&gt; tuple[object, dict]:\n  \"\"\"\n  Load a JSON file and return both data and metadata if present.\n\n  Args:\n      file_path (str | Path): Path to the JSON file.\n      json_options (dict | None): Optional options passed to `json.load`.\n\n  Returns:\n      tuple:\n          - object: Deserialized data in \"_data_\" (or full file if not present).\n          - dict: Deserialized Metadata in \"_metadata_\" (or empty if not present).\n\n  Example:\n      &gt;&gt;&gt; data, meta = json_load_with_meta(\"example.json\")\n\n  Notes:\n      If the json file is a dictionary with a key _data_, then the _data_ and _metadata_\n      (if possible) will  be extracted.  Otherwise, the data is assumed\n      to be the whole JSON file with no metadata.\n  \"\"\"\n  logger.debug(f\"json_load_with_meta() called with {file_path=}, {json_options=}\")\n\n  all_data = json_load(file_path, json_options=json_options)\n  if isinstance(all_data, dict) and \"_data_\" in all_data:\n    return all_data[\"_data_\"], all_data.get(\"_metadata_\", {})\n\n  return all_data, {}\n</code></pre>"},{"location":"reference/arb/utils/json/#arb.utils.json.json_save","title":"<code>json_save(file_path, data, json_options=None)</code>","text":"<p>Save a Python object to a JSON file with optional serialization settings.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to write the JSON file.</p> required <code>data</code> <code>object</code> <p>Data to serialize and write.</p> required <code>json_options</code> <code>dict | None</code> <p>Options to pass to <code>json.dump</code>.</p> <code>None</code> <p>Example:     &gt;&gt;&gt; json_save(\"output.json\", {\"x\": decimal.Decimal(\"1.23\")})</p> Source code in <code>arb\\utils\\json.py</code> <pre><code>def json_save(\n    file_path: str | pathlib.Path,\n    data: object,\n    json_options: dict | None = None\n) -&gt; None:\n  \"\"\"\n  Save a Python object to a JSON file with optional serialization settings.\n\n  Args:\n      file_path (str | Path): Path to write the JSON file.\n      data (object): Data to serialize and write.\n      json_options (dict | None): Options to pass to `json.dump`.\n  Example:\n      &gt;&gt;&gt; json_save(\"output.json\", {\"x\": decimal.Decimal(\"1.23\")})\n  \"\"\"\n  logger.debug(f\"json_save() called with {file_path=}, {json_options=}, {data=}\")\n\n  if json_options is None:\n    json_options = {\"default\": json_serializer, \"indent\": 4}\n\n  with open(file_path, \"w\", encoding=\"utf-8\") as f:\n    json.dump(data, f, **json_options)\n\n  logger.debug(f\"JSON saved to file: '{file_path}'.\")\n</code></pre>"},{"location":"reference/arb/utils/json/#arb.utils.json.json_save_with_meta","title":"<code>json_save_with_meta(file_path, data, metadata=None, json_options=None)</code>","text":"<p>Save data with metadata to a JSON file under special keys.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Output JSON file path.</p> required <code>data</code> <code>object</code> <p>Primary data to store under \"data\".</p> required <code>metadata</code> <code>dict | None</code> <p>Optional metadata under \"metadata\".</p> <code>None</code> <code>json_options</code> <code>dict | None</code> <p>Options for <code>json.dump</code>.</p> <code>None</code> <p>Example:     &gt;&gt;&gt; json_save_with_meta(\"log.json\", {\"key\": \"value\"}, {\"source\": \"generated\"})</p> Source code in <code>arb\\utils\\json.py</code> <pre><code>def json_save_with_meta(\n    file_path: str | pathlib.Path,\n    data: object,\n    metadata: dict | None = None,\n    json_options: dict | None = None\n) -&gt; None:\n  \"\"\"\n  Save data with metadata to a JSON file under special keys.\n\n  Args:\n      file_path (str | Path): Output JSON file path.\n      data (object): Primary data to store under \"_data_\".\n      metadata (dict | None): Optional metadata under \"_metadata_\".\n      json_options (dict | None): Options for `json.dump`.\n  Example:\n      &gt;&gt;&gt; json_save_with_meta(\"log.json\", {\"key\": \"value\"}, {\"source\": \"generated\"})\n  \"\"\"\n  logger.debug(f\"json_save_with_meta() called with {file_path=}, {json_options=}, {metadata=}, {data=}\")\n\n  if metadata is None:\n    metadata = {}\n\n  metadata.update({\n    \"File created at\": datetime.datetime.now(ZoneInfo(\"UTC\")).isoformat(),\n    \"Serialized with\": \"utils.json.json_save_with_meta\",\n    \"Deserialize with\": \"utils.json.json_load_with_meta\",\n  })\n\n  wrapped = {\n    \"_metadata_\": metadata,\n    \"_data_\": data,\n  }\n\n  json_save(file_path, wrapped, json_options=json_options)\n</code></pre>"},{"location":"reference/arb/utils/json/#arb.utils.json.json_serializer","title":"<code>json_serializer(obj)</code>","text":"<p>Custom JSON serializer for objects not natively serializable by <code>json.dump</code>.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>object</code> <p>The object to serialize.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A JSON-compatible dictionary representation of the object.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the object type is unsupported.</p> Example <p>json.dumps(datetime.datetime.now(), default=json_serializer)</p> Source code in <code>arb\\utils\\json.py</code> <pre><code>def json_serializer(obj: object) -&gt; dict:\n  \"\"\"\n  Custom JSON serializer for objects not natively serializable by `json.dump`.\n\n  Args:\n      obj (object): The object to serialize.\n\n  Returns:\n      dict: A JSON-compatible dictionary representation of the object.\n\n  Raises:\n      TypeError: If the object type is unsupported.\n\n  Example:\n      &gt;&gt;&gt; json.dumps(datetime.datetime.now(), default=json_serializer)\n  \"\"\"\n  if isinstance(obj, type):\n    return {\"__class__\": obj.__name__, \"__module__\": obj.__module__}\n  elif isinstance(obj, datetime.datetime):\n    return {\"__type__\": \"datetime.datetime\", \"value\": obj.isoformat()}\n  elif isinstance(obj, decimal.Decimal):\n    return {\"__type__\": \"decimal.Decimal\", \"value\": str(obj)}\n\n  raise TypeError(f\"Object of type {type(obj).__name__} is not JSON serializable\")\n</code></pre>"},{"location":"reference/arb/utils/json/#arb.utils.json.make_dict_serializeable","title":"<code>make_dict_serializeable(input_dict, type_map=None, convert_time_to_ca=False)</code>","text":"<p>Transform a dictionary to ensure JSON compatibility of its values.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>dict</code> <p>Original dictionary to process.</p> required <code>type_map</code> <code>dict[str, type] | None</code> <p>Optional field-to-type map for casting.</p> <code>None</code> <code>convert_time_to_ca</code> <code>bool</code> <p>Convert datetimes to CA time before serialization.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with only JSON-serializable values.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any key is not a string.</p> <code>ValueError</code> <p>If value cannot be cast to expected type.</p> Source code in <code>arb\\utils\\json.py</code> <pre><code>def make_dict_serializeable(\n    input_dict: dict,\n    type_map: dict[str, type] | None = None,\n    convert_time_to_ca: bool = False\n) -&gt; dict:\n  \"\"\"\n  Transform a dictionary to ensure JSON compatibility of its values.\n\n  Args:\n      input_dict (dict): Original dictionary to process.\n      type_map (dict[str, type] | None): Optional field-to-type map for casting.\n      convert_time_to_ca (bool): Convert datetimes to CA time before serialization.\n\n  Returns:\n      dict: A dictionary with only JSON-serializable values.\n\n  Raises:\n      TypeError: If any key is not a string.\n      ValueError: If value cannot be cast to expected type.\n  \"\"\"\n  result = {}\n\n  for key, value in input_dict.items():\n    if not isinstance(key, str):\n      raise TypeError(f\"All keys must be strings. Invalid key: {key} ({type(key)})\")\n\n    if type_map and key in type_map:\n      try:\n        value = safe_cast(value, type_map[key])\n      except Exception as e:\n        raise ValueError(f\"Failed to cast key '{key}' to {type_map[key]}: {e}\")\n\n    if isinstance(value, datetime.datetime):\n      if convert_time_to_ca:\n        value = ca_naive_to_utc_datetime(value)\n      value = value.isoformat()\n\n    elif isinstance(value, decimal.Decimal):\n      value = float(value)\n\n    result[key] = value\n\n  return result\n</code></pre>"},{"location":"reference/arb/utils/json/#arb.utils.json.run_diagnostics","title":"<code>run_diagnostics()</code>","text":"<p>Run internal validation for all JSON utilities.</p> Tests <ul> <li>Custom serializer/deserializer</li> <li>JSON saving and loading (with and without metadata)</li> <li>Metadata updating</li> <li>File comparison</li> </ul> <p>Raises:</p> Type Description <code>Exception</code> <p>If any test fails.</p> Source code in <code>arb\\utils\\json.py</code> <pre><code>def run_diagnostics() -&gt; None:\n  \"\"\"\n  Run internal validation for all JSON utilities.\n\n  Tests:\n    - Custom serializer/deserializer\n    - JSON saving and loading (with and without metadata)\n    - Metadata updating\n    - File comparison\n\n  Raises:\n      Exception: If any test fails.\n  \"\"\"\n  import tempfile\n  import shutil\n\n  print(\"Running diagnostics for JSON utilities...\")\n\n  temp_dir = pathlib.Path(tempfile.gettempdir()) / \"json_utils_test\"\n  if temp_dir.exists():\n    shutil.rmtree(temp_dir)\n  temp_dir.mkdir(parents=True)\n\n  try:\n    # Test data\n    data = {\n      \"decimal\": decimal.Decimal(\"123.45\"),\n      \"timestamp\": datetime.datetime(2025, 5, 5, 13, 30, 0, tzinfo=ZoneInfo(\"UTC\")),\n      \"nested\": {\"a\": 1, \"b\": 2},\n    }\n\n    # File paths\n    json_file_1 = temp_dir / \"test_1.json\"\n    json_file_2 = temp_dir / \"test_2.json\"\n    plain_file = temp_dir / \"plain.json\"\n\n    # Save and load using json_save/json_load\n    json_save(json_file_1, data)\n    loaded_1 = json_load(json_file_1)\n    assert loaded_1 == data, \"Basic save/load failed\"\n\n    # Save with metadata and reload\n    meta_info = {\"note\": \"test file\"}\n    json_save_with_meta(json_file_2, data, metadata=meta_info)\n    loaded_data, loaded_meta = json_load_with_meta(json_file_2)\n    assert loaded_data == data, \"Data mismatch in metadata test\"\n    assert \"note\" in loaded_meta, \"Metadata not found\"\n\n    # Write plain file, with serializer included, then enrich with metadata\n    with open(plain_file, \"w\", encoding=\"utf-8\") as f:\n      json.dump(data, f, indent=2, default=json_serializer)\n    add_metadata_to_json(plain_file)\n\n    # Compare metadata-enriched files\n    compare_json_files(json_file_2, plain_file)\n\n    print(\"All diagnostics completed successfully.\")\n\n  except Exception as e:\n    print(f\"Diagnostics failed: {e}\")\n    raise\n</code></pre>"},{"location":"reference/arb/utils/json/#arb.utils.json.wtform_types_and_values","title":"<code>wtform_types_and_values(wtform)</code>","text":"<p>Extract field types and current data values from a WTForm.</p> <p>Parameters:</p> Name Type Description Default <code>wtform</code> <code>FlaskForm</code> <p>WTForms instance.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[dict[str, type], dict[str, object]]</code> <ul> <li>dict[str, type]: Field name to type mapping for deserialization.</li> <li>dict[str, object]: Field name to current value mapping (may include 'Please Select').</li> </ul> Source code in <code>arb\\utils\\json.py</code> <pre><code>def wtform_types_and_values(\n    wtform\n) -&gt; tuple[dict[str, type], dict[str, object]]:\n  \"\"\"\n  Extract field types and current data values from a WTForm.\n\n  Args:\n      wtform (FlaskForm): WTForms instance.\n\n  Returns:\n      tuple:\n          - dict[str, type]: Field name to type mapping for deserialization.\n          - dict[str, object]: Field name to current value mapping (may include 'Please Select').\n  \"\"\"\n  type_map = {}\n  field_data = {}\n\n  for name, field in wtform._fields.items():\n    value = field.data\n    field_data[name] = value\n\n    # Identify complex field types for type mapping\n    if isinstance(field, DateTimeField):\n      type_map[name] = datetime.datetime\n    elif isinstance(field, DecimalField):\n      type_map[name] = decimal.Decimal\n    elif isinstance(field, BooleanField):\n      type_map[name] = bool\n    elif isinstance(field, IntegerField):\n      type_map[name] = int\n    elif isinstance(field, SelectField):\n      type_map[name] = str  # 'Please Select' is valid\n\n  return type_map, field_data\n</code></pre>"},{"location":"reference/arb/utils/log_util/","title":"<code>arb.utils.log_util</code>","text":"<p>log_util.py</p> <p>Logging utilities to trace function calls and parameter values across the application.</p> <p>This module provides two main tools for logging function arguments:</p> <pre><code>1. log_function_parameters(): Logs the name and arguments of the current function.\n2. log_parameters(): A decorator that logs all arguments of decorated functions.\n</code></pre> <p>It also includes a logging filter, FlaskUserContextFilter, to inject the current Flask user into all log records when inside a request context.</p> Features <ul> <li>Logs arguments from both positional and keyword inputs.</li> <li>Automatically derives the correct logger based on caller/module context.</li> <li>Supports optional printing to stdout for real-time debugging.</li> <li>Integrates Flask <code>g.user</code> context when available, aiding request traceability.</li> </ul> Intended Use <ul> <li>Diagnostic tracing and observability in Flask applications.</li> <li>Debugging individual functions without modifying logic.</li> <li>Enhancing structured logging with contextual request user information.</li> </ul> Dependencies <ul> <li>Python standard library (inspect, logging)</li> <li>Flask (optional, for FlaskUserContextFilter)</li> </ul> Version <p>1.0.0</p>"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util--example-usage","title":"Example Usage:","text":"<p>import arb.utils.log_util as log_util logging.basicConfig(level=logging.DEBUG)</p> <p>@log_parameters(print_to_console=True) def greet(name, lang=\"en\"):     return f\"Hello {name} [{lang}]\"</p> <p>def example():     log_function_parameters(print_to_console=True)</p> <p>greet(\"Alice\", lang=\"fr\") example()</p>"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util--output","title":"Output:","text":"<p>greet(name='Alice', lang='fr') example()</p>"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util--recommendations","title":"Recommendations:","text":"<ul> <li>Use <code>log_parameters</code> for consistent tracing of function calls across modules.</li> <li>Use <code>log_function_parameters</code> for temporary instrumentation or detailed inline debugging.</li> </ul> <p>This module is safe to import in any Python project and requires only the standard library.</p>"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util.FlaskUserContextFilter","title":"<code>FlaskUserContextFilter</code>","text":"<p>               Bases: <code>Filter</code></p> <p>Logging filter that injects Flask's <code>g.user</code> into log records, if available.</p> <p>This allows log formats to include the active Flask user for traceability.</p> Adds <p>record.user (str): User identifier from Flask's request context, or \"n/a\" if unavailable.</p> Source code in <code>arb\\utils\\log_util.py</code> <pre><code>class FlaskUserContextFilter(logging.Filter):\n  \"\"\"\n  Logging filter that injects Flask's `g.user` into log records, if available.\n\n  This allows log formats to include the active Flask user for traceability.\n\n  Adds:\n      record.user (str): User identifier from Flask's request context, or \"n/a\" if unavailable.\n  \"\"\"\n\n  def filter(self, record):\n    if has_request_context() and hasattr(g, \"user\"):\n      record.user = g.user\n    else:\n      record.user = \"n/a\"\n    return True\n</code></pre>"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util.log_function_parameters","title":"<code>log_function_parameters(logger=None, print_to_console=False)</code>","text":"<p>Log the current function's name and arguments using debug-level logging.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger | None</code> <p>Optional logger. If None, derives one from caller's module.</p> <code>None</code> <code>print_to_console</code> <code>bool</code> <p>If True, also prints the message to stdout.</p> <code>False</code> Example <p>def example(a, b=2): log_function_parameters() example(1)</p> Source code in <code>arb\\utils\\log_util.py</code> <pre><code>def log_function_parameters(\n    logger: logging.Logger | None = None,\n    print_to_console: bool = False\n) -&gt; None:\n  \"\"\"\n  Log the current function's name and arguments using debug-level logging.\n\n  Args:\n      logger (logging.Logger | None): Optional logger. If None, derives one from caller's module.\n      print_to_console (bool): If True, also prints the message to stdout.\n\n  Example:\n      &gt;&gt;&gt; def example(a, b=2): log_function_parameters()\n      &gt;&gt;&gt; example(1)\n      # Logs: example(a=1, b=2)\n  \"\"\"\n  frame = inspect.currentframe().f_back\n  func_name = frame.f_code.co_name\n\n  if logger is None:\n    module_name = frame.f_globals.get(\"__name__\", \"default_logger\")\n    logger = logging.getLogger(module_name)\n\n  args_info = inspect.getargvalues(frame)\n  params = []\n\n  for arg in args_info.args:\n    value = args_info.locals.get(arg)\n    params.append(f\"{arg}={value!r}\")\n\n  if args_info.varargs:\n    value = args_info.locals.get(args_info.varargs)\n    params.append(f\"{args_info.varargs}={value!r}\")\n\n  if args_info.keywords:\n    kwargs = args_info.locals.get(args_info.keywords, {})\n    for k, v in kwargs.items():\n      params.append(f\"{k}={v!r}\")\n\n  log_line = f\"{func_name}({', '.join(params)})\"\n  logger.debug(log_line)\n  if print_to_console:\n    print(log_line)\n</code></pre>"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util.log_function_parameters--logs-examplea1-b2","title":"Logs: example(a=1, b=2)","text":""},{"location":"reference/arb/utils/log_util/#arb.utils.log_util.log_parameters","title":"<code>log_parameters(logger=None, print_to_console=False)</code>","text":"<p>Decorator to log all arguments passed to a function upon each invocation.</p> <p>Parameters:</p> Name Type Description Default <code>logger</code> <code>Logger | None</code> <p>Optional logger instance. Defaults to caller's module logger.</p> <code>None</code> <code>print_to_console</code> <code>bool</code> <p>If True, also prints the log message to stdout.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>A decorator that logs parameter values each time the function is called.</p> Example <p>@log_parameters(print_to_console=True) def greet(name, lang=\"en\"):     return f\"Hello {name} [{lang}]\"</p> Source code in <code>arb\\utils\\log_util.py</code> <pre><code>def log_parameters(\n    logger: logging.Logger | None = None,\n    print_to_console: bool = False\n) -&gt; Callable:\n  \"\"\"\n  Decorator to log all arguments passed to a function upon each invocation.\n\n  Args:\n      logger (logging.Logger | None): Optional logger instance. Defaults to caller's module logger.\n      print_to_console (bool): If True, also prints the log message to stdout.\n\n  Returns:\n      Callable: A decorator that logs parameter values each time the function is called.\n\n  Example:\n      &gt;&gt;&gt; @log_parameters(print_to_console=True)\n      &gt;&gt;&gt; def greet(name, lang=\"en\"):\n      &gt;&gt;&gt;     return f\"Hello {name} [{lang}]\"\n  \"\"\"\n\n  def decorator(func: Callable) -&gt; Callable:\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n      nonlocal logger\n      if logger is None:\n        logger = logging.getLogger(func.__module__)\n      bound = inspect.signature(func).bind(*args, **kwargs)\n      bound.apply_defaults()\n      param_str = \", \".join(f\"{k}={v!r}\" for k, v in bound.arguments.items())\n      log_line = f\"{func.__name__}({param_str})\"\n      logger.debug(log_line)\n      if print_to_console:\n        print(log_line)\n      return func(*args, **kwargs)\n\n    return wrapper\n\n  return decorator\n</code></pre>"},{"location":"reference/arb/utils/misc/","title":"<code>arb.utils.misc</code>","text":"<p>misc.py</p> <p>Miscellaneous utility functions for common tasks including dictionary traversal, default injection, argument formatting, exception logging, and safe type casting.</p> Functions included <ul> <li>get_nested_value: Safely access deeply nested values in a dictionary.</li> <li>ensure_key_value_pair: Add missing keys to sub-dictionaries using defaults.</li> <li>replace_list_occurrences: Modify list elements in-place based on a mapping.</li> <li>args_to_string: Format argument lists into padded strings.</li> <li>log_error: Log full exception tracebacks and re-raise.</li> <li>safe_cast: Convert values to expected types if needed.</li> <li>run_diagnostics: Test suite for all utilities.</li> </ul> Intended Use <ul> <li>Shared helpers for Flask or CLI-based Python applications.</li> <li>Improves code reuse and diagnostic traceability.</li> </ul> Dependencies <ul> <li>Python standard library</li> <li>Logging provided by arb.__get_logger</li> </ul> Version <p>1.0.0</p> <p>TODO:     - Consider converting log_error into a structured 500 error response for Flask apps</p>"},{"location":"reference/arb/utils/misc/#arb.utils.misc.args_to_string","title":"<code>args_to_string(args)</code>","text":"<p>Convert a list or tuple of arguments into a single space-separated string with padding.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>list | tuple | None</code> <p>Arguments to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Space-separated string representation.</p> Example <p>args_to_string([\"--debug\", \"--log\", \"file.txt\"]) ' --debug --log file.txt '</p> Source code in <code>arb\\utils\\misc.py</code> <pre><code>def args_to_string(args: list | tuple | None) -&gt; str:\n  \"\"\"\n  Convert a list or tuple of arguments into a single space-separated string with padding.\n\n  Args:\n      args (list | tuple | None): Arguments to convert.\n\n  Returns:\n      str: Space-separated string representation.\n\n  Example:\n      &gt;&gt;&gt; args_to_string([\"--debug\", \"--log\", \"file.txt\"])\n      ' --debug --log file.txt '\n  \"\"\"\n  if not args:\n    return ''\n  else:\n    args = [str(arg) for arg in args]\n    return_string = \" \" + \" \".join(args) + \" \"\n    return return_string\n</code></pre>"},{"location":"reference/arb/utils/misc/#arb.utils.misc.ensure_key_value_pair","title":"<code>ensure_key_value_pair(dict_, default_dict, sub_key)</code>","text":"<p>Ensure each sub-dictionary in dict_ has a given key, populating it from default_dict if missing.</p> <p>Parameters:</p> Name Type Description Default <code>dict_</code> <code>dict[str, dict]</code> <p>A dictionary whose values are sub-dictionaries.</p> required <code>default_dict</code> <code>dict</code> <p>A lookup dictionary to supply missing key-value pairs.</p> required <code>sub_key</code> <code>str</code> <p>The key that must exist in each sub-dictionary.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the sub_key is missing, and no fallback is found in default_dict.</p> Example <p>dict_ = {\"a\": {\"x\": 1}, \"b\": {\"x\": 2}, \"c\": {}} defaults = {\"c\": 99} ensure_key_value_pair(dict_, defaults, \"x\") dict_[\"c\"][\"x\"] 99</p> Source code in <code>arb\\utils\\misc.py</code> <pre><code>def ensure_key_value_pair(dict_: dict[str, dict], default_dict: dict, sub_key: str) -&gt; None:\n  \"\"\"\n  Ensure each sub-dictionary in dict_ has a given key, populating it from default_dict if missing.\n\n  Args:\n      dict_ (dict[str, dict]): A dictionary whose values are sub-dictionaries.\n      default_dict (dict): A lookup dictionary to supply missing key-value pairs.\n      sub_key (str): The key that must exist in each sub-dictionary.\n\n  Raises:\n      TypeError: If the sub_key is missing, and no fallback is found in default_dict.\n\n  Example:\n      &gt;&gt;&gt; dict_ = {\"a\": {\"x\": 1}, \"b\": {\"x\": 2}, \"c\": {}}\n      &gt;&gt;&gt; defaults = {\"c\": 99}\n      &gt;&gt;&gt; ensure_key_value_pair(dict_, defaults, \"x\")\n      &gt;&gt;&gt; dict_[\"c\"][\"x\"]\n      99\n  \"\"\"\n  for key, sub_dict in dict_.items():\n    logger.debug(f\"{key=}, {sub_dict=}\")\n    if sub_key not in sub_dict:\n      if key in default_dict:\n        sub_dict[sub_key] = default_dict[key]\n      else:\n        raise TypeError(\n          f\"{sub_key} is not present in sub dictionary for key '{key}' \"\n          f\"and no default provided in default_dict\"\n        )\n</code></pre>"},{"location":"reference/arb/utils/misc/#arb.utils.misc.get_nested_value","title":"<code>get_nested_value(nested_dict, keys)</code>","text":"<p>Retrieve a value from a nested dictionary using a key path.</p> <p>Parameters:</p> Name Type Description Default <code>nested_dict</code> <code>dict</code> <p>The dictionary to search.</p> required <code>keys</code> <code>list | tuple | str</code> <p>A sequence of keys to traverse the dictionary, or a single key.</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>The value found at the specified key path.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a key is missing at any level.</p> <code>TypeError</code> <p>If a non-dictionary value is encountered mid-traversal.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99}\n&gt;&gt;&gt; get_nested_value(data, (\"a\", \"b\", \"c\"))\n42\n&gt;&gt;&gt; get_nested_value(data, \"x\")\n99\n</code></pre> Source code in <code>arb\\utils\\misc.py</code> <pre><code>def get_nested_value(nested_dict: dict, keys: list | tuple | str) -&gt; object:\n  \"\"\"\n  Retrieve a value from a nested dictionary using a key path.\n\n  Args:\n      nested_dict (dict): The dictionary to search.\n      keys (list | tuple | str): A sequence of keys to traverse the dictionary, or a single key.\n\n  Returns:\n      object: The value found at the specified key path.\n\n  Raises:\n      KeyError: If a key is missing at any level.\n      TypeError: If a non-dictionary value is encountered mid-traversal.\n\n  Examples:\n      &gt;&gt;&gt; data = {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99}\n      &gt;&gt;&gt; get_nested_value(data, (\"a\", \"b\", \"c\"))\n      42\n      &gt;&gt;&gt; get_nested_value(data, \"x\")\n      99\n  \"\"\"\n  if not isinstance(keys, (list, tuple)):\n    # Single key case\n    if keys not in nested_dict:\n      raise KeyError(f\"Key '{keys}' not found in the dictionary\")\n    return nested_dict[keys]\n\n  current = nested_dict\n  for key in keys:\n    if not isinstance(current, dict):\n      raise TypeError(f\"Expected a dictionary at key '{key}', found {type(current).__name__}\")\n    if key not in current:\n      raise KeyError(f\"Key '{key}' not found in the dictionary\")\n    current = current[key]\n  return current\n</code></pre>"},{"location":"reference/arb/utils/misc/#arb.utils.misc.log_error","title":"<code>log_error(e)</code>","text":"<p>Log an exception and its stack trace, then re-raise the exception.</p> <p>Parameters:</p> Name Type Description Default <code>e</code> <code>Exception</code> <p>The exception to log.</p> required <p>Raises:</p> Type Description <code>Exception</code> <p>Always re-raises the input exception after logging.</p> Notes <ul> <li>Outputs full traceback to logger.</li> <li>Re-raises the original exception.</li> <li>Useful during development or structured exception monitoring.</li> </ul> TODO <p>Consider wrapping this in Flask to render a 500 error page instead.</p> Source code in <code>arb\\utils\\misc.py</code> <pre><code>def log_error(e: Exception) -&gt; None:\n  \"\"\"\n  Log an exception and its stack trace, then re-raise the exception.\n\n  Args:\n      e (Exception): The exception to log.\n\n  Raises:\n      Exception: Always re-raises the input exception after logging.\n\n  Notes:\n      - Outputs full traceback to logger.\n      - Re-raises the original exception.\n      - Useful during development or structured exception monitoring.\n\n  TODO:\n      Consider wrapping this in Flask to render a 500 error page instead.\n  \"\"\"\n  logger.error(e, exc_info=True)\n  stack = traceback.extract_stack()\n  logger.error(stack)\n  raise e\n</code></pre>"},{"location":"reference/arb/utils/misc/#arb.utils.misc.replace_list_occurrences","title":"<code>replace_list_occurrences(list_, lookup_dict)</code>","text":"<p>Replace elements of a list in-place using a lookup dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>list_</code> <code>list</code> <p>The list whose elements may be replaced.</p> required <code>lookup_dict</code> <code>dict</code> <p>A dictionary mapping old values to new values.</p> required Example <p>values = [\"cat\", \"dog\", \"bird\"] lookup = {\"dog\": \"puppy\", \"bird\": \"parrot\"} replace_list_occurrences(values, lookup) values ['cat', 'puppy', 'parrot']</p> Source code in <code>arb\\utils\\misc.py</code> <pre><code>def replace_list_occurrences(list_: list, lookup_dict: dict) -&gt; None:\n  \"\"\"\n  Replace elements of a list in-place using a lookup dictionary.\n\n  Args:\n      list_ (list): The list whose elements may be replaced.\n      lookup_dict (dict): A dictionary mapping old values to new values.\n\n  Example:\n      &gt;&gt;&gt; values = [\"cat\", \"dog\", \"bird\"]\n      &gt;&gt;&gt; lookup = {\"dog\": \"puppy\", \"bird\": \"parrot\"}\n      &gt;&gt;&gt; replace_list_occurrences(values, lookup)\n      &gt;&gt;&gt; values\n      ['cat', 'puppy', 'parrot']\n  \"\"\"\n  for i in range(len(list_)):\n    if list_[i] in lookup_dict:\n      list_[i] = lookup_dict[list_[i]]\n</code></pre>"},{"location":"reference/arb/utils/misc/#arb.utils.misc.run_diagnostics","title":"<code>run_diagnostics()</code>","text":"<p>Run diagnostics to validate functionality of misc.py utilities.</p> This includes <ul> <li>Nested dictionary access</li> <li>Default key/value injection into sub-dictionaries</li> <li>In-place replacement of list values</li> <li>Argument string formatting</li> <li>Error logging (non-raising test only)</li> </ul> Example <p>run_diagnostics()</p> Source code in <code>arb\\utils\\misc.py</code> <pre><code>def run_diagnostics() -&gt; None:\n  \"\"\"\n  Run diagnostics to validate functionality of misc.py utilities.\n\n  This includes:\n    - Nested dictionary access\n    - Default key/value injection into sub-dictionaries\n    - In-place replacement of list values\n    - Argument string formatting\n    - Error logging (non-raising test only)\n\n  Example:\n      &gt;&gt;&gt; run_diagnostics()\n  \"\"\"\n  print(\"Running diagnostics for misc.py utilities...\")\n\n  # --- Test get_nested_value ---\n  test_dict = {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99}\n  assert get_nested_value(test_dict, (\"a\", \"b\", \"c\")) == 42, \"Nested dict access failed\"\n  assert get_nested_value(test_dict, \"x\") == 99, \"Single key access failed\"\n\n  # --- Test ensure_key_value_pair ---\n  dict_with_sub = {\"apple\": {\"color\": \"red\"}, \"banana\": {}}\n  defaults = {\"banana\": \"yellow\"}\n  ensure_key_value_pair(dict_with_sub, defaults, \"color\")\n  assert dict_with_sub[\"banana\"][\"color\"] == \"yellow\", \"Default insertion failed\"\n\n  # --- Test replace_list_occurrences ---\n  items = [\"dog\", \"cat\", \"parrot\"]\n  replace_list_occurrences(items, {\"dog\": \"puppy\", \"parrot\": \"bird\"})\n  assert items == [\"puppy\", \"cat\", \"bird\"], \"List replacement failed\"\n\n  # --- Test args_to_string ---\n  assert args_to_string([\"--a\", \"--b\", \"value\"]) == \" --a --b value \", \"args_to_string failed\"\n  assert args_to_string(None) == \"\", \"args_to_string empty case failed\"\n\n  # --- Test log_error (without raising) ---\n  try:\n    try:\n      raise ValueError(\"Test exception for log_error\")\n    except Exception as e:\n      # Simulate logging only \u2014 comment out re-raise\n      logger.error(e, exc_info=True)\n  except Exception:\n    assert False, \"log_error should not re-raise during diagnostics\"\n\n  print(\"All diagnostics passed.\")\n</code></pre>"},{"location":"reference/arb/utils/misc/#arb.utils.misc.safe_cast","title":"<code>safe_cast(value, expected_type)</code>","text":"<p>Cast a value to the expected type only if it's not already of that type.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to check and potentially cast.</p> required <code>expected_type</code> <code>type</code> <p>The target Python type to cast to.</p> required <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>The original or casted value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the cast fails or is inappropriate for the type.</p> Source code in <code>arb\\utils\\misc.py</code> <pre><code>def safe_cast(value, expected_type: type) -&gt; object:\n  \"\"\"\n  Cast a value to the expected type only if it's not already of that type.\n\n  Args:\n      value (Any): The value to check and potentially cast.\n      expected_type (type): The target Python type to cast to.\n\n  Returns:\n      object: The original or casted value.\n\n  Raises:\n      ValueError: If the cast fails or is inappropriate for the type.\n  \"\"\"\n\n  try:\n    if not isinstance(value, expected_type):\n      value = expected_type(value)\n    return value\n  except Exception as e:\n    raise ValueError(f\"Failed to cast value {value!r} to {expected_type}: {e}\")\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/","title":"<code>arb.utils.sql_alchemy</code>","text":"<p>SQLAlchemy utility functions for Flask applications using declarative or automap base models.</p> <p>This module provides introspection, diagnostics, and model-row operations for SQLAlchemy-based Flask apps. It supports both declarative and automapped models.</p>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy--included-utilities","title":"Included Utilities:","text":"<ul> <li>Model diagnostics (<code>sa_model_diagnostics</code>)</li> <li>Field and column type inspection (<code>get_sa_fields</code>, <code>get_sa_column_types</code>)</li> <li>Full automap type mapping (<code>get_sa_automap_types</code>)</li> <li>Dictionary conversions (<code>sa_model_to_dict</code>, <code>sa_model_dict_compare</code>)</li> <li>Table-to-dict exports (<code>table_to_list</code>)</li> <li>Table/class lookups (<code>get_class_from_table_name</code>)</li> <li>Row fetch and sort utilities (<code>get_rows_by_table_name</code>)</li> <li>Model add/delete with logging (<code>add_commit_and_log_model</code>, <code>delete_commit_and_log_model</code>)</li> <li>Foreign key traversal (<code>get_foreign_value</code>)</li> <li>PostgreSQL sequence inspection (<code>find_auto_increment_value</code>)</li> <li>JSON column loader (<code>load_model_json_column</code>)</li> </ul>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy--type-hints","title":"Type Hints:","text":"<ul> <li><code>db (SQLAlchemy)</code>: Flask-SQLAlchemy database instance with .session and .engine.</li> <li><code>base (DeclarativeMeta)</code>: Declarative or automapped SQLAlchemy base (e.g., via automap_base()).</li> <li><code>model (DeclarativeMeta)</code>: SQLAlchemy ORM model class or instance.</li> <li><code>session (Session)</code>: SQLAlchemy session object.</li> </ul>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy--usage-notes","title":"Usage Notes:","text":"<ul> <li>Supports PostgreSQL features like sequence inspection via <code>pg_get_serial_sequence</code>.</li> <li>Logging is integrated for debugging and auditing.</li> <li>Compatible with Python 3.10+ syntax (PEP 604 union types).</li> </ul> Version <p>1.0.0</p>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.add_commit_and_log_model","title":"<code>add_commit_and_log_model(db, model_row, comment='', model_before=None)</code>","text":"<p>Add or update a model instance, log changes, and commit.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy instance bound to the Flask app.</p> required <code>model_row</code> <code>DeclarativeMeta</code> <p>ORM model instance.</p> required <code>comment</code> <code>str</code> <p>Optional log comment.</p> <code>''</code> <code>model_before</code> <code>dict | None</code> <p>Optional snapshot before changes.</p> <code>None</code> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def add_commit_and_log_model(\n    db: SQLAlchemy,\n    model_row: DeclarativeMeta,\n    comment: str = \"\",\n    model_before: dict | None = None\n) -&gt; None:\n  \"\"\"\n  Add or update a model instance, log changes, and commit.\n\n  Args:\n      db (SQLAlchemy): SQLAlchemy instance bound to the Flask app.\n      model_row (DeclarativeMeta): ORM model instance.\n      comment (str): Optional log comment.\n      model_before (dict | None): Optional snapshot before changes.\n  \"\"\"\n  # todo (update) - use the payload routine apply_json_patch_and_log\n  if model_before:\n    logger.info(f\"Before commit {comment=}: {model_before}\")\n\n  try:\n    db.session.add(model_row)\n    db.session.commit()\n    model_after = sa_model_to_dict(model_row)\n    logger.info(f\"After commit: {model_after}\")\n\n    if model_before:\n      changes = sa_model_dict_compare(model_before, model_after)\n      logger.info(f\"Changed values: {changes}\")\n  except Exception as e:\n    log_error(e)\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.delete_commit_and_log_model","title":"<code>delete_commit_and_log_model(db, model_row, comment='')</code>","text":"<p>Delete a model instance, log it, and commit the change.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy instance bound to the Flask app.</p> required <code>model_row</code> <code>DeclarativeMeta</code> <p>ORM model instance.</p> required <code>comment</code> <code>str</code> <p>Optional log comment.</p> <code>''</code> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def delete_commit_and_log_model(db: SQLAlchemy, model_row: DeclarativeMeta, comment: str = \"\") -&gt; None:\n  \"\"\"\n  Delete a model instance, log it, and commit the change.\n\n  Args:\n      db (SQLAlchemy): SQLAlchemy instance bound to the Flask app.\n      model_row (DeclarativeMeta): ORM model instance.\n      comment (str): Optional log comment.\n  \"\"\"\n  # todo (update) - use the payload routine apply_json_patch_and_log and or some way to track change\n  logger.info(f\"Deleting model {comment=}: {sa_model_to_dict(model_row)}\")\n\n  try:\n    db.session.delete(model_row)\n    db.session.commit()\n  except Exception as e:\n    log_error(e)\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.find_auto_increment_value","title":"<code>find_auto_increment_value(db, table_name, column_name)</code>","text":"<p>Find the next auto-increment value for a table column (PostgreSQL only).</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy instance bound to the Flask app.</p> required <code>table_name</code> <code>str</code> <p>Table name.</p> required <code>column_name</code> <code>str</code> <p>Column name.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Human-readable summary of the next sequence value.</p> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def find_auto_increment_value(db: SQLAlchemy, table_name: str, column_name: str) -&gt; str:\n  \"\"\"\n  Find the next auto-increment value for a table column (PostgreSQL only).\n\n  Args:\n      db (SQLAlchemy): SQLAlchemy instance bound to the Flask app.\n      table_name (str): Table name.\n      column_name (str): Column name.\n\n  Returns:\n      str: Human-readable summary of the next sequence value.\n  \"\"\"\n  with db.engine.connect() as connection:\n    sql_seq = f\"SELECT pg_get_serial_sequence('{table_name}', '{column_name}')\"\n    sequence_name = connection.execute(text(sql_seq)).scalar()\n\n    sql_nextval = f\"SELECT nextval('{sequence_name}')\"\n    next_val = connection.execute(text(sql_nextval)).scalar()\n\n    return f\"Table '{table_name}' column '{column_name}' sequence '{sequence_name}' next value is '{next_val}'\"\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_class_from_table_name","title":"<code>get_class_from_table_name(base, table_name)</code>","text":"<p>Retrieves the mapped class for a given table name.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>DeclarativeMeta</code> <p>SQLAlchemy declarative base.</p> required <code>table_name</code> <code>str</code> <p>Database table name.</p> required <p>Returns:</p> Type Description <code>DeclarativeMeta | None</code> <p>DeclarativeMeta | None: Mapped SQLAlchemy ORM class, or None if not found.</p> <p>Notes: - To access the class mapped to a specific table name in SQLAlchemy without directly using _class_registry,   you can use the Base.metadata object, which stores information about all mapped tables.</p> <ul> <li>get_class_from_table_name seems to fail from gpt refactor, so i kept my original code here.</li> <li>it failed because my old test of is not None was changed to if - be on the lookout for other subtle bugs like this</li> </ul> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def get_class_from_table_name(base: DeclarativeMeta | None,\n                              table_name: str) -&gt; DeclarativeMeta | None:\n  \"\"\"\n  Retrieves the mapped class for a given table name.\n\n  Args:\n      base (DeclarativeMeta): SQLAlchemy declarative base.\n      table_name (str): Database table name.\n\n  Returns:\n      DeclarativeMeta | None: Mapped SQLAlchemy ORM class, or None if not found.\n\n  Notes:\n  - To access the class mapped to a specific table name in SQLAlchemy without directly using _class_registry,\n    you can use the Base.metadata object, which stores information about all mapped tables.\n\n  - get_class_from_table_name seems to fail from gpt refactor, so i kept my original code here.\n  - it failed because my old test of is not None was changed to if - be on the lookout for other subtle bugs like this\n  \"\"\"\n  try:\n    # Look up the table in metadata and find its mapped class\n    table = base.metadata.tables.get(table_name)\n    if table is not None:\n      for mapper in base.registry.mappers:\n        if mapper.local_table == table:\n          return mapper.class_\n    return None\n  except Exception as e:\n    msg = f\"Exception occurred when trying to get table named {table_name}\"\n    logger.error(msg, exc_info=True)\n    logger.error(f\"exception info: {e}\")\n\n  return None\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_foreign_value","title":"<code>get_foreign_value(db, base, primary_table_name, foreign_table_name, primary_table_fk_name, foreign_table_column_name, primary_table_pk_value, primary_table_pk_name=None, foreign_table_pk_name=None)</code>","text":"<p>Resolve a foreign key reference and return its value.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy instance bound to the Flask app.</p> required <code>base</code> <code>DeclarativeMeta</code> <p>Declarative base.</p> required <code>primary_table_name</code> <code>str</code> <p>Table with FK.</p> required <code>foreign_table_name</code> <code>str</code> <p>Table with desired value.</p> required <code>primary_table_fk_name</code> <code>str</code> <p>FK field name.</p> required <code>foreign_table_column_name</code> <code>str</code> <p>Target field name in foreign table.</p> required <code>primary_table_pk_value</code> <code>int</code> <p>PK value of row in primary table.</p> required <code>primary_table_pk_name</code> <code>str | None</code> <p>Optional PK field override.</p> <code>None</code> <code>foreign_table_pk_name</code> <code>str | None</code> <p>Optional PK override in foreign table.</p> <code>None</code> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: Foreign value if found, else None.</p> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def get_foreign_value(\n    db: SQLAlchemy,\n    base: DeclarativeMeta,\n    primary_table_name: str,\n    foreign_table_name: str,\n    primary_table_fk_name: str,\n    foreign_table_column_name: str,\n    primary_table_pk_value: int,\n    primary_table_pk_name: str | None = None,\n    foreign_table_pk_name: str | None = None\n) -&gt; str | None:\n  \"\"\"\n    Resolve a foreign key reference and return its value.\n\n    Args:\n        db (SQLAlchemy): SQLAlchemy instance bound to the Flask app.\n        base (DeclarativeMeta): Declarative base.\n        primary_table_name (str): Table with FK.\n        foreign_table_name (str): Table with desired value.\n        primary_table_fk_name (str): FK field name.\n        foreign_table_column_name (str): Target field name in foreign table.\n        primary_table_pk_value (int): PK value of row in primary table.\n        primary_table_pk_name (str | None): Optional PK field override.\n        foreign_table_pk_name (str | None): Optional PK override in foreign table.\n\n    Returns:\n        str | None: Foreign value if found, else None.\n  \"\"\"\n  logger.debug(f\"Looking up foreign value: {locals()}\")\n  result = None\n\n  primary_table = get_class_from_table_name(base, primary_table_name)\n  foreign_table = get_class_from_table_name(base, foreign_table_name)\n\n  if primary_table_pk_name:\n    pk_column = getattr(primary_table, primary_table_pk_name)\n    primary_row = db.session.query(primary_table).filter(pk_column == primary_table_pk_value).first()\n  else:\n    primary_row = db.session.query(primary_table).get(primary_table_pk_value)\n\n  if primary_row:\n    fk_value = getattr(primary_row, primary_table_fk_name)\n    if foreign_table_pk_name:\n      fk_column = getattr(foreign_table, foreign_table_pk_name)\n      foreign_row = db.session.query(foreign_table).filter(fk_column == fk_value).first()\n    else:\n      foreign_row = db.session.query(foreign_table).get(fk_value)\n\n    if foreign_row:\n      result = getattr(foreign_row, foreign_table_column_name)\n\n  logger.debug(f\"Foreign key result: {result}\")\n  return result\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_rows_by_table_name","title":"<code>get_rows_by_table_name(db, base, table_name, colum_name_pk=None, ascending=True)</code>","text":"<p>Retrieve all rows from a table, optionally sorted by a column.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy db object.</p> required <code>base</code> <code>DeclarativeMeta</code> <p>Declarative base.</p> required <code>table_name</code> <code>str</code> <p>Table name.</p> required <code>colum_name_pk</code> <code>str | None</code> <p>Column to sort by.</p> <code>None</code> <code>ascending</code> <code>bool</code> <p>Sort order.</p> <code>True</code> <p>Returns:</p> Type Description <code>list</code> <p>list[DeclarativeMeta]: List of ORM model instances.</p> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def get_rows_by_table_name(\n    db: SQLAlchemy,\n    base: DeclarativeMeta,\n    table_name: str,\n    colum_name_pk: str | None = None,\n    ascending: bool = True\n) -&gt; list:\n  \"\"\"\n    Retrieve all rows from a table, optionally sorted by a column.\n\n    Args:\n        db: SQLAlchemy db object.\n        base (DeclarativeMeta): Declarative base.\n        table_name (str): Table name.\n        colum_name_pk (str | None): Column to sort by.\n        ascending (bool): Sort order.\n\n    Returns:\n        list[DeclarativeMeta]: List of ORM model instances.\n  \"\"\"\n  table = get_class_from_table_name(base, table_name)\n  logger.info(f\"{type(table)=}\")\n\n  query = db.session.query(table)\n  if colum_name_pk:\n    column = getattr(table, colum_name_pk)\n    query = query.order_by(column if ascending else desc(column))\n\n  rows = query.all()\n  logger.debug(f\"Query result: {type(rows)=}\")\n  return rows\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_sa_automap_types","title":"<code>get_sa_automap_types(engine, base)</code>","text":"<p>Return column type metadata for all mapped classes.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>SQLAlchemy engine instance.</p> required <code>base</code> <code>DeclarativeMeta</code> <p>Automap base prepared with reflected metadata.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Nested mapping: table -&gt; column -&gt; type category.   The dict is database table names, columns names, and datatypes with the structure:     result[table_name][column_name][kind] = type     where: kind can be 'database_type', 'sqlalchemy_type', or 'python_type'</p> Notes <ul> <li>base likely created with:     base = automap_base()     base.prepare(db.engine, reflect=True)</li> </ul> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def get_sa_automap_types(engine: Engine, base: DeclarativeMeta) -&gt; dict:\n  \"\"\"\n  Return column type metadata for all mapped classes.\n\n  Args:\n      engine (Engine): SQLAlchemy engine instance.\n      base (DeclarativeMeta): Automap base prepared with reflected metadata.\n\n  Returns:\n      dict: Nested mapping: table -&gt; column -&gt; type category.\n            The dict is database table names, columns names, and datatypes with the structure:\n              result[table_name][column_name][kind] = type\n              where: kind can be 'database_type', 'sqlalchemy_type', or 'python_type'\n\n  Notes:\n    - base likely created with:\n        base = automap_base()\n        base.prepare(db.engine, reflect=True)\n  \"\"\"\n  logger.debug(f\"calling get_sa_automap_types()\")\n\n  result = {}\n  inspector = inspect(engine)\n\n  # Loop through all the mapped classes (tables)\n  # print(f\"{type(base)=}\")\n  # print(f\"{type(base.classes)=}\")\n  for class_name, mapped_class in base.classes.items():\n    # print(f\"Table: {class_name}\")\n    result[class_name] = {}\n\n    # Get the table columns\n    columns = mapped_class.__table__.columns\n\n    # Loop through columns to get types\n    for column in columns:\n      # print(f\"Column: {column.name}\")\n      result[class_name][column.name] = {}\n      db_type = None\n      sa_type = None\n      py_type = None\n\n      # Database (SQL) column type\n      db_column_type = inspector.get_columns(class_name)\n      for col in db_column_type:\n        if col['name'] == column.name:\n          db_type = col['type']\n          # print(f\"  Database type (SQL): {db_type}\")\n\n          # SQLAlchemy type\n          sa_type = type(db_type).__name__\n          # print(f\"  SQLAlchemy type: {sa_type}\")\n\n      # Python type\n      try:\n        py_type = column.type.python_type\n      except Exception as e:\n        msg = f\"{column.name} is of type: {sa_type} that is not implemented in python.  Setting python type to None.\"\n        logger.warning(msg)\n        logger.warning(e)\n\n      # print(f\"  Python type: {py_type}\")\n      result[class_name][column.name][\"python_type\"] = py_type\n      result[class_name][column.name][\"database_type\"] = db_type\n      result[class_name][column.name][\"sqlalchemy_type\"] = sa_type\n\n  logger.debug(f\"returning from get_sa_automap_types()\")\n  return result\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_sa_column_types","title":"<code>get_sa_column_types(model, is_instance=False)</code>","text":"<p>Return a mapping of each column to its SQLAlchemy and Python types.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DeclarativeMeta</code> <p>SQLAlchemy model instance or class.</p> required <code>is_instance</code> <code>bool</code> <p>True if <code>model</code> is an instance, False if a class.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Mapping from column names to a dict with 'sqlalchemy_type' and 'python_type'.</p> Example <p>get_sa_column_types(User) {   'id': {'sa_type': 'Integer', 'py_type': 'int'},   'email': {'sa_type': 'String', 'py_type': 'str'} }</p> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def get_sa_column_types(model: DeclarativeMeta, is_instance: bool = False) -&gt; dict:\n  \"\"\"\n  Return a mapping of each column to its SQLAlchemy and Python types.\n\n  Args:\n      model (DeclarativeMeta): SQLAlchemy model instance or class.\n      is_instance (bool): True if `model` is an instance, False if a class.\n\n  Returns:\n      dict: Mapping from column names to a dict with 'sqlalchemy_type' and 'python_type'.\n\n  Example:\n    &gt;&gt;&gt; get_sa_column_types(User)\n    {\n      'id': {'sa_type': 'Integer', 'py_type': 'int'},\n      'email': {'sa_type': 'String', 'py_type': 'str'}\n    }\n  \"\"\"\n\n  # Get the table inspector for the model\n  if is_instance:\n    inspector = inspect(type(model))\n  else:\n    inspector = inspect(model)\n\n  logger.debug(f\"\\t{model=}\")\n  logger.debug(f\"\\t{inspector=}\")\n\n  columns_info = {}\n  for column in inspector.columns:\n    col_name = column.name\n    try:\n      columns_info[col_name] = {\n        'sqlalchemy_type': column.type,\n        'python_type': column.type.python_type\n      }\n    except Exception as e:\n      logger.warning(f\"{col_name} has unsupported Python type.\")\n      logger.warning(e)\n      columns_info[col_name] = {\n        'sqlalchemy_type': column.type,\n        'python_type': None\n      }\n      raise  # Re-raises the current exception with original traceback - comment out if you don't to warn rather than fail\n\n  return columns_info\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_sa_fields","title":"<code>get_sa_fields(model)</code>","text":"<p>Get a sorted list of column names for a SQLAlchemy model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DeclarativeMeta</code> <p>SQLAlchemy model instance or class.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Alphabetically sorted list of column attribute names.</p> Example <p>get_sa_fields(User) ['email', 'id', 'name']</p> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def get_sa_fields(model: DeclarativeMeta) -&gt; list[str]:\n  \"\"\"\n  Get a sorted list of column names for a SQLAlchemy model.\n\n  Args:\n      model (DeclarativeMeta): SQLAlchemy model instance or class.\n\n  Returns:\n      list[str]: Alphabetically sorted list of column attribute names.\n\n  Example:\n    &gt;&gt;&gt; get_sa_fields(User)\n    ['email', 'id', 'name']\n  \"\"\"\n  inst = inspect(model)\n  model_fields = [c_attr.key for c_attr in inst.mapper.column_attrs]\n  model_fields.sort()\n  return model_fields\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_table_row_and_column","title":"<code>get_table_row_and_column(db, base, table_name, column_name, id_)</code>","text":"<p>Fetch a row and column value given table name and primary key.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy instance bound to the Flask app.</p> required <code>base</code> <code>DeclarativeMeta</code> <p>Declarative base.</p> required <code>table_name</code> <code>str</code> <p>Table name.</p> required <code>column_name</code> <code>str</code> <p>Column of interest.</p> required <code>id_</code> <code>int</code> <p>Primary key value.</p> required <p>Returns:</p> Type Description <code>tuple | None</code> <p>tuple | None: (row, value) if found, else (None, None).</p> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def get_table_row_and_column(\n    db: SQLAlchemy,\n    base: DeclarativeMeta,\n    table_name: str,\n    column_name: str,\n    id_: int\n) -&gt; tuple | None:\n  \"\"\"\n  Fetch a row and column value given table name and primary key.\n\n  Args:\n      db (SQLAlchemy): SQLAlchemy instance bound to the Flask app.\n      base (DeclarativeMeta): Declarative base.\n      table_name (str): Table name.\n      column_name (str): Column of interest.\n      id_ (int): Primary key value.\n\n  Returns:\n      tuple | None: (row, value) if found, else (None, None).\n  \"\"\"\n  logger.debug(f\"Getting {column_name} from {table_name} where pk={id_}\")\n  column_value = None\n  table = get_class_from_table_name(base, table_name)\n  row = db.session.query(table).get(id_)\n\n  if row:\n    column_value = getattr(row, column_name)\n\n  logger.debug(f\"{row=}, {column_value=}\")\n  return row, column_value\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.load_model_json_column","title":"<code>load_model_json_column(model, column_name)</code>","text":"<p>Safely extract and normalize a JSON dictionary from a model's column.</p> <p>This helper ensures that the value stored in a model's JSON column is returned as a Python dictionary, regardless of whether it's stored as a JSON string or native dict in the database.</p> <p>If the value is a malformed JSON string, a warning is logged and an empty dict is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DeclarativeMeta</code> <p>SQLAlchemy ORM model instance.</p> required <code>column_name</code> <code>str</code> <p>Name of the attribute on the model (e.g., 'misc_json').</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Parsed dictionary from the JSON column. Defaults to {} on failure or None.</p> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def load_model_json_column(model: DeclarativeMeta, column_name: str) -&gt; dict:\n  \"\"\"\n  Safely extract and normalize a JSON dictionary from a model's column.\n\n  This helper ensures that the value stored in a model's JSON column is returned\n  as a Python dictionary, regardless of whether it's stored as a JSON string or\n  native dict in the database.\n\n  If the value is a malformed JSON string, a warning is logged and an empty dict is returned.\n\n  Args:\n      model (DeclarativeMeta): SQLAlchemy ORM model instance.\n      column_name (str): Name of the attribute on the model (e.g., 'misc_json').\n\n  Returns:\n      dict: Parsed dictionary from the JSON column. Defaults to {} on failure or None.\n  \"\"\"\n  raw_value = getattr(model, column_name)\n  if isinstance(raw_value, str):\n    try:\n      return json.loads(raw_value)\n    except json.JSONDecodeError:\n      logger.warning(f\"Corrupt JSON found in {column_name}, resetting to empty dict.\")\n      return {}\n  elif raw_value is None:\n    return {}\n  elif isinstance(raw_value, dict):\n    return raw_value\n  else:\n    raise TypeError(f\"Expected str, dict, or None for {column_name}, got {type(raw_value).__name__}\")\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.run_diagnostics","title":"<code>run_diagnostics(db, base, session)</code>","text":"<p>Run diagnostics to validate SQLAlchemy utilities in this module.</p> This function performs <ul> <li>Model diagnostics</li> <li>Type inspection</li> <li>Conversion to dictionary</li> <li>Change detection</li> <li>Table row extraction</li> <li>Primary and foreign key access</li> </ul> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>SQLAlchemy</code> <p>SQLAlchemy db object.</p> required <code>base</code> <code>DeclarativeMeta</code> <p>Declarative or automap base.</p> required <code>session</code> <code>Session</code> <p>Active SQLAlchemy session.</p> required Example <p>run_diagnostics(db, base, db.session)</p> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def run_diagnostics(db: SQLAlchemy, base: DeclarativeMeta, session: Session) -&gt; None:\n  \"\"\"\n  Run diagnostics to validate SQLAlchemy utilities in this module.\n\n  This function performs:\n    - Model diagnostics\n    - Type inspection\n    - Conversion to dictionary\n    - Change detection\n    - Table row extraction\n    - Primary and foreign key access\n\n  Args:\n      db: SQLAlchemy db object.\n      base (DeclarativeMeta): Declarative or automap base.\n      session (Session): Active SQLAlchemy session.\n\n  Example:\n      &gt;&gt;&gt; run_diagnostics(db, base, db.session)\n  \"\"\"\n  print(\"Running diagnostics for sqlalchemy_util.py...\")\n\n  # Use first available mapped class\n  class_names = list(base.classes.keys())\n  assert class_names, \"No mapped tables found\"\n\n  test_table = class_names[0]\n  cls = base.classes[test_table]\n\n  row = session.query(cls).first()\n  assert row, f\"No rows in table '{test_table}'\"\n\n  print(f\"Using table: {test_table}\")\n\n  # Field listing\n  fields = get_sa_fields(row)\n  print(f\"Fields: {fields}\")\n\n  # Type info\n  types = get_sa_column_types(row)\n  print(f\"Column types: {types}\")\n\n  # Dict conversion and comparison\n  d1 = sa_model_to_dict(row)\n  d2 = d1.copy()\n  d2[fields[0]] = \"__CHANGED__\"\n  changes = sa_model_dict_compare(d1, d2)\n  assert fields[0] in changes, \"Change detection failed\"\n\n  # Table to list\n  all_rows = table_to_list(base, session, test_table)\n  assert isinstance(all_rows, list) and all_rows, \"table_to_list failed\"\n\n  # Try auto-increment value if PK exists\n  pk_column = fields[0]\n  if hasattr(cls, pk_column):\n    result = find_auto_increment_value(db, test_table, pk_column)\n    print(f\"Auto-increment: {result}\")\n\n  print(\"All diagnostics completed successfully.\")\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.sa_model_diagnostics","title":"<code>sa_model_diagnostics(model, comment='')</code>","text":"<p>Log diagnostic details about a SQLAlchemy model instance.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DeclarativeMeta</code> <p>SQLAlchemy model instance.</p> required <code>comment</code> <code>str</code> <p>Optional comment header for log output.</p> <code>''</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>sa_model_diagnostics(user, comment=\"Inspecting User\")</p> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def sa_model_diagnostics(model: DeclarativeMeta, comment: str = \"\") -&gt; None:\n  \"\"\"\n  Log diagnostic details about a SQLAlchemy model instance.\n\n  Args:\n      model (DeclarativeMeta): SQLAlchemy model instance.\n      comment (str): Optional comment header for log output.\n\n  Returns:\n      None\n\n  Example:\n    &gt;&gt;&gt; sa_model_diagnostics(user, comment=\"Inspecting User\")\n  \"\"\"\n  logger.debug(f\"Diagnostics for model of type {type(model)=}\")\n  if comment:\n    logger.debug(f\"{comment}\")\n  logger.debug(f\"{model=}\")\n\n  fields = get_sa_fields(model)\n  for key in fields:\n    value = getattr(model, key)\n    logger.debug(f\"{key} {type(value)} = ({value})\")\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.sa_model_dict_compare","title":"<code>sa_model_dict_compare(model_before, model_after)</code>","text":"<p>Compare two model dictionaries and return changed fields.</p> <p>Parameters:</p> Name Type Description Default <code>model_before</code> <code>dict</code> <p>Original state.</p> required <code>model_after</code> <code>dict</code> <p>New state.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Changed fields and their new values.</p> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def sa_model_dict_compare(model_before: dict, model_after: dict) -&gt; dict:\n  \"\"\"\n  Compare two model dictionaries and return changed fields.\n\n  Args:\n      model_before (dict): Original state.\n      model_after (dict): New state.\n\n  Returns:\n      dict: Changed fields and their new values.\n  \"\"\"\n  changes = {}\n  for field in model_after:\n    if field not in model_before or model_before[field] != model_after[field]:\n      changes[field] = model_after[field]\n  return changes\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.sa_model_to_dict","title":"<code>sa_model_to_dict(model)</code>","text":"<p>Convert a SQLAlchemy model to a Python dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DeclarativeMeta</code> <p>SQLAlchemy model.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with column names as keys and values from the model.</p> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def sa_model_to_dict(model: DeclarativeMeta) -&gt; dict:\n  \"\"\"\n  Convert a SQLAlchemy model to a Python dictionary.\n\n  Args:\n      model (DeclarativeMeta): SQLAlchemy model.\n\n  Returns:\n      dict: Dictionary with column names as keys and values from the model.\n  \"\"\"\n  model_as_dict = {}\n  fields = get_sa_fields(model)\n  for field in fields:\n    value = getattr(model, field)\n    model_as_dict[field] = value\n\n  return model_as_dict\n</code></pre>"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.table_to_list","title":"<code>table_to_list(base, session, table_name)</code>","text":"<p>Convert all rows of a mapped table to a list of dicts.</p> <p>Parameters:</p> Name Type Description Default <code>base</code> <code>DeclarativeMeta</code> <p>Automap base.</p> required <code>session</code> <code>Session</code> <p>SQLAlchemy session.</p> required <code>table_name</code> <code>str</code> <p>Table name to query.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: List of row dictionaries.</p> Source code in <code>arb\\utils\\sql_alchemy.py</code> <pre><code>def table_to_list(base: DeclarativeMeta, session: Session, table_name: str) -&gt; list[dict]:\n  \"\"\"\n  Convert all rows of a mapped table to a list of dicts.\n\n  Args:\n      base (DeclarativeMeta): Automap base.\n      session (Session): SQLAlchemy session.\n      table_name (str): Table name to query.\n\n  Returns:\n      list[dict]: List of row dictionaries.\n  \"\"\"\n  result = []\n  table = base.classes.get(table_name)\n\n  if table:\n    logger.debug(f\"Selecting data from: {table_name}\")\n    rows = session.query(table).all()\n    col_names = table.__table__.columns.keys()\n\n    for row in rows:\n      row_data = {col: getattr(row, col) for col in col_names}\n      result.append(row_data)\n  else:\n    logger.warning(f\"Table '{table_name}' not found in metadata.\")\n\n  return result\n</code></pre>"},{"location":"reference/arb/utils/web_html/","title":"<code>arb.utils.web_html</code>","text":"<p>HTML and WTForms utility functions for form handling and file uploads.</p> This module provides helper functions for <ul> <li>Uploading user files with sanitized names</li> <li>Generating WTForms-compatible selector lists</li> <li>Managing triple tuples for dynamic dropdown metadata</li> </ul> Notes <ul> <li>Avoids circular imports by not depending on other utility modules.</li> <li>Other utility modules (e.g., Excel, DB) may safely import this one.</li> <li>Adds \"Please Select\" logic to dropdowns using <code>arb.utils.constants</code>.</li> </ul> Example Usage <p>from arb.utils.web_html import upload_single_file, selector_list_to_tuples</p>"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.ensure_placeholder_option","title":"<code>ensure_placeholder_option(tuple_list, item=PLEASE_SELECT, item_dict={'disabled': True}, ensure_first=True)</code>","text":"<p>Ensure a placeholder entry is present in the tuple list.</p> <p>This function ensures that a specified \"placeholder\" option (typically used to prompt users to select a value, such as \"Please Select\") exists in the given list of selector options. If the placeholder is not present, it is inserted at the top. If it exists but is not the first item, it is optionally moved to the first position.</p> <p>Parameters:</p> Name Type Description Default <code>tuple_list</code> <code>list[tuple[str, str, dict]]</code> <p>Original selector list.</p> required <code>item</code> <code>str</code> <p>Value for the placeholder. Default is \"Please Select\".</p> <code>PLEASE_SELECT</code> <code>item_dict</code> <code>dict</code> <p>Metadata for the placeholder. Default disables the option.</p> <code>{'disabled': True}</code> <code>ensure_first</code> <code>bool</code> <p>If True, move placeholder to top if found elsewhere.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[tuple[str, str, dict]]</code> <p>list[tuple[str, str, dict]]: Updated tuple list with ensured placeholder.</p> Example <p>ensure_placeholder_option([(\"A\", \"A\", {})]) [('Please Select', 'Please Select', {'disabled': True}), ('A', 'A', {})]</p> Source code in <code>arb\\utils\\web_html.py</code> <pre><code>def ensure_placeholder_option(\n    tuple_list: list[tuple[str, str, dict]],\n    item: str = PLEASE_SELECT,\n    item_dict: dict = {\"disabled\": True},\n    ensure_first: bool = True\n) -&gt; list[tuple[str, str, dict]]:\n  \"\"\"\n  Ensure a placeholder entry is present in the tuple list.\n\n  This function ensures that a specified \"placeholder\" option (typically used to prompt\n  users to select a value, such as \"Please Select\") exists in the given list of\n  selector options. If the placeholder is not present, it is inserted at the top. If it\n  exists but is not the first item, it is optionally moved to the first position.\n\n  Args:\n      tuple_list (list[tuple[str, str, dict]]): Original selector list.\n      item (str): Value for the placeholder. Default is \"Please Select\".\n      item_dict (dict): Metadata for the placeholder. Default disables the option.\n      ensure_first (bool): If True, move placeholder to top if found elsewhere.\n\n  Returns:\n      list[tuple[str, str, dict]]: Updated tuple list with ensured placeholder.\n\n  Example:\n      &gt;&gt;&gt; ensure_placeholder_option([(\"A\", \"A\", {})])\n      [('Please Select', 'Please Select', {'disabled': True}), ('A', 'A', {})]\n  \"\"\"\n\n  if item is None:\n    item = PLEASE_SELECT\n\n  if item_dict is None:\n    item_dict = {\"disabled\": True}\n\n  placeholder = (item, item, item_dict)\n\n  # Find the index of any existing placeholder (based on value match)\n  # Explanation:\n  #   - `enumerate(tuple_list)` produces (index, tuple) pairs.\n  #   - `t[0] == item` checks if the first element (value) matches the placeholder value.\n  #   - `next(...)` returns the index of the first match, or `None` if no match is found.\n  index = next((i for i, t in enumerate(tuple_list) if t[0] == item), None)\n\n  if index is None:\n    # Placeholder not found; insert it at the beginning of the list.\n    return [placeholder] + tuple_list\n\n  elif ensure_first and index != 0:\n    # Placeholder found but not in first position and `ensure_first` is True.\n    # Move it to the front while preserving the order of the rest.\n    reordered = [t for i, t in enumerate(tuple_list) if i != index]\n    return [tuple_list[index]] + reordered\n\n  # Placeholder exists and is already in the correct position; return unchanged.\n  return tuple_list\n</code></pre>"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.list_to_triple_tuple","title":"<code>list_to_triple_tuple(values)</code>","text":"<p>Convert a list of strings into WTForms triple tuples.</p> <p>Each tuple contains (value, label, metadata).</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>list[str]</code> <p>List of form options.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str, dict]]</code> <p>list[tuple[str, str, dict]]: Triple tuples for WTForms SelectField.</p> Example <p>list_to_triple_tuple([\"A\", \"B\"]) [('A', 'A', {}), ('B', 'B', {})]</p> Source code in <code>arb\\utils\\web_html.py</code> <pre><code>def list_to_triple_tuple(values: list[str]) -&gt; list[tuple[str, str, dict]]:\n  \"\"\"\n  Convert a list of strings into WTForms triple tuples.\n\n  Each tuple contains (value, label, metadata).\n\n  Args:\n      values (list[str]): List of form options.\n\n  Returns:\n      list[tuple[str, str, dict]]: Triple tuples for WTForms SelectField.\n\n  Example:\n      &gt;&gt;&gt; list_to_triple_tuple([\"A\", \"B\"])\n      [('A', 'A', {}), ('B', 'B', {})]\n  \"\"\"\n  return [(v, v, {}) for v in values]\n</code></pre>"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.remove_items","title":"<code>remove_items(tuple_list, remove_items)</code>","text":"<p>Remove one or more values from a tuple list by matching the first element.</p> <p>Parameters:</p> Name Type Description Default <code>tuple_list</code> <code>list[tuple[str, str, dict]]</code> <p>Selector tuples.</p> required <code>remove_items</code> <code>str | list[str]</code> <p>One or more values to remove by key match.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str, dict]]</code> <p>list[tuple[str, str, dict]]: Filtered list excluding the removed values.</p> Example <p>remove_items([(\"A\", \"A\", {}), (\"B\", \"B\", {})], \"B\") [('A', 'A', {})]</p> Source code in <code>arb\\utils\\web_html.py</code> <pre><code>def remove_items(tuple_list: list[tuple[str, str, dict]],\n                 remove_items: str | list[str]\n                 ) -&gt; list[tuple[str, str, dict]]:\n  \"\"\"\n  Remove one or more values from a tuple list by matching the first element.\n\n  Args:\n      tuple_list (list[tuple[str, str, dict]]): Selector tuples.\n      remove_items (str | list[str]): One or more values to remove by key match.\n\n  Returns:\n      list[tuple[str, str, dict]]: Filtered list excluding the removed values.\n\n  Example:\n      &gt;&gt;&gt; remove_items([(\"A\", \"A\", {}), (\"B\", \"B\", {})], \"B\")\n      [('A', 'A', {})]\n  \"\"\"\n  remove_set = {remove_items} if isinstance(remove_items, str) else set(remove_items)\n  return [t for t in tuple_list if t[0] not in remove_set]\n</code></pre>"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.run_diagnostics","title":"<code>run_diagnostics()</code>","text":"<p>Run assertions to validate selector utility behavior.</p> Tests <ul> <li>Conversion of string lists to selector tuples</li> <li>Tuple updating with metadata</li> <li>Placeholder insertion</li> <li>Value removal from selector lists</li> <li>Dict transformation to tuple selectors</li> </ul> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>run_diagnostics()</p> Source code in <code>arb\\utils\\web_html.py</code> <pre><code>def run_diagnostics() -&gt; None:\n  \"\"\"\n  Run assertions to validate selector utility behavior.\n\n  Tests:\n    - Conversion of string lists to selector tuples\n    - Tuple updating with metadata\n    - Placeholder insertion\n    - Value removal from selector lists\n    - Dict transformation to tuple selectors\n\n  Returns:\n      None\n\n  Example:\n      &gt;&gt;&gt; run_diagnostics()\n  \"\"\"\n  print(\"Running diagnostics for web_html.py...\")\n\n  test_values = [\"A\", \"B\", \"C\"]\n\n  # Test selector_list_to_tuples\n  selector = selector_list_to_tuples(test_values)\n  assert selector[0][0] == PLEASE_SELECT\n  assert (\"A\", \"A\") in selector\n\n  # Test list_to_triple_tuple\n  triple = list_to_triple_tuple([\"X\", \"Y\"])\n  assert triple == [(\"X\", \"X\", {}), (\"Y\", \"Y\", {})]\n\n  # Test update_triple_tuple_dict\n  updated = update_triple_tuple_dict(triple, [\"Y\"], {\"selected\": True})\n  assert updated[1][2].get(\"selected\") is True\n\n  # Test update_selector_dict\n  test_dict = {\"colors\": [\"red\", \"green\"]}\n  updated_dict = update_selector_dict(test_dict)\n  assert PLEASE_SELECT in [x[0] for x in updated_dict[\"colors\"]]\n\n  # Test ensure_placeholder_option\n  reordered = ensure_placeholder_option([(\"X\", \"X\", {})])\n  assert reordered[0][0] == PLEASE_SELECT\n\n  # Test remove_items\n  cleaned = remove_items(triple, \"X\")\n  assert all(t[0] != \"X\" for t in cleaned)\n\n  print(\"All selector diagnostics passed.\")\n</code></pre>"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.selector_list_to_tuples","title":"<code>selector_list_to_tuples(values)</code>","text":"<p>Convert a list of values into WTForms-compatible dropdown tuples.</p> <p>Adds a disabled \"Please Select\" entry at the top of the list.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>list[str]</code> <p>Dropdown options (excluding \"Please Select\").</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str] | tuple[str, str, dict]]</code> <p>list[tuple[str, str] | tuple[str, str, dict]]: WTForms selector list including a disabled \"Please Select\" entry.</p> Example <p>selector_list_to_tuples([\"Red\", \"Green\"]) [('Please Select', 'Please Select', {'disabled': True}),  ('Red', 'Red'), ('Green', 'Green')]</p> Source code in <code>arb\\utils\\web_html.py</code> <pre><code>def selector_list_to_tuples(values: list[str]) -&gt; list[tuple[str, str] | tuple[str, str, dict]]:\n  \"\"\"\n  Convert a list of values into WTForms-compatible dropdown tuples.\n\n  Adds a disabled \"Please Select\" entry at the top of the list.\n\n  Args:\n      values (list[str]): Dropdown options (excluding \"Please Select\").\n\n  Returns:\n      list[tuple[str, str] | tuple[str, str, dict]]:\n          WTForms selector list including a disabled \"Please Select\" entry.\n\n  Example:\n      &gt;&gt;&gt; selector_list_to_tuples([\"Red\", \"Green\"])\n      [('Please Select', 'Please Select', {'disabled': True}),\n       ('Red', 'Red'), ('Green', 'Green')]\n  \"\"\"\n  result = [(PLEASE_SELECT, PLEASE_SELECT, {\"disabled\": True})]\n  result += [(v, v) for v in values]\n  return result\n</code></pre>"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.update_selector_dict","title":"<code>update_selector_dict(input_dict)</code>","text":"<p>Convert dictionary of string lists into selector-style tuple lists.</p> <p>Each list is transformed to include a \"Please Select\" disabled option followed by (value, label) tuples.</p> <p>Parameters:</p> Name Type Description Default <code>input_dict</code> <code>dict[str, list[str]]</code> <p>Dict of dropdown options per field.</p> required <p>Returns:</p> Type Description <code>dict[str, list[tuple[str, str] | tuple[str, str, dict]]]</code> <p>dict[str, list[tuple[str, str] | tuple[str, str, dict]]]: Dict with WTForms-ready selector tuples.</p> Example <p>update_selector_dict({\"colors\": [\"Red\", \"Blue\"]}) {   \"colors\": [     (\"Please Select\", \"Please Select\", {\"disabled\": True}),     (\"Red\", \"Red\"),     (\"Blue\", \"Blue\")   ] }</p> Source code in <code>arb\\utils\\web_html.py</code> <pre><code>def update_selector_dict(input_dict: dict[str, list[str]]) -&gt; dict[str, list[tuple[str, str] | tuple[str, str, dict]]]:\n  \"\"\"\n  Convert dictionary of string lists into selector-style tuple lists.\n\n  Each list is transformed to include a \"Please Select\" disabled option\n  followed by (value, label) tuples.\n\n  Args:\n      input_dict (dict[str, list[str]]): Dict of dropdown options per field.\n\n  Returns:\n      dict[str, list[tuple[str, str] | tuple[str, str, dict]]]:\n          Dict with WTForms-ready selector tuples.\n\n  Example:\n      &gt;&gt;&gt; update_selector_dict({\"colors\": [\"Red\", \"Blue\"]})\n      {\n        \"colors\": [\n          (\"Please Select\", \"Please Select\", {\"disabled\": True}),\n          (\"Red\", \"Red\"),\n          (\"Blue\", \"Blue\")\n        ]\n      }\n  \"\"\"\n  return {key: selector_list_to_tuples(values) for key, values in input_dict.items()}\n</code></pre>"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.update_triple_tuple_dict","title":"<code>update_triple_tuple_dict(tuple_list, match_list, match_update_dict, unmatch_update_dict=None)</code>","text":"<p>Update the metadata dict of each WTForms triple tuple based on value match.</p> <p>Parameters:</p> Name Type Description Default <code>tuple_list</code> <code>list[tuple[str, str, dict]]</code> <p>Existing list of selector tuples.</p> required <code>match_list</code> <code>list[str]</code> <p>Values to match against.</p> required <code>match_update_dict</code> <code>dict</code> <p>Metadata to apply if value is in <code>match_list</code>.</p> required <code>unmatch_update_dict</code> <code>dict | None</code> <p>Metadata to apply otherwise (optional).</p> <code>None</code> <p>Returns:</p> Type Description <code>list[tuple[str, str, dict]]</code> <p>list[tuple[str, str, dict]]: Updated list of selector tuples.</p> Example <p>update_triple_tuple_dict( ...     [('A', 'A', {}), ('B', 'B', {})], ...     ['A'], ...     {'disabled': True}, ...     {'class': 'available'} ... ) [('A', 'A', {'disabled': True}), ('B', 'B', {'class': 'available'})]</p> Source code in <code>arb\\utils\\web_html.py</code> <pre><code>def update_triple_tuple_dict(\n    tuple_list: list[tuple[str, str, dict]],\n    match_list: list[str],\n    match_update_dict: dict,\n    unmatch_update_dict: dict | None = None\n) -&gt; list[tuple[str, str, dict]]:\n  \"\"\"\n  Update the metadata dict of each WTForms triple tuple based on value match.\n\n  Args:\n      tuple_list (list[tuple[str, str, dict]]): Existing list of selector tuples.\n      match_list (list[str]): Values to match against.\n      match_update_dict (dict): Metadata to apply if value is in `match_list`.\n      unmatch_update_dict (dict | None): Metadata to apply otherwise (optional).\n\n  Returns:\n      list[tuple[str, str, dict]]: Updated list of selector tuples.\n\n  Example:\n      &gt;&gt;&gt; update_triple_tuple_dict(\n      ...     [('A', 'A', {}), ('B', 'B', {})],\n      ...     ['A'],\n      ...     {'disabled': True},\n      ...     {'class': 'available'}\n      ... )\n      [('A', 'A', {'disabled': True}), ('B', 'B', {'class': 'available'})]\n  \"\"\"\n  if unmatch_update_dict is None:\n    unmatch_update_dict = {}\n\n  result = []\n  for key, value, meta in tuple_list:\n    meta.update(match_update_dict if key in match_list else unmatch_update_dict)\n    result.append((key, value, meta))\n  return result\n</code></pre>"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.upload_single_file","title":"<code>upload_single_file(upload_dir, request_file)</code>","text":"<p>Save a user-uploaded file to the server using a secure, timestamped filename.</p> <p>Parameters:</p> Name Type Description Default <code>upload_dir</code> <code>str | Path</code> <p>Directory to save the uploaded file.</p> required <code>request_file</code> <code>FileStorage</code> <p>Werkzeug object from <code>request.files['&lt;field&gt;']</code>.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Full path to the uploaded file on disk.</p> <p>Raises:</p> Type Description <code>OSError</code> <p>If the file cannot be written to disk.</p> Example <p>file = request.files['data'] path = upload_single_file(\"/data/uploads\", file)</p> Source code in <code>arb\\utils\\web_html.py</code> <pre><code>def upload_single_file(upload_dir: str | Path, request_file: FileStorage) -&gt; Path:\n  \"\"\"\n  Save a user-uploaded file to the server using a secure, timestamped filename.\n\n  Args:\n      upload_dir (str | Path): Directory to save the uploaded file.\n      request_file (FileStorage): Werkzeug object from `request.files['&lt;field&gt;']`.\n\n  Returns:\n      Path: Full path to the uploaded file on disk.\n\n  Raises:\n      OSError: If the file cannot be written to disk.\n\n  Example:\n      &gt;&gt;&gt; file = request.files['data']\n      &gt;&gt;&gt; path = upload_single_file(\"/data/uploads\", file)\n  \"\"\"\n  logger.debug(f\"Attempting to upload {request_file.filename=}\")\n  file_name = get_secure_timestamped_file_name(upload_dir, request_file.filename)\n  logger.debug(f\"Upload single file as: {file_name}\")\n  request_file.save(file_name)\n  return file_name\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/","title":"<code>arb.utils.wtf_forms_util</code>","text":"<p>Functions and helper classes to support WTForms models.</p> Notes <ul> <li>WTForm model classes should remain adjacent to Flask views (e.g., in <code>wtf_landfill.py</code>)</li> <li>This module is for shared utilities, validators, and form-to-model conversion logic.</li> </ul>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.IfTruthy","title":"<code>IfTruthy</code>","text":"<p>WTForms validator: Dynamically switches between InputRequired and Optional.</p> <p>The validator behavior is conditional on another field\u2019s truthiness. Depending on <code>mode</code>, this field becomes required or optional.</p> <p>Parameters:</p> Name Type Description Default <code>other_field_name</code> <code>str</code> <p>Name of the field to evaluate.</p> required <code>falsy_values</code> <code>list | None</code> <p>Custom falsy value list (defaults to standard values).</p> <code>None</code> <code>mode</code> <code>str</code> <p>Either 'required on truthy' or 'optional on truthy'.</p> <code>'required on truthy'</code> <code>message</code> <code>str | None</code> <p>Optional validation error message.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If an invalid mode is provided.</p> Example <p>class MyForm(FlaskForm):     toggle = BooleanField()     extra = StringField(validators=[IfTruthy(\"toggle\", mode=\"required on truthy\")])</p> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>class IfTruthy:\n  \"\"\"\n  WTForms validator: Dynamically switches between InputRequired and Optional.\n\n  The validator behavior is conditional on another field\u2019s truthiness.\n  Depending on `mode`, this field becomes required or optional.\n\n  Args:\n      other_field_name (str): Name of the field to evaluate.\n      falsy_values (list | None): Custom falsy value list (defaults to standard values).\n      mode (str): Either 'required on truthy' or 'optional on truthy'.\n      message (str | None): Optional validation error message.\n\n  Raises:\n      TypeError: If an invalid mode is provided.\n\n  Example:\n      class MyForm(FlaskForm):\n          toggle = BooleanField()\n          extra = StringField(validators=[IfTruthy(\"toggle\", mode=\"required on truthy\")])\n  \"\"\"\n\n  field_flags = (\"iftruthy\",)\n\n  def __init__(self, other_field_name: str, falsy_values: list | None = None, mode: str = 'required on truthy', message: str | None = None):\n    self.other_field_name = other_field_name\n    if falsy_values is None:\n      falsy_values = [False, [], {}, (), '', '0', '0.0', 0, 0.0]\n    self.falsy_values = falsy_values\n\n    if mode == 'required on truthy':\n      self.validators = {'truthy': InputRequired, 'falsy': Optional}\n    elif mode == 'optional on truthy':\n      self.validators = {'truthy': Optional, 'falsy': InputRequired}\n    else:\n      raise TypeError(f\"Unknown mode: {mode}\")\n\n    self.message = message\n\n  def __call__(self, form, field):\n    other_field = form[self.other_field_name]\n    if other_field is None:\n      raise Exception(f'No field named \"{self.other_field_name}\" in form')\n\n    validator_class = self.validators['truthy'] if other_field.data not in self.falsy_values else self.validators['falsy']\n    validator_class(self.message).__call__(form, field)\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.RequiredIfTruthy","title":"<code>RequiredIfTruthy</code>","text":"<p>WTForms validator: Applies InputRequired or Optional based on another field's truthiness.</p> <p>If the referenced field is \"truthy\" (not in a falsy list), then this field is required. If it is \"falsy\", this field becomes optional.</p> <p>Parameters:</p> Name Type Description Default <code>other_field_name</code> <code>str</code> <p>Name of the field to check.</p> required <code>message</code> <code>str | None</code> <p>Optional custom validation error message.</p> <code>None</code> <code>other_field_invalid_values</code> <code>list | None</code> <p>Values considered falsy (defaults to standard empty/zero/null values).</p> <code>None</code> Example <p>class MyForm(FlaskForm):     confirm = BooleanField()     notes = StringField(validators=[RequiredIfTruthy(\"confirm\")])</p> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>class RequiredIfTruthy:\n  \"\"\"\n  WTForms validator: Applies InputRequired or Optional based on another field's truthiness.\n\n  If the referenced field is \"truthy\" (not in a falsy list), then this field is required.\n  If it is \"falsy\", this field becomes optional.\n\n  Args:\n      other_field_name (str): Name of the field to check.\n      message (str | None): Optional custom validation error message.\n      other_field_invalid_values (list | None): Values considered falsy (defaults to standard empty/zero/null values).\n\n  Example:\n      class MyForm(FlaskForm):\n          confirm = BooleanField()\n          notes = StringField(validators=[RequiredIfTruthy(\"confirm\")])\n  \"\"\"\n\n  field_flags = (\"requirediftruthy\",)\n\n  def __init__(self, other_field_name: str, message: str | None = None, other_field_invalid_values: list | None = None):\n    self.other_field_name = other_field_name\n    self.message = message\n    if other_field_invalid_values is None:\n      other_field_invalid_values = [False, [], {}, (), '', '0', '0.0', 0, 0.0]\n    self.other_field_invalid_values = other_field_invalid_values\n    logger.debug(\"RequiredIfTruthy initialized\")\n\n  def __call__(self, form, field):\n    other_field = form[self.other_field_name]\n    if other_field is None:\n      raise Exception(f'No field named \"{self.other_field_name}\" in form')\n\n    if other_field.data not in self.other_field_invalid_values:\n      logger.debug(\"other_field is truthy \u2192 requiring this field\")\n      InputRequired(self.message).__call__(form, field)\n    else:\n      logger.debug(\"other_field is falsy \u2192 allowing this field to be optional\")\n      Optional(self.message).__call__(form, field)\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.build_choices","title":"<code>build_choices(header, items)</code>","text":"<p>Combine header and dynamic items into a list of triple-tuples for WTForms SelectFields.</p> <p>Parameters:</p> Name Type Description Default <code>header</code> <code>list[tuple[str, str, dict]]</code> <p>Static options to appear first in the dropdown.</p> required <code>items</code> <code>list[str]</code> <p>Dynamic option values to convert into (value, label, {}) format.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, str, dict]]</code> <p>list[tuple[str, str, dict]]: Combined list of header and generated item tuples.</p> Example <p>build_choices( ...   [(\"Please Select\", \"Please Select\", {\"disabled\": True})], ...   [\"One\", \"Two\"] ... ) [   (\"Please Select\", \"Please Select\", {\"disabled\": True}),   (\"One\", \"One\", {}),   (\"Two\", \"Two\", {}) ]</p> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def build_choices(header: list[tuple[str, str, dict]], items: list[str]) -&gt; list[tuple[str, str, dict]]:\n  \"\"\"\n  Combine header and dynamic items into a list of triple-tuples for WTForms SelectFields.\n\n  Args:\n    header (list[tuple[str, str, dict]]): Static options to appear first in the dropdown.\n    items (list[str]): Dynamic option values to convert into (value, label, {}) format.\n\n  Returns:\n    list[tuple[str, str, dict]]: Combined list of header and generated item tuples.\n\n  Example:\n    &gt;&gt;&gt; build_choices(\n    ...   [(\"Please Select\", \"Please Select\", {\"disabled\": True})],\n    ...   [\"One\", \"Two\"]\n    ... )\n    [\n      (\"Please Select\", \"Please Select\", {\"disabled\": True}),\n      (\"One\", \"One\", {}),\n      (\"Two\", \"Two\", {})\n    ]\n  \"\"\"\n  footer = [(item, item, {}) for item in items]\n  return header + footer\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.change_validators","title":"<code>change_validators(form, field_names_to_change, old_validator, new_validator)</code>","text":"<p>Replace one validator type with another on a list of WTForms fields.</p> <p>Parameters:</p> Name Type Description Default <code>form</code> <code>FlaskForm</code> <p>WTForms form instance.</p> required <code>field_names_to_change</code> <code>list[str]</code> <p>List of fields to alter.</p> required <code>old_validator</code> <code>type</code> <p>Validator class to remove (e.g., Optional).</p> required <code>new_validator</code> <code>type</code> <p>Validator class to add (e.g., InputRequired).</p> required Notes <ul> <li>The replacement is done in-place on each field's <code>validators</code> list.</li> <li>Useful for dynamically changing required status.</li> </ul> Example <p>change_validators(form, [\"name\"], Optional, InputRequired)</p> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def change_validators(form: FlaskForm,\n                      field_names_to_change: list[str],\n                      old_validator: type,\n                      new_validator: type) -&gt; None:\n  \"\"\"\n  Replace one validator type with another on a list of WTForms fields.\n\n  Args:\n    form (FlaskForm): WTForms form instance.\n    field_names_to_change (list[str]): List of fields to alter.\n    old_validator (type): Validator class to remove (e.g., Optional).\n    new_validator (type): Validator class to add (e.g., InputRequired).\n\n  Notes:\n    - The replacement is done in-place on each field's `validators` list.\n    - Useful for dynamically changing required status.\n\n  Example:\n      &gt;&gt;&gt; change_validators(form, [\"name\"], Optional, InputRequired)\n  \"\"\"\n  field_names = get_wtforms_fields(form, include_csrf_token=False)\n  for field_name in field_names:\n    if field_name in field_names_to_change:\n      validators = form[field_name].validators\n      for i, validator in enumerate(validators):\n        if isinstance(validator, old_validator):\n          validators[i] = new_validator()\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.change_validators_on_test","title":"<code>change_validators_on_test(form, bool_test, required_if_true, optional_if_true=None)</code>","text":"<p>Conditionally switch validators on selected form fields based on a boolean test.</p> If bool_test is True <ul> <li>Fields in required_if_true become required (InputRequired).</li> <li>Fields in optional_if_true become optional (Optional).</li> </ul> If bool_test is False <ul> <li>Fields in required_if_true become optional.</li> <li>Fields in optional_if_true become required.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>form</code> <code>FlaskForm</code> <p>The form to update.</p> required <code>bool_test</code> <code>bool</code> <p>If True, required/optional fields are swapped accordingly.</p> required <code>required_if_true</code> <code>list[str]</code> <p>Field names that become required when bool_test is True.</p> required <code>optional_if_true</code> <code>list[str] | None</code> <p>Field names that become optional when bool_test is True.</p> <code>None</code> Example <p>change_validators_on_test(form, bool_test=is_active, ...                           required_if_true=[\"comment\"], ...                           optional_if_true=[\"note\"])</p> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def change_validators_on_test(form: FlaskForm,\n                              bool_test: bool,\n                              required_if_true: list[str],\n                              optional_if_true: list[str] | None = None) -&gt; None:\n  \"\"\"\n  Conditionally switch validators on selected form fields based on a boolean test.\n\n  If bool_test is True:\n    - Fields in required_if_true become required (InputRequired).\n    - Fields in optional_if_true become optional (Optional).\n\n  If bool_test is False:\n    - Fields in required_if_true become optional.\n    - Fields in optional_if_true become required.\n\n  Args:\n    form (FlaskForm): The form to update.\n    bool_test (bool): If True, required/optional fields are swapped accordingly.\n    required_if_true (list[str]): Field names that become required when bool_test is True.\n    optional_if_true (list[str] | None): Field names that become optional when bool_test is True.\n\n  Example:\n      &gt;&gt;&gt; change_validators_on_test(form, bool_test=is_active,\n      ...                           required_if_true=[\"comment\"],\n      ...                           optional_if_true=[\"note\"])\n  \"\"\"\n  if optional_if_true is None:\n    optional_if_true = []\n\n  if bool_test:\n    change_validators(form,\n                      field_names_to_change=required_if_true,\n                      old_validator=Optional,\n                      new_validator=InputRequired,\n                      )\n\n    change_validators(form,\n                      field_names_to_change=optional_if_true,\n                      old_validator=InputRequired,\n                      new_validator=Optional,\n                      )\n  else:\n    change_validators(form,\n                      field_names_to_change=required_if_true,\n                      old_validator=InputRequired,\n                      new_validator=Optional,\n                      )\n\n    change_validators(form,\n                      field_names_to_change=optional_if_true,\n                      old_validator=Optional,\n                      new_validator=InputRequired,\n                      )\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.ensure_field_choice","title":"<code>ensure_field_choice(field_name, field, choices=None)</code>","text":"<p>Ensure a field\u2019s current value is among its valid choices, or reset it to a placeholder.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the WTForms field (for logging purposes).</p> required <code>field</code> <code>Field</code> <p>WTForms-compatible field (typically a SelectField).</p> required <code>choices</code> <code>list[tuple[str, str]] | list[tuple[str, str, dict]] | None</code> <p>Valid choices to enforce. If None, uses the field's existing choices.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Notes: - If <code>choices</code> is provided, this function sets <code>field.choices</code> to the new list. - If <code>choices</code> is None, it uses the field's existing <code>.choices</code>. In either case,   it validates that the current <code>field.data</code> is among the available options and   resets it to \"Please Select\" if not. - Both <code>field.data</code> and <code>field.raw_data</code> are reset to keep form behavior consistent. - Each choice tuple should be in the form:     - (value, label), or     - (value, label, metadata_dict)     - Only the first element (<code>value</code>) is used for validation. - Use this with SelectField or similar fields where <code>.choices</code> must be explicitly defined. - The reset value \"Please Select\" should match a placeholder value if one is used in your app.</p> Example <p>ensure_field_choice(\"sector\", form.sector, [(\"oil\", \"Oil &amp; Gas\"), (\"land\", \"Landfill\")]) form.sector.choices [('oil', 'Oil &amp; Gas'), ('land', 'Landfill')]</p> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def ensure_field_choice(field_name: str,\n                        field,\n                        choices: list[tuple[str, str] | tuple[str, str, dict]] | None = None) -&gt; None:\n  \"\"\"\n  Ensure a field\u2019s current value is among its valid choices, or reset it to a placeholder.\n\n  Args:\n    field_name (str): Name of the WTForms field (for logging purposes).\n    field (Field): WTForms-compatible field (typically a SelectField).\n    choices (list[tuple[str, str]] | list[tuple[str, str, dict]] | None):\n      Valid choices to enforce. If None, uses the field's existing choices.\n\n  Returns:\n    None\n\n  Notes:\n  - If `choices` is provided, this function sets `field.choices` to the new list.\n  - If `choices` is None, it uses the field's existing `.choices`. In either case,\n    it validates that the current `field.data` is among the available options and\n    resets it to \"Please Select\" if not.\n  - Both `field.data` and `field.raw_data` are reset to keep form behavior consistent.\n  - Each choice tuple should be in the form:\n      - (value, label), or\n      - (value, label, metadata_dict)\n      - Only the first element (`value`) is used for validation.\n  - Use this with SelectField or similar fields where `.choices` must be explicitly defined.\n  - The reset value \"Please Select\" should match a placeholder value if one is used in your app.\n\n  Example:\n      &gt;&gt;&gt; ensure_field_choice(\"sector\", form.sector, [(\"oil\", \"Oil &amp; Gas\"), (\"land\", \"Landfill\")])\n      &gt;&gt;&gt; form.sector.choices\n      [('oil', 'Oil &amp; Gas'), ('land', 'Landfill')]\n  \"\"\"\n\n  if choices is None:\n    # Use existing field choices if none are supplied\n    choices = field.choices\n  else:\n    # Apply a new set of choices to the field\n    field.choices = choices\n\n  valid_values = {c[0] for c in choices}\n\n  if field.data not in valid_values:\n    logger.debug(f\"{field_name}.data={field.data!r} not in valid options, resetting to '{PLEASE_SELECT}'\")\n    field.data = PLEASE_SELECT\n    field.raw_data = [field.data]\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.format_raw_data","title":"<code>format_raw_data(field, value)</code>","text":"<p>Convert a field value to a format suitable for WTForms <code>.raw_data</code>.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>Field</code> <p>A WTForms field instance (e.g., DecimalField, DateTimeField).</p> required <code>value</code> <code>str | int | float | Decimal | datetime | None</code> <p>The field's data value.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of string values to assign to <code>field.raw_data</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value type is unsupported.</p> Example <p>format_raw_data(field, Decimal(\"10.5\")) ['10.5']</p> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def format_raw_data(field: Field, value) -&gt; list[str]:\n  \"\"\"\n  Convert a field value to a format suitable for WTForms `.raw_data`.\n\n  Args:\n    field (Field): A WTForms field instance (e.g., DecimalField, DateTimeField).\n    value (str | int | float | Decimal | datetime.datetime | None): The field's data value.\n\n  Returns:\n    list[str]: List of string values to assign to `field.raw_data`.\n\n  Raises:\n    ValueError: If the value type is unsupported.\n\n  Example:\n    &gt;&gt;&gt; format_raw_data(field, Decimal(\"10.5\"))\n    ['10.5']\n  \"\"\"\n  if value is None:\n    return []\n  elif isinstance(value, (str, int, float)):\n    return [str(value)]\n  elif isinstance(value, Decimal):\n    return [str(float(value))]  # Cast to float before converting to string\n  elif isinstance(value, datetime.datetime):\n    return [value.isoformat()]\n  else:\n    raise ValueError(f\"Unsupported type for raw_data: {type(value)} with value {value}\")\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.get_payloads","title":"<code>get_payloads(model, wtform, ignore_fields=None)</code>","text":"<p>DEPRECATED: Use <code>wtform_to_model()</code> instead.</p> <p>Extract all field values and changed values from a WTForm.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DeclarativeMeta</code> <p>SQLAlchemy model with JSON column <code>misc_json</code>.</p> required <code>wtform</code> <code>FlaskForm</code> <p>The form to extract values from.</p> required <code>ignore_fields</code> <code>list[str] | None</code> <p>List of fields to skip during comparison.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict, dict]</code> <p>tuple[dict, dict]: Tuple of (payload_all, payload_changes) - payload_all: All form fields - payload_changes: Subset of fields with changed values vs. model</p> Notes <ul> <li>Performs a naive comparison (==) without deserializing types.</li> <li>Use skip_empty_fields = True to suppress null-like values.</li> </ul> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def get_payloads(model: DeclarativeMeta,\n                 wtform: FlaskForm,\n                 ignore_fields: list[str] | None = None) -&gt; tuple[dict, dict]:\n  \"\"\"\n  DEPRECATED: Use `wtform_to_model()` instead.\n\n  Extract all field values and changed values from a WTForm.\n\n  Args:\n    model (DeclarativeMeta): SQLAlchemy model with JSON column `misc_json`.\n    wtform (FlaskForm): The form to extract values from.\n    ignore_fields (list[str] | None): List of fields to skip during comparison.\n\n  Returns:\n    tuple[dict, dict]: Tuple of (payload_all, payload_changes)\n      - payload_all: All form fields\n      - payload_changes: Subset of fields with changed values vs. model\n\n  Notes:\n    - Performs a naive comparison (==) without deserializing types.\n    - Use skip_empty_fields = True to suppress null-like values.\n  \"\"\"\n  if ignore_fields is None:\n    ignore_fields = []\n\n  skip_empty_fields = False  # Yes if you wish to skip blank fields from being updated when feasible\n\n  payload_all = {}\n  payload_changes = {}\n\n  model_json_dict = getattr(model, \"misc_json\") or {}\n  logger.debug(f\"{model_json_dict=}\")\n\n  model_field_names = list(model_json_dict.keys())\n  form_field_names = get_wtforms_fields(wtform)\n\n  list_differences(model_field_names,\n                   form_field_names,\n                   iterable_01_name=\"SQLAlchemy Model\",\n                   iterable_02_name=\"WTForm Fields\",\n                   print_warning=False,\n                   )\n\n  for form_field_name in form_field_names:\n    field = getattr(wtform, form_field_name)\n    field_value = field.data\n    model_value = model_json_dict.get(form_field_name)\n\n    if form_field_name in ignore_fields:\n      continue\n\n    if skip_empty_fields is True:\n      # skipping empty strings if the model is \"\" or None\n      if field_value == \"\":\n        if model_value in [None, \"\"]:\n          continue\n\n      # Only persist \"Please Select\" if overwriting a meaningful value.\n      if isinstance(field, SelectField) and field_value == PLEASE_SELECT:\n        if model_value in [None, \"\"]:\n          continue\n\n    payload_all[form_field_name] = field_value\n\n    # todo (depreciated) - object types are not being seen as equivalent (because they are serialized strings)\n    #        need to update logic - check out prep_payload_for_json for uniform approach\n    if model_value != field_value:\n      payload_changes[form_field_name] = field_value\n\n  return payload_all, payload_changes\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.get_wtforms_fields","title":"<code>get_wtforms_fields(form, include_csrf_token=False)</code>","text":"<p>Return the sorted field names associated with a WTForms form.</p> <p>Parameters:</p> Name Type Description Default <code>form</code> <code>FlaskForm</code> <p>The WTForms form instance.</p> required <code>include_csrf_token</code> <code>bool</code> <p>If True, include 'csrf_token' in the result.</p> <code>False</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Alphabetically sorted list of field names in the form.</p> Example <p>get_wtforms_fields(form) ['name', 'sector']</p> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def get_wtforms_fields(form: FlaskForm,\n                       include_csrf_token: bool = False) -&gt; list[str]:\n  \"\"\"\n  Return the sorted field names associated with a WTForms form.\n\n  Args:\n    form (FlaskForm): The WTForms form instance.\n    include_csrf_token (bool): If True, include 'csrf_token' in the result.\n\n  Returns:\n    list[str]: Alphabetically sorted list of field names in the form.\n\n  Example:\n    &gt;&gt;&gt; get_wtforms_fields(form)\n    ['name', 'sector']\n  \"\"\"\n  field_names = [\n    name for name in form.data\n    if include_csrf_token or name != \"csrf_token\"\n  ]\n  field_names.sort()\n  return field_names\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.initialize_drop_downs","title":"<code>initialize_drop_downs(form, default=None)</code>","text":"<p>Set default values for uninitialized WTForms SelectFields.</p> <p>Parameters:</p> Name Type Description Default <code>form</code> <code>FlaskForm</code> <p>The form containing SelectField fields to be initialized.</p> required <code>default</code> <code>str | None</code> <p>The value to assign to a field if its current value is None. If not provided, uses the application's global placeholder (e.g., \"Please Select\").</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>initialize_drop_downs(form, default=\"Please Select\")</p> Notes <ul> <li>Fields that already have a value (even a falsy one like an empty string) are not modified.</li> <li>Only fields of type <code>SelectField</code> are affected.</li> <li>This function is typically used after form construction but before rendering or validation.</li> </ul> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def initialize_drop_downs(form: FlaskForm, default: str = None) -&gt; None:\n  \"\"\"\n  Set default values for uninitialized WTForms SelectFields.\n\n  Args:\n    form (FlaskForm): The form containing SelectField fields to be initialized.\n    default (str | None): The value to assign to a field if its current value is None.\n      If not provided, uses the application's global placeholder (e.g., \"Please Select\").\n\n  Returns:\n    None\n\n  Example:\n    &gt;&gt;&gt; initialize_drop_downs(form, default=\"Please Select\")\n\n  Notes:\n    - Fields that already have a value (even a falsy one like an empty string) are not modified.\n    - Only fields of type `SelectField` are affected.\n    - This function is typically used after form construction but before rendering or validation.\n  \"\"\"\n  if default is None:\n    default = PLEASE_SELECT\n\n  logger.debug(\"Initializing drop-downs...\")\n  for field in form:\n    if isinstance(field, SelectField) and field.data is None:\n      logger.debug(f\"{field.name} set to default value: {default}\")\n      field.data = default\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.min_decimal_precision","title":"<code>min_decimal_precision(min_digits)</code>","text":"<p>Return a validator for WTForms DecimalField enforcing minimum decimal precision.</p> <p>Parameters:</p> Name Type Description Default <code>min_digits</code> <code>int</code> <p>Minimum number of digits required after the decimal.</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>WTForms-compatible validator that raises ValidationError if decimal places are insufficient.</p> Example <p>field = DecimalField(\"Amount\", validators=[min_decimal_precision(2)])</p> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def min_decimal_precision(min_digits: int) -&gt; Callable:\n  \"\"\"\n  Return a validator for WTForms DecimalField enforcing minimum decimal precision.\n\n  Args:\n      min_digits (int): Minimum number of digits required after the decimal.\n\n  Returns:\n      Callable: WTForms-compatible validator that raises ValidationError if decimal places are insufficient.\n\n  Example:\n      &gt;&gt;&gt; field = DecimalField(\"Amount\", validators=[min_decimal_precision(2)])\n  \"\"\"\n\n  def _min_decimal_precision(form, field):\n    if field.data is None:\n      return\n\n    try:\n      value_str = str(field.data)\n      if '.' in value_str:\n        _, decimals = value_str.split('.')\n        if len(decimals) &lt; min_digits:\n          raise ValidationError()\n      elif min_digits &gt; 0:\n        raise ValidationError()\n    except (ValueError, TypeError):\n      raise ValidationError(\n        f\"Field must be a valid numeric value with at least {min_digits} decimal places.\"\n      )\n\n  return _min_decimal_precision\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.model_to_wtform","title":"<code>model_to_wtform(model, wtform, json_column='misc_json')</code>","text":"<p>Populate a WTForm from a SQLAlchemy model's JSON column.</p> <p>This function loads the model's JSON field (typically 'misc_json') and sets WTForms field <code>.data</code> and <code>.raw_data</code> accordingly. Required for correct rendering and validation of pre-filled forms.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DeclarativeMeta</code> <p>SQLAlchemy model instance containing a JSON column.</p> required <code>wtform</code> <code>FlaskForm</code> <p>The WTForm instance to populate.</p> required <code>json_column</code> <code>str</code> <p>The attribute name of the JSON column. Defaults to \"misc_json\".</p> <code>'misc_json'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a datetime value cannot be parsed.</p> <code>TypeError</code> <p>If the field type is unsupported.</p> Notes <ul> <li>Supports preloading DateTimeField and DecimalField types.</li> <li>Converts ISO8601 UTC \u2192 localized Pacific time.</li> <li>Ignores JSON fields that don\u2019t map to WTForm fields.</li> </ul> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def model_to_wtform(model: DeclarativeMeta,\n                    wtform: FlaskForm,\n                    json_column: str = \"misc_json\") -&gt; None:\n  \"\"\"\n  Populate a WTForm from a SQLAlchemy model's JSON column.\n\n  This function loads the model's JSON field (typically 'misc_json') and\n  sets WTForms field `.data` and `.raw_data` accordingly. Required for correct rendering\n  and validation of pre-filled forms.\n\n  Args:\n    model (DeclarativeMeta): SQLAlchemy model instance containing a JSON column.\n    wtform (FlaskForm): The WTForm instance to populate.\n    json_column (str): The attribute name of the JSON column. Defaults to \"misc_json\".\n\n  Raises:\n    ValueError: If a datetime value cannot be parsed.\n    TypeError: If the field type is unsupported.\n\n  Notes:\n    - Supports preloading DateTimeField and DecimalField types.\n    - Converts ISO8601 UTC \u2192 localized Pacific time.\n    - Ignores JSON fields that don\u2019t map to WTForm fields.\n  \"\"\"\n  model_json_dict = getattr(model, json_column)\n  logger.debug(f\"model_to_wtform called with model={model}, json={model_json_dict}\")\n\n  # Ensure dict, not str\n  if isinstance(model_json_dict, str):\n    try:\n      model_json_dict = json.loads(model_json_dict)\n      logger.debug(\"Parsed JSON string into dict.\")\n    except json.JSONDecodeError:\n      logger.warning(f\"Invalid JSON in model's '{json_column}' column.\")\n      model_json_dict = {}\n\n  if model_json_dict is None:\n    model_json_dict = {}\n\n  if \"id_incidence\" in model_json_dict and model_json_dict[\"id_incidence\"] != model.id_incidence:\n    logger.warning(f\"[model_to_wtform] MISMATCH: model.id_incidence={model.id_incidence} \"\n                   f\"!= misc_json['id_incidence']={model_json_dict['id_incidence']}\")\n\n  form_fields = get_wtforms_fields(wtform)\n  model_fields = list(model_json_dict.keys())\n\n  list_differences(\n    model_fields, form_fields,\n    iterable_01_name=\"SQLAlchemy Model JSON\",\n    iterable_02_name=\"WTForm Fields\",\n    print_warning=False\n  )\n\n  # Use utilities to get type map and convert model dict\n  type_map, _ = wtform_types_and_values(wtform)\n  parsed_dict = deserialize_dict(model_json_dict, type_map, convert_time_to_ca=True)\n\n  for field_name in form_fields:\n    field = getattr(wtform, field_name)\n    model_value = parsed_dict.get(field_name)\n\n    # Set field data and raw_data for proper rendering/validation\n    field.data = model_value\n    field.raw_data = format_raw_data(field, model_value)\n\n    logger.debug(f\"Set {field_name=}, data={field.data}, raw_data={field.raw_data}\")\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.prep_payload_for_json","title":"<code>prep_payload_for_json(payload, type_matching_dict=None)</code>","text":"<p>Prepare a payload dictionary for JSON-safe serialization.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>dict</code> <p>Key-value updates extracted from a WTForm or other source.</p> required <code>type_matching_dict</code> <code>dict[str, type] | None</code> <p>Optional type coercion rules. e.g., {\"id_incidence\": int, \"some_flag\": bool}</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Transformed version of the payload, suitable for use in a model's JSON field.</p> Notes <ul> <li>Applies datetime to ISO, Decimal to float</li> <li>Respects \"Please Select\" for placeholders</li> <li>Values in <code>type_matching_dict</code> are explicitly cast to the specified types</li> </ul> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def prep_payload_for_json(payload: dict,\n                          type_matching_dict: dict[str, type] | None = None) -&gt; dict:\n  \"\"\"\n  Prepare a payload dictionary for JSON-safe serialization.\n\n  Args:\n    payload (dict): Key-value updates extracted from a WTForm or other source.\n    type_matching_dict (dict[str, type] | None): Optional type coercion rules.\n      e.g., {\"id_incidence\": int, \"some_flag\": bool}\n\n  Returns:\n    dict: Transformed version of the payload, suitable for use in a model's JSON field.\n\n  Notes:\n    - Applies datetime to ISO, Decimal to float\n    - Respects \"Please Select\" for placeholders\n    - Values in `type_matching_dict` are explicitly cast to the specified types\n  \"\"\"\n\n  type_matching_dict = type_matching_dict or {\"id_incidence\": int}\n\n  return make_dict_serializeable(payload,\n                                 type_map=type_matching_dict,\n                                 convert_time_to_ca=True)\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.remove_validators","title":"<code>remove_validators(form, field_names, validators_to_remove=None)</code>","text":"<p>Remove specified validators from selected WTForms fields.</p> <p>Parameters:</p> Name Type Description Default <code>form</code> <code>FlaskForm</code> <p>The WTForms form instance.</p> required <code>field_names</code> <code>list[str]</code> <p>List of field names to examine and modify.</p> required <code>validators_to_remove</code> <code>list[type] | None</code> <p>Validator classes to remove. Defaults to [InputRequired] if not provided.</p> <code>None</code> Notes <p>This modifies the validators list in-place and is useful when conditional field requirements apply.</p> Example <p>remove_validators(form, [\"name\", \"email\"], [InputRequired])</p> Notes <ul> <li>Useful when validator logic depends on user input or view context.</li> <li>Not currently in use</li> </ul> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def remove_validators(form: FlaskForm,\n                      field_names: list[str],\n                      validators_to_remove: list[type] | None = None) -&gt; None:\n  \"\"\"\n  Remove specified validators from selected WTForms fields.\n\n  Args:\n    form (FlaskForm): The WTForms form instance.\n    field_names (list[str]): List of field names to examine and modify.\n    validators_to_remove (list[type] | None): Validator classes to remove.\n      Defaults to [InputRequired] if not provided.\n\n  Notes:\n    This modifies the validators list in-place and is useful when\n    conditional field requirements apply.\n\n  Example:\n      &gt;&gt;&gt; remove_validators(form, [\"name\", \"email\"], [InputRequired])\n\n  Notes:\n    - Useful when validator logic depends on user input or view context.\n    - Not currently in use\n\n  \"\"\"\n  if validators_to_remove is None:\n    validators_to_remove = [InputRequired]\n\n  fields = get_wtforms_fields(form, include_csrf_token=False)\n  for field in fields:\n    if field in field_names:\n      validators = form[field].validators\n      for validator in validators:\n        for validator_to_remove in validators_to_remove:\n          if isinstance(validator, validator_to_remove):\n            # logger.debug(f\"{type(validators)}, {validators=}, {validator=}\")\n            validators.remove(validator)\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.run_diagnostics","title":"<code>run_diagnostics()</code>","text":"<p>Run a full diagnostics suite for WTForms utility testing.</p> Simulates a complete form lifecycle <ul> <li>Initializes a mock form and model.</li> <li>Transfers model \u2192 form \u2192 model.</li> <li>Adjusts validators dynamically.</li> <li>Simulates invalid user input.</li> <li>Verifies selector enforcement and error tracking.</li> <li>Validates without CSRF enforcement.</li> </ul> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <ul> <li>Logs key actions and field state.</li> <li>Does not require a Flask app or real DB connection.</li> <li>Intended for standalone execution and manual inspection.</li> </ul> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def run_diagnostics() -&gt; None:\n  \"\"\"\n  Run a full diagnostics suite for WTForms utility testing.\n\n  Simulates a complete form lifecycle:\n    - Initializes a mock form and model.\n    - Transfers model \u2192 form \u2192 model.\n    - Adjusts validators dynamically.\n    - Simulates invalid user input.\n    - Verifies selector enforcement and error tracking.\n    - Validates without CSRF enforcement.\n\n  Returns:\n    None\n\n  Notes:\n    - Logs key actions and field state.\n    - Does not require a Flask app or real DB connection.\n    - Intended for standalone execution and manual inspection.\n  \"\"\"\n\n  class DummyModel:\n    \"\"\"Mock SQLAlchemy model with JSON-like attribute.\"\"\"\n\n    def __init__(self):\n      self.misc_json = {\n        \"name\": \"Alice\",\n        \"age\": 30,\n        \"created_at\": \"2024-01-01T08:00:00Z\"\n      }\n\n  class TestForm(FlaskForm):\n    \"\"\"Test WTForm.\"\"\"\n    name = SelectField('Name', choices=[(PLEASE_SELECT, PLEASE_SELECT), (\"Alice\", \"Alice\"), (\"Bob\", \"Bob\")],\n                       validators=[InputRequired()])\n    age = DecimalField('Age', validators=[InputRequired()])\n    created_at = DateTimeField('Created At', format=\"%Y-%m-%dT%H:%M\", validators=[InputRequired()])\n\n  from werkzeug.datastructures import MultiDict\n\n  logger.info(\"Running WTForms diagnostics...\")\n\n  form = TestForm(formdata=MultiDict({\n    \"name\": \"Alice\",\n    \"age\": \"30.00\",\n    \"created_at\": \"2024-01-01T00:00\"\n  }))\n\n  model = DummyModel()\n\n  # Ensure defaults work\n  initialize_drop_downs(form)\n\n  # Transfer model \u2192 form\n  model_to_wtform(model, form)\n  logger.info(f\"Model \u2192 Form: name={form.name.data}, age={form.age.data}, created_at={form.created_at.data}\")\n\n  # Form \u2192 model (round-trip)\n  wtform_to_model(model, form)\n  logger.info(f\"Updated model.misc_json: {model.misc_json}\")\n\n  # Count errors\n  error_summary = wtf_count_errors(form, log_errors=True)\n  logger.info(f\"Error summary: {error_summary}\")\n\n  # Test validator manipulation\n  logger.info(\"Testing change_validators...\")\n  change_validators(form, field_names_to_change=[\"age\"], old_validator=InputRequired, new_validator=Optional)\n  for field_name in [\"name\", \"age\", \"created_at\"]:\n    logger.debug(f\"{field_name} validators: {form[field_name].validators}\")\n\n  # Test selector validation (simulate bad input)\n  form.name.data = PLEASE_SELECT\n  validate_selectors(form)\n  logger.info(f\"Name field errors after selector validation: {form.name.errors}\")\n\n  # Test CSRF-less validation\n  result = validate_no_csrf(form)\n  logger.info(f\"validate_no_csrf result: {result}, errors: {form.errors}\")\n\n  logger.info(\"WTForms diagnostics completed successfully.\")\n\n  if __name__ == '__main__':\n    run_diagnostics()\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.update_model_with_payload","title":"<code>update_model_with_payload(model, payload, json_field='misc_json', comment='')</code>","text":"<p>Apply a JSON-safe payload to a model's JSON column and mark it as changed.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DeclarativeMeta</code> <p>SQLAlchemy model instance to update.</p> required <code>payload</code> <code>dict</code> <p>Dictionary of updates to apply.</p> required <code>json_field</code> <code>str</code> <p>Name of the model's JSON column (default is \"misc_json\").</p> <code>'misc_json'</code> <code>comment</code> <code>str</code> <p>Optional comment to include with update logging.</p> <code>''</code> Notes <ul> <li>Calls <code>prep_payload_for_json</code> to ensure data integrity.</li> <li>Uses <code>apply_json_patch_and_log</code> to track and log changes.</li> <li>Deep-copies the existing JSON field to avoid side effects.</li> </ul> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def update_model_with_payload(model: DeclarativeMeta,\n                              payload: dict,\n                              json_field: str = \"misc_json\",\n                              comment: str = \"\") -&gt; None:\n  \"\"\"\n  Apply a JSON-safe payload to a model's JSON column and mark it as changed.\n\n  Args:\n    model (DeclarativeMeta): SQLAlchemy model instance to update.\n    payload (dict): Dictionary of updates to apply.\n    json_field (str): Name of the model's JSON column (default is \"misc_json\").\n    comment (str): Optional comment to include with update logging.\n\n  Notes:\n    - Calls `prep_payload_for_json` to ensure data integrity.\n    - Uses `apply_json_patch_and_log` to track and log changes.\n    - Deep-copies the existing JSON field to avoid side effects.\n  \"\"\"\n  logger.debug(f\"update_model_with_payload: {model=}, {payload=}\")\n\n  model_json = copy.deepcopy(getattr(model, json_field) or {})\n  new_payload = prep_payload_for_json(payload)\n  model_json.update(new_payload)\n\n  apply_json_patch_and_log(\n    model,\n    json_field=json_field,\n    updates=model_json,\n    user=\"anonymous\",\n    comments=comment,\n  )\n\n  logger.debug(f\"Model JSON updated: {getattr(model, json_field)=}\")\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.validate_no_csrf","title":"<code>validate_no_csrf(form, extra_validators=None)</code>","text":"<p>Validate a WTForm while skipping CSRF errors (useful for GET-submitted forms).</p> <p>Parameters:</p> Name Type Description Default <code>form</code> <code>FlaskForm</code> <p>The form to validate.</p> required <code>extra_validators</code> <code>dict | None</code> <p>Optional per-field validators to apply.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the form is valid after removing CSRF errors, otherwise False.</p> Example <p>if validate_no_csrf(form): handle_form()</p> Notes <ul> <li>This allows validation to succeed even when CSRF tokens are missing or invalid.</li> <li>It logs before and after validation for debug purposes.</li> </ul> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def validate_no_csrf(form: FlaskForm, extra_validators: dict | None = None) -&gt; bool:\n  \"\"\"\n  Validate a WTForm while skipping CSRF errors (useful for GET-submitted forms).\n\n  Args:\n    form (FlaskForm): The form to validate.\n    extra_validators (dict | None): Optional per-field validators to apply.\n\n  Returns:\n    bool: True if the form is valid after removing CSRF errors, otherwise False.\n\n  Example:\n    &gt;&gt;&gt; if validate_no_csrf(form): handle_form()\n\n  Notes:\n    - This allows validation to succeed even when CSRF tokens are missing or invalid.\n    - It logs before and after validation for debug purposes.\n  \"\"\"\n  logger.debug(f\"validate_no_csrf() called:\")\n  form.validate(extra_validators=extra_validators)\n\n  if form.errors and 'csrf_token' in form.errors:\n    del form.errors['csrf_token']\n\n  csrf_field = getattr(form, 'csrf_token', None)\n  if csrf_field:\n    if csrf_field.errors:\n      if 'The CSRF token is missing.' in csrf_field.errors:\n        csrf_field.errors.remove('The CSRF token is missing.')\n\n  form_valid = not bool(form.errors)\n\n  logger.debug(f\"after validate_no_csrf() called: {form_valid=}, {form.errors=}\")\n  return form_valid\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.validate_selectors","title":"<code>validate_selectors(form, default=None)</code>","text":"<p>Append validation errors for SelectFields left at default placeholder values.</p> <p>Parameters:</p> Name Type Description Default <code>form</code> <code>FlaskForm</code> <p>WTForm instance containing SelectFields.</p> required <code>default</code> <code>str | None</code> <p>Placeholder value to treat as invalid (default: \"Please Select\").</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <ul> <li>Typically used for GET-submitted forms where default values are not caught automatically.</li> <li>Adds \"This field is required.\" error to fields that are InputRequired but still at default.</li> </ul> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def validate_selectors(form: FlaskForm, default: str = None) -&gt; None:\n  \"\"\"\n  Append validation errors for SelectFields left at default placeholder values.\n\n  Args:\n    form (FlaskForm): WTForm instance containing SelectFields.\n    default (str | None): Placeholder value to treat as invalid (default: \"Please Select\").\n\n  Returns:\n    None\n\n  Notes:\n    - Typically used for GET-submitted forms where default values are not caught automatically.\n    - Adds \"This field is required.\" error to fields that are InputRequired but still at default.\n  \"\"\"\n  if default is None:\n    default = PLEASE_SELECT\n\n  for field in form:\n    if isinstance(field, SelectField):\n      if field.data is None or field.data == default:\n        for validator in field.validators:\n          if isinstance(validator, InputRequired):\n            msg = \"This field is required.\"\n            if msg not in field.errors:\n              field.errors.append(msg)\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.wtf_count_errors","title":"<code>wtf_count_errors(form, log_errors=False)</code>","text":"<p>Count validation errors on a WTForm instance.</p> <p>Parameters:</p> Name Type Description Default <code>form</code> <code>FlaskForm</code> <p>The form to inspect.</p> required <code>log_errors</code> <code>bool</code> <p>If True, log the form's errors using debug log level.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, int]</code> <p>dict[str, int]: Dictionary with error counts: - 'elements_with_errors': number of fields that had one or more errors - 'element_error_count': total number of field-level errors - 'wtf_form_error_count': number of form-level (non-field) errors - 'total_error_count': sum of all error types</p> Notes <p>Ensure <code>form.validate_on_submit()</code> or <code>form.validate()</code> has been called first, or the error counts will be inaccurate.</p> Example <p>error_summary = wtf_count_errors(form) print(error_summary[\"total_error_count\"])</p> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def wtf_count_errors(form: FlaskForm, log_errors: bool = False) -&gt; dict[str, int]:\n  \"\"\"\n  Count validation errors on a WTForm instance.\n\n  Args:\n    form (FlaskForm): The form to inspect.\n    log_errors (bool): If True, log the form's errors using debug log level.\n\n  Returns:\n    dict[str, int]: Dictionary with error counts:\n      - 'elements_with_errors': number of fields that had one or more errors\n      - 'element_error_count': total number of field-level errors\n      - 'wtf_form_error_count': number of form-level (non-field) errors\n      - 'total_error_count': sum of all error types\n\n  Notes:\n    Ensure `form.validate_on_submit()` or `form.validate()` has been called first,\n    or the error counts will be inaccurate.\n\n  Example:\n    &gt;&gt;&gt; error_summary = wtf_count_errors(form)\n    &gt;&gt;&gt; print(error_summary[\"total_error_count\"])\n\n  \"\"\"\n  error_count_dict = {\n    'elements_with_errors': 0,\n    'element_error_count': 0,\n    'wtf_form_error_count': 0,\n    'total_error_count': 0,\n  }\n\n  if log_errors:\n    logger.debug(f\"Form errors are: {form.errors}\")\n\n  for field, error_list in form.errors.items():\n    if field is None:\n      error_count_dict['wtf_form_error_count'] += len(error_list)\n    else:\n      error_count_dict['elements_with_errors'] += 1\n      error_count_dict['element_error_count'] += len(error_list)\n\n  error_count_dict['total_error_count'] = (\n      error_count_dict['element_error_count'] +\n      error_count_dict['wtf_form_error_count']\n  )\n\n  return error_count_dict\n</code></pre>"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.wtform_to_model","title":"<code>wtform_to_model(model, wtform, json_column='misc_json', user='anonymous', comments='', ignore_fields=None, type_matching_dict=None)</code>","text":"<p>Extract data from a WTForm and update the model's JSON column. Logs all changes.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>DeclarativeMeta</code> <p>SQLAlchemy model instance.</p> required <code>wtform</code> <code>FlaskForm</code> <p>WTForm with typed Python values.</p> required <code>json_column</code> <code>str</code> <p>JSON column name on the model.</p> <code>'misc_json'</code> <code>user</code> <code>str</code> <p>Username for logging purposes.</p> <code>'anonymous'</code> <code>comments</code> <code>str</code> <p>Optional comment for logging context.</p> <code>''</code> <code>ignore_fields</code> <code>list[str] | None</code> <p>Fields to exclude from update.</p> <code>None</code> <code>type_matching_dict</code> <code>dict[str, type] | None</code> <p>Optional override for type enforcement.</p> <code>None</code> Notes <ul> <li>Uses make_dict_serializeable and get_changed_fields to compare values.</li> <li>Delegates to apply_json_patch_and_log to persist and log changes.</li> </ul> Source code in <code>arb\\utils\\wtf_forms_util.py</code> <pre><code>def wtform_to_model(model: DeclarativeMeta,\n                    wtform: FlaskForm,\n                    json_column: str = \"misc_json\",\n                    user: str = \"anonymous\",\n                    comments: str = \"\",\n                    ignore_fields: list[str] | None = None,\n                    type_matching_dict: dict[str, type] | None = None) -&gt; None:\n  \"\"\"\n  Extract data from a WTForm and update the model's JSON column. Logs all changes.\n\n  Args:\n    model (DeclarativeMeta): SQLAlchemy model instance.\n    wtform (FlaskForm): WTForm with typed Python values.\n    json_column (str): JSON column name on the model.\n    user (str): Username for logging purposes.\n    comments (str): Optional comment for logging context.\n    ignore_fields (list[str] | None): Fields to exclude from update.\n    type_matching_dict (dict[str, type] | None): Optional override for type enforcement.\n\n  Notes:\n    - Uses make_dict_serializeable and get_changed_fields to compare values.\n    - Delegates to apply_json_patch_and_log to persist and log changes.\n  \"\"\"\n  ignore_fields = set(ignore_fields or [])\n\n  payload_all = {\n    field_name: getattr(wtform, field_name).data\n    for field_name in get_wtforms_fields(wtform)\n    if field_name not in ignore_fields\n  }\n\n  # Use manual overrides only \u2014 no type_map from form\n  payload_all = make_dict_serializeable(payload_all, type_map=type_matching_dict, convert_time_to_ca=True)\n\n  existing_json = load_model_json_column(model, json_column)\n  existing_serialized = make_dict_serializeable(existing_json, type_map=type_matching_dict, convert_time_to_ca=True)\n\n  payload_changes = get_changed_fields(payload_all, existing_serialized)\n  if payload_changes:\n    logger.info(f\"wtform_to_model payload_changes: {payload_changes}\")\n    apply_json_patch_and_log(model, payload_changes, json_column, user=user, comments=comments)\n\n  logger.info(f\"wtform_to_model payload_all: {payload_all}\")\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/","title":"<code>arb.utils.excel.xl_create</code>","text":"<p>Module to prepare Excel templates and generate new Excel files using Jinja-rendered payloads.</p> <p>This module performs schema-based templating of Excel spreadsheets for feedback forms, injects metadata, applies default values, and renders Excel files based on structured JSON payloads.</p> Typical Workflow <ol> <li>Use a spreadsheet with named ranges and Jinja placeholders.</li> <li>Generate an initial JSON schema using a VBA macro.</li> <li>Refine the schema by injecting typing information.</li> <li>Generate default and test payloads.</li> <li>Populate Excel files using these payloads.</li> </ol> <p>Run this file directly to create all schema and payload artifacts for landfill, oil and gas, and energy templates.</p>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.create_default_types_schema","title":"<code>create_default_types_schema(diagnostics=False)</code>","text":"<p>Create a JSON file that maps variable names to their default Python value types.</p> <p>Parameters:</p> Name Type Description Default <code>diagnostics</code> <code>bool</code> <p>If True, logs each variable name and its type to the debug logger.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping variable names to Python types (e.g., str, int, datetime).</p> Example <p>types = create_default_types_schema(diagnostics=True)</p> Notes <ul> <li>Output is saved to 'xl_schemas/default_value_types_v01_00.json'.</li> <li>A backup is compared against the newly generated file if present.</li> <li>Field names and types are sourced from <code>xl_hardcoded.default_value_types_v01_00</code>.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def create_default_types_schema(diagnostics: bool = False) -&gt; dict:\n  \"\"\"\n  Create a JSON file that maps variable names to their default Python value types.\n\n  Args:\n    diagnostics (bool): If True, logs each variable name and its type to the debug logger.\n\n  Returns:\n    dict: Dictionary mapping variable names to Python types (e.g., str, int, datetime).\n\n  Example:\n    &gt;&gt;&gt; types = create_default_types_schema(diagnostics=True)\n\n  Notes:\n    - Output is saved to 'xl_schemas/default_value_types_v01_00.json'.\n    - A backup is compared against the newly generated file if present.\n    - Field names and types are sourced from `xl_hardcoded.default_value_types_v01_00`.\n  \"\"\"\n  from arb.utils.excel.xl_hardcoded import default_value_types_v01_00\n\n  logger.debug(\"create_default_types_schema() called\")\n\n  file_name = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00.json\"\n  file_backup = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00_backup.json\"\n\n  field_types = dict(sorted(default_value_types_v01_00.items()))\n\n  if diagnostics:\n    for name, typ in field_types.items():\n      logger.debug(f\"'{name}': {typ.__name__},\")\n\n  metadata = {\"schema_version\": \"default_value_types_v01_00\"}\n  json_save_with_meta(file_name, field_types, metadata=metadata)\n\n  if file_name.is_file() and file_backup.is_file():\n    compare_json_files(file_name, file_backup)\n\n  return field_types\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.create_payload","title":"<code>create_payload(payload, file_name, schema_version, metadata=None)</code>","text":"<p>Create a JSON payload file with embedded metadata describing the schema version.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>dict</code> <p>Dictionary of values to serialize to JSON.</p> required <code>file_name</code> <code>Path</code> <p>Path to output the payload JSON file.</p> required <code>schema_version</code> <code>str</code> <p>Identifier for the schema the payload conforms to.</p> required <code>metadata</code> <code>dict</code> <p>Additional metadata to embed. If None, a new dict is created.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>create_payload({\"id_case\": \"A42\"}, Path(\"payload.json\"), \"v01_00\")</p> Notes <ul> <li>Adds 'schema_version' and a default payload description to metadata.</li> <li>Uses <code>json_save_with_meta()</code> to embed metadata into the JSON file.</li> <li>Logs all key actions and file paths for diagnostics.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def create_payload(payload: dict, file_name: Path, schema_version: str, metadata: dict = None) -&gt; None:\n  \"\"\"\n  Create a JSON payload file with embedded metadata describing the schema version.\n\n  Args:\n    payload (dict): Dictionary of values to serialize to JSON.\n    file_name (Path): Path to output the payload JSON file.\n    schema_version (str): Identifier for the schema the payload conforms to.\n    metadata (dict, optional): Additional metadata to embed. If None, a new dict is created.\n\n  Returns:\n    None\n\n  Example:\n    &gt;&gt;&gt; create_payload({\"id_case\": \"A42\"}, Path(\"payload.json\"), \"v01_00\")\n\n  Notes:\n    - Adds 'schema_version' and a default payload description to metadata.\n    - Uses `json_save_with_meta()` to embed metadata into the JSON file.\n    - Logs all key actions and file paths for diagnostics.\n  \"\"\"\n\n  logger.debug(\"create_payload() called\")\n\n  if metadata is None:\n    metadata = {}\n\n  metadata[\"schema_version\"] = schema_version\n  metadata[\"payload description\"] = \"Test of Excel jinja templating system\"\n\n  logger.debug(f\"Writing payload to {file_name} with metadata: {metadata}\")\n  json_save_with_meta(file_name, data=payload, metadata=metadata)\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.create_payloads","title":"<code>create_payloads()</code>","text":"<p>Generate and save example payload files for each supported sector (landfill, oil &amp; gas, energy).</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>create_payloads()</p> Notes <ul> <li>Each payload is saved to <code>xl_payloads/{schema_version}_payload_01.json</code>.</li> <li>If a backup file exists, the new payload is compared against it for consistency.</li> <li>The energy payload reuses the oil &amp; gas example data for demonstration purposes.</li> <li>Uses <code>create_payload()</code> to handle serialization and metadata embedding.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def create_payloads() -&gt; None:\n  \"\"\"\n  Generate and save example payload files for each supported sector (landfill, oil &amp; gas, energy).\n\n  Returns:\n    None\n\n  Example:\n    &gt;&gt;&gt; create_payloads()\n\n  Notes:\n    - Each payload is saved to `xl_payloads/{schema_version}_payload_01.json`.\n    - If a backup file exists, the new payload is compared against it for consistency.\n    - The energy payload reuses the oil &amp; gas example data for demonstration purposes.\n    - Uses `create_payload()` to handle serialization and metadata embedding.\n  \"\"\"\n\n  logger.debug(\"create_payloads() called\")\n\n  from arb.utils.excel.xl_hardcoded import landfill_payload_01, oil_and_gas_payload_01\n\n  test_sets = [\n    (\"landfill_v01_00\", landfill_payload_01),\n    (\"oil_and_gas_v01_00\", oil_and_gas_payload_01),\n    (\"energy_v00_01\", oil_and_gas_payload_01),  # Reuse oil &amp; gas payload\n  ]\n\n  for schema_version, payload in test_sets:\n    file_name = PROCESSED_VERSIONS / f\"xl_payloads/{schema_version}_payload_01.json\"\n    file_backup = PROCESSED_VERSIONS / f\"xl_payloads/{schema_version}_payload_01_backup.json\"\n\n    create_payload(payload, file_name, schema_version)\n\n    if file_name.is_file() and file_backup.is_file():\n      compare_json_files(file_name, file_backup)\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.create_schemas_and_payloads","title":"<code>create_schemas_and_payloads()</code>","text":"<p>Generate all schema, payload, and Excel artifacts for the feedback system.</p> This orchestration function performs the full pipeline <ul> <li>Creates the default value types schema.</li> <li>Processes all sector-specific schema files (landfill, oil &amp; gas, energy).</li> <li>Writes default payloads for each schema.</li> <li>Generates test payloads and renders Excel templates.</li> </ul> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>create_schemas_and_payloads()</p> Notes <ul> <li>Creates all required directories under <code>processed_versions</code>.</li> <li>Intended for one-time use during development or deployment setup.</li> <li>Logs each operation and file path for debugging.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def create_schemas_and_payloads() -&gt; None:\n  \"\"\"\n  Generate all schema, payload, and Excel artifacts for the feedback system.\n\n  This orchestration function performs the full pipeline:\n    - Creates the default value types schema.\n    - Processes all sector-specific schema files (landfill, oil &amp; gas, energy).\n    - Writes default payloads for each schema.\n    - Generates test payloads and renders Excel templates.\n\n  Returns:\n    None\n\n  Example:\n    &gt;&gt;&gt; create_schemas_and_payloads()\n\n  Notes:\n    - Creates all required directories under `processed_versions`.\n    - Intended for one-time use during development or deployment setup.\n    - Logs each operation and file path for debugging.\n  \"\"\"\n\n  logger.debug(\"create_schemas_and_payloads() called\")\n\n  ensure_dir_exists(PROCESSED_VERSIONS / \"xl_schemas\")\n  ensure_dir_exists(PROCESSED_VERSIONS / \"xl_workbooks\")\n  ensure_dir_exists(PROCESSED_VERSIONS / \"xl_payloads\")\n\n  create_default_types_schema(diagnostics=True)\n  prep_xl_templates()\n  create_payloads()\n  test_update_xlsx_payloads_01()\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.prep_xl_templates","title":"<code>prep_xl_templates()</code>","text":"<p>Prepare processed Excel templates and payloads for landfill, oil &amp; gas, and energy sectors.</p> This function <ul> <li>Copies original schema and Excel files to the processed directory.</li> <li>Converts VBA-generated schema files by injecting type info.</li> <li>Writes upgraded schema and default payload JSON files.</li> <li>Produces Jinja-compatible Excel workbook versions for templating.</li> </ul> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>prep_xl_templates()</p> Notes <ul> <li>File paths are derived from structured configs for each sector.</li> <li>Overwrites files in the output directory if they already exist.</li> <li>Output directories are created if they don't exist.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def prep_xl_templates() -&gt; None:\n  \"\"\"\n  Prepare processed Excel templates and payloads for landfill, oil &amp; gas, and energy sectors.\n\n  This function:\n    - Copies original schema and Excel files to the processed directory.\n    - Converts VBA-generated schema files by injecting type info.\n    - Writes upgraded schema and default payload JSON files.\n    - Produces Jinja-compatible Excel workbook versions for templating.\n\n  Returns:\n    None\n\n  Example:\n    &gt;&gt;&gt; prep_xl_templates()\n\n  Notes:\n    - File paths are derived from structured configs for each sector.\n    - Overwrites files in the output directory if they already exist.\n    - Output directories are created if they don't exist.\n  \"\"\"\n  logger.debug(\"prep_xl_templates() called for landfill, oil &amp; gas, and energy schemas\")\n\n  file_specs = []\n  input_dir = PROJECT_ROOT / \"feedback_forms/current_versions\"\n  output_dir = PROJECT_ROOT / \"feedback_forms/processed_versions\"\n\n  ensure_dir_exists(output_dir / \"xl_schemas\")\n  ensure_dir_exists(output_dir / \"xl_workbooks\")\n  ensure_dir_exists(output_dir / \"xl_payloads\")\n\n  template_configs = [\n    (\"landfill_v01_00\", \"landfill_operator_feedback\", LANDFILL_VERSION),\n    (\"oil_and_gas_v01_00\", \"oil_and_gas_operator_feedback\", OIL_AND_GAS_VERSION),\n    (\"energy_v00_01\", \"energy_operator_feedback\", ENERGY_VERSION),\n  ]\n\n  for schema_version, prefix, version in template_configs:\n    spec = {\n      \"schema_version\": schema_version,\n      \"input_schema_vba_path\": input_dir / f\"{schema_version}_vba.json\",\n      \"input_xl_path\": input_dir / f\"{prefix}_{version}.xlsx\",\n      \"input_xl_jinja_path\": input_dir / f\"{prefix}_{version}_jinja_.xlsx\",\n      \"output_schema_vba_path\": output_dir / \"xl_schemas\" / f\"{schema_version}_vba.json\",\n      \"output_schema_path\": output_dir / \"xl_schemas\" / f\"{schema_version}.json\",\n      \"output_xl_path\": output_dir / \"xl_workbooks\" / f\"{prefix}_{version}.xlsx\",\n      \"output_xl_jinja_path\": output_dir / \"xl_workbooks\" / f\"{prefix}_{version}_jinja_.xlsx\",\n      \"output_payload_path\": output_dir / \"xl_payloads\" / f\"{schema_version}_defaults.json\",\n    }\n    file_specs.append(spec)\n\n  for spec in file_specs:\n    logger.debug(f\"Processing schema_version {spec['schema_version']}\")\n\n    file_map = [(spec[\"input_schema_vba_path\"], spec[\"output_schema_vba_path\"]),\n                (spec[\"input_xl_path\"], spec[\"output_xl_path\"]),\n                (spec[\"input_xl_jinja_path\"], spec[\"output_xl_jinja_path\"]), ]\n\n    for file_old, file_new in file_map:\n      logger.debug(f\"Copying file from: {file_old} to: {file_new}\")\n      shutil.copy(file_old, file_new)\n\n    update_vba_schema(spec[\"schema_version\"],\n                      file_name_in=spec[\"output_schema_vba_path\"],\n                      file_name_out=spec[\"output_schema_path\"])\n\n    schema_to_default_json(file_name_in=spec[\"output_schema_path\"],\n                           file_name_out=spec[\"output_payload_path\"])\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.run_diagnostics","title":"<code>run_diagnostics()</code>","text":"<p>Execute a full suite of diagnostic routines to verify Excel templating functionality.</p> This includes <ul> <li>Creating default value types schema.</li> <li>Generating upgraded schema files and default payloads.</li> <li>Running test payload injection for Jinja-enabled Excel files.</li> </ul> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>run_diagnostics()</p> Notes <ul> <li>Logs each step of the process to the application logger.</li> <li>Catches and logs any exceptions that occur during testing.</li> <li>Intended for developers to verify schema and workbook generation end-to-end.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def run_diagnostics() -&gt; None:\n  \"\"\"\n  Execute a full suite of diagnostic routines to verify Excel templating functionality.\n\n  This includes:\n    - Creating default value types schema.\n    - Generating upgraded schema files and default payloads.\n    - Running test payload injection for Jinja-enabled Excel files.\n\n  Returns:\n    None\n\n  Example:\n    &gt;&gt;&gt; run_diagnostics()\n\n  Notes:\n    - Logs each step of the process to the application logger.\n    - Catches and logs any exceptions that occur during testing.\n    - Intended for developers to verify schema and workbook generation end-to-end.\n  \"\"\"\n  logger.info(\"Running diagnostics...\")\n\n  try:\n    logger.info(\"Step 1: Creating default type schema\")\n    create_default_types_schema(diagnostics=True)\n\n    logger.info(\"Step 2: Creating and verifying schema files and payloads\")\n    prep_xl_templates()\n    create_payloads()\n\n    logger.info(\"Step 3: Performing test Excel generation\")\n    test_update_xlsx_payloads_01()\n\n    logger.info(\"Diagnostics complete. Check output directory and logs for details.\")\n\n  except Exception as e:\n    logger.exception(f\"Diagnostics failed: {e}\")\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.schema_to_default_dict","title":"<code>schema_to_default_dict(schema_file_name)</code>","text":"<p>Generate default values and metadata from an Excel schema JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>schema_file_name</code> <code>Path</code> <p>Path to the schema JSON file.</p> required <p>Returns:</p> Type Description <code>tuple[dict, dict]</code> <p>tuple[dict, dict]: - defaults: Dictionary mapping variable names to default values.     * Drop-downs get \"Please Select\".     * All other fields get an empty string. - metadata: Metadata dictionary from the schema file.</p> Example <p>defaults, meta = schema_to_default_dict(Path(\"xl_schemas/landfill_v01_00.json\"))</p> Notes <ul> <li>The schema must include an \"is_drop_down\" flag for correct default generation.</li> <li>Useful for pre-populating forms with valid placeholder values.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def schema_to_default_dict(schema_file_name: Path) -&gt; tuple[dict, dict]:\n  \"\"\"\n  Generate default values and metadata from an Excel schema JSON file.\n\n  Args:\n    schema_file_name (Path): Path to the schema JSON file.\n\n  Returns:\n    tuple[dict, dict]:\n      - defaults: Dictionary mapping variable names to default values.\n          * Drop-downs get \"Please Select\".\n          * All other fields get an empty string.\n      - metadata: Metadata dictionary from the schema file.\n\n  Example:\n    &gt;&gt;&gt; defaults, meta = schema_to_default_dict(Path(\"xl_schemas/landfill_v01_00.json\"))\n\n  Notes:\n    - The schema must include an \"is_drop_down\" flag for correct default generation.\n    - Useful for pre-populating forms with valid placeholder values.\n  \"\"\"\n  logger.debug(f\"schema_to_default_dict() called for {schema_file_name=}\")\n\n  data, metadata = json_load_with_meta(schema_file_name)\n  logger.debug(f\"{metadata=}\")\n\n  defaults = {\n    variable: PLEASE_SELECT if sub_schema.get(\"is_drop_down\") else \"\"\n    for variable, sub_schema in data.items()\n  }\n\n  return defaults, metadata\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.schema_to_default_json","title":"<code>schema_to_default_json(file_name_in, file_name_out=None)</code>","text":"<p>Save default values extracted from a schema into a JSON file with metadata.</p> <p>Parameters:</p> Name Type Description Default <code>file_name_in</code> <code>Path</code> <p>Input path to the schema JSON file.</p> required <code>file_name_out</code> <code>Path</code> <p>Output path for the defaults JSON. If None, defaults to 'xl_payloads/{schema_version}_defaults.json'.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict, dict]</code> <p>tuple[dict, dict]: - defaults: Dictionary of default values derived from the schema. - metadata: Metadata dictionary included in the output JSON.</p> Example <p>schema_to_default_json(Path(\"xl_schemas/landfill_v01_00.json\"))</p> Notes <ul> <li>Drop-down fields default to \"Please Select\".</li> <li>Other fields default to an empty string.</li> <li>Adds a note to metadata explaining default value behavior.</li> <li>Ensures output directory exists before writing.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def schema_to_default_json(file_name_in: Path, file_name_out: Path = None) -&gt; tuple[dict, dict]:\n  \"\"\"\n  Save default values extracted from a schema into a JSON file with metadata.\n\n  Args:\n    file_name_in (Path): Input path to the schema JSON file.\n    file_name_out (Path, optional): Output path for the defaults JSON.\n      If None, defaults to 'xl_payloads/{schema_version}_defaults.json'.\n\n  Returns:\n    tuple[dict, dict]:\n      - defaults: Dictionary of default values derived from the schema.\n      - metadata: Metadata dictionary included in the output JSON.\n\n  Example:\n    &gt;&gt;&gt; schema_to_default_json(Path(\"xl_schemas/landfill_v01_00.json\"))\n\n  Notes:\n    - Drop-down fields default to \"Please Select\".\n    - Other fields default to an empty string.\n    - Adds a note to metadata explaining default value behavior.\n    - Ensures output directory exists before writing.\n  \"\"\"\n  logger.debug(f\"schema_to_default_json() called for {file_name_in=}\")\n\n  defaults, metadata = schema_to_default_dict(file_name_in)\n  metadata['notes'] = (\n    \"Default values are empty strings unless the field is a drop-down cell. \"\n    \"For drop-down cells, the default is 'Please Select'.\"\n  )\n\n  if file_name_out is None:\n    file_name_out = f\"xl_payloads/{metadata['schema_version']}_defaults.json\"\n\n  ensure_parent_dirs(file_name_out)\n\n  json_save_with_meta(file_name_out, data=defaults, metadata=metadata, json_options=None)\n  return defaults, metadata\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.schema_to_json_file","title":"<code>schema_to_json_file(data, schema_version, file_name=None)</code>","text":"<p>Save an Excel schema to a JSON file with metadata and validate the round-trip.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>The Excel schema to be serialized and written to disk.</p> required <code>schema_version</code> <code>str</code> <p>Schema version identifier to include in metadata.</p> required <code>file_name</code> <code>str</code> <p>Output file path. Defaults to \"xl_schemas/{schema_version}.json\".</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>schema_to_json_file(my_schema, schema_version=\"v01_00\")</p> Notes <ul> <li>If <code>file_name</code> is not provided, the output will be saved to   \"xl_schemas/{schema_version}.json\".</li> <li>Metadata will include the schema version.</li> <li>Performs a round-trip serialization test to verify that the saved data   and metadata match the originals.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def schema_to_json_file(data: dict, schema_version: str, file_name: str = None) -&gt; None:\n  \"\"\"\n  Save an Excel schema to a JSON file with metadata and validate the round-trip.\n\n  Args:\n    data (dict): The Excel schema to be serialized and written to disk.\n    schema_version (str): Schema version identifier to include in metadata.\n    file_name (str, optional): Output file path. Defaults to \"xl_schemas/{schema_version}.json\".\n\n  Returns:\n    None\n\n  Example:\n    &gt;&gt;&gt; schema_to_json_file(my_schema, schema_version=\"v01_00\")\n\n  Notes:\n    - If `file_name` is not provided, the output will be saved to\n      \"xl_schemas/{schema_version}.json\".\n    - Metadata will include the schema version.\n    - Performs a round-trip serialization test to verify that the saved data\n      and metadata match the originals.\n  \"\"\"\n\n  logger.debug(f\"schema_to_json_file() called with {schema_version=}, {file_name=}\")\n\n  if file_name is None:\n    file_name = f\"xl_schemas/{schema_version}.json\"\n\n  ensure_parent_dirs(file_name)\n  metadata = {'schema_version': schema_version}\n\n  logger.debug(f\"Saving schema to: {file_name} with metadata: {metadata}\")\n  json_save_with_meta(file_name, data=data, metadata=metadata, json_options=None)\n\n  # Verify round-trip serialization\n  read_data, read_metadata = json_load_with_meta(file_name)\n  if read_data == data and read_metadata == metadata:\n    logger.debug(\"SUCCESS: JSON serialization round-trip matches original.\")\n  else:\n    logger.warning(\"FAILURE: Mismatch in JSON serialization round-trip.\")\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.sort_xl_schema","title":"<code>sort_xl_schema(xl_schema, sort_by='variable_name')</code>","text":"<p>Sort an Excel schema and its sub-schema dictionaries for easier comparison.</p> <p>This function modifies sub-schemas in place and returns a new dictionary with the top-level keys sorted according to the selected strategy.</p> <p>Sub-schemas are reordered so that keys appear in the order: \"label\", \"label_address\", \"value_address\", \"value_type\", then others.</p> <p>Parameters:</p> Name Type Description Default <code>xl_schema</code> <code>dict</code> <p>Dictionary where keys are variable names and values are sub-schema dicts.</p> required <code>sort_by</code> <code>str</code> <p>Sorting strategy: - \"variable_name\": Sort top-level keys alphabetically (default). - \"label_address\": Sort based on Excel row order of each sub-schema's 'label_address'.</p> <code>'variable_name'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A new dictionary with reordered sub-schemas and sorted top-level keys.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unrecognized sorting strategy is provided.</p> Example <p>sorted_schema = sort_xl_schema(schema, sort_by=\"label_address\")</p> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def sort_xl_schema(xl_schema: dict,\n                   sort_by: str = \"variable_name\") -&gt; dict:\n  \"\"\"\n  Sort an Excel schema and its sub-schema dictionaries for easier comparison.\n\n  This function modifies sub-schemas in place and returns a new dictionary with the\n  top-level keys sorted according to the selected strategy.\n\n  Sub-schemas are reordered so that keys appear in the order:\n  \"label\", \"label_address\", \"value_address\", \"value_type\", then others.\n\n  Args:\n    xl_schema (dict): Dictionary where keys are variable names and values are sub-schema dicts.\n    sort_by (str): Sorting strategy:\n      - \"variable_name\": Sort top-level keys alphabetically (default).\n      - \"label_address\": Sort based on Excel row order of each sub-schema's 'label_address'.\n\n  Returns:\n    dict: A new dictionary with reordered sub-schemas and sorted top-level keys.\n\n  Raises:\n    ValueError: If an unrecognized sorting strategy is provided.\n\n  Example:\n    &gt;&gt;&gt; sorted_schema = sort_xl_schema(schema, sort_by=\"label_address\")\n  \"\"\"\n  logger.debug(\"sort_xl_schema() called\")\n\n  # Reorder each sub-schema dict\n  for variable_name, sub_schema in xl_schema.items():\n    reordered = {}\n    for key in (\"label\", \"label_address\", \"value_address\", \"value_type\"):\n      if key in sub_schema:\n        reordered[key] = sub_schema.pop(key)\n    reordered.update(sub_schema)\n    xl_schema[variable_name] = reordered\n\n  # Sort top-level dictionary\n  if sort_by == \"variable_name\":\n    logger.debug(\"Sorting schema by variable_name\")\n    sorted_items = dict(sorted(xl_schema.items(), key=lambda item: item[0]))\n  elif sort_by == \"label_address\":\n    logger.debug(\"Sorting schema by label_address\")\n    get_xl_row = partial(\n      xl_address_sort, address_location=\"value\", sort_by=\"row\", sub_keys=\"label_address\"\n    )\n    sorted_items = dict(sorted(xl_schema.items(), key=get_xl_row))\n  else:\n    raise ValueError(\"sort_by must be 'variable_name' or 'label_address'\")\n\n  return sorted_items\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.test_update_xlsx_payloads_01","title":"<code>test_update_xlsx_payloads_01()</code>","text":"<p>Run test cases that populate Jinja-templated Excel files with known payloads.</p> <p>This test routine helps validate that Excel generation is functioning correctly for all supported sectors (landfill, oil &amp; gas, energy).</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>test_update_xlsx_payloads_01()</p> Notes <ul> <li>Writes populated Excel files to the <code>xl_workbooks</code> directory.</li> <li>Uses both file-based and inline payloads.</li> <li>Intended for development and diagnostic use, not production.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def test_update_xlsx_payloads_01() -&gt; None:\n  \"\"\"\n  Run test cases that populate Jinja-templated Excel files with known payloads.\n\n  This test routine helps validate that Excel generation is functioning correctly\n  for all supported sectors (landfill, oil &amp; gas, energy).\n\n  Returns:\n    None\n\n  Example:\n    &gt;&gt;&gt; test_update_xlsx_payloads_01()\n\n  Notes:\n    - Writes populated Excel files to the `xl_workbooks` directory.\n    - Uses both file-based and inline payloads.\n    - Intended for development and diagnostic use, not production.\n  \"\"\"\n  logger.debug(\"test_update_xlsx_payloads_01() called\")\n\n  # Landfill test with two payloads from file\n  update_xlsx_payloads(\n    PROCESSED_VERSIONS / f\"xl_workbooks/landfill_operator_feedback_{LANDFILL_VERSION}_jinja_.xlsx\",\n    PROCESSED_VERSIONS / f\"xl_workbooks/landfill_operator_feedback_{LANDFILL_VERSION}_populated_01.xlsx\",\n    [\n      PROCESSED_VERSIONS / \"xl_payloads/landfill_v01_00_defaults.json\",\n      PROCESSED_VERSIONS / \"xl_payloads/landfill_v01_00_payload_01.json\",\n    ]\n  )\n\n  # Landfill test with one file payload and one inline dict\n  update_xlsx_payloads(\n    PROCESSED_VERSIONS / f\"xl_workbooks/landfill_operator_feedback_{LANDFILL_VERSION}_jinja_.xlsx\",\n    PROCESSED_VERSIONS / f\"xl_workbooks/landfill_operator_feedback_{LANDFILL_VERSION}_populated_02.xlsx\",\n    [\n      PROCESSED_VERSIONS / \"xl_payloads/landfill_v01_00_payload_01.json\",\n      {\"id_incidence\": \"123456\"},\n    ]\n  )\n\n  # Oil and gas test\n  update_xlsx_payloads(\n    PROCESSED_VERSIONS / f\"xl_workbooks/oil_and_gas_operator_feedback_{OIL_AND_GAS_VERSION}_jinja_.xlsx\",\n    PROCESSED_VERSIONS / f\"xl_workbooks/oil_and_gas_operator_feedback_{OIL_AND_GAS_VERSION}_populated_01.xlsx\",\n    [\n      PROCESSED_VERSIONS / \"xl_payloads/oil_and_gas_v01_00_defaults.json\",\n      PROCESSED_VERSIONS / \"xl_payloads/oil_and_gas_v01_00_payload_01.json\",\n    ]\n  )\n\n  # Energy test with inline payload\n  update_xlsx_payloads(\n    PROCESSED_VERSIONS / f\"xl_workbooks/energy_operator_feedback_{ENERGY_VERSION}_jinja_.xlsx\",\n    PROCESSED_VERSIONS / f\"xl_workbooks/energy_operator_feedback_{ENERGY_VERSION}_populated_01.xlsx\",\n    [\n      PROCESSED_VERSIONS / \"xl_payloads/energy_v00_01_defaults.json\",\n      {\"id_incidence\": \"654321\"},\n    ]\n  )\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.update_vba_schema","title":"<code>update_vba_schema(schema_version, file_name_in=None, file_name_out=None, file_name_default_value_types=None)</code>","text":"<p>Update a VBA-generated Excel schema with value_type info and re-sort it.</p> <p>Parameters:</p> Name Type Description Default <code>schema_version</code> <code>str</code> <p>Identifier for the schema version.</p> required <code>file_name_in</code> <code>Path</code> <p>Path to the raw VBA schema JSON file. Defaults to \"processed_versions/xl_schemas/{schema_version}_vba.json\".</p> <code>None</code> <code>file_name_out</code> <code>Path</code> <p>Path to output the upgraded schema JSON. Defaults to \"processed_versions/xl_schemas/{schema_version}.json\".</p> <code>None</code> <code>file_name_default_value_types</code> <code>Path</code> <p>Path to JSON file defining default value types. Defaults to \"processed_versions/xl_schemas/default_value_types_v01_00.json\".</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated and sorted schema dictionary.</p> Example <p>updated = update_vba_schema(\"landfill_v01_00\")</p> Notes <ul> <li>This function ensures that all schema entries include a 'value_type'.</li> <li>Applies <code>sort_xl_schema()</code> with sort_by=\"label_address\".</li> <li>Uses <code>schema_to_json_file()</code> to write the result to disk.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def update_vba_schema(\n    schema_version: str,\n    file_name_in: Path = None,\n    file_name_out: Path = None,\n    file_name_default_value_types: Path = None\n) -&gt; dict:\n  \"\"\"\n  Update a VBA-generated Excel schema with value_type info and re-sort it.\n\n  Args:\n    schema_version (str): Identifier for the schema version.\n    file_name_in (Path, optional): Path to the raw VBA schema JSON file.\n      Defaults to \"processed_versions/xl_schemas/{schema_version}_vba.json\".\n    file_name_out (Path, optional): Path to output the upgraded schema JSON.\n      Defaults to \"processed_versions/xl_schemas/{schema_version}.json\".\n    file_name_default_value_types (Path, optional): Path to JSON file defining\n      default value types. Defaults to \"processed_versions/xl_schemas/default_value_types_v01_00.json\".\n\n  Returns:\n    dict: The updated and sorted schema dictionary.\n\n  Example:\n    &gt;&gt;&gt; updated = update_vba_schema(\"landfill_v01_00\")\n\n  Notes:\n    - This function ensures that all schema entries include a 'value_type'.\n    - Applies `sort_xl_schema()` with sort_by=\"label_address\".\n    - Uses `schema_to_json_file()` to write the result to disk.\n  \"\"\"\n\n  logger.debug(f\"update_vba_schema() called with {schema_version=}, {file_name_in=}, \"\n               f\"{file_name_out=}, {file_name_default_value_types=}\")\n\n  if file_name_in is None:\n    file_name_in = PROCESSED_VERSIONS / \"xl_schemas\" / f\"{schema_version}_vba.json\"\n  if file_name_out is None:\n    file_name_out = PROCESSED_VERSIONS / \"xl_schemas\" / f\"{schema_version}.json\"\n  if file_name_default_value_types is None:\n    file_name_default_value_types = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00.json\"\n\n  ensure_parent_dirs(file_name_in)\n  ensure_parent_dirs(file_name_out)\n  ensure_parent_dirs(file_name_default_value_types)\n\n  default_value_types, _ = json_load_with_meta(file_name_default_value_types)\n  schema = json_load(file_name_in, json_options=None)\n\n  ensure_key_value_pair(schema, default_value_types, \"value_type\")\n  schema = sort_xl_schema(schema, sort_by=\"label_address\")\n\n  schema_to_json_file(schema, schema_version, file_name=file_name_out)\n  return schema\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.update_vba_schemas","title":"<code>update_vba_schemas()</code>","text":"<p>Batch update of known VBA-generated schemas using <code>update_vba_schema()</code>.</p> <p>This function applies schema upgrades to a predefined list of versions, typically for supported sectors like landfill and oil &amp; gas.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Notes <ul> <li>Automatically processes \"landfill_v01_00\" and \"oil_and_gas_v01_00\".</li> <li>Calls <code>update_vba_schema()</code> for each version.</li> <li>Output schemas are written to the processed_versions/xl_schemas directory.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def update_vba_schemas() -&gt; None:\n  \"\"\"\n  Batch update of known VBA-generated schemas using `update_vba_schema()`.\n\n  This function applies schema upgrades to a predefined list of versions,\n  typically for supported sectors like landfill and oil &amp; gas.\n\n  Returns:\n    None\n\n  Notes:\n    - Automatically processes \"landfill_v01_00\" and \"oil_and_gas_v01_00\".\n    - Calls `update_vba_schema()` for each version.\n    - Output schemas are written to the processed_versions/xl_schemas directory.\n  \"\"\"\n  logger.debug(\"update_vba_schemas() called\")\n\n  for schema_version in [\"landfill_v01_00\", \"oil_and_gas_v01_00\"]:\n    update_vba_schema(schema_version)\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.update_xlsx","title":"<code>update_xlsx(file_in, file_out, jinja_dict)</code>","text":"<p>Render a Jinja-templated Excel (.xlsx) file by replacing placeholders with dictionary values.</p> <p>Parameters:</p> Name Type Description Default <code>file_in</code> <code>Path</code> <p>Path to the input Excel file containing Jinja placeholders.</p> required <code>file_out</code> <code>Path</code> <p>Path where the rendered Excel file will be saved.</p> required <code>jinja_dict</code> <code>dict</code> <p>Dictionary mapping Jinja template variables to replacement values.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>update_xlsx(Path(\"template.xlsx\"), Path(\"output.xlsx\"), {\"site_name\": \"Landfill A\"})</p> Notes <ul> <li>Only modifies 'xl/sharedStrings.xml' within the XLSX zip archive.</li> <li>All other file contents are passed through unchanged.</li> <li>Useful for populating pre-tagged Excel templates with form data.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def update_xlsx(file_in: Path, file_out: Path, jinja_dict: dict) -&gt; None:\n  \"\"\"\n  Render a Jinja-templated Excel (.xlsx) file by replacing placeholders with dictionary values.\n\n  Args:\n    file_in (Path): Path to the input Excel file containing Jinja placeholders.\n    file_out (Path): Path where the rendered Excel file will be saved.\n    jinja_dict (dict): Dictionary mapping Jinja template variables to replacement values.\n\n  Returns:\n    None\n\n  Example:\n    &gt;&gt;&gt; update_xlsx(Path(\"template.xlsx\"), Path(\"output.xlsx\"), {\"site_name\": \"Landfill A\"})\n\n  Notes:\n    - Only modifies 'xl/sharedStrings.xml' within the XLSX zip archive.\n    - All other file contents are passed through unchanged.\n    - Useful for populating pre-tagged Excel templates with form data.\n  \"\"\"\n\n  logger.debug(f\"Rendering Excel from {file_in} to {file_out} using {jinja_dict}\")\n\n  with zipfile.ZipFile(file_in, 'r') as xlsx, zipfile.ZipFile(file_out, 'w') as new_xlsx:\n    for filename in xlsx.namelist():\n      with xlsx.open(filename) as file:\n        contents = file.read()\n        if filename == 'xl/sharedStrings.xml':\n          contents = jinja2.Template(contents.decode('utf-8')).render(jinja_dict).encode('utf-8')\n        new_xlsx.writestr(filename, contents)\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.update_xlsx_payloads","title":"<code>update_xlsx_payloads(file_in, file_out, payloads)</code>","text":"<p>Apply multiple payloads to a Jinja-templated Excel file and render the result.</p> <p>Parameters:</p> Name Type Description Default <code>file_in</code> <code>Path</code> <p>Path to the input Excel file containing Jinja placeholders.</p> required <code>file_out</code> <code>Path</code> <p>Path where the populated Excel file will be written.</p> required <code>payloads</code> <code>list | tuple</code> <p>Sequence of dictionaries or JSON file paths. - Payloads are merged in order, with later values overriding earlier ones.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>update_xlsx_payloads( ...   Path(\"template.xlsx\"), ...   Path(\"output.xlsx\"), ...   [Path(\"defaults.json\"), {\"id_case\": \"X42\"}] ... )</p> Notes <ul> <li>Each payload may be a dictionary or a Path to a JSON file with metadata.</li> <li>Designed to support tiered rendering: default payload + override.</li> <li>Uses <code>json_load_with_meta()</code> for file payloads.</li> <li>Passes final merged dictionary to <code>update_xlsx()</code>.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_create.py</code> <pre><code>def update_xlsx_payloads(file_in: Path, file_out: Path, payloads: list | tuple) -&gt; None:\n  \"\"\"\n  Apply multiple payloads to a Jinja-templated Excel file and render the result.\n\n  Args:\n    file_in (Path): Path to the input Excel file containing Jinja placeholders.\n    file_out (Path): Path where the populated Excel file will be written.\n    payloads (list | tuple): Sequence of dictionaries or JSON file paths.\n      - Payloads are merged in order, with later values overriding earlier ones.\n\n  Returns:\n    None\n\n  Example:\n    &gt;&gt;&gt; update_xlsx_payloads(\n    ...   Path(\"template.xlsx\"),\n    ...   Path(\"output.xlsx\"),\n    ...   [Path(\"defaults.json\"), {\"id_case\": \"X42\"}]\n    ... )\n\n  Notes:\n    - Each payload may be a dictionary or a Path to a JSON file with metadata.\n    - Designed to support tiered rendering: default payload + override.\n    - Uses `json_load_with_meta()` for file payloads.\n    - Passes final merged dictionary to `update_xlsx()`.\n  \"\"\"\n\n  logger.debug(f\"update_xlsx_payloads() called with: {file_in=}, {file_out=}, {payloads=}\")\n\n  new_dict = {}\n  for payload in payloads:\n    if isinstance(payload, dict):\n      data = payload\n    else:\n      data, _ = json_load_with_meta(payload)\n    new_dict.update(data)\n\n  update_xlsx(file_in, file_out, new_dict)\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_file_structure/","title":"<code>arb.utils.excel.xl_file_structure</code>","text":"<p>Module to determine the path to the root of the feedback_portal project in a platform-independent way.</p> <p>This module can be invoked from multiple runtime contexts, including: - The <code>utils.excel</code> directory (e.g., for standalone Excel/VBA payload generation). - The <code>portal</code> Flask app directory.</p> <p>Directory structure reference:</p> <p>/feedback_portal/                   &lt;-- Base of project directory tree   \u251c\u2500\u2500 feedback_forms/        \u251c\u2500\u2500 current_versions/        &lt;-- Current versions of feedback forms (created in Excel/VBA)        \u2514\u2500\u2500 processed_versions/      &lt;-- Updated versions created in Python             \u251c\u2500\u2500 xl_payloads/             \u251c\u2500\u2500 xl_schemas/             \u2514\u2500\u2500 xl_workbooks/   \u251c\u2500\u2500 source/        \u2514\u2500\u2500 production/             \u2514\u2500\u2500 arb/                  \u251c\u2500\u2500 portal/        &lt;-- Flask app                  \u2514\u2500\u2500 utils/                       \u2514\u2500\u2500 excel/    &lt;-- Excel generation scripts</p> <p>Attributes:</p> Name Type Description <code>PROJECT_ROOT</code> <code>Path</code> <p>Resolved root directory of the project.</p> <code>FEEDBACK_FORMS</code> <code>Path</code> <p>Path to the 'feedback_forms' directory.</p> <code>CURRENT_VERSIONS</code> <code>Path</code> <p>Path to Excel files from current official versions.</p> <code>PROCESSED_VERSIONS</code> <code>Path</code> <p>Path to output files generated via Python processing.</p>"},{"location":"reference/arb/utils/excel/xl_hardcoded/","title":"<code>arb.utils.excel.xl_hardcoded</code>","text":"<p>Hardcoded schema definitions and sample payloads for Excel template processing.</p> <p>The new versioning systems uses the naming scheme vxx_yy, where xx represents a major version and  yy represents a minor version (without the 'old' in the prefix).</p> <p>These were manually created by inspecting old_v01 and old_v02 versions of now outdated Excel spreadsheets.</p> Contents <ul> <li><code>default_value_types_v01_00</code>: field types for v01_00 based on old_v01 and old_v02 schemas</li> <li>Sample payloads for oil &amp; gas and landfill forms</li> <li><code>jinja_names_set</code>: manually compiled field names used in Jinja templates</li> <li>Diagnostic comparison for field coverage (see <code>__main__</code>)</li> </ul>"},{"location":"reference/arb/utils/excel/xl_misc/","title":"<code>arb.utils.excel.xl_misc</code>","text":"<p>Excel address parsing and sorting utilities.</p> <p>This module provides helper functions for interpreting Excel-style address strings (such as \"$A$1\") and using them for sorting data structures. These utilities are used during schema generation, payload creation, and Excel form manipulation.</p> <p>Functions:</p> Name Description <code>- get_excel_row_column</code> <p>Parses an Excel address into column and row components.</p> <code>- xl_address_sort</code> <p>Extracts a sortable row or column value from an Excel address.</p> <code>- run_diagnostics</code> <p>Test harness for verifying address parsing and sorting behavior.</p> Typical Use Case <p>These functions are primarily invoked when organizing Excel schema dictionaries by their physical layout in the worksheet, either by row or column position.</p> Notes <ul> <li>Assumes absolute Excel address formatting (e.g., \"$A$1\").</li> <li>Designed to be used by other modules like xl_create and xl_file_structure.</li> </ul>"},{"location":"reference/arb/utils/excel/xl_misc/#arb.utils.excel.xl_misc.get_excel_row_column","title":"<code>get_excel_row_column(xl_address)</code>","text":"<p>Extract the Excel column letters and row number from an address string.</p> <p>Excel absolute references take the form \"$A$1\" or \"$BB$12\", where both the column and row are prefixed with dollar signs.</p> <p>Parameters:</p> Name Type Description Default <code>xl_address</code> <code>str</code> <p>The Excel address to parse (must be in absolute format like \"$A$1\").</p> required <p>Returns:</p> Type Description <code>tuple[str, int]</code> <p>tuple[str, int]: A tuple of (column letters, row number).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the format is invalid (e.g., not exactly two dollar signs, or row not an integer).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_excel_row_column(\"$Z$9\")\n('Z', 9)\n</code></pre> <pre><code>&gt;&gt;&gt; get_excel_row_column(\"$AA$105\")\n('AA', 105)\n</code></pre> Source code in <code>arb\\utils\\excel\\xl_misc.py</code> <pre><code>def get_excel_row_column(xl_address: str) -&gt; tuple[str, int]:\n  \"\"\"\n  Extract the Excel column letters and row number from an address string.\n\n  Excel absolute references take the form \"$A$1\" or \"$BB$12\", where both the column and row\n  are prefixed with dollar signs.\n\n  Args:\n    xl_address (str): The Excel address to parse (must be in absolute format like \"$A$1\").\n\n  Returns:\n    tuple[str, int]: A tuple of (column letters, row number).\n\n  Raises:\n    ValueError: If the format is invalid (e.g., not exactly two dollar signs, or row not an integer).\n\n  Examples:\n    &gt;&gt;&gt; get_excel_row_column(\"$Z$9\")\n    ('Z', 9)\n\n    &gt;&gt;&gt; get_excel_row_column(\"$AA$105\")\n    ('AA', 105)\n  \"\"\"\n\n  if xl_address.count('$') != 2:\n    raise ValueError(f\"Excel address must contain exactly two '$' characters: {xl_address}\")\n\n  first_dollar = xl_address.find('$')\n  last_dollar = xl_address.rfind('$')\n\n  column = xl_address[first_dollar + 1:last_dollar]\n  try:\n    row = int(xl_address[last_dollar + 1:])\n  except ValueError as e:\n    raise ValueError(f\"Could not parse row number from Excel address: {xl_address}\") from e\n\n  return column, row\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_misc/#arb.utils.excel.xl_misc.run_diagnostics","title":"<code>run_diagnostics()</code>","text":"<p>Run demonstration tests for get_excel_row_column() and xl_address_sort(). This function is only called if this module is run directly.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; run_diagnostics()\n</code></pre> Source code in <code>arb\\utils\\excel\\xl_misc.py</code> <pre><code>def run_diagnostics() -&gt; None:\n  \"\"\"\n  Run demonstration tests for get_excel_row_column() and xl_address_sort().\n  This function is only called if this module is run directly.\n\n  Examples:\n    &gt;&gt;&gt; run_diagnostics()\n  \"\"\"\n  pp = pprint.PrettyPrinter(indent=4, sort_dicts=False)\n\n  print(\"=== Testing get_excel_row_column ===\")\n  valid_addresses = [\"$C$42\", \"$AA$99\"]\n  for addr in valid_addresses:\n    try:\n      result = get_excel_row_column(addr)\n      print(f\"  Address: {addr} =&gt; {result}\")\n    except Exception as e:\n      print(f\"  ERROR for {addr}: {e}\")\n\n  print(\"\\n=== Testing get_excel_row_column (invalid formats) ===\")\n  invalid_addresses = [\"A$1\", \"$A1\", \"$A$1$\", \"$AB$\", \"$AB$XYZ\"]\n  for addr in invalid_addresses:\n    try:\n      result = get_excel_row_column(addr)\n      print(f\"  UNEXPECTED SUCCESS: {addr} =&gt; {result}\")\n    except Exception as e:\n      print(f\"  Expected failure for {addr}: {e}\")\n\n  print(\"\\n=== Testing xl_address_sort ===\")\n  test_tuple = (\"$B$10\", \"Example\")\n  print(f\"  Tuple: {test_tuple} =&gt; Row: {xl_address_sort(test_tuple, 'key', 'row')}\")\n\n  nested = (\"key\", {\"nested\": {\"cell\": \"$D$20\"}})\n  print(f\"  Tuple: {nested} =&gt; Row: {xl_address_sort(nested, 'value', 'row', sub_keys=['nested', 'cell'])}\")\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_misc/#arb.utils.excel.xl_misc.xl_address_sort","title":"<code>xl_address_sort(xl_tuple, address_location='key', sort_by='row', sub_keys=None)</code>","text":"<p>Extract the Excel row or column value from a tuple of key-value pairs for sorting.</p> <p>This is used when sorting collections of data where either the key or the value contains an Excel-style address string. Supports sorting by either row or column.</p> <p>Parameters:</p> Name Type Description Default <code>xl_tuple</code> <code>tuple</code> <p>A (key, value) tuple where one element contains a string like \"$A$1\".</p> required <code>address_location</code> <code>str</code> <p>Which element contains the Excel address (\"key\" or \"value\").</p> <code>'key'</code> <code>sort_by</code> <code>str</code> <p>Whether to sort by \"row\" (int) or \"column\" (str).</p> <code>'row'</code> <code>sub_keys</code> <code>str | list[str] | None</code> <p>Key(s) to retrieve nested address if inside a dict.</p> <code>None</code> <p>Returns:</p> Type Description <code>int | str</code> <p>int | str: The row (int) or column (str) extracted from the address.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>address_location</code> or <code>sort_by</code> has an invalid value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; xl_address_sort((\"$B$3\", \"data\"), address_location=\"key\", sort_by=\"row\")\n3\n</code></pre> <pre><code>&gt;&gt;&gt; xl_address_sort((\"item\", {\"pos\": \"$C$7\"}), address_location=\"value\", sort_by=\"column\", sub_keys=\"pos\")\n'C'\n</code></pre> Source code in <code>arb\\utils\\excel\\xl_misc.py</code> <pre><code>def xl_address_sort(\n  xl_tuple: tuple,\n  address_location: str = \"key\",\n  sort_by: str = \"row\",\n  sub_keys: str | list[str] | None = None\n) -&gt; int | str:\n  \"\"\"\n  Extract the Excel row or column value from a tuple of key-value pairs for sorting.\n\n  This is used when sorting collections of data where either the key or the value\n  contains an Excel-style address string. Supports sorting by either row or column.\n\n  Args:\n    xl_tuple (tuple): A (key, value) tuple where one element contains a string like \"$A$1\".\n    address_location (str): Which element contains the Excel address (\"key\" or \"value\").\n    sort_by (str): Whether to sort by \"row\" (int) or \"column\" (str).\n    sub_keys (str | list[str] | None): Key(s) to retrieve nested address if inside a dict.\n\n  Returns:\n    int | str: The row (int) or column (str) extracted from the address.\n\n  Raises:\n    ValueError: If `address_location` or `sort_by` has an invalid value.\n\n  Examples:\n    &gt;&gt;&gt; xl_address_sort((\"$B$3\", \"data\"), address_location=\"key\", sort_by=\"row\")\n    3\n\n    &gt;&gt;&gt; xl_address_sort((\"item\", {\"pos\": \"$C$7\"}), address_location=\"value\", sort_by=\"column\", sub_keys=\"pos\")\n    'C'\n  \"\"\"\n\n  if address_location == \"key\":\n    address = xl_tuple[0]\n  elif address_location == \"value\":\n    if sub_keys is None:\n      address = xl_tuple[1]\n    else:\n      address = get_nested_value(xl_tuple[1], sub_keys)\n  else:\n    raise ValueError(\"address_location must be 'key' or 'value'\")\n\n  column, row = get_excel_row_column(address)\n\n  if sort_by == \"row\":\n    return_value = row\n  elif sort_by == \"column\":\n    return_value = column\n  else:\n    raise ValueError(\"sort_by must be 'row' or 'column'\")\n\n  return return_value\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/","title":"<code>arb.utils.excel.xl_parse</code>","text":"<p>Module to parse and ingest Excel spreadsheet contents.</p> <p>This module provides logic to convert Excel forms into structured dictionary representations, including extraction of tab contents, metadata, and schema references. It is primarily used to support automated feedback template parsing.</p> Notes <ul> <li><code>schema_file_map</code> is a dict where keys are schema names and values are paths to JSON files.</li> <li><code>schema_map</code> is a dict where keys are schema names and values are:     {\"schema\": schema_dict, \"metadata\": metadata_dict}</li> </ul> Example <p>xl_schema_map[\"oil_and_gas_v03\"][\"schema\"]</p>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.create_schema_file_map","title":"<code>create_schema_file_map(schema_path=None, schema_names=None)</code>","text":"<p>Create a dictionary mapping schema names to their JSON file paths.</p> <p>Parameters:</p> Name Type Description Default <code>schema_path</code> <code>str | Path | None</code> <p>Folder containing schema files. Defaults to processed versions dir.</p> <code>None</code> <code>schema_names</code> <code>list[str] | None</code> <p>Names of schemas to include. Defaults to known schemas.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Path]</code> <p>dict[str, Path]: Map from schema name to schema file path.</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def create_schema_file_map(schema_path: str | Path | None = None,\n                           schema_names: list[str] | None = None) -&gt; dict[str, Path]:\n  \"\"\"\n  Create a dictionary mapping schema names to their JSON file paths.\n\n  Args:\n    schema_path (str | Path | None): Folder containing schema files. Defaults to processed versions dir.\n    schema_names (list[str] | None): Names of schemas to include. Defaults to known schemas.\n\n  Returns:\n    dict[str, Path]: Map from schema name to schema file path.\n  \"\"\"\n  logger.debug(f\"create_schema_file_map() called with {schema_path=}, {schema_names=}\")\n  if schema_path is None:\n    schema_path = PROCESSED_VERSIONS / \"xl_schemas\"\n  if schema_names is None:\n    schema_names = [\"landfill_v01_00\",\n                    \"oil_and_gas_v01_00\",\n                    \"energy_v00_01\", ]\n\n  schema_file_map = {}\n\n  for schema_name in schema_names:\n    schema_file_name = schema_name + \".json\"\n    schema_file_path = schema_path / schema_file_name\n    schema_file_map[schema_name] = schema_file_path\n\n  return schema_file_map\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.extract_tabs","title":"<code>extract_tabs(wb, schema_map, xl_as_dict)</code>","text":"<p>Extract data from the data tabs that are enumerated in the schema tab.</p> <p>Parameters:</p> Name Type Description Default <code>wb</code> <code>Workbook</code> <p>OpenPyXL workbook object.</p> required <code>schema_map</code> <code>dict[str, dict]</code> <p>Schema map with schema definitions.</p> required <code>xl_as_dict</code> <code>dict</code> <p>Parsed Excel content, including 'schemas' and 'metadata'.                  Dictionary with schema tab where keys are the data tab names and values are the formatting_schema to                  parse the tab.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated xl_as_dict including parsed 'tab_contents'.</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def extract_tabs(wb: openpyxl.workbook.workbook.Workbook,\n                 schema_map: dict[str, dict],\n                 xl_as_dict: dict) -&gt; dict:\n  \"\"\"\n  Extract data from the data tabs that are enumerated in the schema tab.\n\n  Args:\n    wb (Workbook): OpenPyXL workbook object.\n    schema_map (dict[str, dict]): Schema map with schema definitions.\n    xl_as_dict (dict): Parsed Excel content, including 'schemas' and 'metadata'.\n                       Dictionary with schema tab where keys are the data tab names and values are the formatting_schema to\n                       parse the tab.\n\n  Returns:\n    dict: Updated xl_as_dict including parsed 'tab_contents'.\n  \"\"\"\n\n  skip_please_selects = False\n\n  result = copy.deepcopy(xl_as_dict)\n\n  for tab_name, formatting_schema in result['schemas'].items():\n    logger.debug(f\"Extracting data from '{tab_name}', using the formatting schema '{formatting_schema}'\")\n    result['tab_contents'][tab_name] = {}\n\n    ws = wb[tab_name]\n    format_dict = schema_map[formatting_schema]['schema']\n\n    for html_field_name, lookup in format_dict.items():\n      value_address = lookup['value_address']\n      value_type = lookup['value_type']\n      is_drop_down = lookup['is_drop_down']\n      value = ws[value_address].value\n\n      if skip_please_selects is True:\n        if is_drop_down and value == PLEASE_SELECT:\n          logger.debug(f\"Skipping {html_field_name} because it is a drop down and is set to {PLEASE_SELECT}\")\n          continue\n\n      # Try to cast the spreadsheet data to the desired type if possible\n      if value is not None:\n        if not isinstance(value, value_type):\n          # if it is not supposed to be of type string, but it is a zero length string, turn it to None\n          if value == \"\":\n            value = None\n          else:\n            logger.warning(f\"Warning: &lt;{html_field_name}&gt; value at &lt;{lookup['value_address']}&gt; is &lt;{value}&gt; \"\n                           f\"and is of type &lt;{type(value)}&gt; whereas it should be of type &lt;{value_type}&gt;.  \"\n                           f\"Attempting to convert the value to the correct type\")\n            try:\n              # convert to datetime using a parser if possible\n              if value_type == datetime.datetime:\n                local_datetime = parse_unknown_datetime(value)\n                if local_datetime and not is_datetime_naive(local_datetime):\n                  logger.warning(f\"Date time {value} is not a naive datetime, skipping to avoid data corruption\")\n                  continue\n                value = local_datetime\n              else:\n                # Use default repr-like conversion if not a datetime\n                value = value_type(value)\n              logger.info(f\"Type conversion successful.  value is now &lt;{value}&gt; with type: &lt;{type(value)}&gt;\")\n            except (ValueError, TypeError) as e:\n              logger.warning(f\"Type conversion failed, resetting value to None\")\n              value = None\n\n      result['tab_contents'][tab_name][html_field_name] = value\n\n      if 'label_address' in lookup and 'label' in lookup:\n        label_address = lookup['label_address']\n        label_xl = ws[label_address].value\n        label_schema = lookup['label']\n        if label_xl != label_schema:\n          logger.warning(f\"Schema data label and spreadsheet data label differ.\"\n                         f\"\\n\\tschema label = {label_schema}\\n\\tspreadsheet label ({label_address}) = {label_xl}\")\n\n    logger.debug(f\"Initial spreadsheet extraction of '{tab_name}' yields {result['tab_contents'][tab_name]}\")\n    # Some cells should be spit into multiple dictionary entries (such as full name, lat/log)\n    split_compound_keys(result['tab_contents'][tab_name])\n    # Excel drop-downs save the value not the key, so you have to reverse lookup their values\n\n    logger.debug(f\"Final corrected spreadsheet extraction of '{tab_name}' yields {result['tab_contents'][tab_name]}\")\n\n  return result\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.get_json_file_name","title":"<code>get_json_file_name(file_name)</code>","text":"<p>Convert a file name (Excel or JSON) into a JSON file name, parsing if needed.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>Path</code> <p>The uploaded file.</p> required <p>Returns:</p> Type Description <code>Path | None</code> <p>Path | None: JSON file path if parsed or detected, otherwise None.</p> <p>Notes: - If the file_name is a json file (has .json extension), the file_name is the json_file_name. - If the file is an Excel file (.xlsx extension), try to parse it into a json file and return the json file name   of the parsed contents. - If the file is neither a json file nr a spreadsheet that can be parsed into a json file, return None. - If the file was already a json file, return the file name unaltered. - If the file was an Excel file, return the json file that its data was extracted to. - For all other file types return None.</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def get_json_file_name(file_name: Path) -&gt; Path | None:\n  \"\"\"\n  Convert a file name (Excel or JSON) into a JSON file name, parsing if needed.\n\n  Args:\n    file_name (Path): The uploaded file.\n\n  Returns:\n    Path | None: JSON file path if parsed or detected, otherwise None.\n\n  Notes:\n  - If the file_name is a json file (has .json extension), the file_name is the json_file_name.\n  - If the file is an Excel file (.xlsx extension), try to parse it into a json file and return the json file name\n    of the parsed contents.\n  - If the file is neither a json file nr a spreadsheet that can be parsed into a json file, return None.\n  - If the file was already a json file, return the file name unaltered.\n  - If the file was an Excel file, return the json file that its data was extracted to.\n  - For all other file types return None.\n  \"\"\"\n\n  json_file_name = None\n\n  extension = file_name.suffix\n  # logger.debug(f\"{extension=}\")\n  if extension == \".xlsx\":\n    logger.debug(f\"Excel file upload detected.  File name: {file_name}\")\n    # xl_as_dict = xl_path_to_dict(file_name)\n    xl_as_dict = parse_xl_file(file_name)\n    logger.debug(f\"{xl_as_dict=}\")\n    json_file_name = file_name.with_suffix('.json')\n    logger.debug(f\"Saving extracted data from Excel as: {json_file_name}\")\n    json_save_with_meta(json_file_name, xl_as_dict)\n  elif extension == \".json\":\n    logger.debug(f\"Json file upload detected.  File name: {file_name}\")\n    json_file_name = Path(file_name)\n  else:\n    logger.debug(f\"Unknown file type upload detected.  File name: {file_name}\")\n\n  return json_file_name\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.get_spreadsheet_key_value_pairs","title":"<code>get_spreadsheet_key_value_pairs(wb, tab_name, top_left_cell)</code>","text":"<p>Read key-value pairs from a worksheet starting at a given cell.</p> <p>Parameters:</p> Name Type Description Default <code>wb</code> <code>Workbook</code> <p>OpenPyXL workbook object.</p> required <code>tab_name</code> <code>str</code> <p>Name of the worksheet tab.</p> required <code>top_left_cell</code> <code>str</code> <p>Top-left cell of the key/value pair region.</p> required <p>Returns:</p> Type Description <code>dict[str, str | None]</code> <p>dict[str, str | None]: Parsed key-value pairs.</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def get_spreadsheet_key_value_pairs(wb: openpyxl.workbook.workbook.Workbook,\n                                    tab_name: str,\n                                    top_left_cell: str) -&gt; dict[str, str | None]:\n  \"\"\"\n  Read key-value pairs from a worksheet starting at a given cell.\n\n  Args:\n    wb (Workbook): OpenPyXL workbook object.\n    tab_name (str): Name of the worksheet tab.\n    top_left_cell (str): Top-left cell of the key/value pair region.\n\n  Returns:\n    dict[str, str | None]: Parsed key-value pairs.\n  \"\"\"\n\n  # logger.debug(f\"{type(wb)=}, \")\n  ws = wb[tab_name]\n\n  # logger.debug(f\"{type(ws)=}, \")\n\n  return_dict = {}\n\n  row_offset = 0\n  while True:\n    key = ws[top_left_cell].offset(row=row_offset).value\n    value = ws[top_left_cell].offset(row=row_offset, column=1).value\n    if key not in [\"\", None]:\n      return_dict[key] = value\n      row_offset += 1\n    else:\n      break\n\n  return return_dict\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.initialize_module","title":"<code>initialize_module()</code>","text":"<p>Initialize the module by calling set_globals().</p> <p>This function loads default schema mappings into global variables.</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def initialize_module() -&gt; None:\n  \"\"\"\n  Initialize the module by calling set_globals().\n\n  This function loads default schema mappings into global variables.\n  \"\"\"\n  logger.debug(f\"initialize_module() called\")\n  set_globals()\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.load_schema_file_map","title":"<code>load_schema_file_map(schema_file_map)</code>","text":"<p>Load JSON schema and metadata from a mapping of schema name to file path.</p> <p>Parameters:</p> Name Type Description Default <code>schema_file_map</code> <code>dict[str, Path]</code> <p>Keys are schema names, values are JSON schema file paths.</p> required <p>Returns:</p> Type Description <code>dict[str, dict]</code> <p>dict[str, dict]: Dictionary where keys are schema names and values are dicts with: - \"schema\": The schema dictionary. - \"metadata\": Metadata extracted from the JSON.</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def load_schema_file_map(schema_file_map: dict[str, Path]) -&gt; dict[str, dict]:\n  \"\"\"\n  Load JSON schema and metadata from a mapping of schema name to file path.\n\n  Args:\n    schema_file_map (dict[str, Path]): Keys are schema names, values are JSON schema file paths.\n\n  Returns:\n    dict[str, dict]: Dictionary where keys are schema names and values are dicts with:\n      - \"schema\": The schema dictionary.\n      - \"metadata\": Metadata extracted from the JSON.\n  \"\"\"\n  logger.debug(f\"load_schema_file_map() called with {schema_file_map=}\")\n\n  schema_map = {}\n\n  for schema_name, json_path in schema_file_map.items():\n    schema, metadata = json_load_with_meta(json_path)\n    schema_map[schema_name] = {\"schema\": schema, \"metadata\": metadata}\n\n  return schema_map\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.load_xl_schema","title":"<code>load_xl_schema(file_name)</code>","text":"<p>Load schema and metadata from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str | Path</code> <p>Path to a JSON schema file.</p> required <p>Returns:</p> Type Description <code>tuple[dict, dict]</code> <p>tuple[dict, dict]: Tuple of (schema dict, metadata dict).</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def load_xl_schema(file_name: str | Path) -&gt; tuple[dict, dict]:\n  \"\"\"\n  Load schema and metadata from a JSON file.\n\n  Args:\n    file_name (str | Path): Path to a JSON schema file.\n\n  Returns:\n    tuple[dict, dict]: Tuple of (schema dict, metadata dict).\n  \"\"\"\n  logger.debug(f\"load_xl_schema() called with {file_name=}\")\n  schema, metadata = json_load_with_meta(file_name)\n  return schema, metadata\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.main","title":"<code>main()</code>","text":"<p>Run all schema and Excel file parsing test functions for diagnostic purposes.</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def main() -&gt; None:\n  \"\"\"\n  Run all schema and Excel file parsing test functions for diagnostic purposes.\n  \"\"\"\n  test_load_xl_schemas()\n  test_load_schema_file_map()\n  test_parse_xl_file()\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.parse_xl_file","title":"<code>parse_xl_file(xl_path, schema_map=None)</code>","text":"<p>Parse a spreadsheet and return a dictionary representation using the given schema.</p> <p>Parameters:</p> Name Type Description Default <code>xl_path</code> <code>str | Path</code> <p>Path to the Excel spreadsheet.</p> required <code>schema_map</code> <code>dict[str, dict] | None</code> <p>Map of schema names to their definitions.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary with extracted metadata, schemas, and tab contents.</p> <p>Notes: - tutorial on openpyxl: https://openpyxl.readthedocs.io/en/stable/tutorial.html</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def parse_xl_file(xl_path, schema_map=None) -&gt; dict:\n  \"\"\"\n  Parse a spreadsheet and return a dictionary representation using the given schema.\n\n  Args:\n    xl_path (str | Path): Path to the Excel spreadsheet.\n    schema_map (dict[str, dict] | None): Map of schema names to their definitions.\n\n  Returns:\n    dict: Dictionary with extracted metadata, schemas, and tab contents.\n\n  Notes:\n  - tutorial on openpyxl: https://openpyxl.readthedocs.io/en/stable/tutorial.html\n  \"\"\"\n  logger.debug(f\"parse_xl_with_schema_dict() called with {xl_path=}, {schema_map=}\")\n\n  if schema_map is None:\n    schema_map = xl_schema_map\n\n  # Dictionary data structure to store Excel contents\n  result = {}\n  result['metadata'] = {}\n  result['schemas'] = {}\n  result['tab_contents'] = {}\n\n  # Notes on data_only argument.  By default, .value returns the 'formula' in the cell.\n  # If data_only=True, then .value  returns the last 'value' that was evaluated at the cell.\n  wb = openpyxl.load_workbook(xl_path, keep_vba=False, data_only=True)\n\n  # Extract metadata and schema information from hidden tabs\n  if EXCEL_METADATA_TAB_NAME in wb.sheetnames:\n    logger.debug(f\"metadata tab detected in Excel file\")\n    result['metadata'] = get_spreadsheet_key_value_pairs(wb,\n                                                         EXCEL_METADATA_TAB_NAME,\n                                                         EXCEL_TOP_LEFT_KEY_VALUE_CELL\n                                                         )\n\n  if EXCEL_SCHEMA_TAB_NAME in wb.sheetnames:\n    logger.debug(f\"Schema tab detected in Excel file\")\n    result['schemas'] = get_spreadsheet_key_value_pairs(wb,\n                                                        EXCEL_SCHEMA_TAB_NAME,\n                                                        EXCEL_TOP_LEFT_KEY_VALUE_CELL)\n  else:\n    ValueError(f'Spreadsheet must have a {EXCEL_SCHEMA_TAB_NAME} tab')\n\n  # extract data tabs content using specified schemas\n  new_result = extract_tabs(wb, schema_map, result)\n\n  return new_result\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.set_globals","title":"<code>set_globals(xl_schema_file_map_=None)</code>","text":"<p>Set module-level global variables for schema file map and loaded schema map.</p> <p>Parameters:</p> Name Type Description Default <code>xl_schema_file_map_</code> <code>dict[str, Path] | None</code> <p>Optional override for schema file map. If not provided, uses a default list of pre-defined schema files.</p> <code>None</code> Notes <ul> <li>Calls <code>load_schema_file_map()</code> to populate xl_schema_map from JSON files.</li> </ul> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def set_globals(xl_schema_file_map_: dict[str, Path] | None = None) -&gt; None:\n  \"\"\"\n  Set module-level global variables for schema file map and loaded schema map.\n\n  Args:\n    xl_schema_file_map_ (dict[str, Path] | None): Optional override for schema file map.\n      If not provided, uses a default list of pre-defined schema files.\n\n  Notes:\n    - Calls `load_schema_file_map()` to populate xl_schema_map from JSON files.\n  \"\"\"\n  global xl_schema_file_map, xl_schema_map\n  # todo - update default roots with module paths, may make sense to remove globals and have\n  # a different logic since this is outdated given the project root approach\n\n  logger.debug(f\"set_globals() called with {xl_schema_file_map_=}\")\n\n  # todo - not sure if these should be hard coded here ...\n  if xl_schema_file_map_ is None:\n    xl_schema_file_map = {\n      \"landfill_v01_00\": PROCESSED_VERSIONS / \"xl_schemas\" / \"landfill_v01_00.json\",\n      \"oil_and_gas_v01_00\": PROCESSED_VERSIONS / \"xl_schemas\" / \"oil_and_gas_v01_00.json\",\n      \"energy_v00_01\": PROCESSED_VERSIONS / \"xl_schemas\" / \"energy_v00_01.json\",\n    }\n  else:\n    xl_schema_file_map = xl_schema_file_map_\n\n  # load all the schemas\n  if xl_schema_file_map:\n    xl_schema_map = load_schema_file_map(xl_schema_file_map)\n\n  logger.debug(f\"globals are now: {xl_schema_file_map=}, {xl_schema_map=}\")\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.split_compound_keys","title":"<code>split_compound_keys(dict_)</code>","text":"<p>Decompose compound keys into atomic fields.</p> <p>Remove key/value pairs of entries that potentially contain compound keys and replace them with key value pairs that are more atomic.</p> <p>Parameters:</p> Name Type Description Default <code>dict_</code> <code>dict</code> <p>Dictionary with potentially compound fields (e.g., lat_and_long).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'lat_and_long' is improperly formatted.</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def split_compound_keys(dict_: dict) -&gt; None:\n  \"\"\"\n  Decompose compound keys into atomic fields.\n\n  Remove key/value pairs of entries that potentially contain compound keys and replace them with key value pairs\n  that are more atomic.\n\n  Args:\n    dict_ (dict): Dictionary with potentially compound fields (e.g., lat_and_long).\n\n  Raises:\n    ValueError: If 'lat_and_long' is improperly formatted.\n  \"\"\"\n  for html_field_name in list(dict_.keys()):\n    value = dict_[html_field_name]\n\n    if html_field_name == 'lat_and_long':\n      if value:\n        lat_longs = value.split(',')\n        if len(lat_longs) == 2:\n          dict_['lat_arb'] = lat_longs[0]\n          dict_['long_arb'] = lat_longs[1]\n        else:\n          raise ValueError(f\"Lat long must be a blank or a comma separated list of lat/long pairs\")\n      del dict_[html_field_name]\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.test_load_schema_file_map","title":"<code>test_load_schema_file_map()</code>","text":"<p>Debug test for loading schema file map and displaying contents.</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def test_load_schema_file_map() -&gt; None:\n  \"\"\"\n  Debug test for loading schema file map and displaying contents.\n  \"\"\"\n  logger.debug(f\"test_load_schema_file_map() called\")\n  schema_map = create_schema_file_map()\n  schemas = load_schema_file_map(schema_map)\n  logger.debug(f\"{schemas=}\")\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.test_load_xl_schemas","title":"<code>test_load_xl_schemas()</code>","text":"<p>Debug test for loading default schemas from xl_schema_file_map.</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def test_load_xl_schemas() -&gt; None:\n  \"\"\"\n  Debug test for loading default schemas from xl_schema_file_map.\n  \"\"\"\n  logger.debug(f\"Testing load_xl_schemas() with test_load_xl_schemas\")\n  schemas = load_schema_file_map(xl_schema_file_map)\n  logger.debug(f\"Testing load_xl_schemas() with test_load_xl_schemas\")\n  logging.debug(f\"schemas = {pp_log(schemas)}\")\n</code></pre>"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.test_parse_xl_file","title":"<code>test_parse_xl_file()</code>","text":"<p>Debug test to parse a known Excel file into dictionary form using schemas.</p> Source code in <code>arb\\utils\\excel\\xl_parse.py</code> <pre><code>def test_parse_xl_file() -&gt; None:\n  \"\"\"\n  Debug test to parse a known Excel file into dictionary form using schemas.\n  \"\"\"\n  logger.debug(f\"test_parse_xl_file() called\")\n  xl_path = PROCESSED_VERSIONS / \"xl_workbooks\" / \"landfill_operator_feedback_v070_populated_01.xlsx\"\n  print(f\"{xl_path=}\")\n  result = parse_xl_file(xl_path, xl_schema_map)\n  logger.debug(f\"{result=}\")\n</code></pre>"}]}