{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome & Instructions Welcome to the backend documentation site for the CalSMP Operator Feedback Portal. This documentation is auto-generated from Python source code and includes: Flask route handlers and form logic Utility functions and helpers Excel file schema generation and parsing SQLAlchemy integration and metadata support Use the left sidebar to explore the available modules.","title":"Welcome &amp; Instructions"},{"location":"#welcome-instructions","text":"Welcome to the backend documentation site for the CalSMP Operator Feedback Portal. This documentation is auto-generated from Python source code and includes: Flask route handlers and form logic Utility functions and helpers Excel file schema generation and parsing SQLAlchemy integration and metadata support Use the left sidebar to explore the available modules.","title":"Welcome &amp; Instructions"},{"location":"reference/arb/__get_logger/","text":"arb.__get_logger Centralized logging utility for use across the ARB portal (or any Python project). This module should be imported first in any file that requires logging. It is deliberately named __get_logger.py to ensure it appears first when imports are sorted alphabetically\u2014ensuring logging is configured before any modules emit log messages. Key Features: Initializes logging once per Python process to prevent redundant configurations. Automatically creates log files under a logs/ directory, named after the entry-point script (e.g., running wsgi.py results in logs/wsgi.log ). Provides a built-in PrettyPrinter helper for human-readable, structured log output. Supports console logging (optional) and custom output paths for log files. Usage Examples: Import and initialize the logger in any module: from arb import get_logger as get_logger logger, pp_log = get_logger(__name ) logger.debug(\"Basic log message\") logger.debug(pp_log({\"structured\": \"data\", \"for\": \"inspection\"})) To customize behavior: logger, pp_log = get_logger( file_stem= name , log_to_console=True, force_command_line=False, file_path=\"custom_logs/\" ) Configuration Behavior: If file_path is provided, logs are written to that directory using the provided file_stem . If file_path is not provided, the logger writes to logs/<stem>.log . If executed from __main__ or __init__ , the log file defaults to logs/app_logger.log . All log files use UTF-8 encoding and include timestamps with millisecond precision. Recommendation: Place __get_logger.py near the root of your source tree and import it as early as possible in each module to guarantee consistent logging setup. get_logger ( file_stem = 'app_logger' , file_path = None , log_to_console = False , force_command_line = False ) Return a configured logger instance and a structured logging helper. Parameters: file_stem ( str | None , default: 'app_logger' ) \u2013 Name used for the logger and log filename. file_path ( str | Path | None , default: None ) \u2013 Directory path to store logs (default: logs/ ). log_to_console ( bool , default: False ) \u2013 Whether to also output logs to the console. force_command_line ( bool , default: False ) \u2013 Use command-line filename as logger name. Returns: tuple [ Logger , any ] \u2013 tuple[Logger, callable]: A configured logger and a pretty-print function. Notes Logging setup is only performed once per process. Default log filename: logs/<file_stem>.log . Source code in arb\\__get_logger.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def get_logger ( file_stem : str | None = \"app_logger\" , file_path : str | Path | None = None , log_to_console : bool = False , force_command_line : bool = False ) -> tuple [ Logger , any ]: \"\"\" Return a configured logger instance and a structured logging helper. Args: file_stem (str | None): Name used for the logger and log filename. file_path (str | Path | None): Directory path to store logs (default: `logs/`). log_to_console (bool): Whether to also output logs to the console. force_command_line (bool): Use command-line filename as logger name. Returns: tuple[Logger, callable]: A configured logger and a pretty-print function. Notes: - Logging setup is only performed once per process. - Default log filename: `logs/<file_stem>.log`. \"\"\" # log_format_old = \"+%(asctime)s.%(msecs)03d | %(levelname)-8s | %(name)s | %(filename)s | %(lineno)d | %(message)s\" log_format_proposed = \"+ %(asctime)s . %(msecs)03d | %(levelname)-8s | %(name)s | %(filename)s | %(lineno)d | user: %(user)s | %(message)s \" log_format = \"+ %(asctime)s . %(msecs)03d | %(levelname)-8s | %(name)-16s | user:anonymous | %(lineno)-5d | %(filename)-20s | %(message)s \" log_datefmt = \"%Y-%m- %d %H:%M:%S\" # Determine file stem based on command-line script if requested if force_command_line : script_path = Path ( sys . argv [ 0 ]) file_stem = script_path . stem if file_stem in [ None , \"\" , \"__init__\" , \"__main__\" ]: file_stem = \"app_logger\" file_stem = file_stem . replace ( \".\" , \"_\" ) # Default to logs/ directory next to this file if file_path is None : file_path = Path ( __file__ ) . parent / \"logs\" else : file_path = Path ( file_path ) file_name = file_path / f \" { file_stem } .log\" # Create or retrieve logger logger = logging . getLogger ( file_stem ) is_logger_already_configured = bool ( logging . getLogger () . handlers ) if not is_logger_already_configured : file_name . parent . mkdir ( parents = True , exist_ok = True ) logging . basicConfig ( level = logging . DEBUG , format = log_format , datefmt = log_datefmt , filename = str ( file_name ), encoding = \"utf-8\" , ) # Optional console logging (only add if one doesn't already exist) if log_to_console : root_logger = logging . getLogger () has_console_handler = any ( isinstance ( handler , logging . StreamHandler ) and not isinstance ( handler , logging . FileHandler ) for handler in root_logger . handlers ) if not has_console_handler : console_handler = logging . StreamHandler () console_handler . setFormatter ( logging . Formatter ( log_format , datefmt = log_datefmt )) root_logger . addHandler ( console_handler ) logger . debug ( f \"get_logger() called with { file_stem = } , { file_path =} , { log_to_console =} , { force_command_line =} , { sys . argv = } \" ) if is_logger_already_configured : logging . debug ( \"Logging has already been initialized; configuration will not be changed.\" ) else : logging . debug ( f \"Logging was initialized on first usage. Outputting logs to { file_name } \" ) _ , pp_log = get_pretty_printer () return logger , pp_log get_pretty_printer ( ** kwargs ) Return a PrettyPrinter instance and a formatting function for structured logging. This is useful for debugging or logging nested data structures like dictionaries or deeply nested lists. Parameters: **kwargs ( Any , default: {} ) \u2013 Options passed to pprint.PrettyPrinter , including: - indent (int): Indentation level (default: 4). - sort_dicts (bool): Whether to sort dictionary keys (default: False). - width (int): Max character width per line (default: 120). Returns: tuple [ PrettyPrinter , callable ] \u2013 tuple[PrettyPrinter, callable]: Printer object and its .pformat method. Source code in arb\\__get_logger.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def get_pretty_printer ( ** kwargs : Any ) -> tuple [ pprint . PrettyPrinter , callable ]: \"\"\" Return a `PrettyPrinter` instance and a formatting function for structured logging. This is useful for debugging or logging nested data structures like dictionaries or deeply nested lists. Args: **kwargs (Any): Options passed to `pprint.PrettyPrinter`, including: - indent (int): Indentation level (default: 4). - sort_dicts (bool): Whether to sort dictionary keys (default: False). - width (int): Max character width per line (default: 120). Returns: tuple[PrettyPrinter, callable]: Printer object and its .pformat method. \"\"\" options = { \"indent\" : 4 , \"sort_dicts\" : False , \"width\" : 120 } options . update ( kwargs ) pp = pprint . PrettyPrinter ( ** options ) return pp , pp . pformat","title":"arb.__get_logger"},{"location":"reference/arb/__get_logger/#arb__get_logger","text":"Centralized logging utility for use across the ARB portal (or any Python project). This module should be imported first in any file that requires logging. It is deliberately named __get_logger.py to ensure it appears first when imports are sorted alphabetically\u2014ensuring logging is configured before any modules emit log messages.","title":"arb.__get_logger"},{"location":"reference/arb/__get_logger/#arb.__get_logger--key-features","text":"Initializes logging once per Python process to prevent redundant configurations. Automatically creates log files under a logs/ directory, named after the entry-point script (e.g., running wsgi.py results in logs/wsgi.log ). Provides a built-in PrettyPrinter helper for human-readable, structured log output. Supports console logging (optional) and custom output paths for log files.","title":"Key Features:"},{"location":"reference/arb/__get_logger/#arb.__get_logger--usage-examples","text":"Import and initialize the logger in any module: from arb import get_logger as get_logger logger, pp_log = get_logger(__name ) logger.debug(\"Basic log message\") logger.debug(pp_log({\"structured\": \"data\", \"for\": \"inspection\"})) To customize behavior: logger, pp_log = get_logger( file_stem= name , log_to_console=True, force_command_line=False, file_path=\"custom_logs/\" )","title":"Usage Examples:"},{"location":"reference/arb/__get_logger/#arb.__get_logger--configuration-behavior","text":"If file_path is provided, logs are written to that directory using the provided file_stem . If file_path is not provided, the logger writes to logs/<stem>.log . If executed from __main__ or __init__ , the log file defaults to logs/app_logger.log . All log files use UTF-8 encoding and include timestamps with millisecond precision.","title":"Configuration Behavior:"},{"location":"reference/arb/__get_logger/#arb.__get_logger--recommendation","text":"Place __get_logger.py near the root of your source tree and import it as early as possible in each module to guarantee consistent logging setup.","title":"Recommendation:"},{"location":"reference/arb/__get_logger/#arb.__get_logger.get_logger","text":"Return a configured logger instance and a structured logging helper. Parameters: file_stem ( str | None , default: 'app_logger' ) \u2013 Name used for the logger and log filename. file_path ( str | Path | None , default: None ) \u2013 Directory path to store logs (default: logs/ ). log_to_console ( bool , default: False ) \u2013 Whether to also output logs to the console. force_command_line ( bool , default: False ) \u2013 Use command-line filename as logger name. Returns: tuple [ Logger , any ] \u2013 tuple[Logger, callable]: A configured logger and a pretty-print function. Notes Logging setup is only performed once per process. Default log filename: logs/<file_stem>.log . Source code in arb\\__get_logger.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def get_logger ( file_stem : str | None = \"app_logger\" , file_path : str | Path | None = None , log_to_console : bool = False , force_command_line : bool = False ) -> tuple [ Logger , any ]: \"\"\" Return a configured logger instance and a structured logging helper. Args: file_stem (str | None): Name used for the logger and log filename. file_path (str | Path | None): Directory path to store logs (default: `logs/`). log_to_console (bool): Whether to also output logs to the console. force_command_line (bool): Use command-line filename as logger name. Returns: tuple[Logger, callable]: A configured logger and a pretty-print function. Notes: - Logging setup is only performed once per process. - Default log filename: `logs/<file_stem>.log`. \"\"\" # log_format_old = \"+%(asctime)s.%(msecs)03d | %(levelname)-8s | %(name)s | %(filename)s | %(lineno)d | %(message)s\" log_format_proposed = \"+ %(asctime)s . %(msecs)03d | %(levelname)-8s | %(name)s | %(filename)s | %(lineno)d | user: %(user)s | %(message)s \" log_format = \"+ %(asctime)s . %(msecs)03d | %(levelname)-8s | %(name)-16s | user:anonymous | %(lineno)-5d | %(filename)-20s | %(message)s \" log_datefmt = \"%Y-%m- %d %H:%M:%S\" # Determine file stem based on command-line script if requested if force_command_line : script_path = Path ( sys . argv [ 0 ]) file_stem = script_path . stem if file_stem in [ None , \"\" , \"__init__\" , \"__main__\" ]: file_stem = \"app_logger\" file_stem = file_stem . replace ( \".\" , \"_\" ) # Default to logs/ directory next to this file if file_path is None : file_path = Path ( __file__ ) . parent / \"logs\" else : file_path = Path ( file_path ) file_name = file_path / f \" { file_stem } .log\" # Create or retrieve logger logger = logging . getLogger ( file_stem ) is_logger_already_configured = bool ( logging . getLogger () . handlers ) if not is_logger_already_configured : file_name . parent . mkdir ( parents = True , exist_ok = True ) logging . basicConfig ( level = logging . DEBUG , format = log_format , datefmt = log_datefmt , filename = str ( file_name ), encoding = \"utf-8\" , ) # Optional console logging (only add if one doesn't already exist) if log_to_console : root_logger = logging . getLogger () has_console_handler = any ( isinstance ( handler , logging . StreamHandler ) and not isinstance ( handler , logging . FileHandler ) for handler in root_logger . handlers ) if not has_console_handler : console_handler = logging . StreamHandler () console_handler . setFormatter ( logging . Formatter ( log_format , datefmt = log_datefmt )) root_logger . addHandler ( console_handler ) logger . debug ( f \"get_logger() called with { file_stem = } , { file_path =} , { log_to_console =} , { force_command_line =} , { sys . argv = } \" ) if is_logger_already_configured : logging . debug ( \"Logging has already been initialized; configuration will not be changed.\" ) else : logging . debug ( f \"Logging was initialized on first usage. Outputting logs to { file_name } \" ) _ , pp_log = get_pretty_printer () return logger , pp_log","title":"get_logger"},{"location":"reference/arb/__get_logger/#arb.__get_logger.get_pretty_printer","text":"Return a PrettyPrinter instance and a formatting function for structured logging. This is useful for debugging or logging nested data structures like dictionaries or deeply nested lists. Parameters: **kwargs ( Any , default: {} ) \u2013 Options passed to pprint.PrettyPrinter , including: - indent (int): Indentation level (default: 4). - sort_dicts (bool): Whether to sort dictionary keys (default: False). - width (int): Max character width per line (default: 120). Returns: tuple [ PrettyPrinter , callable ] \u2013 tuple[PrettyPrinter, callable]: Printer object and its .pformat method. Source code in arb\\__get_logger.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def get_pretty_printer ( ** kwargs : Any ) -> tuple [ pprint . PrettyPrinter , callable ]: \"\"\" Return a `PrettyPrinter` instance and a formatting function for structured logging. This is useful for debugging or logging nested data structures like dictionaries or deeply nested lists. Args: **kwargs (Any): Options passed to `pprint.PrettyPrinter`, including: - indent (int): Indentation level (default: 4). - sort_dicts (bool): Whether to sort dictionary keys (default: False). - width (int): Max character width per line (default: 120). Returns: tuple[PrettyPrinter, callable]: Printer object and its .pformat method. \"\"\" options = { \"indent\" : 4 , \"sort_dicts\" : False , \"width\" : 120 } options . update ( kwargs ) pp = pprint . PrettyPrinter ( ** options ) return pp , pp . pformat","title":"get_pretty_printer"},{"location":"reference/arb/wsgi/","text":"arb.wsgi WSGI entry point for serving the Flask app. This file enables the application to be run via a WSGI server (e.g., Gunicorn or uWSGI) or directly via flask run or python wsgi.py . It provides detailed notes for various execution contexts, Flask CLI behavior, debugging strategies, and developer workflows including PyCharm integration. Note on running Flask Apps: 1) You can run a flask app from the CLI in two ways: python run flask app directly from python without the flask CLI Errors shown in terminal; browser only shows generic 500 unless debug=True in source code flask run uses the Flask CLI makes use of Flask related environment variables and command line arguments 2) Flask configuration precedence: The effective behavior of your Flask app depends on how it's launched and which configuration values are set at various levels. The following precedence applies: Precedence Order (from strongest to weakest): 1. Arguments passed directly to `app.run(...)` - These override everything else, including CLI flags and environment variables. 2. Flask CLI command-line options - e.g., `flask run --port=8000` overrides any FLASK_RUN_PORT setting. 3. Environment variables - e.g., FLASK_ENV, FLASK_DEBUG, FLASK_RUN_PORT 2) Environmental variables and running from the Flask CLI FLASK_APP: sets the default name for the flask app if not specified. \"flask run\" is equivalent to \"flask --app FLASK_APP run\" Likely FLASK_APP=app.py or FLASK_APP=wsgi FLASK_ENV: can be 'development' or 'production' development enables debug mode, auto-reload, and detailed error pages, production disables them. Likely will allways want FLASK_ENV=development for CARB development FLASK_DEBUG: 1 enables the interactive browser debugger (Werkzeug); 0 disables it. PYTHONPATH: Adds directories to Python's module resolution path (sometimes needed for imports) 3) Flask CLI arguments * key options * --app * --debug or --no-debug * determines if the Werkzeug browser debugger is on/off * --no-reload <-- faster load time and does not restart app on source code changes 4) Source code app arguments: Commonly used arguments for `app.run()`: Args: * host (str, optional): The IP address to bind to. Defaults to `'127.0.0.1'`. Use `'0.0.0.0'` to make the app publicly accessible (e.g., on a local network). * port (int, optional): The port number to listen on. Defaults to `5000`. * debug (bool, optional): Enables debug mode, which activates auto-reload and the interactive browser debugger. Defaults to `None`. * use_reloader (bool, optional): Enables the auto-reloader to restart the server on code changes. Defaults to `True` if debug is enabled. * use_debugger (bool, optional): Enables the interactive debugger in the browser when errors occur. Defaults to `True` if debug is enabled. * threaded (bool, optional): Run the server in multithreaded mode. Useful for handling multiple concurrent requests. Defaults to `False`. * processes (int, optional): Number of worker processes for handling requests. Mutually exclusive with `threaded=True`. Defaults to `1`. * load_dotenv (bool, optional): Whether to load environment variables from a `.env` file. Defaults to `True`. 5) Best practices: 1. use app.run(debug=True) in the wsgi file except for official release to 3rd parties * will give you access to Browser-based call trace or python debugger depending on other factors 2. Use 'development' over 'production' until product is final. 3. testing web interactions in browser without python debugger * flask run --app wsgi 4. Debugging with PyCharm (breakpoints + console) * Use a Run Configuration: * Script: wsgi.py * Working Dir: production/arb * Env vars: FLASK_ENV=development, FLASK_DEBUG=0 * app.run(debug=True) in wsgi.py 5. Combined Workflow (PyCharm + Browser) * Run wsgi.py in PyCharm with debug=True * Set FLASK_ENV=development, FLASK_DEBUG=1 * This allows: * PyCharm to log & capture * Browser to display detailed error trace * Breakpoints still work (though sometimes suppressed by Werkzeug internals) 6) Root directory notes: - The project root directory is \"feedback_portal\" - if the app is run from wsgi.py file with path: feedback_portal/source/production/arb/wsgi.py - Path( file ).resolve().parents[3] \u2192 .../feedback_portal todo - work this in, run with: flask --app wsgi run --no-reload","title":"arb.wsgi"},{"location":"reference/arb/wsgi/#arbwsgi","text":"WSGI entry point for serving the Flask app. This file enables the application to be run via a WSGI server (e.g., Gunicorn or uWSGI) or directly via flask run or python wsgi.py . It provides detailed notes for various execution contexts, Flask CLI behavior, debugging strategies, and developer workflows including PyCharm integration. Note on running Flask Apps: 1) You can run a flask app from the CLI in two ways: python run flask app directly from python without the flask CLI Errors shown in terminal; browser only shows generic 500 unless debug=True in source code flask run uses the Flask CLI makes use of Flask related environment variables and command line arguments 2) Flask configuration precedence: The effective behavior of your Flask app depends on how it's launched and which configuration values are set at various levels. The following precedence applies: Precedence Order (from strongest to weakest): 1. Arguments passed directly to `app.run(...)` - These override everything else, including CLI flags and environment variables. 2. Flask CLI command-line options - e.g., `flask run --port=8000` overrides any FLASK_RUN_PORT setting. 3. Environment variables - e.g., FLASK_ENV, FLASK_DEBUG, FLASK_RUN_PORT 2) Environmental variables and running from the Flask CLI FLASK_APP: sets the default name for the flask app if not specified. \"flask run\" is equivalent to \"flask --app FLASK_APP run\" Likely FLASK_APP=app.py or FLASK_APP=wsgi FLASK_ENV: can be 'development' or 'production' development enables debug mode, auto-reload, and detailed error pages, production disables them. Likely will allways want FLASK_ENV=development for CARB development FLASK_DEBUG: 1 enables the interactive browser debugger (Werkzeug); 0 disables it. PYTHONPATH: Adds directories to Python's module resolution path (sometimes needed for imports) 3) Flask CLI arguments * key options * --app * --debug or --no-debug * determines if the Werkzeug browser debugger is on/off * --no-reload <-- faster load time and does not restart app on source code changes 4) Source code app arguments: Commonly used arguments for `app.run()`: Args: * host (str, optional): The IP address to bind to. Defaults to `'127.0.0.1'`. Use `'0.0.0.0'` to make the app publicly accessible (e.g., on a local network). * port (int, optional): The port number to listen on. Defaults to `5000`. * debug (bool, optional): Enables debug mode, which activates auto-reload and the interactive browser debugger. Defaults to `None`. * use_reloader (bool, optional): Enables the auto-reloader to restart the server on code changes. Defaults to `True` if debug is enabled. * use_debugger (bool, optional): Enables the interactive debugger in the browser when errors occur. Defaults to `True` if debug is enabled. * threaded (bool, optional): Run the server in multithreaded mode. Useful for handling multiple concurrent requests. Defaults to `False`. * processes (int, optional): Number of worker processes for handling requests. Mutually exclusive with `threaded=True`. Defaults to `1`. * load_dotenv (bool, optional): Whether to load environment variables from a `.env` file. Defaults to `True`. 5) Best practices: 1. use app.run(debug=True) in the wsgi file except for official release to 3rd parties * will give you access to Browser-based call trace or python debugger depending on other factors 2. Use 'development' over 'production' until product is final. 3. testing web interactions in browser without python debugger * flask run --app wsgi 4. Debugging with PyCharm (breakpoints + console) * Use a Run Configuration: * Script: wsgi.py * Working Dir: production/arb * Env vars: FLASK_ENV=development, FLASK_DEBUG=0 * app.run(debug=True) in wsgi.py 5. Combined Workflow (PyCharm + Browser) * Run wsgi.py in PyCharm with debug=True * Set FLASK_ENV=development, FLASK_DEBUG=1 * This allows: * PyCharm to log & capture * Browser to display detailed error trace * Breakpoints still work (though sometimes suppressed by Werkzeug internals) 6) Root directory notes: - The project root directory is \"feedback_portal\" - if the app is run from wsgi.py file with path: feedback_portal/source/production/arb/wsgi.py - Path( file ).resolve().parents[3] \u2192 .../feedback_portal","title":"arb.wsgi"},{"location":"reference/arb/wsgi/#arb.wsgi--todo-work-this-in-run-with-flask-app-wsgi-run-no-reload","text":"","title":"todo - work this in, run with: flask --app wsgi run --no-reload"},{"location":"reference/arb/portal/app/","text":"arb.portal.app Application factory for the ARB Feedback Portal (Flask app). This module defines the create_app() function, which initializes and configures the Flask application with required extensions, startup behavior, routing, and globals. Key Responsibilities: Load Flask configuration dynamically using get_config() Apply global app settings via configure_flask_app() Initialize SQLAlchemy and optionally CSRF protection Reflect and optionally create the application database schema Load dropdowns and type mappings into the app context Register Flask blueprints (e.g., main ) Usage: Used by WSGI, CLI tools, or testing utilities: from arb.portal.app import create_app app = create_app() create_app () Create and configure the ARB Feedback Portal Flask application. Follows the Flask application factory pattern. This function loads configuration, initializes extensions, binds SQLAlchemy to the app, and registers route blueprints and global utilities. Returns: Flask ( Flask ) \u2013 A fully initialized Flask application instance with: - App context globals (dropdowns, types) - SQLAlchemy base metadata ( app.base ) - Registered routes via blueprints Source code in arb\\portal\\app.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def create_app () -> Flask : \"\"\" Create and configure the ARB Feedback Portal Flask application. Follows the Flask application factory pattern. This function loads configuration, initializes extensions, binds SQLAlchemy to the app, and registers route blueprints and global utilities. Returns: Flask: A fully initialized Flask application instance with: - App context globals (dropdowns, types) - SQLAlchemy base metadata (`app.base`) - Registered routes via blueprints \"\"\" app : Flask = Flask ( __name__ ) # Load configuration from config/settings.py app . config . from_object ( get_config ()) # Setup Jinja2, logging, and app-wide config configure_flask_app ( app ) # Initialize Flask extensions db . init_app ( app ) # GPT recommends this, but I'm commenting it out for now # csrf.init_app(app) # Database initialization and reflection (within app context) with app . app_context (): db_initialize_and_create () reflect_database () # Load dropdowns, mappings, and other global data base : AutomapBase = get_reflected_base ( db ) # reuse db.metadata without hitting DB again app . base = base # \u2705 Attach automap base to app object Globals . load_type_mapping ( app , db , base ) Globals . load_drop_downs ( app , db ) # Register route blueprints app . register_blueprint ( main ) return app","title":"arb.portal.app"},{"location":"reference/arb/portal/app/#arbportalapp","text":"Application factory for the ARB Feedback Portal (Flask app). This module defines the create_app() function, which initializes and configures the Flask application with required extensions, startup behavior, routing, and globals.","title":"arb.portal.app"},{"location":"reference/arb/portal/app/#arb.portal.app--key-responsibilities","text":"Load Flask configuration dynamically using get_config() Apply global app settings via configure_flask_app() Initialize SQLAlchemy and optionally CSRF protection Reflect and optionally create the application database schema Load dropdowns and type mappings into the app context Register Flask blueprints (e.g., main )","title":"Key Responsibilities:"},{"location":"reference/arb/portal/app/#arb.portal.app--usage","text":"Used by WSGI, CLI tools, or testing utilities: from arb.portal.app import create_app app = create_app()","title":"Usage:"},{"location":"reference/arb/portal/app/#arb.portal.app.create_app","text":"Create and configure the ARB Feedback Portal Flask application. Follows the Flask application factory pattern. This function loads configuration, initializes extensions, binds SQLAlchemy to the app, and registers route blueprints and global utilities. Returns: Flask ( Flask ) \u2013 A fully initialized Flask application instance with: - App context globals (dropdowns, types) - SQLAlchemy base metadata ( app.base ) - Registered routes via blueprints Source code in arb\\portal\\app.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def create_app () -> Flask : \"\"\" Create and configure the ARB Feedback Portal Flask application. Follows the Flask application factory pattern. This function loads configuration, initializes extensions, binds SQLAlchemy to the app, and registers route blueprints and global utilities. Returns: Flask: A fully initialized Flask application instance with: - App context globals (dropdowns, types) - SQLAlchemy base metadata (`app.base`) - Registered routes via blueprints \"\"\" app : Flask = Flask ( __name__ ) # Load configuration from config/settings.py app . config . from_object ( get_config ()) # Setup Jinja2, logging, and app-wide config configure_flask_app ( app ) # Initialize Flask extensions db . init_app ( app ) # GPT recommends this, but I'm commenting it out for now # csrf.init_app(app) # Database initialization and reflection (within app context) with app . app_context (): db_initialize_and_create () reflect_database () # Load dropdowns, mappings, and other global data base : AutomapBase = get_reflected_base ( db ) # reuse db.metadata without hitting DB again app . base = base # \u2705 Attach automap base to app object Globals . load_type_mapping ( app , db , base ) Globals . load_drop_downs ( app , db ) # Register route blueprints app . register_blueprint ( main ) return app","title":"create_app"},{"location":"reference/arb/portal/app_util/","text":"arb.portal.app_util Application-specific utility functions for the ARB Feedback Portal. This module provides helpers for resolving sector data, handling file uploads, preparing database rows, and integrating WTForms with SQLAlchemy models. Key Capabilities: Resolve sector and sector_type for an incidence Insert or update rows from Excel/JSON payloads Reflect and verify database schema and rows Track uploaded files via the UploadedFile table Apply filter logic to the portal_updates log view Generate context and form logic for feedback pages Typical Usage: File ingestion and incidence row creation Dynamic form loading from model rows Sector/type resolution from related tables Upload tracking and file diagnostics add_file_to_upload_table ( db , file_name , status = None , description = None ) Insert a record into the UploadedFile table for audit and diagnostics. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. file_name ( str | Path ) \u2013 File path or name to be recorded. status ( str | None , default: None ) \u2013 Optional upload status label. description ( str | None , default: None ) \u2013 Optional notes for the upload event. Returns: None \u2013 None Source code in arb\\portal\\app_util.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def add_file_to_upload_table ( db , file_name : str | Path , status = None , description = None ) -> None : \"\"\" Insert a record into the `UploadedFile` table for audit and diagnostics. Args: db (SQLAlchemy): SQLAlchemy database instance. file_name (str | Path): File path or name to be recorded. status (str | None): Optional upload status label. description (str | None): Optional notes for the upload event. Returns: None \"\"\" # todo (consider) to wrap commit in log? from arb.portal.sqla_models import UploadedFile logger . debug ( f \"Adding uploaded file to upload table: { file_name =} \" ) model_uploaded_file = UploadedFile ( path = str ( file_name ), status = status , description = description , ) db . session . add ( model_uploaded_file ) db . session . commit () logger . debug ( f \" { model_uploaded_file =} \" ) apply_portal_update_filters ( query , PortalUpdate , args ) Apply user-defined filters to a PortalUpdate SQLAlchemy query. Parameters: query ( SQLAlchemy Query ) \u2013 Query to be filtered. PortalUpdate ( Base ) \u2013 ORM model class for the portal_updates table. args ( dict ) \u2013 Typically from request.args , containing filter values. Supported filters Substring matches on key, user, comments ID exact match or range parsing (e.g. \"100-200, 250\") Date range filtering via start_date and end_date Supported ID formats (via filter_id_incidence): \"123\" \u2192 Matches ID 123 exactly \"100-200\" \u2192 Matches IDs from 100 to 200 inclusive \"-250\" \u2192 Matches all IDs \u2264 250 \"300-\" \u2192 Matches all IDs \u2265 300 \"123,150-200,250-\" \u2192 Mixed exacts and ranges \"abc, 100-xyz, 222\" \u2192 Invalid parts are ignored Returns (SQLAlchemy Query): SQLAlchemy query: Modified query with filters applied. Source code in arb\\portal\\app_util.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def apply_portal_update_filters ( query , PortalUpdate , args : dict ): \"\"\" Apply user-defined filters to a `PortalUpdate` SQLAlchemy query. Args: query (SQLAlchemy Query): Query to be filtered. PortalUpdate (Base): ORM model class for the portal_updates table. args (dict): Typically from `request.args`, containing filter values. Supported filters: - Substring matches on key, user, comments - ID exact match or range parsing (e.g. \"100-200, 250\") - Date range filtering via `start_date` and `end_date` Supported ID formats (via filter_id_incidence): ------------------------------------------------ - \"123\" \u2192 Matches ID 123 exactly - \"100-200\" \u2192 Matches IDs from 100 to 200 inclusive - \"-250\" \u2192 Matches all IDs \u2264 250 - \"300-\" \u2192 Matches all IDs \u2265 300 - \"123,150-200,250-\" \u2192 Mixed exacts and ranges - \"abc, 100-xyz, 222\" \u2192 Invalid parts are ignored Returns (SQLAlchemy Query): SQLAlchemy query: Modified query with filters applied. \"\"\" filter_key = args . get ( \"filter_key\" , \"\" ) . strip () filter_user = args . get ( \"filter_user\" , \"\" ) . strip () filter_comments = args . get ( \"filter_comments\" , \"\" ) . strip () filter_id_incidence = args . get ( \"filter_id_incidence\" , \"\" ) . strip () start_date_str = args . get ( \"start_date\" , \"\" ) . strip () end_date_str = args . get ( \"end_date\" , \"\" ) . strip () if filter_key : query = query . filter ( PortalUpdate . key . ilike ( f \"% { filter_key } %\" )) if filter_user : query = query . filter ( PortalUpdate . user . ilike ( f \"% { filter_user } %\" )) if filter_comments : query = query . filter ( PortalUpdate . comments . ilike ( f \"% { filter_comments } %\" )) if filter_id_incidence : id_exact = set () id_range_clauses = [] for part in filter_id_incidence . split ( \",\" ): part = part . strip () if not part : continue if \"-\" in part : try : start , end = part . split ( \"-\" ) start = start . strip () end = end . strip () if start and end : start_val = int ( start ) end_val = int ( end ) if start_val <= end_val : id_range_clauses . append ( PortalUpdate . id_incidence . between ( start_val , end_val )) elif start : start_val = int ( start ) id_range_clauses . append ( PortalUpdate . id_incidence >= start_val ) elif end : end_val = int ( end ) id_range_clauses . append ( PortalUpdate . id_incidence <= end_val ) except ValueError : continue # Ignore malformed part elif part . isdigit (): id_exact . add ( int ( part )) clause_list = [] if id_exact : clause_list . append ( PortalUpdate . id_incidence . in_ ( sorted ( id_exact ))) clause_list . extend ( id_range_clauses ) if clause_list : query = query . filter ( or_ ( * clause_list )) try : if start_date_str : start_dt = datetime . strptime ( start_date_str , \"%Y-%m- %d \" ) query = query . filter ( PortalUpdate . timestamp >= start_dt ) if end_date_str : end_dt = datetime . strptime ( end_date_str , \"%Y-%m- %d \" ) end_dt = end_dt . replace ( hour = 23 , minute = 59 , second = 59 ) query = query . filter ( PortalUpdate . timestamp <= end_dt ) except ValueError : pass # Silently ignore invalid date inputs return query dict_to_database ( db , base , data_dict , table_name = 'incidences' , primary_key = 'id_incidence' , json_field = 'misc_json' ) Insert or update a row in the specified table using a dictionary payload. The payload is merged into a model instance and committed to the database. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. base ( AutomapBase ) \u2013 Reflected SQLAlchemy base metadata. data_dict ( dict ) \u2013 Dictionary containing payload data. table_name ( str , default: 'incidences' ) \u2013 Table name to modify. Defaults to 'incidences'. primary_key ( str , default: 'id_incidence' ) \u2013 Name of the primary key field. Defaults to 'id_incidence'. json_field ( str , default: 'misc_json' ) \u2013 Name of the JSON field to update. Defaults to 'misc_json'. Returns: int ( int ) \u2013 Final value of the primary key for the affected row. Raises: ValueError \u2013 If data_dict is empty. AttributeError \u2013 If the resulting model does not expose the primary key. Source code in arb\\portal\\app_util.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 def dict_to_database ( db , base , data_dict : dict , table_name = \"incidences\" , primary_key = \"id_incidence\" , json_field = \"misc_json\" ) -> int : \"\"\" Insert or update a row in the specified table using a dictionary payload. The payload is merged into a model instance and committed to the database. Args: db (SQLAlchemy): SQLAlchemy database instance. base (AutomapBase): Reflected SQLAlchemy base metadata. data_dict (dict): Dictionary containing payload data. table_name (str): Table name to modify. Defaults to 'incidences'. primary_key (str): Name of the primary key field. Defaults to 'id_incidence'. json_field (str): Name of the JSON field to update. Defaults to 'misc_json'. Returns: int: Final value of the primary key for the affected row. Raises: ValueError: If data_dict is empty. AttributeError: If the resulting model does not expose the primary key. \"\"\" from arb.utils.wtf_forms_util import update_model_with_payload if not data_dict : msg = \"Attempt to add empty entry to database\" logger . warning ( msg ) raise ValueError ( msg ) id_ = data_dict . get ( primary_key ) model , id_ , is_new_row = get_ensured_row ( db = db , base = base , table_name = table_name , primary_key_name = primary_key , id_ = id_ ) # Backfill generated primary key into payload if it was not supplied if is_new_row : logger . debug ( f \"Backfilling { primary_key } = { id_ } into payload\" ) data_dict [ primary_key ] = id_ update_model_with_payload ( model , data_dict , json_field = json_field ) session = db . session session . add ( model ) session . commit () # Final safety: extract final PK from model try : return getattr ( model , primary_key ) except AttributeError as e : logger . error ( f \"Model has no attribute ' { primary_key } ': { e } \" ) raise get_ensured_row ( db , base , table_name = 'incidences' , primary_key_name = 'id_incidence' , id_ = None ) Retrieve or create a row in the specified table using a primary key. If the row exists, it is returned. Otherwise, a new row is created and committed. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. base ( AutomapBase ) \u2013 Reflected SQLAlchemy base metadata. table_name ( str , default: 'incidences' ) \u2013 Table name to operate on. Defaults to 'incidences'. primary_key_name ( str , default: 'id_incidence' ) \u2013 Name of the primary key column. Defaults to 'id_incidence'. id_ ( int | None , default: None ) \u2013 Primary key value. If None, a new row is created. Returns: tuple ( tuple ) \u2013 (model, id_, is_new_row) - model: SQLAlchemy ORM instance - id_: Primary key value - is_new_row: Whether a new row was created (True/False) Raises: AttributeError \u2013 If the model class lacks the specified primary key. Source code in arb\\portal\\app_util.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def get_ensured_row ( db , base , table_name = \"incidences\" , primary_key_name = \"id_incidence\" , id_ = None ) -> tuple : \"\"\" Retrieve or create a row in the specified table using a primary key. If the row exists, it is returned. Otherwise, a new row is created and committed. Args: db (SQLAlchemy): SQLAlchemy database instance. base (AutomapBase): Reflected SQLAlchemy base metadata. table_name (str): Table name to operate on. Defaults to 'incidences'. primary_key_name (str): Name of the primary key column. Defaults to 'id_incidence'. id_ (int | None): Primary key value. If None, a new row is created. Returns: tuple: (model, id_, is_new_row) - model: SQLAlchemy ORM instance - id_: Primary key value - is_new_row: Whether a new row was created (True/False) Raises: AttributeError: If the model class lacks the specified primary key. \"\"\" is_new_row = False session = db . session table = get_class_from_table_name ( base , table_name ) if id_ is not None : logger . debug ( f \"Retrieving { table_name } row with { primary_key_name } = { id_ } \" ) model = session . get ( table , id_ ) if model is None : is_new_row = True logger . debug ( f \"No existing row found; creating new { table_name } row with { primary_key_name } = { id_ } \" ) model = table ( ** { primary_key_name : id_ }) else : is_new_row = True logger . debug ( f \"Creating new { table_name } row with auto-generated { primary_key_name } \" ) model = table ( ** { primary_key_name : None }) session . add ( model ) session . commit () id_ = getattr ( model , primary_key_name ) logger . debug ( f \" { table_name } row created with { primary_key_name } = { id_ } \" ) return model , id_ , is_new_row get_sector_info ( db , base , id_ ) Resolve the sector and sector_type for a given incidence ID. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. base ( AutomapBase ) \u2013 SQLAlchemy automapped declarative base. id_ ( int ) \u2013 ID of the row in the incidences table. Returns: tuple [ str , str ] \u2013 tuple[str, str]: (sector, sector_type) Source code in arb\\portal\\app_util.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_sector_info ( db : SQLAlchemy , base : AutomapBase , id_ : int ) -> tuple [ str , str ]: \"\"\" Resolve the sector and sector_type for a given incidence ID. Args: db (SQLAlchemy): SQLAlchemy database instance. base (AutomapBase): SQLAlchemy automapped declarative base. id_ (int): ID of the row in the `incidences` table. Returns: tuple[str, str]: (sector, sector_type) \"\"\" logger . debug ( f \"get_sector_info() called to determine sector & sector type for { id_ =} \" ) primary_table_name = \"incidences\" json_column = \"misc_json\" sector = None sector_type = None # Find the sector from the foreign table if incidence was created by plume tracker. sector_by_foreign_key = get_foreign_value ( db , base , primary_table_name = primary_table_name , foreign_table_name = \"sources\" , primary_table_fk_name = \"source_id\" , foreign_table_column_name = \"sector\" , primary_table_pk_value = id_ , ) # Get the row and misc_json field from the incidence table row , misc_json = get_table_row_and_column ( db , base , table_name = primary_table_name , column_name = json_column , id_ = id_ , ) if misc_json is None : misc_json = {} sector = resolve_sector ( sector_by_foreign_key , row , misc_json ) sector_type = get_sector_type ( sector ) logger . debug ( f \"get_sector_info() returning { sector =} { sector_type =} \" ) return sector , sector_type get_sector_type ( sector ) Map a sector name to its broad classification. Parameters: sector ( str ) \u2013 Input sector label. Returns: str ( str ) \u2013 One of \"Oil & Gas\" or \"Landfill\". Raises: ValueError \u2013 On unknown sector input. Source code in arb\\portal\\app_util.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def get_sector_type ( sector : str ) -> str : \"\"\" Map a sector name to its broad classification. Args: sector (str): Input sector label. Returns: str: One of \"Oil & Gas\" or \"Landfill\". Raises: ValueError: On unknown sector input. \"\"\" if sector in OIL_AND_GAS_SECTORS : return \"Oil & Gas\" elif sector in LANDFILL_SECTORS : return \"Landfill\" else : raise ValueError ( f \"Unknown sector type: ' { sector } '.\" ) incidence_prep ( model_row , crud_type , sector_type , default_dropdown ) Generate the context and render the HTML template for a feedback record. Populates WTForms fields from the model and applies validation logic depending on the request method (GET/POST). Integrates conditional dropdown resets, CSRF-less validation, and feedback record persistence. Parameters: model_row ( DeclarativeMeta ) \u2013 SQLAlchemy model row for the feedback entry. crud_type ( str ) \u2013 'create' or 'update'. sector_type ( str ) \u2013 'Oil & Gas' or 'Landfill'. default_dropdown ( str ) \u2013 Value used to fill in blank selects. Returns: str ( str ) \u2013 Rendered HTML from the appropriate feedback template. Raises: ValueError \u2013 If the sector type is invalid. Source code in arb\\portal\\app_util.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 def incidence_prep ( model_row : DeclarativeMeta , crud_type : str , sector_type : str , default_dropdown : str ) -> str : \"\"\" Generate the context and render the HTML template for a feedback record. Populates WTForms fields from the model and applies validation logic depending on the request method (GET/POST). Integrates conditional dropdown resets, CSRF-less validation, and feedback record persistence. Args: model_row (DeclarativeMeta): SQLAlchemy model row for the feedback entry. crud_type (str): 'create' or 'update'. sector_type (str): 'Oil & Gas' or 'Landfill'. default_dropdown (str): Value used to fill in blank selects. Returns: str: Rendered HTML from the appropriate feedback template. Raises: ValueError: If the sector type is invalid. \"\"\" # imports below can't be moved to top of file because they require Globals to be initialized # prior to first use (Globals.load_drop_downs(app, db)). from arb.portal.wtf_landfill import LandfillFeedback from arb.portal.wtf_oil_and_gas import OGFeedback logger . debug ( f \"incidence_prep() called with { crud_type =} , { sector_type =} \" ) sa_model_diagnostics ( model_row ) if default_dropdown is None : default_dropdown = PLEASE_SELECT if sector_type == \"Oil & Gas\" : logger . debug ( f \"( { sector_type =} ) will use an Oil & Gas Feedback Form\" ) wtf_form = OGFeedback () template_file = 'feedback_oil_and_gas.html' elif sector_type == \"Landfill\" : logger . debug ( f \"( { sector_type =} ) will use a Landfill Feedback Form\" ) wtf_form = LandfillFeedback () template_file = 'feedback_landfill.html' else : raise ValueError ( f \"Unknown sector type: ' { sector_type } '.\" ) if request . method == 'GET' : # Populate wtform from model data model_to_wtform ( model_row , wtf_form ) # todo - maybe put update contingencies here? # obj_diagnostics(wtf_form, message=\"wtf_form in incidence_prep() after model_to_wtform\") # For GET requests for row creation, don't validate and error_count_dict will be all zeros # For GET requests for row update, validate (except for the csrf token that is only present for a POST) if crud_type == 'update' : validate_no_csrf ( wtf_form , extra_validators = None ) # todo - trying to make sure invalid drop-downs become \"Please Select\" # may want to look into using validate_no_csrf or initialize_drop_downs (or combo) # Set all select elements that are a default value (None) to \"Please Select\" value initialize_drop_downs ( wtf_form , default = default_dropdown ) # logger.debug(f\"\\n\\t{wtf_form.data=}\") if request . method == 'POST' : # Validate and count errors wtf_form . validate () error_count_dict = wtf_count_errors ( wtf_form , log_errors = True ) # Diagnostics of model before updating with wtform values # Likely can comment out model_before and add_commit_and_log_model # if you want less diagnostics and redundant commits model_before = sa_model_to_dict ( model_row ) wtform_to_model ( model_row , wtf_form , ignore_fields = [ \"id_incidence\" ]) add_commit_and_log_model ( db , model_row , comment = 'call to wtform_to_model()' , model_before = model_before ) # Determine course of action for successful database update based on which button was submitted button = request . form . get ( 'submit_button' ) # todo - change the button name to save? if button == 'validate_and_submit' : logger . debug ( f \"validate_and_submit was pressed\" ) if wtf_form . validate (): return redirect ( url_for ( 'main.index' )) error_count_dict = wtf_count_errors ( wtf_form , log_errors = True ) logger . debug ( f \"incidence_prep() about to render get template\" ) return render_template ( template_file , wtf_form = wtf_form , crud_type = crud_type , error_count_dict = error_count_dict , id_incidence = model_row . id_incidence , ) json_file_to_db ( db , file_name , base ) Load a JSON file and insert its contents into the incidences table. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy session used to commit the new row. file_name ( str | Path ) \u2013 Path to the JSON file on disk. base ( AutomapBase ) \u2013 SQLAlchemy automapped metadata base. Returns: tuple [ int , str ] \u2013 tuple[int, str]: The (id_incidence, sector) extracted from the inserted row. Raises: FileNotFoundError \u2013 If the specified file path does not exist. JSONDecodeError \u2013 If the file is not valid JSON. Source code in arb\\portal\\app_util.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def json_file_to_db ( db : SQLAlchemy , file_name : str | Path , base : AutomapBase ) -> tuple [ int , str ]: \"\"\" Load a JSON file and insert its contents into the `incidences` table. Args: db (SQLAlchemy): SQLAlchemy session used to commit the new row. file_name (str | Path): Path to the JSON file on disk. base (AutomapBase): SQLAlchemy automapped metadata base. Returns: tuple[int, str]: The (id_incidence, sector) extracted from the inserted row. Raises: FileNotFoundError: If the specified file path does not exist. json.JSONDecodeError: If the file is not valid JSON. \"\"\" json_as_dict , metadata = json_load_with_meta ( file_name ) return xl_dict_to_database ( db , base , json_as_dict ) resolve_sector ( sector_by_foreign_key , row , misc_json ) Determine the appropriate sector from FK and JSON sources. Parameters: sector_by_foreign_key ( str | None ) \u2013 Sector from sources table. row ( DeclarativeMeta ) \u2013 Row from incidences table (SQLAlchemy result). misc_json ( dict ) \u2013 Parsed misc_json content. Returns: str ( str ) \u2013 Sector string. Raises: ValueError \u2013 If values are missing or conflict. Source code in arb\\portal\\app_util.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def resolve_sector ( sector_by_foreign_key : str | None , row : DeclarativeMeta , misc_json : dict ) -> str : \"\"\" Determine the appropriate sector from FK and JSON sources. Args: sector_by_foreign_key (str | None): Sector from `sources` table. row (DeclarativeMeta): Row from `incidences` table (SQLAlchemy result). misc_json (dict): Parsed `misc_json` content. Returns: str: Sector string. Raises: ValueError: If values are missing or conflict. \"\"\" logger . debug ( f \"resolve_sector() called with { sector_by_foreign_key =} , { row =} , { misc_json =} \" ) sector = None sector_by_json = misc_json . get ( \"sector\" ) if sector_by_foreign_key is None : logger . warning ( \"sector column value in sources table is None.\" ) if sector_by_json is None : logger . warning ( \"'sector' not in misc_json\" ) if sector_by_foreign_key is None and sector_by_json is None : logger . error ( \"Can't determine incidence sector\" ) raise ValueError ( \"Can't determine incidence sector\" ) if sector_by_foreign_key is not None and sector_by_json is not None : if sector_by_foreign_key != sector_by_json : logger . error ( f \"Sector mismatch: { sector_by_foreign_key =} , { sector_by_json =} \" ) raise ValueError ( \"Can't determine incidence sector\" ) sector = sector_by_foreign_key or sector_by_json logger . debug ( f \"resolve_sector() returning { sector =} \" ) return sector upload_and_update_db ( db , upload_dir , request_file , base ) Save uploaded file, parse contents, and insert or update DB rows. Parameters: db ( SQLAlchemy ) \u2013 Database instance. upload_dir ( str | Path ) \u2013 Directory where file will be saved. request_file ( FileStorage ) \u2013 Flask request.files[...] object. base ( AutomapBase ) \u2013 Automapped schema metadata. Returns: tuple [ str , int | None, str | None] \u2013 tuple[str, int | None, str | None]: Filename, id_incidence, sector. Source code in arb\\portal\\app_util.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def upload_and_update_db ( db : SQLAlchemy , upload_dir : str | Path , request_file : FileStorage , base : AutomapBase ) -> tuple [ str , int | None , str | None ]: \"\"\" Save uploaded file, parse contents, and insert or update DB rows. Args: db (SQLAlchemy): Database instance. upload_dir (str | Path): Directory where file will be saved. request_file (FileStorage): Flask `request.files[...]` object. base (AutomapBase): Automapped schema metadata. Returns: tuple[str, int | None, str | None]: Filename, id_incidence, sector. \"\"\" logger . debug ( f \"upload_and_update_db() called with { request_file =} \" ) id_ = None sector = None file_name = upload_single_file ( upload_dir , request_file ) add_file_to_upload_table ( db , file_name , status = \"File Added\" , description = None ) # if file is xl and can be converted to json, # save a json version of the file and return the filename json_file_name = get_json_file_name ( file_name ) if json_file_name : id_ , sector = json_file_to_db ( db , json_file_name , base ) return file_name , id_ , sector xl_dict_to_database ( db , base , xl_dict , tab_name ) Insert or update a row from an Excel-parsed JSON dictionary into the database. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. base ( AutomapBase ) \u2013 Reflected SQLAlchemy base metadata. xl_dict ( dict ) \u2013 Parsed Excel document with 'metadata' and 'tab_contents'. tab_name ( str ) \u2013 Name of the worksheet tab to extract. Returns: tuple [ int , str ] \u2013 tuple[int, str]: Tuple of (id_incidence, sector) after row insertion. Source code in arb\\portal\\app_util.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def xl_dict_to_database ( db , base , xl_dict : dict , tab_name : str ) -> tuple [ int , str ]: \"\"\" Insert or update a row from an Excel-parsed JSON dictionary into the database. Args: db (SQLAlchemy): SQLAlchemy database instance. base (AutomapBase): Reflected SQLAlchemy base metadata. xl_dict (dict): Parsed Excel document with 'metadata' and 'tab_contents'. tab_name (str): Name of the worksheet tab to extract. Returns: tuple[int, str]: Tuple of (id_incidence, sector) after row insertion. \"\"\" logger . debug ( f \"xl_dict_to_database() called with { xl_dict =} \" ) metadata = xl_dict [ \"metadata\" ] sector = metadata [ \"sector\" ] tab_data = xl_dict [ \"tab_contents\" ][ tab_name ] tab_data [ \"sector\" ] = sector id_ = dict_to_database ( db , base , tab_data ) return id_ , sector","title":"arb.portal.app_util"},{"location":"reference/arb/portal/app_util/#arbportalapp_util","text":"Application-specific utility functions for the ARB Feedback Portal. This module provides helpers for resolving sector data, handling file uploads, preparing database rows, and integrating WTForms with SQLAlchemy models.","title":"arb.portal.app_util"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util--key-capabilities","text":"Resolve sector and sector_type for an incidence Insert or update rows from Excel/JSON payloads Reflect and verify database schema and rows Track uploaded files via the UploadedFile table Apply filter logic to the portal_updates log view Generate context and form logic for feedback pages","title":"Key Capabilities:"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util--typical-usage","text":"File ingestion and incidence row creation Dynamic form loading from model rows Sector/type resolution from related tables Upload tracking and file diagnostics","title":"Typical Usage:"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.add_file_to_upload_table","text":"Insert a record into the UploadedFile table for audit and diagnostics. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. file_name ( str | Path ) \u2013 File path or name to be recorded. status ( str | None , default: None ) \u2013 Optional upload status label. description ( str | None , default: None ) \u2013 Optional notes for the upload event. Returns: None \u2013 None Source code in arb\\portal\\app_util.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 def add_file_to_upload_table ( db , file_name : str | Path , status = None , description = None ) -> None : \"\"\" Insert a record into the `UploadedFile` table for audit and diagnostics. Args: db (SQLAlchemy): SQLAlchemy database instance. file_name (str | Path): File path or name to be recorded. status (str | None): Optional upload status label. description (str | None): Optional notes for the upload event. Returns: None \"\"\" # todo (consider) to wrap commit in log? from arb.portal.sqla_models import UploadedFile logger . debug ( f \"Adding uploaded file to upload table: { file_name =} \" ) model_uploaded_file = UploadedFile ( path = str ( file_name ), status = status , description = description , ) db . session . add ( model_uploaded_file ) db . session . commit () logger . debug ( f \" { model_uploaded_file =} \" )","title":"add_file_to_upload_table"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.apply_portal_update_filters","text":"Apply user-defined filters to a PortalUpdate SQLAlchemy query. Parameters: query ( SQLAlchemy Query ) \u2013 Query to be filtered. PortalUpdate ( Base ) \u2013 ORM model class for the portal_updates table. args ( dict ) \u2013 Typically from request.args , containing filter values. Supported filters Substring matches on key, user, comments ID exact match or range parsing (e.g. \"100-200, 250\") Date range filtering via start_date and end_date","title":"apply_portal_update_filters"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.apply_portal_update_filters--supported-id-formats-via-filter_id_incidence","text":"\"123\" \u2192 Matches ID 123 exactly \"100-200\" \u2192 Matches IDs from 100 to 200 inclusive \"-250\" \u2192 Matches all IDs \u2264 250 \"300-\" \u2192 Matches all IDs \u2265 300 \"123,150-200,250-\" \u2192 Mixed exacts and ranges \"abc, 100-xyz, 222\" \u2192 Invalid parts are ignored Returns (SQLAlchemy Query): SQLAlchemy query: Modified query with filters applied. Source code in arb\\portal\\app_util.py 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 def apply_portal_update_filters ( query , PortalUpdate , args : dict ): \"\"\" Apply user-defined filters to a `PortalUpdate` SQLAlchemy query. Args: query (SQLAlchemy Query): Query to be filtered. PortalUpdate (Base): ORM model class for the portal_updates table. args (dict): Typically from `request.args`, containing filter values. Supported filters: - Substring matches on key, user, comments - ID exact match or range parsing (e.g. \"100-200, 250\") - Date range filtering via `start_date` and `end_date` Supported ID formats (via filter_id_incidence): ------------------------------------------------ - \"123\" \u2192 Matches ID 123 exactly - \"100-200\" \u2192 Matches IDs from 100 to 200 inclusive - \"-250\" \u2192 Matches all IDs \u2264 250 - \"300-\" \u2192 Matches all IDs \u2265 300 - \"123,150-200,250-\" \u2192 Mixed exacts and ranges - \"abc, 100-xyz, 222\" \u2192 Invalid parts are ignored Returns (SQLAlchemy Query): SQLAlchemy query: Modified query with filters applied. \"\"\" filter_key = args . get ( \"filter_key\" , \"\" ) . strip () filter_user = args . get ( \"filter_user\" , \"\" ) . strip () filter_comments = args . get ( \"filter_comments\" , \"\" ) . strip () filter_id_incidence = args . get ( \"filter_id_incidence\" , \"\" ) . strip () start_date_str = args . get ( \"start_date\" , \"\" ) . strip () end_date_str = args . get ( \"end_date\" , \"\" ) . strip () if filter_key : query = query . filter ( PortalUpdate . key . ilike ( f \"% { filter_key } %\" )) if filter_user : query = query . filter ( PortalUpdate . user . ilike ( f \"% { filter_user } %\" )) if filter_comments : query = query . filter ( PortalUpdate . comments . ilike ( f \"% { filter_comments } %\" )) if filter_id_incidence : id_exact = set () id_range_clauses = [] for part in filter_id_incidence . split ( \",\" ): part = part . strip () if not part : continue if \"-\" in part : try : start , end = part . split ( \"-\" ) start = start . strip () end = end . strip () if start and end : start_val = int ( start ) end_val = int ( end ) if start_val <= end_val : id_range_clauses . append ( PortalUpdate . id_incidence . between ( start_val , end_val )) elif start : start_val = int ( start ) id_range_clauses . append ( PortalUpdate . id_incidence >= start_val ) elif end : end_val = int ( end ) id_range_clauses . append ( PortalUpdate . id_incidence <= end_val ) except ValueError : continue # Ignore malformed part elif part . isdigit (): id_exact . add ( int ( part )) clause_list = [] if id_exact : clause_list . append ( PortalUpdate . id_incidence . in_ ( sorted ( id_exact ))) clause_list . extend ( id_range_clauses ) if clause_list : query = query . filter ( or_ ( * clause_list )) try : if start_date_str : start_dt = datetime . strptime ( start_date_str , \"%Y-%m- %d \" ) query = query . filter ( PortalUpdate . timestamp >= start_dt ) if end_date_str : end_dt = datetime . strptime ( end_date_str , \"%Y-%m- %d \" ) end_dt = end_dt . replace ( hour = 23 , minute = 59 , second = 59 ) query = query . filter ( PortalUpdate . timestamp <= end_dt ) except ValueError : pass # Silently ignore invalid date inputs return query","title":"Supported ID formats (via filter_id_incidence):"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.dict_to_database","text":"Insert or update a row in the specified table using a dictionary payload. The payload is merged into a model instance and committed to the database. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. base ( AutomapBase ) \u2013 Reflected SQLAlchemy base metadata. data_dict ( dict ) \u2013 Dictionary containing payload data. table_name ( str , default: 'incidences' ) \u2013 Table name to modify. Defaults to 'incidences'. primary_key ( str , default: 'id_incidence' ) \u2013 Name of the primary key field. Defaults to 'id_incidence'. json_field ( str , default: 'misc_json' ) \u2013 Name of the JSON field to update. Defaults to 'misc_json'. Returns: int ( int ) \u2013 Final value of the primary key for the affected row. Raises: ValueError \u2013 If data_dict is empty. AttributeError \u2013 If the resulting model does not expose the primary key. Source code in arb\\portal\\app_util.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 def dict_to_database ( db , base , data_dict : dict , table_name = \"incidences\" , primary_key = \"id_incidence\" , json_field = \"misc_json\" ) -> int : \"\"\" Insert or update a row in the specified table using a dictionary payload. The payload is merged into a model instance and committed to the database. Args: db (SQLAlchemy): SQLAlchemy database instance. base (AutomapBase): Reflected SQLAlchemy base metadata. data_dict (dict): Dictionary containing payload data. table_name (str): Table name to modify. Defaults to 'incidences'. primary_key (str): Name of the primary key field. Defaults to 'id_incidence'. json_field (str): Name of the JSON field to update. Defaults to 'misc_json'. Returns: int: Final value of the primary key for the affected row. Raises: ValueError: If data_dict is empty. AttributeError: If the resulting model does not expose the primary key. \"\"\" from arb.utils.wtf_forms_util import update_model_with_payload if not data_dict : msg = \"Attempt to add empty entry to database\" logger . warning ( msg ) raise ValueError ( msg ) id_ = data_dict . get ( primary_key ) model , id_ , is_new_row = get_ensured_row ( db = db , base = base , table_name = table_name , primary_key_name = primary_key , id_ = id_ ) # Backfill generated primary key into payload if it was not supplied if is_new_row : logger . debug ( f \"Backfilling { primary_key } = { id_ } into payload\" ) data_dict [ primary_key ] = id_ update_model_with_payload ( model , data_dict , json_field = json_field ) session = db . session session . add ( model ) session . commit () # Final safety: extract final PK from model try : return getattr ( model , primary_key ) except AttributeError as e : logger . error ( f \"Model has no attribute ' { primary_key } ': { e } \" ) raise","title":"dict_to_database"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.get_ensured_row","text":"Retrieve or create a row in the specified table using a primary key. If the row exists, it is returned. Otherwise, a new row is created and committed. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. base ( AutomapBase ) \u2013 Reflected SQLAlchemy base metadata. table_name ( str , default: 'incidences' ) \u2013 Table name to operate on. Defaults to 'incidences'. primary_key_name ( str , default: 'id_incidence' ) \u2013 Name of the primary key column. Defaults to 'id_incidence'. id_ ( int | None , default: None ) \u2013 Primary key value. If None, a new row is created. Returns: tuple ( tuple ) \u2013 (model, id_, is_new_row) - model: SQLAlchemy ORM instance - id_: Primary key value - is_new_row: Whether a new row was created (True/False) Raises: AttributeError \u2013 If the model class lacks the specified primary key. Source code in arb\\portal\\app_util.py 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def get_ensured_row ( db , base , table_name = \"incidences\" , primary_key_name = \"id_incidence\" , id_ = None ) -> tuple : \"\"\" Retrieve or create a row in the specified table using a primary key. If the row exists, it is returned. Otherwise, a new row is created and committed. Args: db (SQLAlchemy): SQLAlchemy database instance. base (AutomapBase): Reflected SQLAlchemy base metadata. table_name (str): Table name to operate on. Defaults to 'incidences'. primary_key_name (str): Name of the primary key column. Defaults to 'id_incidence'. id_ (int | None): Primary key value. If None, a new row is created. Returns: tuple: (model, id_, is_new_row) - model: SQLAlchemy ORM instance - id_: Primary key value - is_new_row: Whether a new row was created (True/False) Raises: AttributeError: If the model class lacks the specified primary key. \"\"\" is_new_row = False session = db . session table = get_class_from_table_name ( base , table_name ) if id_ is not None : logger . debug ( f \"Retrieving { table_name } row with { primary_key_name } = { id_ } \" ) model = session . get ( table , id_ ) if model is None : is_new_row = True logger . debug ( f \"No existing row found; creating new { table_name } row with { primary_key_name } = { id_ } \" ) model = table ( ** { primary_key_name : id_ }) else : is_new_row = True logger . debug ( f \"Creating new { table_name } row with auto-generated { primary_key_name } \" ) model = table ( ** { primary_key_name : None }) session . add ( model ) session . commit () id_ = getattr ( model , primary_key_name ) logger . debug ( f \" { table_name } row created with { primary_key_name } = { id_ } \" ) return model , id_ , is_new_row","title":"get_ensured_row"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.get_sector_info","text":"Resolve the sector and sector_type for a given incidence ID. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. base ( AutomapBase ) \u2013 SQLAlchemy automapped declarative base. id_ ( int ) \u2013 ID of the row in the incidences table. Returns: tuple [ str , str ] \u2013 tuple[str, str]: (sector, sector_type) Source code in arb\\portal\\app_util.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def get_sector_info ( db : SQLAlchemy , base : AutomapBase , id_ : int ) -> tuple [ str , str ]: \"\"\" Resolve the sector and sector_type for a given incidence ID. Args: db (SQLAlchemy): SQLAlchemy database instance. base (AutomapBase): SQLAlchemy automapped declarative base. id_ (int): ID of the row in the `incidences` table. Returns: tuple[str, str]: (sector, sector_type) \"\"\" logger . debug ( f \"get_sector_info() called to determine sector & sector type for { id_ =} \" ) primary_table_name = \"incidences\" json_column = \"misc_json\" sector = None sector_type = None # Find the sector from the foreign table if incidence was created by plume tracker. sector_by_foreign_key = get_foreign_value ( db , base , primary_table_name = primary_table_name , foreign_table_name = \"sources\" , primary_table_fk_name = \"source_id\" , foreign_table_column_name = \"sector\" , primary_table_pk_value = id_ , ) # Get the row and misc_json field from the incidence table row , misc_json = get_table_row_and_column ( db , base , table_name = primary_table_name , column_name = json_column , id_ = id_ , ) if misc_json is None : misc_json = {} sector = resolve_sector ( sector_by_foreign_key , row , misc_json ) sector_type = get_sector_type ( sector ) logger . debug ( f \"get_sector_info() returning { sector =} { sector_type =} \" ) return sector , sector_type","title":"get_sector_info"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.get_sector_type","text":"Map a sector name to its broad classification. Parameters: sector ( str ) \u2013 Input sector label. Returns: str ( str ) \u2013 One of \"Oil & Gas\" or \"Landfill\". Raises: ValueError \u2013 On unknown sector input. Source code in arb\\portal\\app_util.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def get_sector_type ( sector : str ) -> str : \"\"\" Map a sector name to its broad classification. Args: sector (str): Input sector label. Returns: str: One of \"Oil & Gas\" or \"Landfill\". Raises: ValueError: On unknown sector input. \"\"\" if sector in OIL_AND_GAS_SECTORS : return \"Oil & Gas\" elif sector in LANDFILL_SECTORS : return \"Landfill\" else : raise ValueError ( f \"Unknown sector type: ' { sector } '.\" )","title":"get_sector_type"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.incidence_prep","text":"Generate the context and render the HTML template for a feedback record. Populates WTForms fields from the model and applies validation logic depending on the request method (GET/POST). Integrates conditional dropdown resets, CSRF-less validation, and feedback record persistence. Parameters: model_row ( DeclarativeMeta ) \u2013 SQLAlchemy model row for the feedback entry. crud_type ( str ) \u2013 'create' or 'update'. sector_type ( str ) \u2013 'Oil & Gas' or 'Landfill'. default_dropdown ( str ) \u2013 Value used to fill in blank selects. Returns: str ( str ) \u2013 Rendered HTML from the appropriate feedback template. Raises: ValueError \u2013 If the sector type is invalid. Source code in arb\\portal\\app_util.py 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 def incidence_prep ( model_row : DeclarativeMeta , crud_type : str , sector_type : str , default_dropdown : str ) -> str : \"\"\" Generate the context and render the HTML template for a feedback record. Populates WTForms fields from the model and applies validation logic depending on the request method (GET/POST). Integrates conditional dropdown resets, CSRF-less validation, and feedback record persistence. Args: model_row (DeclarativeMeta): SQLAlchemy model row for the feedback entry. crud_type (str): 'create' or 'update'. sector_type (str): 'Oil & Gas' or 'Landfill'. default_dropdown (str): Value used to fill in blank selects. Returns: str: Rendered HTML from the appropriate feedback template. Raises: ValueError: If the sector type is invalid. \"\"\" # imports below can't be moved to top of file because they require Globals to be initialized # prior to first use (Globals.load_drop_downs(app, db)). from arb.portal.wtf_landfill import LandfillFeedback from arb.portal.wtf_oil_and_gas import OGFeedback logger . debug ( f \"incidence_prep() called with { crud_type =} , { sector_type =} \" ) sa_model_diagnostics ( model_row ) if default_dropdown is None : default_dropdown = PLEASE_SELECT if sector_type == \"Oil & Gas\" : logger . debug ( f \"( { sector_type =} ) will use an Oil & Gas Feedback Form\" ) wtf_form = OGFeedback () template_file = 'feedback_oil_and_gas.html' elif sector_type == \"Landfill\" : logger . debug ( f \"( { sector_type =} ) will use a Landfill Feedback Form\" ) wtf_form = LandfillFeedback () template_file = 'feedback_landfill.html' else : raise ValueError ( f \"Unknown sector type: ' { sector_type } '.\" ) if request . method == 'GET' : # Populate wtform from model data model_to_wtform ( model_row , wtf_form ) # todo - maybe put update contingencies here? # obj_diagnostics(wtf_form, message=\"wtf_form in incidence_prep() after model_to_wtform\") # For GET requests for row creation, don't validate and error_count_dict will be all zeros # For GET requests for row update, validate (except for the csrf token that is only present for a POST) if crud_type == 'update' : validate_no_csrf ( wtf_form , extra_validators = None ) # todo - trying to make sure invalid drop-downs become \"Please Select\" # may want to look into using validate_no_csrf or initialize_drop_downs (or combo) # Set all select elements that are a default value (None) to \"Please Select\" value initialize_drop_downs ( wtf_form , default = default_dropdown ) # logger.debug(f\"\\n\\t{wtf_form.data=}\") if request . method == 'POST' : # Validate and count errors wtf_form . validate () error_count_dict = wtf_count_errors ( wtf_form , log_errors = True ) # Diagnostics of model before updating with wtform values # Likely can comment out model_before and add_commit_and_log_model # if you want less diagnostics and redundant commits model_before = sa_model_to_dict ( model_row ) wtform_to_model ( model_row , wtf_form , ignore_fields = [ \"id_incidence\" ]) add_commit_and_log_model ( db , model_row , comment = 'call to wtform_to_model()' , model_before = model_before ) # Determine course of action for successful database update based on which button was submitted button = request . form . get ( 'submit_button' ) # todo - change the button name to save? if button == 'validate_and_submit' : logger . debug ( f \"validate_and_submit was pressed\" ) if wtf_form . validate (): return redirect ( url_for ( 'main.index' )) error_count_dict = wtf_count_errors ( wtf_form , log_errors = True ) logger . debug ( f \"incidence_prep() about to render get template\" ) return render_template ( template_file , wtf_form = wtf_form , crud_type = crud_type , error_count_dict = error_count_dict , id_incidence = model_row . id_incidence , )","title":"incidence_prep"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.json_file_to_db","text":"Load a JSON file and insert its contents into the incidences table. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy session used to commit the new row. file_name ( str | Path ) \u2013 Path to the JSON file on disk. base ( AutomapBase ) \u2013 SQLAlchemy automapped metadata base. Returns: tuple [ int , str ] \u2013 tuple[int, str]: The (id_incidence, sector) extracted from the inserted row. Raises: FileNotFoundError \u2013 If the specified file path does not exist. JSONDecodeError \u2013 If the file is not valid JSON. Source code in arb\\portal\\app_util.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 def json_file_to_db ( db : SQLAlchemy , file_name : str | Path , base : AutomapBase ) -> tuple [ int , str ]: \"\"\" Load a JSON file and insert its contents into the `incidences` table. Args: db (SQLAlchemy): SQLAlchemy session used to commit the new row. file_name (str | Path): Path to the JSON file on disk. base (AutomapBase): SQLAlchemy automapped metadata base. Returns: tuple[int, str]: The (id_incidence, sector) extracted from the inserted row. Raises: FileNotFoundError: If the specified file path does not exist. json.JSONDecodeError: If the file is not valid JSON. \"\"\" json_as_dict , metadata = json_load_with_meta ( file_name ) return xl_dict_to_database ( db , base , json_as_dict )","title":"json_file_to_db"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.resolve_sector","text":"Determine the appropriate sector from FK and JSON sources. Parameters: sector_by_foreign_key ( str | None ) \u2013 Sector from sources table. row ( DeclarativeMeta ) \u2013 Row from incidences table (SQLAlchemy result). misc_json ( dict ) \u2013 Parsed misc_json content. Returns: str ( str ) \u2013 Sector string. Raises: ValueError \u2013 If values are missing or conflict. Source code in arb\\portal\\app_util.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def resolve_sector ( sector_by_foreign_key : str | None , row : DeclarativeMeta , misc_json : dict ) -> str : \"\"\" Determine the appropriate sector from FK and JSON sources. Args: sector_by_foreign_key (str | None): Sector from `sources` table. row (DeclarativeMeta): Row from `incidences` table (SQLAlchemy result). misc_json (dict): Parsed `misc_json` content. Returns: str: Sector string. Raises: ValueError: If values are missing or conflict. \"\"\" logger . debug ( f \"resolve_sector() called with { sector_by_foreign_key =} , { row =} , { misc_json =} \" ) sector = None sector_by_json = misc_json . get ( \"sector\" ) if sector_by_foreign_key is None : logger . warning ( \"sector column value in sources table is None.\" ) if sector_by_json is None : logger . warning ( \"'sector' not in misc_json\" ) if sector_by_foreign_key is None and sector_by_json is None : logger . error ( \"Can't determine incidence sector\" ) raise ValueError ( \"Can't determine incidence sector\" ) if sector_by_foreign_key is not None and sector_by_json is not None : if sector_by_foreign_key != sector_by_json : logger . error ( f \"Sector mismatch: { sector_by_foreign_key =} , { sector_by_json =} \" ) raise ValueError ( \"Can't determine incidence sector\" ) sector = sector_by_foreign_key or sector_by_json logger . debug ( f \"resolve_sector() returning { sector =} \" ) return sector","title":"resolve_sector"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.upload_and_update_db","text":"Save uploaded file, parse contents, and insert or update DB rows. Parameters: db ( SQLAlchemy ) \u2013 Database instance. upload_dir ( str | Path ) \u2013 Directory where file will be saved. request_file ( FileStorage ) \u2013 Flask request.files[...] object. base ( AutomapBase ) \u2013 Automapped schema metadata. Returns: tuple [ str , int | None, str | None] \u2013 tuple[str, int | None, str | None]: Filename, id_incidence, sector. Source code in arb\\portal\\app_util.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def upload_and_update_db ( db : SQLAlchemy , upload_dir : str | Path , request_file : FileStorage , base : AutomapBase ) -> tuple [ str , int | None , str | None ]: \"\"\" Save uploaded file, parse contents, and insert or update DB rows. Args: db (SQLAlchemy): Database instance. upload_dir (str | Path): Directory where file will be saved. request_file (FileStorage): Flask `request.files[...]` object. base (AutomapBase): Automapped schema metadata. Returns: tuple[str, int | None, str | None]: Filename, id_incidence, sector. \"\"\" logger . debug ( f \"upload_and_update_db() called with { request_file =} \" ) id_ = None sector = None file_name = upload_single_file ( upload_dir , request_file ) add_file_to_upload_table ( db , file_name , status = \"File Added\" , description = None ) # if file is xl and can be converted to json, # save a json version of the file and return the filename json_file_name = get_json_file_name ( file_name ) if json_file_name : id_ , sector = json_file_to_db ( db , json_file_name , base ) return file_name , id_ , sector","title":"upload_and_update_db"},{"location":"reference/arb/portal/app_util/#arb.portal.app_util.xl_dict_to_database","text":"Insert or update a row from an Excel-parsed JSON dictionary into the database. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy database instance. base ( AutomapBase ) \u2013 Reflected SQLAlchemy base metadata. xl_dict ( dict ) \u2013 Parsed Excel document with 'metadata' and 'tab_contents'. tab_name ( str ) \u2013 Name of the worksheet tab to extract. Returns: tuple [ int , str ] \u2013 tuple[int, str]: Tuple of (id_incidence, sector) after row insertion. Source code in arb\\portal\\app_util.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def xl_dict_to_database ( db , base , xl_dict : dict , tab_name : str ) -> tuple [ int , str ]: \"\"\" Insert or update a row from an Excel-parsed JSON dictionary into the database. Args: db (SQLAlchemy): SQLAlchemy database instance. base (AutomapBase): Reflected SQLAlchemy base metadata. xl_dict (dict): Parsed Excel document with 'metadata' and 'tab_contents'. tab_name (str): Name of the worksheet tab to extract. Returns: tuple[int, str]: Tuple of (id_incidence, sector) after row insertion. \"\"\" logger . debug ( f \"xl_dict_to_database() called with { xl_dict =} \" ) metadata = xl_dict [ \"metadata\" ] sector = metadata [ \"sector\" ] tab_data = xl_dict [ \"tab_contents\" ][ tab_name ] tab_data [ \"sector\" ] = sector id_ = dict_to_database ( db , base , tab_data ) return id_ , sector","title":"xl_dict_to_database"},{"location":"reference/arb/portal/constants/","text":"arb.portal.constants Shared application-wide constants for the ARB Methane Feedback Portal. These constants are designed to be immutable and centrally maintained. They support consistent behavior and validation across: Web form defaults and placeholders Geospatial input validation Spreadsheet cell parsing Time zone-aware datetime formatting Filename-safe timestamp generation Structure UI Constants Geographic Boundaries (California-specific) Time Zones and Datetime Formats Module Self-Test Notes Constants should always be imported from this module instead of redefined. Time zone constants use zoneinfo.ZoneInfo and are safe for use with timezone-aware datetime objects. CA_TIME_ZONE = ZoneInfo ( 'America/Los_Angeles' ) module-attribute ZoneInfo: Timezone objects used for datetime conversion and formatting. DATETIME_WITH_SECONDS = '%Y_%m_ %d _%H_%M_%S' module-attribute str: Filename-safe datetime string format (includes seconds). GPS_RESOLUTION = 5 module-attribute int: Desired decimal precision for GPS values. HTML_LOCAL_TIME_FORMAT = '%Y-%m- %d T%H:%M' module-attribute str: HTML5-compatible format string for . LATITUDE_VALIDATION = { 'min' : MIN_LATITUDE , 'max' : MAX_LATITUDE , 'message' : f 'Latitudes must be blank or valid California number between { MIN_LATITUDE } and { MAX_LATITUDE } .' } module-attribute dict: Latitude validation schema for WTForms or other validators. LONGITUDE_VALIDATION = { 'min' : MIN_LONGITUDE , 'max' : MAX_LONGITUDE , 'message' : f 'Longitudes must be blank or valid California number between { MIN_LONGITUDE } and { MAX_LONGITUDE } .' } module-attribute dict: Longitude validation schema for WTForms or other validators. MAX_LATITUDE = 42.0 module-attribute float: Maximum possible CA latitude. MAX_LONGITUDE = - 114.0 module-attribute float: Maximum possible CA longitude. MIN_LATITUDE = 32.0 module-attribute float: Minimum possible CA latitude. MIN_LONGITUDE = - 125.0 module-attribute float: Minimum possible CA longitude. PLEASE_SELECT = 'Please Select' module-attribute str: Placeholder value used for dropdowns where no selection is made. UTC_TIME_ZONE = ZoneInfo ( 'UTC' ) module-attribute ZoneInfo: UTC (Zulu) timezone.","title":"arb.portal.constants"},{"location":"reference/arb/portal/constants/#arbportalconstants","text":"Shared application-wide constants for the ARB Methane Feedback Portal. These constants are designed to be immutable and centrally maintained. They support consistent behavior and validation across: Web form defaults and placeholders Geospatial input validation Spreadsheet cell parsing Time zone-aware datetime formatting Filename-safe timestamp generation Structure UI Constants Geographic Boundaries (California-specific) Time Zones and Datetime Formats Module Self-Test Notes Constants should always be imported from this module instead of redefined. Time zone constants use zoneinfo.ZoneInfo and are safe for use with timezone-aware datetime objects.","title":"arb.portal.constants"},{"location":"reference/arb/portal/constants/#arb.portal.constants.CA_TIME_ZONE","text":"ZoneInfo: Timezone objects used for datetime conversion and formatting.","title":"CA_TIME_ZONE"},{"location":"reference/arb/portal/constants/#arb.portal.constants.DATETIME_WITH_SECONDS","text":"str: Filename-safe datetime string format (includes seconds).","title":"DATETIME_WITH_SECONDS"},{"location":"reference/arb/portal/constants/#arb.portal.constants.GPS_RESOLUTION","text":"int: Desired decimal precision for GPS values.","title":"GPS_RESOLUTION"},{"location":"reference/arb/portal/constants/#arb.portal.constants.HTML_LOCAL_TIME_FORMAT","text":"str: HTML5-compatible format string for .","title":"HTML_LOCAL_TIME_FORMAT"},{"location":"reference/arb/portal/constants/#arb.portal.constants.LATITUDE_VALIDATION","text":"dict: Latitude validation schema for WTForms or other validators.","title":"LATITUDE_VALIDATION"},{"location":"reference/arb/portal/constants/#arb.portal.constants.LONGITUDE_VALIDATION","text":"dict: Longitude validation schema for WTForms or other validators.","title":"LONGITUDE_VALIDATION"},{"location":"reference/arb/portal/constants/#arb.portal.constants.MAX_LATITUDE","text":"float: Maximum possible CA latitude.","title":"MAX_LATITUDE"},{"location":"reference/arb/portal/constants/#arb.portal.constants.MAX_LONGITUDE","text":"float: Maximum possible CA longitude.","title":"MAX_LONGITUDE"},{"location":"reference/arb/portal/constants/#arb.portal.constants.MIN_LATITUDE","text":"float: Minimum possible CA latitude.","title":"MIN_LATITUDE"},{"location":"reference/arb/portal/constants/#arb.portal.constants.MIN_LONGITUDE","text":"float: Minimum possible CA longitude.","title":"MIN_LONGITUDE"},{"location":"reference/arb/portal/constants/#arb.portal.constants.PLEASE_SELECT","text":"str: Placeholder value used for dropdowns where no selection is made.","title":"PLEASE_SELECT"},{"location":"reference/arb/portal/constants/#arb.portal.constants.UTC_TIME_ZONE","text":"ZoneInfo: UTC (Zulu) timezone.","title":"UTC_TIME_ZONE"},{"location":"reference/arb/portal/db_hardcoded/","text":"arb.portal.db_hardcoded Hardcoded testing data and dropdown lookup values for the ARB Methane Feedback Portal. This module provides Dummy incidence records for Oil & Gas and Landfill sectors Lookup values for HTML dropdowns (independent and contingent) Shared test values for local debugging or spreadsheet seeding Notes Intended for use during development and offline diagnostics Not suitable for production database seeding add_og_dummy_data ( db , base , table_name ) (Depreciated) Populate the database with synthetic Oil & Gas incidence rows for diagnostics. Parameters: db ( SQLAlchemy ) \u2013 Active SQLAlchemy session bound to the database. base ( AutomapBase ) \u2013 SQLAlchemy automap base for resolving table classes. table_name ( str ) \u2013 Target table name (e.g., 'incidences'). Notes This routine is likely outdated and is kept only as a template. It is valid, but not necessary to specify 'Please Select' in dummy data. Uses an offset in id_incidence to avoid primary key conflicts. Inserts 9 rows with dummy misc_json fields. Source code in arb\\portal\\db_hardcoded.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def add_og_dummy_data ( db , base , table_name ) -> None : \"\"\" (Depreciated) Populate the database with synthetic Oil & Gas incidence rows for diagnostics. Args: db (SQLAlchemy): Active SQLAlchemy session bound to the database. base (AutomapBase): SQLAlchemy automap base for resolving table classes. table_name (str): Target table name (e.g., 'incidences'). Notes: - This routine is likely outdated and is kept only as a template. - It is valid, but not necessary to specify 'Please Select' in dummy data. - Uses an offset in `id_incidence` to avoid primary key conflicts. - Inserts 9 rows with dummy `misc_json` fields. \"\"\" from arb.utils.sql_alchemy import get_class_from_table_name logger . debug ( \"Adding dummy oil and gas data to populate the database\" ) table = get_class_from_table_name ( base , table_name ) col_name = \"misc_json\" offset = 2000000 # adjust so that you don't have a unique constraint issue for i in range ( 1 , 10 ): id_incidence = i + offset id_plume = i + 100 lat_arb = i + 50. long_arb = i + 75. observation_timestamp = datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ) facility_name = f \"facility_ { i } \" contact_name = f \"contact_name_ { i } \" contact_phone = f \"(555) 555-5555x { i } \" contact_email = f \"my_email_ { i } @server.com\" sector = \"Oil & Gas\" sector_type = \"Oil & Gas\" json_data = { \"id_incidence\" : id_incidence , \"id_plume\" : id_plume , \"lat_arb\" : lat_arb , \"long_arb\" : long_arb , # \"observation_timestamp\": observation_timestamp.strftime(\"%Y-%m-%d %H:%M:%S.%f\"), \"observation_timestamp\" : observation_timestamp , \"facility_name\" : facility_name , \"contact_name\" : contact_name , \"contact_phone\" : contact_phone , \"contact_email\" : contact_email , \"sector\" : sector , \"sector_type\" : sector_type , } model = table ( description = \"Dummy data created by add_og_dummy_data\" , ** { col_name : json_data }) logger . debug ( f \"Adding incidence with json dummy data: { json_data =} \" ) db . session . add ( model ) db . session . commit () get_excel_dropdown_data () Return dropdown lookup values used in Excel and HTML form rendering. Returns: tuple ( tuple [ dict [ str , list [ str ]], dict [ str , dict [ str , list [ str ]]]] ) \u2013 dict[str, list[str]]: Independent dropdowns keyed by HTML field name. dict[str, dict[str, list[str]]]: Contingent dropdowns dependent on parent field values. Notes Dropdown values mirror those found in Excel templates. Each list element is a selectable value; \"Please Select\" is prepended externally. Contingent keys follow the format: field2_contingent_on_field1 . Each tuple is 2 or 3 items in length with the format: (select value, select text, and an optional dictionary of additional html formatting) todo - The new drop-downs are not context dependent like they are in excel and the validate logic needs to be updated. Source code in arb\\portal\\db_hardcoded.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 def get_excel_dropdown_data () -> tuple [ dict [ str , list [ str ]], dict [ str , dict [ str , list [ str ]]]]: \"\"\" Return dropdown lookup values used in Excel and HTML form rendering. Returns: tuple: - dict[str, list[str]]: Independent dropdowns keyed by HTML field name. - dict[str, dict[str, list[str]]]: Contingent dropdowns dependent on parent field values. Notes: - Dropdown values mirror those found in Excel templates. - Each list element is a selectable value; `\"Please Select\"` is prepended externally. - Contingent keys follow the format: `field2_contingent_on_field1`. - Each tuple is 2 or 3 items in length with the format: (select value, select text, and an optional dictionary of additional html formatting) # todo - The new drop-downs are not context dependent like they are in excel and the validate logic needs to be updated. \"\"\" # Oil & Gas drop_downs = { \"venting_exclusion\" : [ \"Yes\" , \"No\" , ], \"ogi_performed\" : [ \"Yes\" , \"No\" , ], \"ogi_result\" : [ \"Not applicable as OGI was not performed\" , \"No source found\" , \"Unintentional-leak\" , \"Unintentional-non-component\" , \"Venting-construction/maintenance\" , \"Venting-routine\" , ], \"method21_performed\" : [ \"Yes\" , \"No\" , ], \"method21_result\" : [ \"Not applicable as Method 21 was not performed\" , \"No source found\" , \"Unintentional-below leak threshold\" , \"Unintentional-leak\" , \"Unintentional-non-component\" , \"Venting-construction/maintenance\" , \"Venting-routine\" , ], \"equipment_at_source\" : [ \"Centrifugal Natural Gas Compressor\" , \"Continuous High Bleed Natural Gas-actuated Pneumatic Device\" , \"Continuous Low Bleed Natural Gas-actuated Pneumatic Device\" , \"Intermittent Bleed Natural Gas-actuated Pneumatic Device\" , \"Natural Gas-actuated Pneumatic Pump\" , \"Pressure Separator\" , \"Reciprocating Natural Gas Compressor\" , \"Separator\" , \"Tank\" , \"Open Well Casing Vent\" , \"Piping\" , \"Well\" , \"Other\" , ], \"component_at_source\" : [ \"Valve\" , \"Connector\" , \"Flange\" , \"Fitting - pressure meter/gauge\" , \"Fitting - not pressure meter/gauge\" , \"Open-ended line\" , \"Plug\" , \"Pressure relief device\" , \"Stuffing box\" , \"Other\" , ], # Landfill \"emission_identified_flag_fk\" : [ \"Operator was aware of the leak prior to receiving the CARB plume notification\" , \"Operator detected a leak during follow-up monitoring after receipt of the CARB plume notification\" , \"No leak was detected\" , ], \"emission_type_fk\" : [ \"Not applicable as no leak was detected\" , \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\" , \"An unintentional leak (i.e., the operator was not aware of, and could be repaired if discovered)\" , \"An intentional or allowable vent (i.e., the operator was aware of, and/or would not repair)\" , \"Due to a temporary activity (i.e., would be resolved without corrective action when the activity is complete)\" , ], \"emission_location\" : [ \"Not applicable as no leak was detected\" , \"Gas Collection System Component (e.g., blower, well, valve, port)\" , \"Gas Control Device/Control System Component\" , \"Landfill Surface: Daily Cover\" , \"Landfill Surface: Final Cover\" , \"Landfill Surface: Intermediate Cover\" , \"Leachate Management System\" , \"Working Face (area where active filling was being conducted at the time of detection)\" , ], \"emission_cause\" : [ \"Not applicable as no leak was detected\" , \"Collection system downtime\" , \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover integrity\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Cracked/Broken Seal\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], \"emission_cause_secondary\" : [ \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , \"Collection system downtime\" , \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover integrity\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Cracked/Broken Seal\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], \"emission_cause_tertiary\" : [ \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , \"Collection system downtime\" , \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover integrity\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Cracked/Broken Seal\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], \"included_in_last_lmr\" : [ \"Yes\" , \"No\" , ], \"planned_for_next_lmr\" : [ \"Yes\" , \"No\" , ], } # keys to the contingent dropdowns follow the patter html_selector2_contingent_on_html_selector1 # for instance, emission_cause_contingent_on_emission_location means that the choices for # emission_cause are based on a lookup of the value of emission_location drop_downs_contingent = { \"emission_cause_contingent_on_emission_location\" : { \"Gas Collection System Component (e.g., blower, well, valve, port)\" : [ \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , ], \"Gas Control Device/Control System Component\" : [ \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Damaged component\" , \"Other\" , ], \"Landfill Surface: Daily Cover\" : [ \"Collection system downtime\" , \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover integrity\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Cracked/Broken Seal\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], \"Landfill Surface: Intermediate Cover\" : [ \"Collection system downtime\" , \"Construction - New Well Installation\" , \"Cover integrity\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Cracked/Broken Seal\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], \"Landfill Surface: Final Cover\" : [ \"Collection system downtime\" , \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover integrity\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Cracked/Broken Seal\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], \"Leachate Management System\" : [ \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Damaged component\" , \"Offline Gas Collection Well(s)\" , \"Other\" , ], \"Working Face (area where active filling was being conducted at the time of detection)\" : [ \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], }, } # Note, the drop_downs get \"Please Select\" prepended, but the drop_down_contingent content is not modified drop_downs = update_selector_dict ( drop_downs ) return drop_downs , drop_downs_contingent get_landfill_dummy_data () Generate dummy Landfill form data as a dictionary. Returns: dict ( dict ) \u2013 Pre-filled key/value pairs used to populate a feedback form. Notes: - It is valid, but not necessary to specify 'Please Select' in dummy data. Source code in arb\\portal\\db_hardcoded.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def get_landfill_dummy_data () -> dict : \"\"\" Generate dummy Landfill form data as a dictionary. Returns: dict: Pre-filled key/value pairs used to populate a feedback form. Notes: - It is valid, but not necessary to specify 'Please Select' in dummy data. \"\"\" logger . debug ( f \"in landfill_dummy_data()\" ) json_data = { \"additional_activities\" : \"additional_activities\" , \"additional_notes\" : \"additional_notes\" , \"contact_email\" : \"my_email@email.com\" , \"contact_name\" : \"contact_name\" , \"contact_phone\" : f \"(555) 555-5555\" , # \"emission_cause\": PLEASE_SELECT, \"emission_cause_notes\" : \"emission_cause_notes\" , # \"emission_cause_secondary\": PLEASE_SELECT, # \"emission_cause_tertiary\": PLEASE_SELECT, # \"emission_identified_flag_fk\": PLEASE_SELECT, # \"emission_location\": PLEASE_SELECT, \"emission_location_notes\" : \"emission_location_notes\" , # \"emission_type_fk\": PLEASE_SELECT, \"facility_name\" : \"facility_name\" , \"id_arb_swis\" : \"id_arb_swis\" , \"id_incidence\" : 2002 , \"id_message\" : \"id_message\" , \"id_plume\" : 1002 , # \"included_in_last_lmr\": PLEASE_SELECT, \"included_in_last_lmr_description\" : \"included_in_last_lmr_description\" , \"initial_leak_concentration\" : 1002.5 , \"inspection_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"instrument\" : \"instrument\" , \"last_component_leak_monitoring_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"last_surface_monitoring_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"lat_carb\" : 102.5 , \"lat_revised\" : 103.5 , \"long_carb\" : 104.5 , \"long_revised\" : 105.5 , \"mitigation_actions\" : \"mitigation_actions\" , \"mitigation_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"observation_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), # \"planned_for_next_lmr\": PLEASE_SELECT, \"planned_for_next_lmr_description\" : \"planned_for_next_lmr_description\" , \"re_monitored_concentration\" : 1002.5 , \"re_monitored_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"sector\" : \"Landfill\" , \"sector_type\" : \"Landfill\" , } return json_data get_og_dummy_data () Generate dummy Oil & Gas form data as a dictionary. Returns: dict ( dict ) \u2013 Pre-filled key/value pairs used to populate a feedback form. Notes: - It is valid, but not necessary to specify 'Please Select' in dummy data. Source code in arb\\portal\\db_hardcoded.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def get_og_dummy_data () -> dict : \"\"\" Generate dummy Oil & Gas form data as a dictionary. Returns: dict: Pre-filled key/value pairs used to populate a feedback form. Notes: - It is valid, but not necessary to specify 'Please Select' in dummy data. \"\"\" json_data = { \"id_incidence\" : 2001 , \"id_plume\" : 1001 , \"observation_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"lat_carb\" : 100.05 , \"long_carb\" : 100.06 , \"id_message\" : \"id_message response\" , \"facility_name\" : \"facility_name response\" , \"id_arb_eggrt\" : \"1001\" , \"contact_name\" : \"contact_name response\" , \"contact_phone\" : f \"(555) 555-5555\" , \"contact_email\" : \"my_email@email.com\" , # \"venting_exclusion\": PLEASE_SELECT, \"venting_description_1\" : \"venting_description_1 response\" , # \"ogi_performed\": PLEASE_SELECT, \"ogi_date\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), # \"ogi_result\": PLEASE_SELECT, # \"method21_performed\": PLEASE_SELECT, \"method21_date\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), # \"method21_result\": PLEASE_SELECT, \"initial_leak_concentration\" : 1004 , \"venting_description_2\" : \"venting_description_2 response\" , \"initial_mitigation_plan\" : \"initial_mitigation_plan response\" , # \"equipment_at_source\": PLEASE_SELECT, \"equipment_other_description\" : \"equipment_other_description response\" , # \"component_at_source\": PLEASE_SELECT, \"component_other_description\" : \"component_other_description response\" , \"repair_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"final_repair_concentration\" : 101.05 , \"repair_description\" : \"repair_description response\" , \"additional_notes\" : \"additional_notes response\" , \"sector\" : \"Oil & Gas\" , \"sector_type\" : \"Oil & Gas\" , } return json_data","title":"arb.portal.db_hardcoded"},{"location":"reference/arb/portal/db_hardcoded/#arbportaldb_hardcoded","text":"Hardcoded testing data and dropdown lookup values for the ARB Methane Feedback Portal. This module provides Dummy incidence records for Oil & Gas and Landfill sectors Lookup values for HTML dropdowns (independent and contingent) Shared test values for local debugging or spreadsheet seeding Notes Intended for use during development and offline diagnostics Not suitable for production database seeding","title":"arb.portal.db_hardcoded"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.add_og_dummy_data","text":"(Depreciated) Populate the database with synthetic Oil & Gas incidence rows for diagnostics. Parameters: db ( SQLAlchemy ) \u2013 Active SQLAlchemy session bound to the database. base ( AutomapBase ) \u2013 SQLAlchemy automap base for resolving table classes. table_name ( str ) \u2013 Target table name (e.g., 'incidences'). Notes This routine is likely outdated and is kept only as a template. It is valid, but not necessary to specify 'Please Select' in dummy data. Uses an offset in id_incidence to avoid primary key conflicts. Inserts 9 rows with dummy misc_json fields. Source code in arb\\portal\\db_hardcoded.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def add_og_dummy_data ( db , base , table_name ) -> None : \"\"\" (Depreciated) Populate the database with synthetic Oil & Gas incidence rows for diagnostics. Args: db (SQLAlchemy): Active SQLAlchemy session bound to the database. base (AutomapBase): SQLAlchemy automap base for resolving table classes. table_name (str): Target table name (e.g., 'incidences'). Notes: - This routine is likely outdated and is kept only as a template. - It is valid, but not necessary to specify 'Please Select' in dummy data. - Uses an offset in `id_incidence` to avoid primary key conflicts. - Inserts 9 rows with dummy `misc_json` fields. \"\"\" from arb.utils.sql_alchemy import get_class_from_table_name logger . debug ( \"Adding dummy oil and gas data to populate the database\" ) table = get_class_from_table_name ( base , table_name ) col_name = \"misc_json\" offset = 2000000 # adjust so that you don't have a unique constraint issue for i in range ( 1 , 10 ): id_incidence = i + offset id_plume = i + 100 lat_arb = i + 50. long_arb = i + 75. observation_timestamp = datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ) facility_name = f \"facility_ { i } \" contact_name = f \"contact_name_ { i } \" contact_phone = f \"(555) 555-5555x { i } \" contact_email = f \"my_email_ { i } @server.com\" sector = \"Oil & Gas\" sector_type = \"Oil & Gas\" json_data = { \"id_incidence\" : id_incidence , \"id_plume\" : id_plume , \"lat_arb\" : lat_arb , \"long_arb\" : long_arb , # \"observation_timestamp\": observation_timestamp.strftime(\"%Y-%m-%d %H:%M:%S.%f\"), \"observation_timestamp\" : observation_timestamp , \"facility_name\" : facility_name , \"contact_name\" : contact_name , \"contact_phone\" : contact_phone , \"contact_email\" : contact_email , \"sector\" : sector , \"sector_type\" : sector_type , } model = table ( description = \"Dummy data created by add_og_dummy_data\" , ** { col_name : json_data }) logger . debug ( f \"Adding incidence with json dummy data: { json_data =} \" ) db . session . add ( model ) db . session . commit ()","title":"add_og_dummy_data"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_excel_dropdown_data","text":"Return dropdown lookup values used in Excel and HTML form rendering. Returns: tuple ( tuple [ dict [ str , list [ str ]], dict [ str , dict [ str , list [ str ]]]] ) \u2013 dict[str, list[str]]: Independent dropdowns keyed by HTML field name. dict[str, dict[str, list[str]]]: Contingent dropdowns dependent on parent field values. Notes Dropdown values mirror those found in Excel templates. Each list element is a selectable value; \"Please Select\" is prepended externally. Contingent keys follow the format: field2_contingent_on_field1 . Each tuple is 2 or 3 items in length with the format: (select value, select text, and an optional dictionary of additional html formatting)","title":"get_excel_dropdown_data"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_excel_dropdown_data--todo-the-new-drop-downs-are-not-context-dependent-like-they-are-in-excel-and-the","text":"validate logic needs to be updated. Source code in arb\\portal\\db_hardcoded.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 def get_excel_dropdown_data () -> tuple [ dict [ str , list [ str ]], dict [ str , dict [ str , list [ str ]]]]: \"\"\" Return dropdown lookup values used in Excel and HTML form rendering. Returns: tuple: - dict[str, list[str]]: Independent dropdowns keyed by HTML field name. - dict[str, dict[str, list[str]]]: Contingent dropdowns dependent on parent field values. Notes: - Dropdown values mirror those found in Excel templates. - Each list element is a selectable value; `\"Please Select\"` is prepended externally. - Contingent keys follow the format: `field2_contingent_on_field1`. - Each tuple is 2 or 3 items in length with the format: (select value, select text, and an optional dictionary of additional html formatting) # todo - The new drop-downs are not context dependent like they are in excel and the validate logic needs to be updated. \"\"\" # Oil & Gas drop_downs = { \"venting_exclusion\" : [ \"Yes\" , \"No\" , ], \"ogi_performed\" : [ \"Yes\" , \"No\" , ], \"ogi_result\" : [ \"Not applicable as OGI was not performed\" , \"No source found\" , \"Unintentional-leak\" , \"Unintentional-non-component\" , \"Venting-construction/maintenance\" , \"Venting-routine\" , ], \"method21_performed\" : [ \"Yes\" , \"No\" , ], \"method21_result\" : [ \"Not applicable as Method 21 was not performed\" , \"No source found\" , \"Unintentional-below leak threshold\" , \"Unintentional-leak\" , \"Unintentional-non-component\" , \"Venting-construction/maintenance\" , \"Venting-routine\" , ], \"equipment_at_source\" : [ \"Centrifugal Natural Gas Compressor\" , \"Continuous High Bleed Natural Gas-actuated Pneumatic Device\" , \"Continuous Low Bleed Natural Gas-actuated Pneumatic Device\" , \"Intermittent Bleed Natural Gas-actuated Pneumatic Device\" , \"Natural Gas-actuated Pneumatic Pump\" , \"Pressure Separator\" , \"Reciprocating Natural Gas Compressor\" , \"Separator\" , \"Tank\" , \"Open Well Casing Vent\" , \"Piping\" , \"Well\" , \"Other\" , ], \"component_at_source\" : [ \"Valve\" , \"Connector\" , \"Flange\" , \"Fitting - pressure meter/gauge\" , \"Fitting - not pressure meter/gauge\" , \"Open-ended line\" , \"Plug\" , \"Pressure relief device\" , \"Stuffing box\" , \"Other\" , ], # Landfill \"emission_identified_flag_fk\" : [ \"Operator was aware of the leak prior to receiving the CARB plume notification\" , \"Operator detected a leak during follow-up monitoring after receipt of the CARB plume notification\" , \"No leak was detected\" , ], \"emission_type_fk\" : [ \"Not applicable as no leak was detected\" , \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\" , \"An unintentional leak (i.e., the operator was not aware of, and could be repaired if discovered)\" , \"An intentional or allowable vent (i.e., the operator was aware of, and/or would not repair)\" , \"Due to a temporary activity (i.e., would be resolved without corrective action when the activity is complete)\" , ], \"emission_location\" : [ \"Not applicable as no leak was detected\" , \"Gas Collection System Component (e.g., blower, well, valve, port)\" , \"Gas Control Device/Control System Component\" , \"Landfill Surface: Daily Cover\" , \"Landfill Surface: Final Cover\" , \"Landfill Surface: Intermediate Cover\" , \"Leachate Management System\" , \"Working Face (area where active filling was being conducted at the time of detection)\" , ], \"emission_cause\" : [ \"Not applicable as no leak was detected\" , \"Collection system downtime\" , \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover integrity\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Cracked/Broken Seal\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], \"emission_cause_secondary\" : [ \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , \"Collection system downtime\" , \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover integrity\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Cracked/Broken Seal\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], \"emission_cause_tertiary\" : [ \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , \"Collection system downtime\" , \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover integrity\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Cracked/Broken Seal\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], \"included_in_last_lmr\" : [ \"Yes\" , \"No\" , ], \"planned_for_next_lmr\" : [ \"Yes\" , \"No\" , ], } # keys to the contingent dropdowns follow the patter html_selector2_contingent_on_html_selector1 # for instance, emission_cause_contingent_on_emission_location means that the choices for # emission_cause are based on a lookup of the value of emission_location drop_downs_contingent = { \"emission_cause_contingent_on_emission_location\" : { \"Gas Collection System Component (e.g., blower, well, valve, port)\" : [ \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , ], \"Gas Control Device/Control System Component\" : [ \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Damaged component\" , \"Other\" , ], \"Landfill Surface: Daily Cover\" : [ \"Collection system downtime\" , \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover integrity\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Cracked/Broken Seal\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], \"Landfill Surface: Intermediate Cover\" : [ \"Collection system downtime\" , \"Construction - New Well Installation\" , \"Cover integrity\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Cracked/Broken Seal\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], \"Landfill Surface: Final Cover\" : [ \"Collection system downtime\" , \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover integrity\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Cracked/Broken Seal\" , \"Damaged component\" , \"Insufficient vacuum\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], \"Leachate Management System\" : [ \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Damaged component\" , \"Offline Gas Collection Well(s)\" , \"Other\" , ], \"Working Face (area where active filling was being conducted at the time of detection)\" : [ \"Construction - New Well Installation\" , \"Construction - Well Raising or Horizontal Extension\" , \"Cover-related Construction (Excavation/ Exposed Operations/ Re-grading)\" , \"Offline Gas Collection Well(s)\" , \"Other\" , \"Uncontrolled Area (no gas collection infrastructure)\" , ], }, } # Note, the drop_downs get \"Please Select\" prepended, but the drop_down_contingent content is not modified drop_downs = update_selector_dict ( drop_downs ) return drop_downs , drop_downs_contingent","title":"todo - The new drop-downs are not context dependent like they are in excel and the"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_landfill_dummy_data","text":"Generate dummy Landfill form data as a dictionary. Returns: dict ( dict ) \u2013 Pre-filled key/value pairs used to populate a feedback form. Notes: - It is valid, but not necessary to specify 'Please Select' in dummy data. Source code in arb\\portal\\db_hardcoded.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def get_landfill_dummy_data () -> dict : \"\"\" Generate dummy Landfill form data as a dictionary. Returns: dict: Pre-filled key/value pairs used to populate a feedback form. Notes: - It is valid, but not necessary to specify 'Please Select' in dummy data. \"\"\" logger . debug ( f \"in landfill_dummy_data()\" ) json_data = { \"additional_activities\" : \"additional_activities\" , \"additional_notes\" : \"additional_notes\" , \"contact_email\" : \"my_email@email.com\" , \"contact_name\" : \"contact_name\" , \"contact_phone\" : f \"(555) 555-5555\" , # \"emission_cause\": PLEASE_SELECT, \"emission_cause_notes\" : \"emission_cause_notes\" , # \"emission_cause_secondary\": PLEASE_SELECT, # \"emission_cause_tertiary\": PLEASE_SELECT, # \"emission_identified_flag_fk\": PLEASE_SELECT, # \"emission_location\": PLEASE_SELECT, \"emission_location_notes\" : \"emission_location_notes\" , # \"emission_type_fk\": PLEASE_SELECT, \"facility_name\" : \"facility_name\" , \"id_arb_swis\" : \"id_arb_swis\" , \"id_incidence\" : 2002 , \"id_message\" : \"id_message\" , \"id_plume\" : 1002 , # \"included_in_last_lmr\": PLEASE_SELECT, \"included_in_last_lmr_description\" : \"included_in_last_lmr_description\" , \"initial_leak_concentration\" : 1002.5 , \"inspection_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"instrument\" : \"instrument\" , \"last_component_leak_monitoring_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"last_surface_monitoring_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"lat_carb\" : 102.5 , \"lat_revised\" : 103.5 , \"long_carb\" : 104.5 , \"long_revised\" : 105.5 , \"mitigation_actions\" : \"mitigation_actions\" , \"mitigation_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"observation_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), # \"planned_for_next_lmr\": PLEASE_SELECT, \"planned_for_next_lmr_description\" : \"planned_for_next_lmr_description\" , \"re_monitored_concentration\" : 1002.5 , \"re_monitored_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"sector\" : \"Landfill\" , \"sector_type\" : \"Landfill\" , } return json_data","title":"get_landfill_dummy_data"},{"location":"reference/arb/portal/db_hardcoded/#arb.portal.db_hardcoded.get_og_dummy_data","text":"Generate dummy Oil & Gas form data as a dictionary. Returns: dict ( dict ) \u2013 Pre-filled key/value pairs used to populate a feedback form. Notes: - It is valid, but not necessary to specify 'Please Select' in dummy data. Source code in arb\\portal\\db_hardcoded.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def get_og_dummy_data () -> dict : \"\"\" Generate dummy Oil & Gas form data as a dictionary. Returns: dict: Pre-filled key/value pairs used to populate a feedback form. Notes: - It is valid, but not necessary to specify 'Please Select' in dummy data. \"\"\" json_data = { \"id_incidence\" : 2001 , \"id_plume\" : 1001 , \"observation_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"lat_carb\" : 100.05 , \"long_carb\" : 100.06 , \"id_message\" : \"id_message response\" , \"facility_name\" : \"facility_name response\" , \"id_arb_eggrt\" : \"1001\" , \"contact_name\" : \"contact_name response\" , \"contact_phone\" : f \"(555) 555-5555\" , \"contact_email\" : \"my_email@email.com\" , # \"venting_exclusion\": PLEASE_SELECT, \"venting_description_1\" : \"venting_description_1 response\" , # \"ogi_performed\": PLEASE_SELECT, \"ogi_date\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), # \"ogi_result\": PLEASE_SELECT, # \"method21_performed\": PLEASE_SELECT, \"method21_date\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), # \"method21_result\": PLEASE_SELECT, \"initial_leak_concentration\" : 1004 , \"venting_description_2\" : \"venting_description_2 response\" , \"initial_mitigation_plan\" : \"initial_mitigation_plan response\" , # \"equipment_at_source\": PLEASE_SELECT, \"equipment_other_description\" : \"equipment_other_description response\" , # \"component_at_source\": PLEASE_SELECT, \"component_other_description\" : \"component_other_description response\" , \"repair_timestamp\" : datetime . datetime . now () . replace ( second = 0 , microsecond = 0 ), \"final_repair_concentration\" : 101.05 , \"repair_description\" : \"repair_description response\" , \"additional_notes\" : \"additional_notes response\" , \"sector\" : \"Oil & Gas\" , \"sector_type\" : \"Oil & Gas\" , } return json_data","title":"get_og_dummy_data"},{"location":"reference/arb/portal/extensions/","text":"arb.portal.extensions Centralized definition of Flask extension instances used throughout the portal. This module avoids circular imports by creating extension objects (e.g., db , csrf ) at the top level, without initializing them until app.init_app() is called elsewhere. Extensions Defined db (SQLAlchemy): SQLAlchemy instance shared across all models and routes. csrf (CSRFProtect): CSRF protection used for form validation. Notes geoalchemy2.Geometry must be imported for spatial field introspection, even if not directly referenced in code. Use with app.app_context(): when accessing db outside a Flask route. Example from arb.portal.extensions import db with app.app_context(): ... db.create_all() csrf = CSRFProtect () module-attribute CSRFProtect: Flask-WTF extension for CSRF form protection. db = SQLAlchemy () module-attribute SQLAlchemy: Flask SQLAlchemy instance for managing ORM and schema.","title":"arb.portal.extensions"},{"location":"reference/arb/portal/extensions/#arbportalextensions","text":"Centralized definition of Flask extension instances used throughout the portal. This module avoids circular imports by creating extension objects (e.g., db , csrf ) at the top level, without initializing them until app.init_app() is called elsewhere. Extensions Defined db (SQLAlchemy): SQLAlchemy instance shared across all models and routes. csrf (CSRFProtect): CSRF protection used for form validation. Notes geoalchemy2.Geometry must be imported for spatial field introspection, even if not directly referenced in code. Use with app.app_context(): when accessing db outside a Flask route. Example from arb.portal.extensions import db with app.app_context(): ... db.create_all()","title":"arb.portal.extensions"},{"location":"reference/arb/portal/extensions/#arb.portal.extensions.csrf","text":"CSRFProtect: Flask-WTF extension for CSRF form protection.","title":"csrf"},{"location":"reference/arb/portal/extensions/#arb.portal.extensions.db","text":"SQLAlchemy: Flask SQLAlchemy instance for managing ORM and schema.","title":"db"},{"location":"reference/arb/portal/globals/","text":"arb.portal.globals Global variables and dropdown selector loading for Flask/SQLAlchemy applications. This module provides the Globals class for holding runtime-initialized data structures such as dropdown selectors and database column type mappings. Primary Uses Prevent circular imports in SQLAlchemy/Flask environments Store shared type and dropdown definitions used throughout the app Enable lazy initialization of values dependent on app context Notes Globals are not intended to be mutable after initialization. Centralizes dropdown and type mapping logic for app-wide reuse. Static values that do not require runtime context should live in constants.py . Globals Central class for holding runtime-global mappings used in the Flask app. Attributes: db_column_types ( dict [ str , dict [ str , dict [ str , Any ]]] ) \u2013 Mapping of table.column to SQLAlchemy type metadata (includes db_type , sa_type , py_type ). drop_downs ( dict [ str , list [ str ]] ) \u2013 Field name to independent dropdown options. drop_downs_contingent ( dict [ str , dict [ str , list [ str ]]] ) \u2013 Parent-dependent options for contingent dropdowns (e.g., county \u2192 list of sub-counties). Source code in arb\\portal\\globals.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class Globals : \"\"\" Central class for holding runtime-global mappings used in the Flask app. Attributes: db_column_types (dict[str, dict[str, dict[str, Any]]]): Mapping of table.column to SQLAlchemy type metadata (includes `db_type`, `sa_type`, `py_type`). drop_downs (dict[str, list[str]]): Field name to independent dropdown options. drop_downs_contingent (dict[str, dict[str, list[str]]]): Parent-dependent options for contingent dropdowns (e.g., county \u2192 list of sub-counties). \"\"\" db_column_types = {} drop_downs = {} drop_downs_contingent = {} @classmethod def load_drop_downs ( cls , flask_app : Flask , db : SQLAlchemy ) -> None : \"\"\" Load dropdown data from hardcoded configuration and cache it globally. Args: flask_app (Flask): The active Flask app instance. db (SQLAlchemy): SQLAlchemy instance (not used in this function but passed for consistency). Returns: None Notes: - Uses `get_excel_dropdown_data()` from `db_hardcoded` to populate form options. - Populates both `Globals.drop_downs` and `Globals.drop_downs_contingent`. - Should be called once after app startup or reflection. \"\"\" from arb.portal.db_hardcoded import get_excel_dropdown_data logger . debug ( \"In load_drop_downs()\" ) Globals . drop_downs , Globals . drop_downs_contingent = get_excel_dropdown_data () logger . debug ( f \"Globals.drop_downs= { Globals . drop_downs } \" ) logger . debug ( f \"Globals.drop_downs_contingent= { Globals . drop_downs_contingent } \" ) @classmethod def load_type_mapping ( cls , flask_app : Flask , db : SQLAlchemy , base ) -> None : \"\"\" Populate column type metadata for all reflected tables in the SQLAlchemy base. Args: flask_app (Flask): The current Flask application (used for context scoping). db (SQLAlchemy): SQLAlchemy instance, already bound to a live database engine. base (AutomapBase): Reflected SQLAlchemy metadata containing all mapped models. Returns: None Example: >>> Globals.load_type_mapping(app, db, base) >>> Globals.db_column_types['incidences']['id_plume'] {`db_type`: `INTEGER`, `sa_type`: Integer, `py_type`: <class 'int'>} Notes: - Uses `arb.utils.sql_alchemy.get_sa_automap_types()` for reflection. - The resulting `Globals.db_column_types` is used in form pre-population and validation. \"\"\" from arb.utils.sql_alchemy import get_sa_automap_types with flask_app . app_context (): engine = db . engine Globals . db_column_types = get_sa_automap_types ( engine , base ) logger . debug ( f \"Database type mapping: Globals.db_column_types= { Globals . db_column_types } \" ) load_drop_downs ( flask_app , db ) classmethod Load dropdown data from hardcoded configuration and cache it globally. Parameters: flask_app ( Flask ) \u2013 The active Flask app instance. db ( SQLAlchemy ) \u2013 SQLAlchemy instance (not used in this function but passed for consistency). Returns: None \u2013 None Notes Uses get_excel_dropdown_data() from db_hardcoded to populate form options. Populates both Globals.drop_downs and Globals.drop_downs_contingent . Should be called once after app startup or reflection. Source code in arb\\portal\\globals.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @classmethod def load_drop_downs ( cls , flask_app : Flask , db : SQLAlchemy ) -> None : \"\"\" Load dropdown data from hardcoded configuration and cache it globally. Args: flask_app (Flask): The active Flask app instance. db (SQLAlchemy): SQLAlchemy instance (not used in this function but passed for consistency). Returns: None Notes: - Uses `get_excel_dropdown_data()` from `db_hardcoded` to populate form options. - Populates both `Globals.drop_downs` and `Globals.drop_downs_contingent`. - Should be called once after app startup or reflection. \"\"\" from arb.portal.db_hardcoded import get_excel_dropdown_data logger . debug ( \"In load_drop_downs()\" ) Globals . drop_downs , Globals . drop_downs_contingent = get_excel_dropdown_data () logger . debug ( f \"Globals.drop_downs= { Globals . drop_downs } \" ) logger . debug ( f \"Globals.drop_downs_contingent= { Globals . drop_downs_contingent } \" ) load_type_mapping ( flask_app , db , base ) classmethod Populate column type metadata for all reflected tables in the SQLAlchemy base. Parameters: flask_app ( Flask ) \u2013 The current Flask application (used for context scoping). db ( SQLAlchemy ) \u2013 SQLAlchemy instance, already bound to a live database engine. base ( AutomapBase ) \u2013 Reflected SQLAlchemy metadata containing all mapped models. Returns: None \u2013 None Example Globals.load_type_mapping(app, db, base) Globals.db_column_types['incidences']['id_plume'] { db_type : INTEGER , sa_type : Integer, py_type : } Notes Uses arb.utils.sql_alchemy.get_sa_automap_types() for reflection. The resulting Globals.db_column_types is used in form pre-population and validation. Source code in arb\\portal\\globals.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @classmethod def load_type_mapping ( cls , flask_app : Flask , db : SQLAlchemy , base ) -> None : \"\"\" Populate column type metadata for all reflected tables in the SQLAlchemy base. Args: flask_app (Flask): The current Flask application (used for context scoping). db (SQLAlchemy): SQLAlchemy instance, already bound to a live database engine. base (AutomapBase): Reflected SQLAlchemy metadata containing all mapped models. Returns: None Example: >>> Globals.load_type_mapping(app, db, base) >>> Globals.db_column_types['incidences']['id_plume'] {`db_type`: `INTEGER`, `sa_type`: Integer, `py_type`: <class 'int'>} Notes: - Uses `arb.utils.sql_alchemy.get_sa_automap_types()` for reflection. - The resulting `Globals.db_column_types` is used in form pre-population and validation. \"\"\" from arb.utils.sql_alchemy import get_sa_automap_types with flask_app . app_context (): engine = db . engine Globals . db_column_types = get_sa_automap_types ( engine , base ) logger . debug ( f \"Database type mapping: Globals.db_column_types= { Globals . db_column_types } \" )","title":"arb.portal.globals"},{"location":"reference/arb/portal/globals/#arbportalglobals","text":"Global variables and dropdown selector loading for Flask/SQLAlchemy applications. This module provides the Globals class for holding runtime-initialized data structures such as dropdown selectors and database column type mappings. Primary Uses Prevent circular imports in SQLAlchemy/Flask environments Store shared type and dropdown definitions used throughout the app Enable lazy initialization of values dependent on app context Notes Globals are not intended to be mutable after initialization. Centralizes dropdown and type mapping logic for app-wide reuse. Static values that do not require runtime context should live in constants.py .","title":"arb.portal.globals"},{"location":"reference/arb/portal/globals/#arb.portal.globals.Globals","text":"Central class for holding runtime-global mappings used in the Flask app. Attributes: db_column_types ( dict [ str , dict [ str , dict [ str , Any ]]] ) \u2013 Mapping of table.column to SQLAlchemy type metadata (includes db_type , sa_type , py_type ). drop_downs ( dict [ str , list [ str ]] ) \u2013 Field name to independent dropdown options. drop_downs_contingent ( dict [ str , dict [ str , list [ str ]]] ) \u2013 Parent-dependent options for contingent dropdowns (e.g., county \u2192 list of sub-counties). Source code in arb\\portal\\globals.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class Globals : \"\"\" Central class for holding runtime-global mappings used in the Flask app. Attributes: db_column_types (dict[str, dict[str, dict[str, Any]]]): Mapping of table.column to SQLAlchemy type metadata (includes `db_type`, `sa_type`, `py_type`). drop_downs (dict[str, list[str]]): Field name to independent dropdown options. drop_downs_contingent (dict[str, dict[str, list[str]]]): Parent-dependent options for contingent dropdowns (e.g., county \u2192 list of sub-counties). \"\"\" db_column_types = {} drop_downs = {} drop_downs_contingent = {} @classmethod def load_drop_downs ( cls , flask_app : Flask , db : SQLAlchemy ) -> None : \"\"\" Load dropdown data from hardcoded configuration and cache it globally. Args: flask_app (Flask): The active Flask app instance. db (SQLAlchemy): SQLAlchemy instance (not used in this function but passed for consistency). Returns: None Notes: - Uses `get_excel_dropdown_data()` from `db_hardcoded` to populate form options. - Populates both `Globals.drop_downs` and `Globals.drop_downs_contingent`. - Should be called once after app startup or reflection. \"\"\" from arb.portal.db_hardcoded import get_excel_dropdown_data logger . debug ( \"In load_drop_downs()\" ) Globals . drop_downs , Globals . drop_downs_contingent = get_excel_dropdown_data () logger . debug ( f \"Globals.drop_downs= { Globals . drop_downs } \" ) logger . debug ( f \"Globals.drop_downs_contingent= { Globals . drop_downs_contingent } \" ) @classmethod def load_type_mapping ( cls , flask_app : Flask , db : SQLAlchemy , base ) -> None : \"\"\" Populate column type metadata for all reflected tables in the SQLAlchemy base. Args: flask_app (Flask): The current Flask application (used for context scoping). db (SQLAlchemy): SQLAlchemy instance, already bound to a live database engine. base (AutomapBase): Reflected SQLAlchemy metadata containing all mapped models. Returns: None Example: >>> Globals.load_type_mapping(app, db, base) >>> Globals.db_column_types['incidences']['id_plume'] {`db_type`: `INTEGER`, `sa_type`: Integer, `py_type`: <class 'int'>} Notes: - Uses `arb.utils.sql_alchemy.get_sa_automap_types()` for reflection. - The resulting `Globals.db_column_types` is used in form pre-population and validation. \"\"\" from arb.utils.sql_alchemy import get_sa_automap_types with flask_app . app_context (): engine = db . engine Globals . db_column_types = get_sa_automap_types ( engine , base ) logger . debug ( f \"Database type mapping: Globals.db_column_types= { Globals . db_column_types } \" )","title":"Globals"},{"location":"reference/arb/portal/globals/#arb.portal.globals.Globals.load_drop_downs","text":"Load dropdown data from hardcoded configuration and cache it globally. Parameters: flask_app ( Flask ) \u2013 The active Flask app instance. db ( SQLAlchemy ) \u2013 SQLAlchemy instance (not used in this function but passed for consistency). Returns: None \u2013 None Notes Uses get_excel_dropdown_data() from db_hardcoded to populate form options. Populates both Globals.drop_downs and Globals.drop_downs_contingent . Should be called once after app startup or reflection. Source code in arb\\portal\\globals.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 @classmethod def load_drop_downs ( cls , flask_app : Flask , db : SQLAlchemy ) -> None : \"\"\" Load dropdown data from hardcoded configuration and cache it globally. Args: flask_app (Flask): The active Flask app instance. db (SQLAlchemy): SQLAlchemy instance (not used in this function but passed for consistency). Returns: None Notes: - Uses `get_excel_dropdown_data()` from `db_hardcoded` to populate form options. - Populates both `Globals.drop_downs` and `Globals.drop_downs_contingent`. - Should be called once after app startup or reflection. \"\"\" from arb.portal.db_hardcoded import get_excel_dropdown_data logger . debug ( \"In load_drop_downs()\" ) Globals . drop_downs , Globals . drop_downs_contingent = get_excel_dropdown_data () logger . debug ( f \"Globals.drop_downs= { Globals . drop_downs } \" ) logger . debug ( f \"Globals.drop_downs_contingent= { Globals . drop_downs_contingent } \" )","title":"load_drop_downs"},{"location":"reference/arb/portal/globals/#arb.portal.globals.Globals.load_type_mapping","text":"Populate column type metadata for all reflected tables in the SQLAlchemy base. Parameters: flask_app ( Flask ) \u2013 The current Flask application (used for context scoping). db ( SQLAlchemy ) \u2013 SQLAlchemy instance, already bound to a live database engine. base ( AutomapBase ) \u2013 Reflected SQLAlchemy metadata containing all mapped models. Returns: None \u2013 None Example Globals.load_type_mapping(app, db, base) Globals.db_column_types['incidences']['id_plume'] { db_type : INTEGER , sa_type : Integer, py_type : } Notes Uses arb.utils.sql_alchemy.get_sa_automap_types() for reflection. The resulting Globals.db_column_types is used in form pre-population and validation. Source code in arb\\portal\\globals.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 @classmethod def load_type_mapping ( cls , flask_app : Flask , db : SQLAlchemy , base ) -> None : \"\"\" Populate column type metadata for all reflected tables in the SQLAlchemy base. Args: flask_app (Flask): The current Flask application (used for context scoping). db (SQLAlchemy): SQLAlchemy instance, already bound to a live database engine. base (AutomapBase): Reflected SQLAlchemy metadata containing all mapped models. Returns: None Example: >>> Globals.load_type_mapping(app, db, base) >>> Globals.db_column_types['incidences']['id_plume'] {`db_type`: `INTEGER`, `sa_type`: Integer, `py_type`: <class 'int'>} Notes: - Uses `arb.utils.sql_alchemy.get_sa_automap_types()` for reflection. - The resulting `Globals.db_column_types` is used in form pre-population and validation. \"\"\" from arb.utils.sql_alchemy import get_sa_automap_types with flask_app . app_context (): engine = db . engine Globals . db_column_types = get_sa_automap_types ( engine , base ) logger . debug ( f \"Database type mapping: Globals.db_column_types= { Globals . db_column_types } \" )","title":"load_type_mapping"},{"location":"reference/arb/portal/json_update_util/","text":"arb.portal.json_update_util Utility functions to apply updates to a SQLAlchemy model's JSON field and log each change to the portal_updates table for auditing purposes. Features Compares current vs. new values in a model's JSON field Logs only meaningful changes to a structured audit table Excludes no-op or default placeholders (e.g., None, \"\") Typical Use Called when a form submission modifies a feedback record, with changes applied to the model and written to the database via SQLAlchemy. apply_json_patch_and_log ( model , updates , json_field = 'misc_json' , user = 'anonymous' , comments = '' ) Apply updates to a model's JSON field and log each change in portal_updates. This function performs a key-by-key comparison between the current JSON field ( model.misc_json by default) and the proposed updates . For each key where the value has changed: - The field is updated - The change is logged to portal_updates with a timestamp and user info - Redundant or placeholder updates are skipped (e.g., None \u2192 None) Parameters: model ( SQLAlchemy model ) \u2013 A SQLAlchemy ORM instance with a JSON column. updates ( dict ) \u2013 Dictionary of key-value updates to apply. json_field ( str , default: 'misc_json' ) \u2013 Name of the JSON field (default: 'misc_json'). user ( str , default: 'anonymous' ) \u2013 Identifier of the user performing the change (default: 'anonymous'). comments ( str , default: '' ) \u2013 Optional comment for the log entry. Returns: None \u2013 None Raises: AttributeError \u2013 If the specified JSON field does not exist on the model. Example apply_json_patch_and_log(incidence, {\"status\": \"Resolved\"}, user=\"admin\") Source code in arb\\portal\\json_update_util.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def apply_json_patch_and_log ( model , updates : dict , json_field : str = \"misc_json\" , user : str = \"anonymous\" , comments : str = \"\" ) -> None : \"\"\" Apply updates to a model's JSON field and log each change in portal_updates. This function performs a key-by-key comparison between the current JSON field (`model.misc_json` by default) and the proposed `updates`. For each key where the value has changed: - The field is updated - The change is logged to `portal_updates` with a timestamp and user info - Redundant or placeholder updates are skipped (e.g., None \u2192 None) Args: model (SQLAlchemy model): A SQLAlchemy ORM instance with a JSON column. updates (dict): Dictionary of key-value updates to apply. json_field (str): Name of the JSON field (default: 'misc_json'). user (str): Identifier of the user performing the change (default: 'anonymous'). comments (str): Optional comment for the log entry. Returns: None Raises: AttributeError: If the specified JSON field does not exist on the model. Example: >>> apply_json_patch_and_log(incidence, {\"status\": \"Resolved\"}, user=\"admin\") \"\"\" # In the future, may want to handle new rows differently json_data = getattr ( model , json_field ) if json_data is None : json_data = {} is_new_row = True else : is_new_row = False # Consistency check if \"id_incidence\" in json_data and json_data [ \"id_incidence\" ] != model . id_incidence : logger . warning ( f \"[apply_json_patch_and_log] MISMATCH: model.id_incidence= { model . id_incidence } \" f \"!= misc_json['id_incidence']= { json_data [ 'id_incidence' ] } \" ) # Remove id_incidence from updates to avoid contaminating misc_json if \"id_incidence\" in updates : if updates [ \"id_incidence\" ] != model . id_incidence : logger . warning ( f \"[json_update] Removing conflicting id_incidence from updates: \" f \" { updates [ 'id_incidence' ] } \" ) del updates [ \"id_incidence\" ] for key , new_value in updates . items (): old_value = json_data . get ( key ) json_data [ key ] = new_value # Filter out non-useful updates if old_value is None and new_value is None : continue if old_value is None and new_value == \"\" : continue # Note, on the rare situation that \"Please Select\" is a valid entry in a string field, it will be filtered out if old_value is None and new_value == PLEASE_SELECT : continue if old_value != new_value : log_entry = PortalUpdate ( timestamp = datetime . datetime . now ( datetime . UTC ), key = key , old_value = str ( old_value ), new_value = str ( new_value ), user = user , comments = comments or \"\" , id_incidence = model . id_incidence , ) db . session . add ( log_entry ) setattr ( model , json_field , json_data ) flag_modified ( model , json_field ) db . session . commit ()","title":"arb.portal.json_update_util"},{"location":"reference/arb/portal/json_update_util/#arbportaljson_update_util","text":"Utility functions to apply updates to a SQLAlchemy model's JSON field and log each change to the portal_updates table for auditing purposes. Features Compares current vs. new values in a model's JSON field Logs only meaningful changes to a structured audit table Excludes no-op or default placeholders (e.g., None, \"\") Typical Use Called when a form submission modifies a feedback record, with changes applied to the model and written to the database via SQLAlchemy.","title":"arb.portal.json_update_util"},{"location":"reference/arb/portal/json_update_util/#arb.portal.json_update_util.apply_json_patch_and_log","text":"Apply updates to a model's JSON field and log each change in portal_updates. This function performs a key-by-key comparison between the current JSON field ( model.misc_json by default) and the proposed updates . For each key where the value has changed: - The field is updated - The change is logged to portal_updates with a timestamp and user info - Redundant or placeholder updates are skipped (e.g., None \u2192 None) Parameters: model ( SQLAlchemy model ) \u2013 A SQLAlchemy ORM instance with a JSON column. updates ( dict ) \u2013 Dictionary of key-value updates to apply. json_field ( str , default: 'misc_json' ) \u2013 Name of the JSON field (default: 'misc_json'). user ( str , default: 'anonymous' ) \u2013 Identifier of the user performing the change (default: 'anonymous'). comments ( str , default: '' ) \u2013 Optional comment for the log entry. Returns: None \u2013 None Raises: AttributeError \u2013 If the specified JSON field does not exist on the model. Example apply_json_patch_and_log(incidence, {\"status\": \"Resolved\"}, user=\"admin\") Source code in arb\\portal\\json_update_util.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def apply_json_patch_and_log ( model , updates : dict , json_field : str = \"misc_json\" , user : str = \"anonymous\" , comments : str = \"\" ) -> None : \"\"\" Apply updates to a model's JSON field and log each change in portal_updates. This function performs a key-by-key comparison between the current JSON field (`model.misc_json` by default) and the proposed `updates`. For each key where the value has changed: - The field is updated - The change is logged to `portal_updates` with a timestamp and user info - Redundant or placeholder updates are skipped (e.g., None \u2192 None) Args: model (SQLAlchemy model): A SQLAlchemy ORM instance with a JSON column. updates (dict): Dictionary of key-value updates to apply. json_field (str): Name of the JSON field (default: 'misc_json'). user (str): Identifier of the user performing the change (default: 'anonymous'). comments (str): Optional comment for the log entry. Returns: None Raises: AttributeError: If the specified JSON field does not exist on the model. Example: >>> apply_json_patch_and_log(incidence, {\"status\": \"Resolved\"}, user=\"admin\") \"\"\" # In the future, may want to handle new rows differently json_data = getattr ( model , json_field ) if json_data is None : json_data = {} is_new_row = True else : is_new_row = False # Consistency check if \"id_incidence\" in json_data and json_data [ \"id_incidence\" ] != model . id_incidence : logger . warning ( f \"[apply_json_patch_and_log] MISMATCH: model.id_incidence= { model . id_incidence } \" f \"!= misc_json['id_incidence']= { json_data [ 'id_incidence' ] } \" ) # Remove id_incidence from updates to avoid contaminating misc_json if \"id_incidence\" in updates : if updates [ \"id_incidence\" ] != model . id_incidence : logger . warning ( f \"[json_update] Removing conflicting id_incidence from updates: \" f \" { updates [ 'id_incidence' ] } \" ) del updates [ \"id_incidence\" ] for key , new_value in updates . items (): old_value = json_data . get ( key ) json_data [ key ] = new_value # Filter out non-useful updates if old_value is None and new_value is None : continue if old_value is None and new_value == \"\" : continue # Note, on the rare situation that \"Please Select\" is a valid entry in a string field, it will be filtered out if old_value is None and new_value == PLEASE_SELECT : continue if old_value != new_value : log_entry = PortalUpdate ( timestamp = datetime . datetime . now ( datetime . UTC ), key = key , old_value = str ( old_value ), new_value = str ( new_value ), user = user , comments = comments or \"\" , id_incidence = model . id_incidence , ) db . session . add ( log_entry ) setattr ( model , json_field , json_data ) flag_modified ( model , json_field ) db . session . commit ()","title":"apply_json_patch_and_log"},{"location":"reference/arb/portal/routes/","text":"arb.portal.routes Blueprint-based route definitions for the ARB Feedback Portal. This module defines all Flask routes originally found in app.py , now organized under the main Blueprint for modularity. Routes cover Incidence form creation, editing, and deletion File upload and viewing Portal update log display and export Diagnostics and developer views Notes All routes assume that create_app() registers the main Blueprint. Developer diagnostics are inlined near the end of the module. diagnostics () Run diagnostics on the 'incidences' table and show next ID. Returns: str ( str ) \u2013 Rendered HTML showing auto-increment ID diagnostic. Source code in arb\\portal\\routes.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 @main . route ( '/diagnostics' ) def diagnostics () -> str : \"\"\" Run diagnostics on the 'incidences' table and show next ID. Returns: str: Rendered HTML showing auto-increment ID diagnostic. \"\"\" logger . info ( f \"diagnostics() called\" ) result = find_auto_increment_value ( db , \"incidences\" , \"id_incidence\" ) html_content = f \"<p><strong>Diagnostic Results=</strong></p> <p> { result } </p>\" return render_template ( 'diagnostics.html' , header = \"Auto-Increment Check\" , subheader = \"Next available ID value in the 'incidences' table.\" , html_content = html_content , modal_title = \"Success\" , modal_message = \"Diagnostics completed successfully.\" , ) export_portal_updates () Export filtered portal update logs as a downloadable CSV file. Returns: Response ( Response ) \u2013 CSV content as an attachment. Notes Respects filters set in the /portal_updates page. Uses standard CSV headers and UTF-8 encoding. Source code in arb\\portal\\routes.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 @main . route ( \"/portal_updates/export\" ) def export_portal_updates () -> Response : \"\"\" Export filtered portal update logs as a downloadable CSV file. Returns: Response: CSV content as an attachment. Notes: - Respects filters set in the `/portal_updates` page. - Uses standard CSV headers and UTF-8 encoding. \"\"\" from arb.portal.sqla_models import PortalUpdate from flask import request , Response from io import StringIO import csv query = db . session . query ( PortalUpdate ) query = apply_portal_update_filters ( query , PortalUpdate , request . args ) updates = query . order_by ( PortalUpdate . timestamp . desc ()) . all () si = StringIO () writer = csv . writer ( si ) writer . writerow ([ \"timestamp\" , \"key\" , \"old_value\" , \"new_value\" , \"user\" , \"comments\" , \"id_incidence\" ]) for u in updates : writer . writerow ([ u . timestamp , u . key , u . old_value , u . new_value , u . user , u . comments , u . id_incidence or \"\" ]) return Response ( si . getvalue (), mimetype = \"text/csv\" , headers = { \"Content-Disposition\" : \"attachment; filename=portal_updates_export.csv\" } ) incidence_delete ( id_ ) Delete a specified incidence from the database. Parameters: id_ ( int ) \u2013 Primary key of the incidence to delete. Returns: Response ( Response ) \u2013 Redirect to the homepage after deletion. Notes Future: consider adding authorization (e.g., CARB password) to restrict access. Source code in arb\\portal\\routes.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 @main . post ( '/incidence_delete/<int:id_>/' ) def incidence_delete ( id_ ) -> Response : \"\"\" Delete a specified incidence from the database. Args: id_ (int): Primary key of the incidence to delete. Returns: Response: Redirect to the homepage after deletion. Notes: - Future: consider adding authorization (e.g., CARB password) to restrict access. \"\"\" logger . debug ( f \"Updating database with route incidence_delete for id= { id_ } :\" ) base : DeclarativeMeta = current_app . base # type: ignore[attr-defined] table_name = 'incidences' table = get_class_from_table_name ( base , table_name ) model_row = db . session . query ( table ) . get_or_404 ( id_ ) # todo - ensure portal changes are properly updated arb . utils . sql_alchemy . delete_commit_and_log_model ( db , model_row , comment = f 'Deleting incidence row { id_ } ' ) return redirect ( url_for ( 'main.index' )) incidence_update ( id_ ) Display and edit a specific incidence record by ID. Parameters: id_ ( int ) \u2013 Primary key of the incidence to edit. Returns: str | Response \u2013 str|Response: Rendered HTML of the feedback form for the selected incidence, or a redirect to the upload page if the ID is missing. Raises: 500 Internal Server Error \u2013 If multiple records are found for the same ID. Notes Redirects if the ID is not found in the database. Assumes each incidence ID is unique. Source code in arb\\portal\\routes.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @main . route ( '/incidence_update/<int:id_>/' , methods = ( 'GET' , 'POST' )) def incidence_update ( id_ ) -> str | Response : \"\"\" Display and edit a specific incidence record by ID. Args: id_ (int): Primary key of the incidence to edit. Returns: str|Response: Rendered HTML of the feedback form for the selected incidence, or a redirect to the upload page if the ID is missing. Raises: 500 Internal Server Error: If multiple records are found for the same ID. Notes: - Redirects if the ID is not found in the database. - Assumes each incidence ID is unique. \"\"\" logger . debug ( f \"incidence_update called with id= { id_ } .\" ) base : DeclarativeMeta = current_app . base # type: ignore[attr-defined] table_name = 'incidences' table = get_class_from_table_name ( base , table_name ) # get_or_404 uses the tables primary key # model_row = db.session.query(table).get_or_404(id_) # todo turn this into a get and if it is null, then redirect? to the spreadsheet upload # todo consider turning into one_or_none and have error handling rows = db . session . query ( table ) . filter_by ( id_incidence = id_ ) . all () if not rows : message = f \"A request was made to edit a non-existent id_incidence ( { id_ } ). Consider uploading the incidence by importing a spreadsheet.\" return redirect ( url_for ( 'main.upload_file' , message = message )) if len ( rows ) > 1 : abort ( 500 , description = f \"Multiple rows found for id= { id_ } \" ) model_row = rows [ 0 ] sector , sector_type = get_sector_info ( db , base , id_ ) logger . debug ( f \"calling incidence_prep()\" ) return incidence_prep ( model_row , crud_type = 'update' , sector_type = sector_type , default_dropdown = PLEASE_SELECT ) index () Display the homepage with a list of all existing incidence records. Queries the 'incidences' table in descending order of ID and renders the results in a summary table on the landing page. Returns: str ( str ) \u2013 Rendered HTML for the homepage with incidence records. Source code in arb\\portal\\routes.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @main . route ( '/' ) def index () -> str : \"\"\" Display the homepage with a list of all existing incidence records. Queries the 'incidences' table in descending order of ID and renders the results in a summary table on the landing page. Returns: str: Rendered HTML for the homepage with incidence records. \"\"\" base : DeclarativeMeta = current_app . base # type: ignore[attr-defined] table_name = 'incidences' colum_name_pk = 'id_incidence' rows = get_rows_by_table_name ( db , base , table_name , colum_name_pk , ascending = False ) return render_template ( 'index.html' , model_rows = rows ) landfill_incidence_create () Create a new dummy Landfill incidence and redirect to its edit form. Returns: Response ( Response ) \u2013 Redirect to the incidence_update page for the newly created ID. Notes Dummy data is loaded from db_hardcoded.get_landfill_dummy_data() . Source code in arb\\portal\\routes.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 @main . route ( '/landfill_incidence_create/' , methods = ( 'GET' , 'POST' )) def landfill_incidence_create () -> Response : \"\"\" Create a new dummy Landfill incidence and redirect to its edit form. Returns: Response: Redirect to the `incidence_update` page for the newly created ID. Notes: - Dummy data is loaded from `db_hardcoded.get_landfill_dummy_data()`. \"\"\" logger . debug ( f \"landfill_incidence_create called.\" ) base : DeclarativeMeta = current_app . base # type: ignore[attr-defined] table_name = 'incidences' col_name = 'misc_json' data_dict = arb . portal . db_hardcoded . get_landfill_dummy_data () id_ = dict_to_database ( db , base , data_dict , table_name = table_name , json_field = col_name , ) logger . debug ( f \"landfill_incidence_create() - leaving.\" ) return redirect ( url_for ( 'main.incidence_update' , id_ = id_ )) list_uploads () List all files in the upload directory. Returns: str ( str ) \u2013 Rendered HTML showing all uploaded Excel files available on disk. Source code in arb\\portal\\routes.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 @main . route ( '/list_uploads' ) def list_uploads () -> str : \"\"\" List all files in the upload directory. Returns: str: Rendered HTML showing all uploaded Excel files available on disk. \"\"\" logger . debug ( f \"in list_uploads\" ) upload_folder = current_app . config [ \"UPLOAD_FOLDER\" ] # up_dir = Path(\"portal/static/uploads\") # print(f\"{type(up_dir)=}: {up_dir=}\") files = [ x . name for x in upload_folder . iterdir () if x . is_file ()] logger . debug ( f \" { files =} \" ) return render_template ( 'uploads_list.html' , files = files ) og_incidence_create () Create a new dummy Oil & Gas incidence and redirect to its edit form. Returns: Response ( Response ) \u2013 Redirect to the incidence_update page for the newly created ID. Notes Dummy data is loaded from db_hardcoded.get_og_dummy_data() . Source code in arb\\portal\\routes.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 @main . route ( '/og_incidence_create/' , methods = ( 'GET' , 'POST' )) def og_incidence_create () -> Response : \"\"\" Create a new dummy Oil & Gas incidence and redirect to its edit form. Returns: Response: Redirect to the `incidence_update` page for the newly created ID. Notes: - Dummy data is loaded from `db_hardcoded.get_og_dummy_data()`. \"\"\" logger . debug ( f \"og_incidence_create() - beginning.\" ) base : DeclarativeMeta = current_app . base # type: ignore[attr-defined] table_name = 'incidences' col_name = 'misc_json' data_dict = arb . portal . db_hardcoded . get_og_dummy_data () id_ = dict_to_database ( db , base , data_dict , table_name = table_name , json_field = col_name , ) logger . debug ( f \"og_incidence_create() - leaving.\" ) return redirect ( url_for ( 'main.incidence_update' , id_ = id_ )) search () Search route triggered by the navigation bar (stub for future use). Returns: str ( str ) \u2013 Renders a search results page showing the query string. Notes Currently echoes the user-submitted query string. Source code in arb\\portal\\routes.py 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 @main . route ( '/search/' , methods = ( 'GET' , 'POST' )) def search () -> str : \"\"\" Search route triggered by the navigation bar (stub for future use). Returns: str: Renders a search results page showing the query string. Notes: - Currently echoes the user-submitted query string. \"\"\" logger . debug ( f \"In search route:\" ) logger . debug ( f \" { request . form =} \" ) search_string = request . form . get ( 'navbar_search' ) logger . debug ( f \" { search_string =} \" ) return render_template ( 'search.html' , search_string = search_string , ) serve_file ( filename ) Serve a file from the server\u2019s upload directory. Parameters: filename ( str ) \u2013 Name of the file to serve. Returns: Response ( Response ) \u2013 Sends file content to the browser or triggers download. Raises: 404 Not Found \u2013 If the file does not exist on disk. Source code in arb\\portal\\routes.py 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 @main . route ( \"/serve_file/<path:filename>\" ) def serve_file ( filename ) -> Response : \"\"\" Serve a file from the server\u2019s upload directory. Args: filename (str): Name of the file to serve. Returns: Response: Sends file content to the browser or triggers download. Raises: 404 Not Found: If the file does not exist on disk. \"\"\" upload_folder = current_app . config [ \"UPLOAD_FOLDER\" ] file_path = os . path . join ( upload_folder , filename ) if not os . path . exists ( file_path ): abort ( 404 ) return send_from_directory ( upload_folder , filename ) show_database_structure () Show structure of all reflected database columns. Returns: str ( str ) \u2013 Rendered HTML with column type information for all tables. Source code in arb\\portal\\routes.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 @main . route ( '/show_database_structure' ) def show_database_structure () -> str : \"\"\" Show structure of all reflected database columns. Returns: str: Rendered HTML with column type information for all tables. \"\"\" logger . info ( f \"Displaying database structure\" ) result = obj_to_html ( Globals . db_column_types ) result = f \"<p><strong>Postgres Database Structure=</strong></p> <p> { result } </p>\" return render_template ( 'diagnostics.html' , header = \"Database Structure Overview\" , subheader = \"Reflecting SQLAlchemy model metadata.\" , html_content = result , ) show_dropdown_dict () Display current dropdown and contingent dropdown values. Returns: str ( str ) \u2013 Rendered HTML table of dropdown structures. Notes Useful for verifying dropdown contents used in WTForms. Source code in arb\\portal\\routes.py 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 @main . route ( '/show_dropdown_dict' ) def show_dropdown_dict () -> str : \"\"\" Display current dropdown and contingent dropdown values. Returns: str: Rendered HTML table of dropdown structures. Notes: - Useful for verifying dropdown contents used in WTForms. \"\"\" logger . info ( f \"Determining dropdown dict\" ) # update drop-down tables Globals . load_drop_downs ( current_app , db ) result1 = obj_to_html ( Globals . drop_downs ) result2 = obj_to_html ( Globals . drop_downs_contingent ) result = ( f \"<p><strong>Globals.drop_downs=</strong></p> <p> { result1 } </p>\" f \"<p><strong>Globals.drop_downs_contingent=</strong></p> <p> { result2 } </p>\" ) return render_template ( 'diagnostics.html' , header = \"Dropdown Dictionaries\" , subheader = \"Loaded dropdown values and contingent mappings.\" , html_content = result , ) show_feedback_form_structure () Inspect and display WTForms structure for feedback forms. Returns: str ( str ) \u2013 Rendered HTML showing field names/types for OG and Landfill forms. Notes Uses get_wtforms_fields() utility to introspect each form. Source code in arb\\portal\\routes.py 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 @main . route ( '/show_feedback_form_structure' ) def show_feedback_form_structure () -> str : \"\"\" Inspect and display WTForms structure for feedback forms. Returns: str: Rendered HTML showing field names/types for OG and Landfill forms. Notes: - Uses `get_wtforms_fields()` utility to introspect each form. \"\"\" from arb.portal.wtf_landfill import LandfillFeedback from arb.portal.wtf_oil_and_gas import OGFeedback logger . info ( f \"Displaying wtforms structure as a diagnostic\" ) form1 = OGFeedback () fields1 = get_wtforms_fields ( form1 ) result1 = obj_to_html ( fields1 ) form2 = LandfillFeedback () fields2 = get_wtforms_fields ( form2 ) result2 = obj_to_html ( fields2 ) result = ( f \"<p><strong>WTF OGFeedback Form Structure=</strong></p> <p> { result1 } </p>\" f \"<p><strong>WTF LandfillFeedback Form Structure=</strong></p> <p> { result2 } </p>\" ) return render_template ( 'diagnostics.html' , header = \"WTForms Feedback Form Structure\" , subheader = \"Inspecting field mappings in Oil & Gas and Landfill feedback forms.\" , html_content = result , ) show_log_file () Display the contents of the server's current log file. Returns: str ( str ) \u2013 Rendered HTML with the full log file shown inside a block. Notes Useful for debugging in development or staging. Source code in arb\\portal\\routes.py 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 @main . route ( '/show_log_file' ) def show_log_file () -> str : \"\"\" Display the contents of the server's current log file. Returns: str: Rendered HTML with the full log file shown inside a <pre> block. Notes: - Useful for debugging in development or staging. \"\"\" logger . info ( f \"Displaying the log file as a diagnostic\" ) with open ( LOG_FILE , 'r' ) as file : file_content = file . read () # result = obj_to_html(file_content) result = f \"<p><strong>Logger file content=</strong></p> <p><pre> { file_content } </pre></p>\" return render_template ( 'diagnostics.html' , header = \"Log File Contents\" , # subheader=\"Full log output from the running server instance.\", html_content = result , ) upload_file ( message = None ) Upload an Excel file and process its contents. Parameters: message ( str | None , default: None ) \u2013 Optional error/info message passed via redirect. Returns: str | Response \u2013 str | Response: Renders the upload form or redirects to incidence update. Notes Supports drag-and-drop Excel upload. Catches and logs exceptions during upload and parsing. Source code in arb\\portal\\routes.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 @main . route ( '/upload' , methods = [ 'GET' , 'POST' ]) @main . route ( '/upload/<message>' , methods = [ 'GET' , 'POST' ]) def upload_file ( message = None ) -> str | Response : \"\"\" Upload an Excel file and process its contents. Args: message (str | None): Optional error/info message passed via redirect. Returns: str | Response: Renders the upload form or redirects to incidence update. Notes: - Supports drag-and-drop Excel upload. - Catches and logs exceptions during upload and parsing. \"\"\" logger . debug ( \"upload_file route called.\" ) base : DeclarativeMeta = current_app . base # type: ignore[attr-defined] form = UploadForm () # Handle optional URL message if message : message = unquote ( message ) logger . debug ( f \"upload_file called with message: { message } \" ) upload_dir = current_app . config [ 'UPLOAD_FOLDER' ] logger . debug ( f \"Upload request with: { request . files =} , upload_dir= { upload_dir } \" ) if request . method == 'POST' : try : if 'file' not in request . files or not request . files [ 'file' ] . filename : logger . warning ( \"No file selected in POST request.\" ) return render_template ( 'upload.html' , upload_message = \"No file selected. Please choose a file.\" ) request_file = request . files [ 'file' ] logger . debug ( f \"Received uploaded file: { request_file . filename } \" ) if request_file : # todo - little confusing how the update logic works cascading from xl to json, etc # consider making the steps and function names a little clearer to help the # update to change logging file_name , id_ , sector = upload_and_update_db ( db , upload_dir , request_file , base ) logger . debug ( f \" { sector =} \" ) if id_ : logger . debug ( f \"Upload successful: redirecting to incidence update for id= { id_ } \" ) return redirect ( url_for ( 'main.incidence_update' , id_ = id_ )) else : logger . debug ( f \"Upload did not match expected format: { file_name =} \" ) return render_template ( 'upload.html' , form = form , upload_message = f \"Uploaded file: { file_name . name } \u2014 format not recognized.\" ) except Exception as e : logger . exception ( \"Error occurred during file upload.\" ) return render_template ( 'upload.html' , upload_message = \"Error: Could not process the uploaded file. \" \"Make sure it is closed and try again.\" ) # GET request return render_template ( 'upload.html' , form = form , upload_message = message ) view_portal_updates () Display a table of all updates recorded in portal_updates . Returns: str ( str ) \u2013 Rendered HTML with sortable and filterable update logs. Notes Supports pagination, filtering, and sorting via query parameters. Default sort is descending by timestamp. Source code in arb\\portal\\routes.py 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 @main . route ( \"/portal_updates\" ) def view_portal_updates () -> str : \"\"\" Display a table of all updates recorded in `portal_updates`. Returns: str: Rendered HTML with sortable and filterable update logs. Notes: - Supports pagination, filtering, and sorting via query parameters. - Default sort is descending by timestamp. \"\"\" from arb.portal.sqla_models import PortalUpdate from flask import request , render_template sort_by = request . args . get ( \"sort_by\" , \"timestamp\" ) direction = request . args . get ( \"direction\" , \"desc\" ) page = int ( request . args . get ( \"page\" , 1 )) per_page = int ( request . args . get ( \"per_page\" , 100 )) query = db . session . query ( PortalUpdate ) query = apply_portal_update_filters ( query , PortalUpdate , request . args ) updates = query . order_by ( PortalUpdate . timestamp . desc ()) . all () return render_template ( \"portal_updates.html\" , updates = updates , sort_by = sort_by , direction = direction , page = page , per_page = per_page , total_pages = 1 , filter_key = request . args . get ( \"filter_key\" , \"\" ) . strip (), filter_user = request . args . get ( \"filter_user\" , \"\" ) . strip (), filter_comments = request . args . get ( \"filter_comments\" , \"\" ) . strip (), filter_id_incidence = request . args . get ( \"filter_id_incidence\" , \"\" ) . strip (), start_date = request . args . get ( \"start_date\" , \"\" ) . strip (), end_date = request . args . get ( \"end_date\" , \"\" ) . strip (), )","title":"arb.portal.routes"},{"location":"reference/arb/portal/routes/#arbportalroutes","text":"Blueprint-based route definitions for the ARB Feedback Portal. This module defines all Flask routes originally found in app.py , now organized under the main Blueprint for modularity. Routes cover Incidence form creation, editing, and deletion File upload and viewing Portal update log display and export Diagnostics and developer views Notes All routes assume that create_app() registers the main Blueprint. Developer diagnostics are inlined near the end of the module.","title":"arb.portal.routes"},{"location":"reference/arb/portal/routes/#arb.portal.routes.diagnostics","text":"Run diagnostics on the 'incidences' table and show next ID. Returns: str ( str ) \u2013 Rendered HTML showing auto-increment ID diagnostic. Source code in arb\\portal\\routes.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 @main . route ( '/diagnostics' ) def diagnostics () -> str : \"\"\" Run diagnostics on the 'incidences' table and show next ID. Returns: str: Rendered HTML showing auto-increment ID diagnostic. \"\"\" logger . info ( f \"diagnostics() called\" ) result = find_auto_increment_value ( db , \"incidences\" , \"id_incidence\" ) html_content = f \"<p><strong>Diagnostic Results=</strong></p> <p> { result } </p>\" return render_template ( 'diagnostics.html' , header = \"Auto-Increment Check\" , subheader = \"Next available ID value in the 'incidences' table.\" , html_content = html_content , modal_title = \"Success\" , modal_message = \"Diagnostics completed successfully.\" , )","title":"diagnostics"},{"location":"reference/arb/portal/routes/#arb.portal.routes.export_portal_updates","text":"Export filtered portal update logs as a downloadable CSV file. Returns: Response ( Response ) \u2013 CSV content as an attachment. Notes Respects filters set in the /portal_updates page. Uses standard CSV headers and UTF-8 encoding. Source code in arb\\portal\\routes.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 @main . route ( \"/portal_updates/export\" ) def export_portal_updates () -> Response : \"\"\" Export filtered portal update logs as a downloadable CSV file. Returns: Response: CSV content as an attachment. Notes: - Respects filters set in the `/portal_updates` page. - Uses standard CSV headers and UTF-8 encoding. \"\"\" from arb.portal.sqla_models import PortalUpdate from flask import request , Response from io import StringIO import csv query = db . session . query ( PortalUpdate ) query = apply_portal_update_filters ( query , PortalUpdate , request . args ) updates = query . order_by ( PortalUpdate . timestamp . desc ()) . all () si = StringIO () writer = csv . writer ( si ) writer . writerow ([ \"timestamp\" , \"key\" , \"old_value\" , \"new_value\" , \"user\" , \"comments\" , \"id_incidence\" ]) for u in updates : writer . writerow ([ u . timestamp , u . key , u . old_value , u . new_value , u . user , u . comments , u . id_incidence or \"\" ]) return Response ( si . getvalue (), mimetype = \"text/csv\" , headers = { \"Content-Disposition\" : \"attachment; filename=portal_updates_export.csv\" } )","title":"export_portal_updates"},{"location":"reference/arb/portal/routes/#arb.portal.routes.incidence_delete","text":"Delete a specified incidence from the database. Parameters: id_ ( int ) \u2013 Primary key of the incidence to delete. Returns: Response ( Response ) \u2013 Redirect to the homepage after deletion. Notes Future: consider adding authorization (e.g., CARB password) to restrict access. Source code in arb\\portal\\routes.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 @main . post ( '/incidence_delete/<int:id_>/' ) def incidence_delete ( id_ ) -> Response : \"\"\" Delete a specified incidence from the database. Args: id_ (int): Primary key of the incidence to delete. Returns: Response: Redirect to the homepage after deletion. Notes: - Future: consider adding authorization (e.g., CARB password) to restrict access. \"\"\" logger . debug ( f \"Updating database with route incidence_delete for id= { id_ } :\" ) base : DeclarativeMeta = current_app . base # type: ignore[attr-defined] table_name = 'incidences' table = get_class_from_table_name ( base , table_name ) model_row = db . session . query ( table ) . get_or_404 ( id_ ) # todo - ensure portal changes are properly updated arb . utils . sql_alchemy . delete_commit_and_log_model ( db , model_row , comment = f 'Deleting incidence row { id_ } ' ) return redirect ( url_for ( 'main.index' ))","title":"incidence_delete"},{"location":"reference/arb/portal/routes/#arb.portal.routes.incidence_update","text":"Display and edit a specific incidence record by ID. Parameters: id_ ( int ) \u2013 Primary key of the incidence to edit. Returns: str | Response \u2013 str|Response: Rendered HTML of the feedback form for the selected incidence, or a redirect to the upload page if the ID is missing. Raises: 500 Internal Server Error \u2013 If multiple records are found for the same ID. Notes Redirects if the ID is not found in the database. Assumes each incidence ID is unique. Source code in arb\\portal\\routes.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 @main . route ( '/incidence_update/<int:id_>/' , methods = ( 'GET' , 'POST' )) def incidence_update ( id_ ) -> str | Response : \"\"\" Display and edit a specific incidence record by ID. Args: id_ (int): Primary key of the incidence to edit. Returns: str|Response: Rendered HTML of the feedback form for the selected incidence, or a redirect to the upload page if the ID is missing. Raises: 500 Internal Server Error: If multiple records are found for the same ID. Notes: - Redirects if the ID is not found in the database. - Assumes each incidence ID is unique. \"\"\" logger . debug ( f \"incidence_update called with id= { id_ } .\" ) base : DeclarativeMeta = current_app . base # type: ignore[attr-defined] table_name = 'incidences' table = get_class_from_table_name ( base , table_name ) # get_or_404 uses the tables primary key # model_row = db.session.query(table).get_or_404(id_) # todo turn this into a get and if it is null, then redirect? to the spreadsheet upload # todo consider turning into one_or_none and have error handling rows = db . session . query ( table ) . filter_by ( id_incidence = id_ ) . all () if not rows : message = f \"A request was made to edit a non-existent id_incidence ( { id_ } ). Consider uploading the incidence by importing a spreadsheet.\" return redirect ( url_for ( 'main.upload_file' , message = message )) if len ( rows ) > 1 : abort ( 500 , description = f \"Multiple rows found for id= { id_ } \" ) model_row = rows [ 0 ] sector , sector_type = get_sector_info ( db , base , id_ ) logger . debug ( f \"calling incidence_prep()\" ) return incidence_prep ( model_row , crud_type = 'update' , sector_type = sector_type , default_dropdown = PLEASE_SELECT )","title":"incidence_update"},{"location":"reference/arb/portal/routes/#arb.portal.routes.index","text":"Display the homepage with a list of all existing incidence records. Queries the 'incidences' table in descending order of ID and renders the results in a summary table on the landing page. Returns: str ( str ) \u2013 Rendered HTML for the homepage with incidence records. Source code in arb\\portal\\routes.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 @main . route ( '/' ) def index () -> str : \"\"\" Display the homepage with a list of all existing incidence records. Queries the 'incidences' table in descending order of ID and renders the results in a summary table on the landing page. Returns: str: Rendered HTML for the homepage with incidence records. \"\"\" base : DeclarativeMeta = current_app . base # type: ignore[attr-defined] table_name = 'incidences' colum_name_pk = 'id_incidence' rows = get_rows_by_table_name ( db , base , table_name , colum_name_pk , ascending = False ) return render_template ( 'index.html' , model_rows = rows )","title":"index"},{"location":"reference/arb/portal/routes/#arb.portal.routes.landfill_incidence_create","text":"Create a new dummy Landfill incidence and redirect to its edit form. Returns: Response ( Response ) \u2013 Redirect to the incidence_update page for the newly created ID. Notes Dummy data is loaded from db_hardcoded.get_landfill_dummy_data() . Source code in arb\\portal\\routes.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 @main . route ( '/landfill_incidence_create/' , methods = ( 'GET' , 'POST' )) def landfill_incidence_create () -> Response : \"\"\" Create a new dummy Landfill incidence and redirect to its edit form. Returns: Response: Redirect to the `incidence_update` page for the newly created ID. Notes: - Dummy data is loaded from `db_hardcoded.get_landfill_dummy_data()`. \"\"\" logger . debug ( f \"landfill_incidence_create called.\" ) base : DeclarativeMeta = current_app . base # type: ignore[attr-defined] table_name = 'incidences' col_name = 'misc_json' data_dict = arb . portal . db_hardcoded . get_landfill_dummy_data () id_ = dict_to_database ( db , base , data_dict , table_name = table_name , json_field = col_name , ) logger . debug ( f \"landfill_incidence_create() - leaving.\" ) return redirect ( url_for ( 'main.incidence_update' , id_ = id_ ))","title":"landfill_incidence_create"},{"location":"reference/arb/portal/routes/#arb.portal.routes.list_uploads","text":"List all files in the upload directory. Returns: str ( str ) \u2013 Rendered HTML showing all uploaded Excel files available on disk. Source code in arb\\portal\\routes.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 @main . route ( '/list_uploads' ) def list_uploads () -> str : \"\"\" List all files in the upload directory. Returns: str: Rendered HTML showing all uploaded Excel files available on disk. \"\"\" logger . debug ( f \"in list_uploads\" ) upload_folder = current_app . config [ \"UPLOAD_FOLDER\" ] # up_dir = Path(\"portal/static/uploads\") # print(f\"{type(up_dir)=}: {up_dir=}\") files = [ x . name for x in upload_folder . iterdir () if x . is_file ()] logger . debug ( f \" { files =} \" ) return render_template ( 'uploads_list.html' , files = files )","title":"list_uploads"},{"location":"reference/arb/portal/routes/#arb.portal.routes.og_incidence_create","text":"Create a new dummy Oil & Gas incidence and redirect to its edit form. Returns: Response ( Response ) \u2013 Redirect to the incidence_update page for the newly created ID. Notes Dummy data is loaded from db_hardcoded.get_og_dummy_data() . Source code in arb\\portal\\routes.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 @main . route ( '/og_incidence_create/' , methods = ( 'GET' , 'POST' )) def og_incidence_create () -> Response : \"\"\" Create a new dummy Oil & Gas incidence and redirect to its edit form. Returns: Response: Redirect to the `incidence_update` page for the newly created ID. Notes: - Dummy data is loaded from `db_hardcoded.get_og_dummy_data()`. \"\"\" logger . debug ( f \"og_incidence_create() - beginning.\" ) base : DeclarativeMeta = current_app . base # type: ignore[attr-defined] table_name = 'incidences' col_name = 'misc_json' data_dict = arb . portal . db_hardcoded . get_og_dummy_data () id_ = dict_to_database ( db , base , data_dict , table_name = table_name , json_field = col_name , ) logger . debug ( f \"og_incidence_create() - leaving.\" ) return redirect ( url_for ( 'main.incidence_update' , id_ = id_ ))","title":"og_incidence_create"},{"location":"reference/arb/portal/routes/#arb.portal.routes.search","text":"Search route triggered by the navigation bar (stub for future use). Returns: str ( str ) \u2013 Renders a search results page showing the query string. Notes Currently echoes the user-submitted query string. Source code in arb\\portal\\routes.py 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 @main . route ( '/search/' , methods = ( 'GET' , 'POST' )) def search () -> str : \"\"\" Search route triggered by the navigation bar (stub for future use). Returns: str: Renders a search results page showing the query string. Notes: - Currently echoes the user-submitted query string. \"\"\" logger . debug ( f \"In search route:\" ) logger . debug ( f \" { request . form =} \" ) search_string = request . form . get ( 'navbar_search' ) logger . debug ( f \" { search_string =} \" ) return render_template ( 'search.html' , search_string = search_string , )","title":"search"},{"location":"reference/arb/portal/routes/#arb.portal.routes.serve_file","text":"Serve a file from the server\u2019s upload directory. Parameters: filename ( str ) \u2013 Name of the file to serve. Returns: Response ( Response ) \u2013 Sends file content to the browser or triggers download. Raises: 404 Not Found \u2013 If the file does not exist on disk. Source code in arb\\portal\\routes.py 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 @main . route ( \"/serve_file/<path:filename>\" ) def serve_file ( filename ) -> Response : \"\"\" Serve a file from the server\u2019s upload directory. Args: filename (str): Name of the file to serve. Returns: Response: Sends file content to the browser or triggers download. Raises: 404 Not Found: If the file does not exist on disk. \"\"\" upload_folder = current_app . config [ \"UPLOAD_FOLDER\" ] file_path = os . path . join ( upload_folder , filename ) if not os . path . exists ( file_path ): abort ( 404 ) return send_from_directory ( upload_folder , filename )","title":"serve_file"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_database_structure","text":"Show structure of all reflected database columns. Returns: str ( str ) \u2013 Rendered HTML with column type information for all tables. Source code in arb\\portal\\routes.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 @main . route ( '/show_database_structure' ) def show_database_structure () -> str : \"\"\" Show structure of all reflected database columns. Returns: str: Rendered HTML with column type information for all tables. \"\"\" logger . info ( f \"Displaying database structure\" ) result = obj_to_html ( Globals . db_column_types ) result = f \"<p><strong>Postgres Database Structure=</strong></p> <p> { result } </p>\" return render_template ( 'diagnostics.html' , header = \"Database Structure Overview\" , subheader = \"Reflecting SQLAlchemy model metadata.\" , html_content = result , )","title":"show_database_structure"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_dropdown_dict","text":"Display current dropdown and contingent dropdown values. Returns: str ( str ) \u2013 Rendered HTML table of dropdown structures. Notes Useful for verifying dropdown contents used in WTForms. Source code in arb\\portal\\routes.py 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 @main . route ( '/show_dropdown_dict' ) def show_dropdown_dict () -> str : \"\"\" Display current dropdown and contingent dropdown values. Returns: str: Rendered HTML table of dropdown structures. Notes: - Useful for verifying dropdown contents used in WTForms. \"\"\" logger . info ( f \"Determining dropdown dict\" ) # update drop-down tables Globals . load_drop_downs ( current_app , db ) result1 = obj_to_html ( Globals . drop_downs ) result2 = obj_to_html ( Globals . drop_downs_contingent ) result = ( f \"<p><strong>Globals.drop_downs=</strong></p> <p> { result1 } </p>\" f \"<p><strong>Globals.drop_downs_contingent=</strong></p> <p> { result2 } </p>\" ) return render_template ( 'diagnostics.html' , header = \"Dropdown Dictionaries\" , subheader = \"Loaded dropdown values and contingent mappings.\" , html_content = result , )","title":"show_dropdown_dict"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_feedback_form_structure","text":"Inspect and display WTForms structure for feedback forms. Returns: str ( str ) \u2013 Rendered HTML showing field names/types for OG and Landfill forms. Notes Uses get_wtforms_fields() utility to introspect each form. Source code in arb\\portal\\routes.py 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 @main . route ( '/show_feedback_form_structure' ) def show_feedback_form_structure () -> str : \"\"\" Inspect and display WTForms structure for feedback forms. Returns: str: Rendered HTML showing field names/types for OG and Landfill forms. Notes: - Uses `get_wtforms_fields()` utility to introspect each form. \"\"\" from arb.portal.wtf_landfill import LandfillFeedback from arb.portal.wtf_oil_and_gas import OGFeedback logger . info ( f \"Displaying wtforms structure as a diagnostic\" ) form1 = OGFeedback () fields1 = get_wtforms_fields ( form1 ) result1 = obj_to_html ( fields1 ) form2 = LandfillFeedback () fields2 = get_wtforms_fields ( form2 ) result2 = obj_to_html ( fields2 ) result = ( f \"<p><strong>WTF OGFeedback Form Structure=</strong></p> <p> { result1 } </p>\" f \"<p><strong>WTF LandfillFeedback Form Structure=</strong></p> <p> { result2 } </p>\" ) return render_template ( 'diagnostics.html' , header = \"WTForms Feedback Form Structure\" , subheader = \"Inspecting field mappings in Oil & Gas and Landfill feedback forms.\" , html_content = result , )","title":"show_feedback_form_structure"},{"location":"reference/arb/portal/routes/#arb.portal.routes.show_log_file","text":"Display the contents of the server's current log file. Returns: str ( str ) \u2013 Rendered HTML with the full log file shown inside a block. Notes Useful for debugging in development or staging. Source code in arb\\portal\\routes.py 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 @main . route ( '/show_log_file' ) def show_log_file () -> str : \"\"\" Display the contents of the server's current log file. Returns: str: Rendered HTML with the full log file shown inside a <pre> block. Notes: - Useful for debugging in development or staging. \"\"\" logger . info ( f \"Displaying the log file as a diagnostic\" ) with open ( LOG_FILE , 'r' ) as file : file_content = file . read () # result = obj_to_html(file_content) result = f \"<p><strong>Logger file content=</strong></p> <p><pre> { file_content } </pre></p>\" return render_template ( 'diagnostics.html' , header = \"Log File Contents\" , # subheader=\"Full log output from the running server instance.\", html_content = result , )","title":"show_log_file"},{"location":"reference/arb/portal/routes/#arb.portal.routes.upload_file","text":"Upload an Excel file and process its contents. Parameters: message ( str | None , default: None ) \u2013 Optional error/info message passed via redirect. Returns: str | Response \u2013 str | Response: Renders the upload form or redirects to incidence update. Notes Supports drag-and-drop Excel upload. Catches and logs exceptions during upload and parsing. Source code in arb\\portal\\routes.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 @main . route ( '/upload' , methods = [ 'GET' , 'POST' ]) @main . route ( '/upload/<message>' , methods = [ 'GET' , 'POST' ]) def upload_file ( message = None ) -> str | Response : \"\"\" Upload an Excel file and process its contents. Args: message (str | None): Optional error/info message passed via redirect. Returns: str | Response: Renders the upload form or redirects to incidence update. Notes: - Supports drag-and-drop Excel upload. - Catches and logs exceptions during upload and parsing. \"\"\" logger . debug ( \"upload_file route called.\" ) base : DeclarativeMeta = current_app . base # type: ignore[attr-defined] form = UploadForm () # Handle optional URL message if message : message = unquote ( message ) logger . debug ( f \"upload_file called with message: { message } \" ) upload_dir = current_app . config [ 'UPLOAD_FOLDER' ] logger . debug ( f \"Upload request with: { request . files =} , upload_dir= { upload_dir } \" ) if request . method == 'POST' : try : if 'file' not in request . files or not request . files [ 'file' ] . filename : logger . warning ( \"No file selected in POST request.\" ) return render_template ( 'upload.html' , upload_message = \"No file selected. Please choose a file.\" ) request_file = request . files [ 'file' ] logger . debug ( f \"Received uploaded file: { request_file . filename } \" ) if request_file : # todo - little confusing how the update logic works cascading from xl to json, etc # consider making the steps and function names a little clearer to help the # update to change logging file_name , id_ , sector = upload_and_update_db ( db , upload_dir , request_file , base ) logger . debug ( f \" { sector =} \" ) if id_ : logger . debug ( f \"Upload successful: redirecting to incidence update for id= { id_ } \" ) return redirect ( url_for ( 'main.incidence_update' , id_ = id_ )) else : logger . debug ( f \"Upload did not match expected format: { file_name =} \" ) return render_template ( 'upload.html' , form = form , upload_message = f \"Uploaded file: { file_name . name } \u2014 format not recognized.\" ) except Exception as e : logger . exception ( \"Error occurred during file upload.\" ) return render_template ( 'upload.html' , upload_message = \"Error: Could not process the uploaded file. \" \"Make sure it is closed and try again.\" ) # GET request return render_template ( 'upload.html' , form = form , upload_message = message )","title":"upload_file"},{"location":"reference/arb/portal/routes/#arb.portal.routes.view_portal_updates","text":"Display a table of all updates recorded in portal_updates . Returns: str ( str ) \u2013 Rendered HTML with sortable and filterable update logs. Notes Supports pagination, filtering, and sorting via query parameters. Default sort is descending by timestamp. Source code in arb\\portal\\routes.py 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 @main . route ( \"/portal_updates\" ) def view_portal_updates () -> str : \"\"\" Display a table of all updates recorded in `portal_updates`. Returns: str: Rendered HTML with sortable and filterable update logs. Notes: - Supports pagination, filtering, and sorting via query parameters. - Default sort is descending by timestamp. \"\"\" from arb.portal.sqla_models import PortalUpdate from flask import request , render_template sort_by = request . args . get ( \"sort_by\" , \"timestamp\" ) direction = request . args . get ( \"direction\" , \"desc\" ) page = int ( request . args . get ( \"page\" , 1 )) per_page = int ( request . args . get ( \"per_page\" , 100 )) query = db . session . query ( PortalUpdate ) query = apply_portal_update_filters ( query , PortalUpdate , request . args ) updates = query . order_by ( PortalUpdate . timestamp . desc ()) . all () return render_template ( \"portal_updates.html\" , updates = updates , sort_by = sort_by , direction = direction , page = page , per_page = per_page , total_pages = 1 , filter_key = request . args . get ( \"filter_key\" , \"\" ) . strip (), filter_user = request . args . get ( \"filter_user\" , \"\" ) . strip (), filter_comments = request . args . get ( \"filter_comments\" , \"\" ) . strip (), filter_id_incidence = request . args . get ( \"filter_id_incidence\" , \"\" ) . strip (), start_date = request . args . get ( \"start_date\" , \"\" ) . strip (), end_date = request . args . get ( \"end_date\" , \"\" ) . strip (), )","title":"view_portal_updates"},{"location":"reference/arb/portal/sqla_models/","text":"arb.portal.sqla_models SQLAlchemy model definitions for the ARB Feedback Portal. This module defines ORM classes that map to key tables in the database, including uploaded file metadata and portal JSON update logs. Notes Only models explicitly defined here will be created by SQLAlchemy via db.create_all() . Most schema inspection and data access for incidences is handled dynamically via reflection. Timezone-aware UTC timestamps are used on all tracked models. All models inherit from db.Model , and can be directly queried with SQLAlchemy syntax. Example file = UploadedFile(path=\"uploads/report.xlsx\", status=\"pending\") db.session.add(file) db.session.commit() PortalUpdate Bases: Model SQLAlchemy model tracking updates to the misc_json field on incidence records. Used for auditing key/value changes made through the portal interface. Each row represents a single change to a single field on a specific incidence. Table Name portal_updates Columns id (int): Primary key. timestamp (datetime): UTC time when the change was logged. key (str): JSON key that was modified. old_value (str | None): Previous value (nullable). new_value (str): New value. user (str): Username or identifier of the user making the change. comments (str): Optional explanatory comment. id_incidence (int | None): Foreign key to the modified incidence (nullable). Notes Automatically populated by apply_json_patch_and_log() . Used for rendering the portal_updates.html table. Source code in arb\\portal\\sqla_models.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class PortalUpdate ( db . Model ): \"\"\" SQLAlchemy model tracking updates to the misc_json field on incidence records. Used for auditing key/value changes made through the portal interface. Each row represents a single change to a single field on a specific incidence. Table Name: portal_updates Columns: id (int): Primary key. timestamp (datetime): UTC time when the change was logged. key (str): JSON key that was modified. old_value (str | None): Previous value (nullable). new_value (str): New value. user (str): Username or identifier of the user making the change. comments (str): Optional explanatory comment. id_incidence (int | None): Foreign key to the modified incidence (nullable). Notes: - Automatically populated by `apply_json_patch_and_log()`. - Used for rendering the `portal_updates.html` table. \"\"\" __tablename__ = \"portal_updates\" id = Column ( Integer , primary_key = True ) timestamp = Column ( DateTime ( timezone = True ), nullable = False , server_default = func . now ()) key = Column ( String ( 255 ), nullable = False ) old_value = Column ( Text , nullable = True ) new_value = Column ( Text , nullable = False ) user = Column ( String ( 255 ), nullable = False , default = \"anonymous\" ) comments = Column ( Text , nullable = False , default = \"\" ) id_incidence = Column ( Integer , nullable = True ) def __repr__ ( self ): return ( f \"<PortalUpdate id= { self . id } key= { self . key !r} old= { self . old_value !r} \" f \"new= { self . new_value !r} user= { self . user !r} at= { self . timestamp } >\" ) UploadedFile Bases: Model SQLAlchemy model representing a user-uploaded file. Stores metadata for files uploaded via the portal interface, including the file path, status, and optional description. Automatically tracks creation and last modification timestamps. Table Name uploaded_files Columns id_ (int): Primary key. path (str): Filesystem path to the uploaded file. description (str | None): Optional human-friendly explanation. status (str | None): Upload status, e.g., 'pending', 'processed', or 'error'. created_timestamp (datetime): UTC timestamp of initial creation. modified_timestamp (datetime): UTC timestamp of last update. Example file = UploadedFile(path=\"uploads/test.xlsx\", status=\"pending\") db.session.add(file) db.session.commit() Notes Timestamps use UTC and are timezone-aware. This table is managed by SQLAlchemy directly (not introspected). Source code in arb\\portal\\sqla_models.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class UploadedFile ( db . Model ): \"\"\" SQLAlchemy model representing a user-uploaded file. Stores metadata for files uploaded via the portal interface, including the file path, status, and optional description. Automatically tracks creation and last modification timestamps. Table Name: uploaded_files Columns: id_ (int): Primary key. path (str): Filesystem path to the uploaded file. description (str | None): Optional human-friendly explanation. status (str | None): Upload status, e.g., 'pending', 'processed', or 'error'. created_timestamp (datetime): UTC timestamp of initial creation. modified_timestamp (datetime): UTC timestamp of last update. Example: >>> file = UploadedFile(path=\"uploads/test.xlsx\", status=\"pending\") >>> db.session.add(file) >>> db.session.commit() Notes: - Timestamps use UTC and are timezone-aware. - This table is managed by SQLAlchemy directly (not introspected). \"\"\" __tablename__ = \"uploaded_files\" id_ = db . Column ( db . Integer , primary_key = True ) path = db . Column ( db . Text , nullable = False ) description = db . Column ( db . Text , nullable = True ) status = db . Column ( db . Text , nullable = True ) created_timestamp = db . Column ( db . DateTime ( timezone = True ), server_default = func . now () ) modified_timestamp = db . Column ( db . DateTime ( timezone = True ), server_default = func . now () ) def __repr__ ( self ) -> str : \"\"\" Return a human-readable string representation of the uploaded file record. Returns: str: Summary string showing the ID, path, description, and status. Example: >>> repr(UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\")) '<Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done>' \"\"\" return ( f '<Uploaded File: { self . id_ } , Path: { self . path } , ' f 'Description: { self . description } , Status: { self . status } >' ) __repr__ () Return a human-readable string representation of the uploaded file record. Returns: str ( str ) \u2013 Summary string showing the ID, path, description, and status. Example repr(UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\")) ' ' Source code in arb\\portal\\sqla_models.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __repr__ ( self ) -> str : \"\"\" Return a human-readable string representation of the uploaded file record. Returns: str: Summary string showing the ID, path, description, and status. Example: >>> repr(UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\")) '<Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done>' \"\"\" return ( f '<Uploaded File: { self . id_ } , Path: { self . path } , ' f 'Description: { self . description } , Status: { self . status } >' ) run_diagnostics () Run a test transaction to validate UploadedFile model functionality. This utility performs an insert, fetch, and rollback on the UploadedFile model to verify that the ORM mapping and database connection are working. Returns: None \u2013 None Raises: RuntimeError \u2013 If database access or fetch fails. Notes Meant for developer use in test environments only. This function leaves no data in the database due to rollback. Logs diagnostic information using the project logger. Source code in arb\\portal\\sqla_models.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def run_diagnostics () -> None : \"\"\" Run a test transaction to validate UploadedFile model functionality. This utility performs an insert, fetch, and rollback on the UploadedFile model to verify that the ORM mapping and database connection are working. Returns: None Raises: RuntimeError: If database access or fetch fails. Notes: - Meant for developer use in test environments only. - This function leaves no data in the database due to rollback. - Logs diagnostic information using the project logger. \"\"\" logger . info ( \"Running UploadedFile diagnostics...\" ) try : logger . debug ( \"Beginning diagnostic transaction...\" ) test_file = UploadedFile ( path = \"uploads/test_file.xlsx\" , description = \"Diagnostic test file\" , status = \"testing\" ) db . session . add ( test_file ) db . session . flush () # Ensures test_file.id_ is populated logger . info ( f \"Inserted test file with ID: { test_file . id_ } \" ) fetched = UploadedFile . query . get ( test_file . id_ ) if fetched is None : raise RuntimeError ( \"Failed to retrieve inserted UploadedFile instance.\" ) logger . info ( f \"Fetched file: { fetched } \" ) logger . debug ( f \"repr: { repr ( fetched ) } \" ) logger . debug ( f \"created_timestamp: { fetched . created_timestamp } \" ) except SQLAlchemyError as e : logger . exception ( \"SQLAlchemy error during diagnostics.\" ) raise RuntimeError ( \"Database error during UploadedFile diagnostics.\" ) from e finally : logger . debug ( \"Rolling back diagnostic transaction.\" ) db . session . rollback () logger . info ( \"Diagnostics completed and transaction rolled back.\" )","title":"arb.portal.sqla_models"},{"location":"reference/arb/portal/sqla_models/#arbportalsqla_models","text":"SQLAlchemy model definitions for the ARB Feedback Portal. This module defines ORM classes that map to key tables in the database, including uploaded file metadata and portal JSON update logs. Notes Only models explicitly defined here will be created by SQLAlchemy via db.create_all() . Most schema inspection and data access for incidences is handled dynamically via reflection. Timezone-aware UTC timestamps are used on all tracked models. All models inherit from db.Model , and can be directly queried with SQLAlchemy syntax. Example file = UploadedFile(path=\"uploads/report.xlsx\", status=\"pending\") db.session.add(file) db.session.commit()","title":"arb.portal.sqla_models"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.PortalUpdate","text":"Bases: Model SQLAlchemy model tracking updates to the misc_json field on incidence records. Used for auditing key/value changes made through the portal interface. Each row represents a single change to a single field on a specific incidence. Table Name portal_updates Columns id (int): Primary key. timestamp (datetime): UTC time when the change was logged. key (str): JSON key that was modified. old_value (str | None): Previous value (nullable). new_value (str): New value. user (str): Username or identifier of the user making the change. comments (str): Optional explanatory comment. id_incidence (int | None): Foreign key to the modified incidence (nullable). Notes Automatically populated by apply_json_patch_and_log() . Used for rendering the portal_updates.html table. Source code in arb\\portal\\sqla_models.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 class PortalUpdate ( db . Model ): \"\"\" SQLAlchemy model tracking updates to the misc_json field on incidence records. Used for auditing key/value changes made through the portal interface. Each row represents a single change to a single field on a specific incidence. Table Name: portal_updates Columns: id (int): Primary key. timestamp (datetime): UTC time when the change was logged. key (str): JSON key that was modified. old_value (str | None): Previous value (nullable). new_value (str): New value. user (str): Username or identifier of the user making the change. comments (str): Optional explanatory comment. id_incidence (int | None): Foreign key to the modified incidence (nullable). Notes: - Automatically populated by `apply_json_patch_and_log()`. - Used for rendering the `portal_updates.html` table. \"\"\" __tablename__ = \"portal_updates\" id = Column ( Integer , primary_key = True ) timestamp = Column ( DateTime ( timezone = True ), nullable = False , server_default = func . now ()) key = Column ( String ( 255 ), nullable = False ) old_value = Column ( Text , nullable = True ) new_value = Column ( Text , nullable = False ) user = Column ( String ( 255 ), nullable = False , default = \"anonymous\" ) comments = Column ( Text , nullable = False , default = \"\" ) id_incidence = Column ( Integer , nullable = True ) def __repr__ ( self ): return ( f \"<PortalUpdate id= { self . id } key= { self . key !r} old= { self . old_value !r} \" f \"new= { self . new_value !r} user= { self . user !r} at= { self . timestamp } >\" )","title":"PortalUpdate"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.UploadedFile","text":"Bases: Model SQLAlchemy model representing a user-uploaded file. Stores metadata for files uploaded via the portal interface, including the file path, status, and optional description. Automatically tracks creation and last modification timestamps. Table Name uploaded_files Columns id_ (int): Primary key. path (str): Filesystem path to the uploaded file. description (str | None): Optional human-friendly explanation. status (str | None): Upload status, e.g., 'pending', 'processed', or 'error'. created_timestamp (datetime): UTC timestamp of initial creation. modified_timestamp (datetime): UTC timestamp of last update. Example file = UploadedFile(path=\"uploads/test.xlsx\", status=\"pending\") db.session.add(file) db.session.commit() Notes Timestamps use UTC and are timezone-aware. This table is managed by SQLAlchemy directly (not introspected). Source code in arb\\portal\\sqla_models.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class UploadedFile ( db . Model ): \"\"\" SQLAlchemy model representing a user-uploaded file. Stores metadata for files uploaded via the portal interface, including the file path, status, and optional description. Automatically tracks creation and last modification timestamps. Table Name: uploaded_files Columns: id_ (int): Primary key. path (str): Filesystem path to the uploaded file. description (str | None): Optional human-friendly explanation. status (str | None): Upload status, e.g., 'pending', 'processed', or 'error'. created_timestamp (datetime): UTC timestamp of initial creation. modified_timestamp (datetime): UTC timestamp of last update. Example: >>> file = UploadedFile(path=\"uploads/test.xlsx\", status=\"pending\") >>> db.session.add(file) >>> db.session.commit() Notes: - Timestamps use UTC and are timezone-aware. - This table is managed by SQLAlchemy directly (not introspected). \"\"\" __tablename__ = \"uploaded_files\" id_ = db . Column ( db . Integer , primary_key = True ) path = db . Column ( db . Text , nullable = False ) description = db . Column ( db . Text , nullable = True ) status = db . Column ( db . Text , nullable = True ) created_timestamp = db . Column ( db . DateTime ( timezone = True ), server_default = func . now () ) modified_timestamp = db . Column ( db . DateTime ( timezone = True ), server_default = func . now () ) def __repr__ ( self ) -> str : \"\"\" Return a human-readable string representation of the uploaded file record. Returns: str: Summary string showing the ID, path, description, and status. Example: >>> repr(UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\")) '<Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done>' \"\"\" return ( f '<Uploaded File: { self . id_ } , Path: { self . path } , ' f 'Description: { self . description } , Status: { self . status } >' )","title":"UploadedFile"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.UploadedFile.__repr__","text":"Return a human-readable string representation of the uploaded file record. Returns: str ( str ) \u2013 Summary string showing the ID, path, description, and status. Example repr(UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\")) ' ' Source code in arb\\portal\\sqla_models.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __repr__ ( self ) -> str : \"\"\" Return a human-readable string representation of the uploaded file record. Returns: str: Summary string showing the ID, path, description, and status. Example: >>> repr(UploadedFile(id_=3, path=\"uploads/data.csv\", description=\"Data\", status=\"done\")) '<Uploaded File: 3, Path: uploads/data.csv, Description: Data, Status: done>' \"\"\" return ( f '<Uploaded File: { self . id_ } , Path: { self . path } , ' f 'Description: { self . description } , Status: { self . status } >' )","title":"__repr__"},{"location":"reference/arb/portal/sqla_models/#arb.portal.sqla_models.run_diagnostics","text":"Run a test transaction to validate UploadedFile model functionality. This utility performs an insert, fetch, and rollback on the UploadedFile model to verify that the ORM mapping and database connection are working. Returns: None \u2013 None Raises: RuntimeError \u2013 If database access or fetch fails. Notes Meant for developer use in test environments only. This function leaves no data in the database due to rollback. Logs diagnostic information using the project logger. Source code in arb\\portal\\sqla_models.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def run_diagnostics () -> None : \"\"\" Run a test transaction to validate UploadedFile model functionality. This utility performs an insert, fetch, and rollback on the UploadedFile model to verify that the ORM mapping and database connection are working. Returns: None Raises: RuntimeError: If database access or fetch fails. Notes: - Meant for developer use in test environments only. - This function leaves no data in the database due to rollback. - Logs diagnostic information using the project logger. \"\"\" logger . info ( \"Running UploadedFile diagnostics...\" ) try : logger . debug ( \"Beginning diagnostic transaction...\" ) test_file = UploadedFile ( path = \"uploads/test_file.xlsx\" , description = \"Diagnostic test file\" , status = \"testing\" ) db . session . add ( test_file ) db . session . flush () # Ensures test_file.id_ is populated logger . info ( f \"Inserted test file with ID: { test_file . id_ } \" ) fetched = UploadedFile . query . get ( test_file . id_ ) if fetched is None : raise RuntimeError ( \"Failed to retrieve inserted UploadedFile instance.\" ) logger . info ( f \"Fetched file: { fetched } \" ) logger . debug ( f \"repr: { repr ( fetched ) } \" ) logger . debug ( f \"created_timestamp: { fetched . created_timestamp } \" ) except SQLAlchemyError as e : logger . exception ( \"SQLAlchemy error during diagnostics.\" ) raise RuntimeError ( \"Database error during UploadedFile diagnostics.\" ) from e finally : logger . debug ( \"Rolling back diagnostic transaction.\" ) db . session . rollback () logger . info ( \"Diagnostics completed and transaction rolled back.\" )","title":"run_diagnostics"},{"location":"reference/arb/portal/wtf_landfill/","text":"arb.portal.wtf_landfill Landfill feedback form definition for the ARB Feedback Portal (WTForms). This module defines the LandfillFeedback class, a comprehensive WTForms-based HTML form for collecting information on methane emission inspections and responses at landfill sites. The form is organized into multiple logical sections and includes dynamic dropdown behavior, conditional validation, and cross-field logic. Key Features: Uses FlaskForm as a base and is rendered using Bootstrap-compatible templates. Dropdowns support conditional dependencies using Globals.drop_downs_contingent . Validators are programmatically adjusted to enforce or relax constraints based on user input. Supports optional \"Other\" fields that are only required when triggered. Final validation is managed via a custom validate() override. Example Usage: form = LandfillFeedback() form.process(request.form) if form.validate_on_submit(): # Process and store form data save_landfill_feedback(form.data) Notes: The update_contingent_selectors() method updates selector/contingent choices. The determine_contingent_fields() method enforces dynamic field-level validation. Intended for use with the landfill_incidence_update route and similar flows. General-purpose WTForms utilities are located in: arb.utils.wtf_forms_util.py LandfillFeedback Bases: FlaskForm WTForms form class for collecting landfill feedback data. Captures user-submitted information about methane emissions, inspections, corrective actions, and contact details related to landfill facility operations. Notes Some fields are conditionally validated depending on selections. The form dynamically updates contingent dropdowns using update_contingent_selectors() . Final validation is enforced in the validate() method. Source code in arb\\portal\\wtf_landfill.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 class LandfillFeedback ( FlaskForm ): \"\"\" WTForms form class for collecting landfill feedback data. Captures user-submitted information about methane emissions, inspections, corrective actions, and contact details related to landfill facility operations. Notes: - Some fields are conditionally validated depending on selections. - The form dynamically updates contingent dropdowns using `update_contingent_selectors()`. - Final validation is enforced in the `validate()` method. \"\"\" # Section 2 # todo - likely have to change these to InputRequired(), Optional(), blank and removed # label = \"1. Incidence/Emission ID\" id_incidence = IntegerField ( \"Incidence/Emission ID\" , validators = [ Optional ()], render_kw = { \"readonly\" : True } ) label = \"2. Plume ID(s)\" id_plume = IntegerField ( label = label , validators = [ InputRequired (), NumberRange ( min = 1 , message = \"Plume ID must be a positive integer\" )], ) # REFERENCES plumes (id_plume) label = \"3. Plume Observation Date\" observation_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"4. Plume Origin CARB Estimated Latitude\" # I think lat/longs are failing because they were renamed ... lat_carb = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LATITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LATITUDE_VALIDATION )], ) label = \"5. Plume Origin CARB Estimated Longitude\" long_carb = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LONGITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LONGITUDE_VALIDATION )], ) label = \"6. CARB Message ID\" id_message = StringField ( label = label , validators = [ Optional ()], ) # Section 3 label = \"Q1. Facility Name\" facility_name = StringField ( label = label , validators = [ InputRequired ()], ) label = \"Q2. Facility SWIS ID\" id_arb_swis = StringField ( label = label , validators = [ Optional ()], ) label = \"Q3. Contact Name\" contact_name = StringField ( label = label , validators = [ InputRequired ()], ) # contact_phone = StringField(label=\"Contact Phone\", validators=[InputRequired()]) label = \"Q4. Contact Phone\" message = \"Invalid phone number. Phone number must be in format '(123) 456-7890' or '(123) 456-7890 x1234567'.\" contact_phone = StringField ( label = label , validators = [ InputRequired (), Regexp ( regex = r \"^\\(\\d {3} \\) \\d {3} -\\d {4} ( x\\d{1,7})?$\" , message = message ) ], ) label = \"Q5. Contact Email\" contact_email = EmailField ( label = label , validators = [ InputRequired (), Email ()]) # Section 4 label = \"Q6. Date of owner/operator\u2019s follow-up ground monitoring.\" inspection_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired (), ], format = HTML_LOCAL_TIME_FORMAT , ) label = \"Q7. Instrument used to locate the leak (e.g., Fisher Scientific TVA2020; RKI Multigas Analyzer Eagle 2; TDL).\" instrument = StringField ( label = label , validators = [ InputRequired ()]) label = \"Q8. Was a leak identified through prior knowledge or by follow-up monitoring after receipt of a CARB plume notice?\" emission_identified_flag_fk = SelectField ( label = label , choices = Globals . drop_downs [ \"emission_identified_flag_fk\" ], validators = [ InputRequired (), ], ) label = ( f \"Q9. If no leaks were found, please describe any events or activities that may have \" f \"contributed to the plume observed on the date provided in Section 2.\" ) additional_activities = TextAreaField ( label = label , validators = [ Optional ()], ) # Section 5 label = \"Q10: Maximum concentration of methane leak (in ppmv).\" initial_leak_concentration = DecimalField ( label = label , validators = [ InputRequired ()], ) label = \"Q11. Please provide a revised latitude if the leak location differs from CARB's estimate in Section 2.\" lat_revised = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LATITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LATITUDE_VALIDATION )], ) label = \"Q12. Please provide a revised longitude if the leak location differs from CARB's estimate in Section 2.\" long_revised = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LONGITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LONGITUDE_VALIDATION )], ) label = \"Q13: Please select from the drop-down menu which option best matches the description of the leak.\" emission_type_fk = SelectField ( label = label , choices = Globals . drop_downs [ \"emission_type_fk\" ], validators = [ InputRequired (), ], ) label = \"Q14. Please select from the drop-down menu which option best describes the location of the leak.\" emission_location = SelectField ( label = label , choices = Globals . drop_downs [ \"emission_location\" ], validators = [ InputRequired (), ], ) label = ( f \"Q15. Please provide a more detailed description of the leak location, \" f \"including grid ID number or component name, if applicable.\" ) emission_location_notes = TextAreaField ( label = label , validators = [], ) label = \"Q16. Please select the most likely cause of the leak.\" emission_cause = SelectField ( label = label , choices = Globals . drop_downs [ \"emission_cause\" ], validators = [ InputRequired (), ], ) label = ( f \"Q17 (Optional). Please select an alternative cause (only if suspected). \" f \"This should not be the same as your Q16 response.\" ) emission_cause_secondary = SelectField ( label = label , choices = Globals . drop_downs [ \"emission_cause_secondary\" ], validators = [ Optional ()], ) label = ( f \"Q18 (Optional). Please select an alternative cause (only if suspected). \" f \"This should not be the same as your Q16 or Q17 responses.\" ) emission_cause_tertiary = SelectField ( label = label , choices = Globals . drop_downs [ \"emission_cause_tertiary\" ], validators = [ Optional ()], ) label = ( f \"Q19. Please provide a more detailed description of the cause(s), \" f \"including the reason for and duration of any construction activity or downtime.\" ) emission_cause_notes = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = \"Q20. Describe any corrective actions taken.\" mitigation_actions = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = \"Q21. Repair date.\" mitigation_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q22. Re-monitored date.\" re_monitored_timestamp = DateTimeLocalField ( label = label , validators = [ Optional ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q23. Re-monitored methane concentration after repair (ppmv).\" re_monitored_concentration = DecimalField ( label = label , validators = [ InputRequired ()], ) label = ( f \"Q24. Was the leak location monitored in the most recent \" f \"prior quarterly/annual surface emissions or quarterly component leak monitoring event?\" ) included_in_last_lmr = SelectField ( label = label , choices = Globals . drop_downs [ \"included_in_last_lmr\" ], validators = [ InputRequired (), ], ) label = \"Q25. If 'No' to Q24, please explain why the area was excluded from monitoring.\" included_in_last_lmr_description = TextAreaField ( label = label , validators = [ InputRequired ()]) label = \"Q26. Is this grid/component planned for inclusion in the next quarterly/annual leak monitoring?\" planned_for_next_lmr = SelectField ( label = label , choices = Globals . drop_downs [ \"planned_for_next_lmr\" ], validators = [ InputRequired (), ], ) label = \"Q27. If 'No' to Q26, please state why the area will not be monitored.\" planned_for_next_lmr_description = TextAreaField ( label = label , validators = [ InputRequired ()]) label = \"Q28. Date of most recent surface emissions monitoring event (prior to this notification).\" last_component_leak_monitoring_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q29. Date of most recent component leak monitoring event (prior to this notification).\" last_surface_monitoring_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q30. Additional notes or comments.\" additional_notes = TextAreaField ( label = label , validators = [], ) label = \"1. CARB internal notes\" carb_notes = TextAreaField ( label = label , validators = [], ) def update_contingent_selectors ( self ) -> None : \"\"\" Update contingent dropdown field choices based on current field selections. This method looks up selector/contingent relationships defined in `Globals.drop_downs_contingent` and dynamically modifies the `choices` for child fields when a selector field has a known dependency. This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of `self.emission_location`. It ensures valid dropdown options and clears invalid selections. Assumes: - `self.emission_location`, `self.emission_cause`, `self.emission_cause_secondary`, and `self.emission_cause_tertiary` are all `SelectField` instances. - `Globals.drop_downs_contingent` contains a nested dictionary of location-contingent dropdown options. Returns: None \"\"\" # todo - update contingent dropdowns? logger . debug ( \"Running update_contingent_selectors()\" ) emission_location = self . emission_location . data logger . debug ( f \"Selected emission_location: { emission_location !r} \" ) emission_cause_dict = Globals . drop_downs_contingent . get ( \"emission_cause_contingent_on_emission_location\" , {} ) choices_raw = emission_cause_dict . get ( emission_location , []) logger . debug ( f \"Available contingent causes: { choices_raw !r} \" ) # Define headers primary_header = [ ( PLEASE_SELECT , PLEASE_SELECT , { \"disabled\" : True }), ( \"Not applicable as no leak was detected\" , \"Not applicable as no leak was detected\" , {}), ] secondary_tertiary_header = primary_header + [ ( \"Not applicable as no additional leak cause suspected\" , \"Not applicable as no additional leak cause suspected\" , {}), ] # Build full choices primary_choices = build_choices ( primary_header , choices_raw ) secondary_tertiary_choices = build_choices ( secondary_tertiary_header , choices_raw ) # Update each field's choices self . emission_cause . choices = primary_choices self . emission_cause_secondary . choices = secondary_tertiary_choices self . emission_cause_tertiary . choices = secondary_tertiary_choices def validate ( self , extra_validators = None ) -> bool : \"\"\" Override WTForms default validation with custom cross-field logic. Ensures required fields are conditionally enforced based on upstream values, including: - Facility activity selections imply required contingent selections - If \"Other\" is chosen, corresponding text input must be filled - If leak is confirmed, additional emission details are required Returns: bool: True if form is valid, False otherwise. Notes: - Calls `determine_contingent_fields()` before validation to ensure field validators are correct. - Uses built-in `super().validate()` after adjusting validators. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with an alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () self . update_contingent_selectors () ################################################################################################### # Set selectors with values not in their choices list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each field's individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(\"in the validator before super\") obj_diagnostics ( self , message = \"in the validator before super\" ) super_return = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### # todo - move field level validation to separate function if self . emission_identified_flag_fk . data == \"No leak was detected\" : valid_options = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data not in valid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data not in valid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) # Q8 and Q13 should be coupled to Operator-aware response elif self . emission_identified_flag_fk . data == \"Operator was aware of the leak prior to receiving the CARB plume notification\" : valid_options = [ PLEASE_SELECT , \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_identified_flag_fk . data != \"No leak was detected\" : invalid_options = [ \"Not applicable as no leak was detected\" , ] if self . emission_type_fk . data in invalid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data in invalid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data in invalid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data in invalid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data in invalid_options : self . emission_cause_tertiary . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) if self . inspection_timestamp . data and self . mitigation_timestamp . data : if self . mitigation_timestamp . data < self . inspection_timestamp . data : self . mitigation_timestamp . errors . append ( \"Date of mitigation cannot be prior to initial site inspection.\" ) # todo - add that 2nd and 3rd can't be repeats ignore_repeats = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if ( self . emission_cause_secondary . data not in ignore_repeats and self . emission_cause_secondary . data in [ self . emission_cause . data ]): self . emission_cause_secondary . errors . append ( f \"Q17 appears to be a repeat\" ) if ( self . emission_cause_tertiary . data not in ignore_repeats and self . emission_cause_tertiary . data in [ self . emission_cause . data , self . emission_cause_secondary . data ]): self . emission_cause_secondary . errors . append ( f \"Q18 appears to be a repeat\" ) # not sure if this test makes sense since they may have know about it prior to the plume (going to comment out) # if self.observation_timestamp.data and self.inspection_timestamp.data: # if self.inspection_timestamp.data < self.observation_timestamp.data: # self.inspection_timestamp.errors.append( # \"Date of inspection cannot be prior to date of initial plume observation.\") ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields to a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) return form_valid def determine_contingent_fields ( self ): \"\"\" Add or remove field validators depending on contingent dropdown selections. Some dropdown options imply that no further input is needed (e.g., selecting \"No leak was detected\" disables required validation on follow-up questions). This function clears or restores validators accordingly. These fields toggle between required and optional depending on related field values (e.g., dropdowns that are set to \"Other\", or location-dependent fields). Some validation rules involve mutually exclusive or fallback logic. Notes: - This function should be called before validation to sync requirements. - May need to re-order exclusions (e.g., venting) for edge cases. \"\"\" # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_emission_identified = [ \"additional_activities\" , \"initial_leak_concentration\" , # \"lat_revised\", # \"long_revised\", \"emission_type_fk\" , \"emission_location\" , # \"emission_location_notes\", \"emission_cause\" , # \"emission_cause_secondary\", # \"emission_cause_tertiary\", \"emission_cause_notes\" , \"mitigation_actions\" , \"mitigation_timestamp\" , \"re_monitored_timestamp\" , \"re_monitored_concentration\" , \"included_in_last_lmr\" , \"included_in_last_lmr_description\" , \"planned_for_next_lmr\" , \"planned_for_next_lmr_description\" , \"last_surface_monitoring_timestamp\" , \"last_component_leak_monitoring_timestamp\" , \"additional_notes\" , ] # todo - update logic for new selectors emission_identified_test = self . emission_identified_flag_fk . data != \"No leak was detected\" # print(f\"{emission_identified_test=}\") change_validators_on_test ( self , emission_identified_test , required_if_emission_identified ) if emission_identified_test : lmr_included_test = self . included_in_last_lmr . data == \"No\" logger . debug ( f \" { lmr_included_test =} \" ) change_validators_on_test ( self , lmr_included_test , [ \"included_in_last_lmr_description\" ]) lmr_planned_test = self . planned_for_next_lmr . data == \"No\" logger . debug ( f \" { lmr_planned_test =} \" ) change_validators_on_test ( self , lmr_planned_test , [ \"planned_for_next_lmr_description\" ]) determine_contingent_fields () Add or remove field validators depending on contingent dropdown selections. Some dropdown options imply that no further input is needed (e.g., selecting \"No leak was detected\" disables required validation on follow-up questions). This function clears or restores validators accordingly. These fields toggle between required and optional depending on related field values (e.g., dropdowns that are set to \"Other\", or location-dependent fields). Some validation rules involve mutually exclusive or fallback logic. Notes This function should be called before validation to sync requirements. May need to re-order exclusions (e.g., venting) for edge cases. Source code in arb\\portal\\wtf_landfill.py 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 def determine_contingent_fields ( self ): \"\"\" Add or remove field validators depending on contingent dropdown selections. Some dropdown options imply that no further input is needed (e.g., selecting \"No leak was detected\" disables required validation on follow-up questions). This function clears or restores validators accordingly. These fields toggle between required and optional depending on related field values (e.g., dropdowns that are set to \"Other\", or location-dependent fields). Some validation rules involve mutually exclusive or fallback logic. Notes: - This function should be called before validation to sync requirements. - May need to re-order exclusions (e.g., venting) for edge cases. \"\"\" # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_emission_identified = [ \"additional_activities\" , \"initial_leak_concentration\" , # \"lat_revised\", # \"long_revised\", \"emission_type_fk\" , \"emission_location\" , # \"emission_location_notes\", \"emission_cause\" , # \"emission_cause_secondary\", # \"emission_cause_tertiary\", \"emission_cause_notes\" , \"mitigation_actions\" , \"mitigation_timestamp\" , \"re_monitored_timestamp\" , \"re_monitored_concentration\" , \"included_in_last_lmr\" , \"included_in_last_lmr_description\" , \"planned_for_next_lmr\" , \"planned_for_next_lmr_description\" , \"last_surface_monitoring_timestamp\" , \"last_component_leak_monitoring_timestamp\" , \"additional_notes\" , ] # todo - update logic for new selectors emission_identified_test = self . emission_identified_flag_fk . data != \"No leak was detected\" # print(f\"{emission_identified_test=}\") change_validators_on_test ( self , emission_identified_test , required_if_emission_identified ) if emission_identified_test : lmr_included_test = self . included_in_last_lmr . data == \"No\" logger . debug ( f \" { lmr_included_test =} \" ) change_validators_on_test ( self , lmr_included_test , [ \"included_in_last_lmr_description\" ]) lmr_planned_test = self . planned_for_next_lmr . data == \"No\" logger . debug ( f \" { lmr_planned_test =} \" ) change_validators_on_test ( self , lmr_planned_test , [ \"planned_for_next_lmr_description\" ]) update_contingent_selectors () Update contingent dropdown field choices based on current field selections. This method looks up selector/contingent relationships defined in Globals.drop_downs_contingent and dynamically modifies the choices for child fields when a selector field has a known dependency. This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of self.emission_location . It ensures valid dropdown options and clears invalid selections. Assumes self.emission_location , self.emission_cause , self.emission_cause_secondary , and self.emission_cause_tertiary are all SelectField instances. Globals.drop_downs_contingent contains a nested dictionary of location-contingent dropdown options. Returns: None \u2013 None Source code in arb\\portal\\wtf_landfill.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def update_contingent_selectors ( self ) -> None : \"\"\" Update contingent dropdown field choices based on current field selections. This method looks up selector/contingent relationships defined in `Globals.drop_downs_contingent` and dynamically modifies the `choices` for child fields when a selector field has a known dependency. This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of `self.emission_location`. It ensures valid dropdown options and clears invalid selections. Assumes: - `self.emission_location`, `self.emission_cause`, `self.emission_cause_secondary`, and `self.emission_cause_tertiary` are all `SelectField` instances. - `Globals.drop_downs_contingent` contains a nested dictionary of location-contingent dropdown options. Returns: None \"\"\" # todo - update contingent dropdowns? logger . debug ( \"Running update_contingent_selectors()\" ) emission_location = self . emission_location . data logger . debug ( f \"Selected emission_location: { emission_location !r} \" ) emission_cause_dict = Globals . drop_downs_contingent . get ( \"emission_cause_contingent_on_emission_location\" , {} ) choices_raw = emission_cause_dict . get ( emission_location , []) logger . debug ( f \"Available contingent causes: { choices_raw !r} \" ) # Define headers primary_header = [ ( PLEASE_SELECT , PLEASE_SELECT , { \"disabled\" : True }), ( \"Not applicable as no leak was detected\" , \"Not applicable as no leak was detected\" , {}), ] secondary_tertiary_header = primary_header + [ ( \"Not applicable as no additional leak cause suspected\" , \"Not applicable as no additional leak cause suspected\" , {}), ] # Build full choices primary_choices = build_choices ( primary_header , choices_raw ) secondary_tertiary_choices = build_choices ( secondary_tertiary_header , choices_raw ) # Update each field's choices self . emission_cause . choices = primary_choices self . emission_cause_secondary . choices = secondary_tertiary_choices self . emission_cause_tertiary . choices = secondary_tertiary_choices validate ( extra_validators = None ) Override WTForms default validation with custom cross-field logic. Ensures required fields are conditionally enforced based on upstream values, including: - Facility activity selections imply required contingent selections - If \"Other\" is chosen, corresponding text input must be filled - If leak is confirmed, additional emission details are required Returns: bool ( bool ) \u2013 True if form is valid, False otherwise. Notes Calls determine_contingent_fields() before validation to ensure field validators are correct. Uses built-in super().validate() after adjusting validators. Source code in arb\\portal\\wtf_landfill.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 def validate ( self , extra_validators = None ) -> bool : \"\"\" Override WTForms default validation with custom cross-field logic. Ensures required fields are conditionally enforced based on upstream values, including: - Facility activity selections imply required contingent selections - If \"Other\" is chosen, corresponding text input must be filled - If leak is confirmed, additional emission details are required Returns: bool: True if form is valid, False otherwise. Notes: - Calls `determine_contingent_fields()` before validation to ensure field validators are correct. - Uses built-in `super().validate()` after adjusting validators. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with an alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () self . update_contingent_selectors () ################################################################################################### # Set selectors with values not in their choices list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each field's individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(\"in the validator before super\") obj_diagnostics ( self , message = \"in the validator before super\" ) super_return = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### # todo - move field level validation to separate function if self . emission_identified_flag_fk . data == \"No leak was detected\" : valid_options = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data not in valid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data not in valid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) # Q8 and Q13 should be coupled to Operator-aware response elif self . emission_identified_flag_fk . data == \"Operator was aware of the leak prior to receiving the CARB plume notification\" : valid_options = [ PLEASE_SELECT , \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_identified_flag_fk . data != \"No leak was detected\" : invalid_options = [ \"Not applicable as no leak was detected\" , ] if self . emission_type_fk . data in invalid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data in invalid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data in invalid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data in invalid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data in invalid_options : self . emission_cause_tertiary . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) if self . inspection_timestamp . data and self . mitigation_timestamp . data : if self . mitigation_timestamp . data < self . inspection_timestamp . data : self . mitigation_timestamp . errors . append ( \"Date of mitigation cannot be prior to initial site inspection.\" ) # todo - add that 2nd and 3rd can't be repeats ignore_repeats = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if ( self . emission_cause_secondary . data not in ignore_repeats and self . emission_cause_secondary . data in [ self . emission_cause . data ]): self . emission_cause_secondary . errors . append ( f \"Q17 appears to be a repeat\" ) if ( self . emission_cause_tertiary . data not in ignore_repeats and self . emission_cause_tertiary . data in [ self . emission_cause . data , self . emission_cause_secondary . data ]): self . emission_cause_secondary . errors . append ( f \"Q18 appears to be a repeat\" ) # not sure if this test makes sense since they may have know about it prior to the plume (going to comment out) # if self.observation_timestamp.data and self.inspection_timestamp.data: # if self.inspection_timestamp.data < self.observation_timestamp.data: # self.inspection_timestamp.errors.append( # \"Date of inspection cannot be prior to date of initial plume observation.\") ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields to a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) return form_valid","title":"arb.portal.wtf_landfill"},{"location":"reference/arb/portal/wtf_landfill/#arbportalwtf_landfill","text":"Landfill feedback form definition for the ARB Feedback Portal (WTForms). This module defines the LandfillFeedback class, a comprehensive WTForms-based HTML form for collecting information on methane emission inspections and responses at landfill sites. The form is organized into multiple logical sections and includes dynamic dropdown behavior, conditional validation, and cross-field logic.","title":"arb.portal.wtf_landfill"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill--key-features","text":"Uses FlaskForm as a base and is rendered using Bootstrap-compatible templates. Dropdowns support conditional dependencies using Globals.drop_downs_contingent . Validators are programmatically adjusted to enforce or relax constraints based on user input. Supports optional \"Other\" fields that are only required when triggered. Final validation is managed via a custom validate() override.","title":"Key Features:"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill--example-usage","text":"form = LandfillFeedback() form.process(request.form) if form.validate_on_submit(): # Process and store form data save_landfill_feedback(form.data)","title":"Example Usage:"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill--notes","text":"The update_contingent_selectors() method updates selector/contingent choices. The determine_contingent_fields() method enforces dynamic field-level validation. Intended for use with the landfill_incidence_update route and similar flows. General-purpose WTForms utilities are located in: arb.utils.wtf_forms_util.py","title":"Notes:"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback","text":"Bases: FlaskForm WTForms form class for collecting landfill feedback data. Captures user-submitted information about methane emissions, inspections, corrective actions, and contact details related to landfill facility operations. Notes Some fields are conditionally validated depending on selections. The form dynamically updates contingent dropdowns using update_contingent_selectors() . Final validation is enforced in the validate() method. Source code in arb\\portal\\wtf_landfill.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 class LandfillFeedback ( FlaskForm ): \"\"\" WTForms form class for collecting landfill feedback data. Captures user-submitted information about methane emissions, inspections, corrective actions, and contact details related to landfill facility operations. Notes: - Some fields are conditionally validated depending on selections. - The form dynamically updates contingent dropdowns using `update_contingent_selectors()`. - Final validation is enforced in the `validate()` method. \"\"\" # Section 2 # todo - likely have to change these to InputRequired(), Optional(), blank and removed # label = \"1. Incidence/Emission ID\" id_incidence = IntegerField ( \"Incidence/Emission ID\" , validators = [ Optional ()], render_kw = { \"readonly\" : True } ) label = \"2. Plume ID(s)\" id_plume = IntegerField ( label = label , validators = [ InputRequired (), NumberRange ( min = 1 , message = \"Plume ID must be a positive integer\" )], ) # REFERENCES plumes (id_plume) label = \"3. Plume Observation Date\" observation_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"4. Plume Origin CARB Estimated Latitude\" # I think lat/longs are failing because they were renamed ... lat_carb = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LATITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LATITUDE_VALIDATION )], ) label = \"5. Plume Origin CARB Estimated Longitude\" long_carb = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LONGITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LONGITUDE_VALIDATION )], ) label = \"6. CARB Message ID\" id_message = StringField ( label = label , validators = [ Optional ()], ) # Section 3 label = \"Q1. Facility Name\" facility_name = StringField ( label = label , validators = [ InputRequired ()], ) label = \"Q2. Facility SWIS ID\" id_arb_swis = StringField ( label = label , validators = [ Optional ()], ) label = \"Q3. Contact Name\" contact_name = StringField ( label = label , validators = [ InputRequired ()], ) # contact_phone = StringField(label=\"Contact Phone\", validators=[InputRequired()]) label = \"Q4. Contact Phone\" message = \"Invalid phone number. Phone number must be in format '(123) 456-7890' or '(123) 456-7890 x1234567'.\" contact_phone = StringField ( label = label , validators = [ InputRequired (), Regexp ( regex = r \"^\\(\\d {3} \\) \\d {3} -\\d {4} ( x\\d{1,7})?$\" , message = message ) ], ) label = \"Q5. Contact Email\" contact_email = EmailField ( label = label , validators = [ InputRequired (), Email ()]) # Section 4 label = \"Q6. Date of owner/operator\u2019s follow-up ground monitoring.\" inspection_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired (), ], format = HTML_LOCAL_TIME_FORMAT , ) label = \"Q7. Instrument used to locate the leak (e.g., Fisher Scientific TVA2020; RKI Multigas Analyzer Eagle 2; TDL).\" instrument = StringField ( label = label , validators = [ InputRequired ()]) label = \"Q8. Was a leak identified through prior knowledge or by follow-up monitoring after receipt of a CARB plume notice?\" emission_identified_flag_fk = SelectField ( label = label , choices = Globals . drop_downs [ \"emission_identified_flag_fk\" ], validators = [ InputRequired (), ], ) label = ( f \"Q9. If no leaks were found, please describe any events or activities that may have \" f \"contributed to the plume observed on the date provided in Section 2.\" ) additional_activities = TextAreaField ( label = label , validators = [ Optional ()], ) # Section 5 label = \"Q10: Maximum concentration of methane leak (in ppmv).\" initial_leak_concentration = DecimalField ( label = label , validators = [ InputRequired ()], ) label = \"Q11. Please provide a revised latitude if the leak location differs from CARB's estimate in Section 2.\" lat_revised = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LATITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LATITUDE_VALIDATION )], ) label = \"Q12. Please provide a revised longitude if the leak location differs from CARB's estimate in Section 2.\" long_revised = DecimalField ( label = label , places = GPS_RESOLUTION , # validators=[Optional(), NumberRange(**LONGITUDE_VALIDATION), min_decimal_precision(GPS_RESOLUTION)], validators = [ Optional (), NumberRange ( ** LONGITUDE_VALIDATION )], ) label = \"Q13: Please select from the drop-down menu which option best matches the description of the leak.\" emission_type_fk = SelectField ( label = label , choices = Globals . drop_downs [ \"emission_type_fk\" ], validators = [ InputRequired (), ], ) label = \"Q14. Please select from the drop-down menu which option best describes the location of the leak.\" emission_location = SelectField ( label = label , choices = Globals . drop_downs [ \"emission_location\" ], validators = [ InputRequired (), ], ) label = ( f \"Q15. Please provide a more detailed description of the leak location, \" f \"including grid ID number or component name, if applicable.\" ) emission_location_notes = TextAreaField ( label = label , validators = [], ) label = \"Q16. Please select the most likely cause of the leak.\" emission_cause = SelectField ( label = label , choices = Globals . drop_downs [ \"emission_cause\" ], validators = [ InputRequired (), ], ) label = ( f \"Q17 (Optional). Please select an alternative cause (only if suspected). \" f \"This should not be the same as your Q16 response.\" ) emission_cause_secondary = SelectField ( label = label , choices = Globals . drop_downs [ \"emission_cause_secondary\" ], validators = [ Optional ()], ) label = ( f \"Q18 (Optional). Please select an alternative cause (only if suspected). \" f \"This should not be the same as your Q16 or Q17 responses.\" ) emission_cause_tertiary = SelectField ( label = label , choices = Globals . drop_downs [ \"emission_cause_tertiary\" ], validators = [ Optional ()], ) label = ( f \"Q19. Please provide a more detailed description of the cause(s), \" f \"including the reason for and duration of any construction activity or downtime.\" ) emission_cause_notes = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = \"Q20. Describe any corrective actions taken.\" mitigation_actions = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = \"Q21. Repair date.\" mitigation_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q22. Re-monitored date.\" re_monitored_timestamp = DateTimeLocalField ( label = label , validators = [ Optional ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q23. Re-monitored methane concentration after repair (ppmv).\" re_monitored_concentration = DecimalField ( label = label , validators = [ InputRequired ()], ) label = ( f \"Q24. Was the leak location monitored in the most recent \" f \"prior quarterly/annual surface emissions or quarterly component leak monitoring event?\" ) included_in_last_lmr = SelectField ( label = label , choices = Globals . drop_downs [ \"included_in_last_lmr\" ], validators = [ InputRequired (), ], ) label = \"Q25. If 'No' to Q24, please explain why the area was excluded from monitoring.\" included_in_last_lmr_description = TextAreaField ( label = label , validators = [ InputRequired ()]) label = \"Q26. Is this grid/component planned for inclusion in the next quarterly/annual leak monitoring?\" planned_for_next_lmr = SelectField ( label = label , choices = Globals . drop_downs [ \"planned_for_next_lmr\" ], validators = [ InputRequired (), ], ) label = \"Q27. If 'No' to Q26, please state why the area will not be monitored.\" planned_for_next_lmr_description = TextAreaField ( label = label , validators = [ InputRequired ()]) label = \"Q28. Date of most recent surface emissions monitoring event (prior to this notification).\" last_component_leak_monitoring_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q29. Date of most recent component leak monitoring event (prior to this notification).\" last_surface_monitoring_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT ) label = \"Q30. Additional notes or comments.\" additional_notes = TextAreaField ( label = label , validators = [], ) label = \"1. CARB internal notes\" carb_notes = TextAreaField ( label = label , validators = [], ) def update_contingent_selectors ( self ) -> None : \"\"\" Update contingent dropdown field choices based on current field selections. This method looks up selector/contingent relationships defined in `Globals.drop_downs_contingent` and dynamically modifies the `choices` for child fields when a selector field has a known dependency. This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of `self.emission_location`. It ensures valid dropdown options and clears invalid selections. Assumes: - `self.emission_location`, `self.emission_cause`, `self.emission_cause_secondary`, and `self.emission_cause_tertiary` are all `SelectField` instances. - `Globals.drop_downs_contingent` contains a nested dictionary of location-contingent dropdown options. Returns: None \"\"\" # todo - update contingent dropdowns? logger . debug ( \"Running update_contingent_selectors()\" ) emission_location = self . emission_location . data logger . debug ( f \"Selected emission_location: { emission_location !r} \" ) emission_cause_dict = Globals . drop_downs_contingent . get ( \"emission_cause_contingent_on_emission_location\" , {} ) choices_raw = emission_cause_dict . get ( emission_location , []) logger . debug ( f \"Available contingent causes: { choices_raw !r} \" ) # Define headers primary_header = [ ( PLEASE_SELECT , PLEASE_SELECT , { \"disabled\" : True }), ( \"Not applicable as no leak was detected\" , \"Not applicable as no leak was detected\" , {}), ] secondary_tertiary_header = primary_header + [ ( \"Not applicable as no additional leak cause suspected\" , \"Not applicable as no additional leak cause suspected\" , {}), ] # Build full choices primary_choices = build_choices ( primary_header , choices_raw ) secondary_tertiary_choices = build_choices ( secondary_tertiary_header , choices_raw ) # Update each field's choices self . emission_cause . choices = primary_choices self . emission_cause_secondary . choices = secondary_tertiary_choices self . emission_cause_tertiary . choices = secondary_tertiary_choices def validate ( self , extra_validators = None ) -> bool : \"\"\" Override WTForms default validation with custom cross-field logic. Ensures required fields are conditionally enforced based on upstream values, including: - Facility activity selections imply required contingent selections - If \"Other\" is chosen, corresponding text input must be filled - If leak is confirmed, additional emission details are required Returns: bool: True if form is valid, False otherwise. Notes: - Calls `determine_contingent_fields()` before validation to ensure field validators are correct. - Uses built-in `super().validate()` after adjusting validators. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with an alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () self . update_contingent_selectors () ################################################################################################### # Set selectors with values not in their choices list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each field's individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(\"in the validator before super\") obj_diagnostics ( self , message = \"in the validator before super\" ) super_return = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### # todo - move field level validation to separate function if self . emission_identified_flag_fk . data == \"No leak was detected\" : valid_options = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data not in valid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data not in valid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) # Q8 and Q13 should be coupled to Operator-aware response elif self . emission_identified_flag_fk . data == \"Operator was aware of the leak prior to receiving the CARB plume notification\" : valid_options = [ PLEASE_SELECT , \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_identified_flag_fk . data != \"No leak was detected\" : invalid_options = [ \"Not applicable as no leak was detected\" , ] if self . emission_type_fk . data in invalid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data in invalid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data in invalid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data in invalid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data in invalid_options : self . emission_cause_tertiary . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) if self . inspection_timestamp . data and self . mitigation_timestamp . data : if self . mitigation_timestamp . data < self . inspection_timestamp . data : self . mitigation_timestamp . errors . append ( \"Date of mitigation cannot be prior to initial site inspection.\" ) # todo - add that 2nd and 3rd can't be repeats ignore_repeats = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if ( self . emission_cause_secondary . data not in ignore_repeats and self . emission_cause_secondary . data in [ self . emission_cause . data ]): self . emission_cause_secondary . errors . append ( f \"Q17 appears to be a repeat\" ) if ( self . emission_cause_tertiary . data not in ignore_repeats and self . emission_cause_tertiary . data in [ self . emission_cause . data , self . emission_cause_secondary . data ]): self . emission_cause_secondary . errors . append ( f \"Q18 appears to be a repeat\" ) # not sure if this test makes sense since they may have know about it prior to the plume (going to comment out) # if self.observation_timestamp.data and self.inspection_timestamp.data: # if self.inspection_timestamp.data < self.observation_timestamp.data: # self.inspection_timestamp.errors.append( # \"Date of inspection cannot be prior to date of initial plume observation.\") ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields to a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) return form_valid def determine_contingent_fields ( self ): \"\"\" Add or remove field validators depending on contingent dropdown selections. Some dropdown options imply that no further input is needed (e.g., selecting \"No leak was detected\" disables required validation on follow-up questions). This function clears or restores validators accordingly. These fields toggle between required and optional depending on related field values (e.g., dropdowns that are set to \"Other\", or location-dependent fields). Some validation rules involve mutually exclusive or fallback logic. Notes: - This function should be called before validation to sync requirements. - May need to re-order exclusions (e.g., venting) for edge cases. \"\"\" # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_emission_identified = [ \"additional_activities\" , \"initial_leak_concentration\" , # \"lat_revised\", # \"long_revised\", \"emission_type_fk\" , \"emission_location\" , # \"emission_location_notes\", \"emission_cause\" , # \"emission_cause_secondary\", # \"emission_cause_tertiary\", \"emission_cause_notes\" , \"mitigation_actions\" , \"mitigation_timestamp\" , \"re_monitored_timestamp\" , \"re_monitored_concentration\" , \"included_in_last_lmr\" , \"included_in_last_lmr_description\" , \"planned_for_next_lmr\" , \"planned_for_next_lmr_description\" , \"last_surface_monitoring_timestamp\" , \"last_component_leak_monitoring_timestamp\" , \"additional_notes\" , ] # todo - update logic for new selectors emission_identified_test = self . emission_identified_flag_fk . data != \"No leak was detected\" # print(f\"{emission_identified_test=}\") change_validators_on_test ( self , emission_identified_test , required_if_emission_identified ) if emission_identified_test : lmr_included_test = self . included_in_last_lmr . data == \"No\" logger . debug ( f \" { lmr_included_test =} \" ) change_validators_on_test ( self , lmr_included_test , [ \"included_in_last_lmr_description\" ]) lmr_planned_test = self . planned_for_next_lmr . data == \"No\" logger . debug ( f \" { lmr_planned_test =} \" ) change_validators_on_test ( self , lmr_planned_test , [ \"planned_for_next_lmr_description\" ])","title":"LandfillFeedback"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback.determine_contingent_fields","text":"Add or remove field validators depending on contingent dropdown selections. Some dropdown options imply that no further input is needed (e.g., selecting \"No leak was detected\" disables required validation on follow-up questions). This function clears or restores validators accordingly. These fields toggle between required and optional depending on related field values (e.g., dropdowns that are set to \"Other\", or location-dependent fields). Some validation rules involve mutually exclusive or fallback logic. Notes This function should be called before validation to sync requirements. May need to re-order exclusions (e.g., venting) for edge cases. Source code in arb\\portal\\wtf_landfill.py 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 def determine_contingent_fields ( self ): \"\"\" Add or remove field validators depending on contingent dropdown selections. Some dropdown options imply that no further input is needed (e.g., selecting \"No leak was detected\" disables required validation on follow-up questions). This function clears or restores validators accordingly. These fields toggle between required and optional depending on related field values (e.g., dropdowns that are set to \"Other\", or location-dependent fields). Some validation rules involve mutually exclusive or fallback logic. Notes: - This function should be called before validation to sync requirements. - May need to re-order exclusions (e.g., venting) for edge cases. \"\"\" # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_emission_identified = [ \"additional_activities\" , \"initial_leak_concentration\" , # \"lat_revised\", # \"long_revised\", \"emission_type_fk\" , \"emission_location\" , # \"emission_location_notes\", \"emission_cause\" , # \"emission_cause_secondary\", # \"emission_cause_tertiary\", \"emission_cause_notes\" , \"mitigation_actions\" , \"mitigation_timestamp\" , \"re_monitored_timestamp\" , \"re_monitored_concentration\" , \"included_in_last_lmr\" , \"included_in_last_lmr_description\" , \"planned_for_next_lmr\" , \"planned_for_next_lmr_description\" , \"last_surface_monitoring_timestamp\" , \"last_component_leak_monitoring_timestamp\" , \"additional_notes\" , ] # todo - update logic for new selectors emission_identified_test = self . emission_identified_flag_fk . data != \"No leak was detected\" # print(f\"{emission_identified_test=}\") change_validators_on_test ( self , emission_identified_test , required_if_emission_identified ) if emission_identified_test : lmr_included_test = self . included_in_last_lmr . data == \"No\" logger . debug ( f \" { lmr_included_test =} \" ) change_validators_on_test ( self , lmr_included_test , [ \"included_in_last_lmr_description\" ]) lmr_planned_test = self . planned_for_next_lmr . data == \"No\" logger . debug ( f \" { lmr_planned_test =} \" ) change_validators_on_test ( self , lmr_planned_test , [ \"planned_for_next_lmr_description\" ])","title":"determine_contingent_fields"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback.update_contingent_selectors","text":"Update contingent dropdown field choices based on current field selections. This method looks up selector/contingent relationships defined in Globals.drop_downs_contingent and dynamically modifies the choices for child fields when a selector field has a known dependency. This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of self.emission_location . It ensures valid dropdown options and clears invalid selections. Assumes self.emission_location , self.emission_cause , self.emission_cause_secondary , and self.emission_cause_tertiary are all SelectField instances. Globals.drop_downs_contingent contains a nested dictionary of location-contingent dropdown options. Returns: None \u2013 None Source code in arb\\portal\\wtf_landfill.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 def update_contingent_selectors ( self ) -> None : \"\"\" Update contingent dropdown field choices based on current field selections. This method looks up selector/contingent relationships defined in `Globals.drop_downs_contingent` and dynamically modifies the `choices` for child fields when a selector field has a known dependency. This method dynamically updates the primary, secondary, and tertiary emission cause fields based on the value of `self.emission_location`. It ensures valid dropdown options and clears invalid selections. Assumes: - `self.emission_location`, `self.emission_cause`, `self.emission_cause_secondary`, and `self.emission_cause_tertiary` are all `SelectField` instances. - `Globals.drop_downs_contingent` contains a nested dictionary of location-contingent dropdown options. Returns: None \"\"\" # todo - update contingent dropdowns? logger . debug ( \"Running update_contingent_selectors()\" ) emission_location = self . emission_location . data logger . debug ( f \"Selected emission_location: { emission_location !r} \" ) emission_cause_dict = Globals . drop_downs_contingent . get ( \"emission_cause_contingent_on_emission_location\" , {} ) choices_raw = emission_cause_dict . get ( emission_location , []) logger . debug ( f \"Available contingent causes: { choices_raw !r} \" ) # Define headers primary_header = [ ( PLEASE_SELECT , PLEASE_SELECT , { \"disabled\" : True }), ( \"Not applicable as no leak was detected\" , \"Not applicable as no leak was detected\" , {}), ] secondary_tertiary_header = primary_header + [ ( \"Not applicable as no additional leak cause suspected\" , \"Not applicable as no additional leak cause suspected\" , {}), ] # Build full choices primary_choices = build_choices ( primary_header , choices_raw ) secondary_tertiary_choices = build_choices ( secondary_tertiary_header , choices_raw ) # Update each field's choices self . emission_cause . choices = primary_choices self . emission_cause_secondary . choices = secondary_tertiary_choices self . emission_cause_tertiary . choices = secondary_tertiary_choices","title":"update_contingent_selectors"},{"location":"reference/arb/portal/wtf_landfill/#arb.portal.wtf_landfill.LandfillFeedback.validate","text":"Override WTForms default validation with custom cross-field logic. Ensures required fields are conditionally enforced based on upstream values, including: - Facility activity selections imply required contingent selections - If \"Other\" is chosen, corresponding text input must be filled - If leak is confirmed, additional emission details are required Returns: bool ( bool ) \u2013 True if form is valid, False otherwise. Notes Calls determine_contingent_fields() before validation to ensure field validators are correct. Uses built-in super().validate() after adjusting validators. Source code in arb\\portal\\wtf_landfill.py 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 def validate ( self , extra_validators = None ) -> bool : \"\"\" Override WTForms default validation with custom cross-field logic. Ensures required fields are conditionally enforced based on upstream values, including: - Facility activity selections imply required contingent selections - If \"Other\" is chosen, corresponding text input must be filled - If leak is confirmed, additional emission details are required Returns: bool: True if form is valid, False otherwise. Notes: - Calls `determine_contingent_fields()` before validation to ensure field validators are correct. - Uses built-in `super().validate()` after adjusting validators. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with an alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () self . update_contingent_selectors () ################################################################################################### # Set selectors with values not in their choices list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each field's individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(\"in the validator before super\") obj_diagnostics ( self , message = \"in the validator before super\" ) super_return = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### # todo - move field level validation to separate function if self . emission_identified_flag_fk . data == \"No leak was detected\" : valid_options = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data not in valid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data not in valid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data not in valid_options : self . emission_cause . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) # Q8 and Q13 should be coupled to Operator-aware response elif self . emission_identified_flag_fk . data == \"Operator was aware of the leak prior to receiving the CARB plume notification\" : valid_options = [ PLEASE_SELECT , \"Operator was aware of the leak prior to receiving the notification, and/or repairs were in progress on the date of the plume observation\" , ] if self . emission_type_fk . data not in valid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_identified_flag_fk . data != \"No leak was detected\" : invalid_options = [ \"Not applicable as no leak was detected\" , ] if self . emission_type_fk . data in invalid_options : self . emission_type_fk . errors . append ( f \"Q8 and Q13 appear to be inconsistent\" ) if self . emission_location . data in invalid_options : self . emission_location . errors . append ( f \"Q8 and Q14 appear to be inconsistent\" ) if self . emission_cause . data in invalid_options : self . emission_cause . errors . append ( f \"Q8 and Q16 appear to be inconsistent\" ) if self . emission_cause_secondary . data in invalid_options : self . emission_cause_secondary . errors . append ( f \"Q8 and Q17 appear to be inconsistent\" ) if self . emission_cause_tertiary . data in invalid_options : self . emission_cause_tertiary . errors . append ( f \"Q8 and Q18 appear to be inconsistent\" ) if self . inspection_timestamp . data and self . mitigation_timestamp . data : if self . mitigation_timestamp . data < self . inspection_timestamp . data : self . mitigation_timestamp . errors . append ( \"Date of mitigation cannot be prior to initial site inspection.\" ) # todo - add that 2nd and 3rd can't be repeats ignore_repeats = [ PLEASE_SELECT , \"Not applicable as no leak was detected\" , \"Not applicable as no additional leak cause suspected\" , ] if ( self . emission_cause_secondary . data not in ignore_repeats and self . emission_cause_secondary . data in [ self . emission_cause . data ]): self . emission_cause_secondary . errors . append ( f \"Q17 appears to be a repeat\" ) if ( self . emission_cause_tertiary . data not in ignore_repeats and self . emission_cause_tertiary . data in [ self . emission_cause . data , self . emission_cause_secondary . data ]): self . emission_cause_secondary . errors . append ( f \"Q18 appears to be a repeat\" ) # not sure if this test makes sense since they may have know about it prior to the plume (going to comment out) # if self.observation_timestamp.data and self.inspection_timestamp.data: # if self.inspection_timestamp.data < self.observation_timestamp.data: # self.inspection_timestamp.errors.append( # \"Date of inspection cannot be prior to date of initial plume observation.\") ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields to a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) return form_valid","title":"validate"},{"location":"reference/arb/portal/wtf_oil_and_gas/","text":"arb.portal.wtf_oil_and_gas Oil & Gas Feedback Form (WTForms) for the ARB Feedback Portal. Defines the OGFeedback class, a complex feedback form used for collecting structured data about methane emission incidents in the oil and gas sector. The form logic mirrors the official O&G spreadsheet and includes conditional field validation, dynamic dropdown dependencies, and timestamp-based consistency checks. Key Features: Enforces correct response flows based on regulatory logic (e.g., 95669.1(b)(1) exclusions). Includes geospatial validation and timestamp sequencing checks. Cross-field validation logic implemented in validate() . Supports conditional validation with custom helpers like change_validators_on_test() . Usage: form = OGFeedback() form.process(request.form) if form.validate_on_submit(): process_feedback_data(form.data) Notes: Fields such as id_incidence are read-only and display-only. Contingent dropdowns are updated via update_contingent_selectors() . Cross-dependencies (e.g., OGI required if no venting exclusion) are enforced dynamically. OGFeedback Bases: FlaskForm WTForms class for collecting feedback on Oil & Gas methane emissions. This form models the structure of the O&G feedback spreadsheet and enforces regulatory logic outlined in California methane rules (e.g., 95669.1). Sections include metadata, inspection information, emissions details, mitigation actions, and contact data. Core Features Uses standard WTForms field types, with conditionally required fields. Dropdowns update dynamically based on user selections. Includes geospatial coordinates and timestamp logic. Implements cross-field validation for inspection results and mitigation status. Used by The web-based feedback form in the ARB Feedback Portal. Routes such as og_incidence_create and incidence_update . Notes Sector-specific contingent dropdowns are handled via Globals. Validators are adjusted at runtime depending on the selected conditions. Source code in arb\\portal\\wtf_oil_and_gas.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 class OGFeedback ( FlaskForm ): \"\"\" WTForms class for collecting feedback on Oil & Gas methane emissions. This form models the structure of the O&G feedback spreadsheet and enforces regulatory logic outlined in California methane rules (e.g., 95669.1). Sections include metadata, inspection information, emissions details, mitigation actions, and contact data. Core Features: - Uses standard WTForms field types, with conditionally required fields. - Dropdowns update dynamically based on user selections. - Includes geospatial coordinates and timestamp logic. - Implements cross-field validation for inspection results and mitigation status. Used by: - The web-based feedback form in the ARB Feedback Portal. - Routes such as `og_incidence_create` and `incidence_update`. Notes: - Sector-specific contingent dropdowns are handled via Globals. - Validators are adjusted at runtime depending on the selected conditions. \"\"\" # venting through inspection (not through the 95669.1(b)(1) exclusion) venting_responses = [ \"Venting-construction/maintenance\" , \"Venting-routine\" , ] # These are considered leaks that require mitigation unintentional_leak = [ \"Unintentional-leak\" , \"Unintentional-non-component\" , ] # Section 3 # This field is read-only and displayed for context only. It should not be edited or submitted. label = \"1. Incidence/Emission ID\" id_incidence = IntegerField ( label , validators = [ Optional ()], render_kw = { \"readonly\" : True } ) label = \"2. Plume ID(s)\" id_plume = IntegerField ( label = label , validators = [ InputRequired (), NumberRange ( min = 1 , message = \"Plume ID must be a positive integer\" )], ) # REFERENCES plumes (id_plume) label = \"3. Plume Observation Timestamp(s)\" observation_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"4. Plume CARB Estimated Latitude\" lat_carb = DecimalField ( label = label , places = GPS_RESOLUTION , validators = [ InputRequired (), NumberRange ( ** LATITUDE_VALIDATION )], ) label = \"5. Plume CARB Estimated Longitude\" long_carb = DecimalField ( label = label , places = GPS_RESOLUTION , validators = [ InputRequired (), NumberRange ( ** LONGITUDE_VALIDATION )], ) label = \"6. CARB Message ID\" id_message = StringField ( label = label , validators = [ Optional ()], ) # Section 4 label = \"Q1. Facility Name\" facility_name = StringField ( label = label , validators = [ InputRequired ()], ) label = \"Q2. Facility's Cal e-GGRT ARB ID (if known)\" id_arb_eggrt = StringField ( label = label , validators = [ Optional ()], ) label = \"Q3. Contact Name\" contact_name = StringField ( label = label , validators = [ InputRequired ()], ) # contact_phone = StringField(label=\"Contact Phone\", validators=[InputRequired()]) label = \"Q4. Contact Phone Number\" message = \"Invalid phone number. Phone number must be in format '(123) 456-7890' or '(123) 456-7890 x1234567'.\" contact_phone = StringField ( label = label , validators = [ InputRequired (), Regexp ( regex = r \"^\\(\\d {3} \\) \\d {3} -\\d {4} ( x\\d{1,7})?$\" , message = message ) ], ) label = \"Q5. Contact Email Address\" contact_email = EmailField ( label = label , validators = [ InputRequired (), Email ()], ) # Section 5 label = ( f \"Q6. Was the plume a result of activity-based venting that is being reported \" f \"per section 95669.1(b)(1) of the Oil and Gas Methane Regulation?\" ) venting_exclusion = SelectField ( label = label , choices = Globals . drop_downs [ \"venting_exclusion\" ], validators = [ InputRequired ()], ) label = ( f \"Q7. If you answered 'Yes' to Q6, please provide a brief summary of the source of the venting \" f \"defined by Regulation 95669.1(b)(1) and why the venting occurred.\" ) message = \"If provided, a description must be at least 30 characters.\" venting_description_1 = TextAreaField ( label = label , validators = [ InputRequired (), Length ( min = 30 , message = message )], ) # Section 6 label = \"Q8. Was an OGI inspection performed?\" ogi_performed = SelectField ( label = label , choices = Globals . drop_downs [ \"ogi_performed\" ], validators = [ InputRequired ()], ) label = \"Q9. If you answered 'Yes' to Q8, what date and time was the OGI inspection performed?\" ogi_date = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"Q10. If you answered 'Yes' to Q8, what type of source was found using OGI?\" ogi_result = SelectField ( label = label , choices = Globals . drop_downs [ \"ogi_result\" ], validators = [ InputRequired ()], ) label = \"Q11. Was a Method 21 inspection performed?\" method21_performed = SelectField ( label = label , choices = Globals . drop_downs [ \"method21_performed\" ], validators = [ InputRequired ()], ) label = \"Q12. If you answered 'Yes' to Q11, what date and time was the Method 21 inspection performed?\" method21_date = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"Q13. If you answered 'Yes' to Q11, what type of source was found using Method 21?\" method21_result = SelectField ( label = label , choices = Globals . drop_downs [ \"method21_result\" ], validators = [ InputRequired ()], ) label = \"Q14. If you answered 'Yes' to Q11, what was the initial leak concentration in ppmv (if applicable)?\" initial_leak_concentration = FloatField ( label = label , validators = [ InputRequired ()], ) label = ( f \"Q15. If you answered 'Venting' to Q10 or Q13, please provide a brief summary of the source \" f \"of the venting discovered during the ground inspection and why the venting occurred.\" ) venting_description_2 = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = ( f \"Q16. If you answered a 'Unintentional-leak' or 'Unintentional-non-component' to Q10 or Q13, \" f \"please provide a description of your initial mitigation plan.\" ) initial_mitigation_plan = TextAreaField ( label = label , validators = [ InputRequired ()], ) # Section 7 label = f \"Q17. What type of equipment is at the source of the emissions?\" equipment_at_source = SelectField ( label = label , choices = Globals . drop_downs [ \"equipment_at_source\" ], validators = [ InputRequired (), ], ) label = \"Q18. If you answered 'Other' for Q17, please provide an additional description of the equipment.\" equipment_other_description = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = f \"Q19. If your source is a component, what type of component is at the source of the emissions?\" component_at_source = SelectField ( label = label , choices = Globals . drop_downs [ \"component_at_source\" ], validators = [], ) label = \"Q20. If you answered 'Other' for Q19, please provide an additional description of the component.\" component_other_description = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = f \"Q21. Repair/mitigation completion date & time (if applicable).\" repair_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = f \"Q22. Final repair concentration in ppmv (if applicable).\" final_repair_concentration = FloatField ( label = label , validators = [ InputRequired ()], ) label = f \"Q23. Repair/Mitigation actions taken (if applicable).\" repair_description = StringField ( label = label , validators = [ InputRequired ()], ) # Section 8 label = f \"Q24. Additional notes or comments.\" additional_notes = TextAreaField ( label = label , validators = [], ) label = \"1. CARB internal notes\" carb_notes = TextAreaField ( label = label , validators = [], ) def update_contingent_selectors ( self ) -> None : \"\"\" Update dropdown field options based on dependent selector fields. Dynamically replaces `.choices` for contingent fields depending on parent selections. Uses the `Globals.drop_downs_contingent` structure for Oil & Gas to determine appropriate mappings. Examples: - not implemented yet Returns: None \"\"\" pass def validate ( self , extra_validators = None ) -> bool : \"\"\" Override the default WTForms validation logic with cross-field rules specific to Oil & Gas reporting. Invokes: - `determine_contingent_fields()` to update validators before validation. - `super().validate()` to apply all field and form-level validations. Custom checks include: - Required fields based on mitigation status or inspection outcomes. - Logical enforcement of conditional relationships between fields. Args: extra_validators (dict, optional): Additional validators provided at runtime. Returns: bool: True if form passes all validation rules, otherwise False. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () ################################################################################################### # Set selectors with values not in their choices list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each fields individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(\"in the validator before super\") super_return = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### if self . observation_timestamp . data and self . ogi_date . data : if self . observation_timestamp . data > self . ogi_date . data : self . ogi_date . errors . append ( \"Initial OGI timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . method21_date . data : if self . observation_timestamp . data > self . method21_date . data : self . method21_date . errors . append ( \"Initial Method 21 timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . repair_timestamp . data : if self . observation_timestamp . data > self . repair_timestamp . data : self . method21_date . errors . append ( \"Repair timestamp must be after the plume observation timestamp\" ) if self . venting_exclusion and self . ogi_result . data : if self . venting_exclusion . data == \"Yes\" : if self . ogi_result . data in [ \"Unintentional-leak\" ]: self . ogi_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with OGI.\" ) if self . venting_exclusion and self . method21_result . data : if self . venting_exclusion . data == \"Yes\" : if self . method21_result . data in [ \"Unintentional-leak\" ]: self . method21_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with Method 21.\" ) if self . ogi_result . data in self . unintentional_leak : if self . method21_performed . data != \"Yes\" : self . method21_performed . errors . append ( \"If a leak was detected via OGI, Method 21 must be performed.\" ) if self . ogi_performed . data == \"No\" : if self . ogi_date . data : self . ogi_date . errors . append ( \"Can't have an OGI inspection date if OGI was not performed\" ) # print(f\"{self.ogi_result.data=}\") if self . ogi_result . data != PLEASE_SELECT : if self . ogi_result . data != \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Can't have an OGI result if OGI was not performed\" ) if self . method21_performed . data == \"No\" : if self . method21_date . data : self . method21_date . errors . append ( \"Can't have an Method 21 inspection date if Method 21 was not performed\" ) if self . initial_leak_concentration . data : self . initial_leak_concentration . errors . append ( \"Can't have an Method 21 concentration if Method 21 was not performed\" ) # print(f\"{self.method21_result.data=}\") if self . method21_result . data != PLEASE_SELECT : if self . method21_result . data != \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Can't have an Method 21 result if Method 21 was not performed\" ) if self . venting_exclusion . data == \"No\" and self . ogi_performed . data == \"No\" and self . method21_performed . data == \"No\" : self . method21_performed . errors . append ( \"If you do not claim a venting exclusion, Method 21 or OGI must be performed.\" ) # todo (consider) - you could also remove the option for not applicable rather than the following two tests if self . ogi_performed . data == \"Yes\" : if self . ogi_result . data == \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Invalid response given your Q8 answer\" ) if self . method21_performed . data == \"Yes\" : if self . method21_result . data == \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Invalid response given your Q11 answer\" ) ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields to a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) logger . debug ( f \"after validate(): { self . errors =} \" ) return form_valid def determine_contingent_fields ( self ) -> None : \"\"\" Adjust validators based on user selections that imply exclusions or optional behavior. Affects validation logic such as: - 95669.1(b)(1) exclusions where OGI inspection is not required. - Skipping downstream fields when \"No leak was detected\" is selected. - Making \"Other\" explanations required only if \"Other\" is selected. Notes: - Should be called before validation to sync rules with input state. - Venting-related exclusions may need careful ordering to preserve business logic. Returns: None. \"\"\" # logger.debug(f\"In determine_contingent_fields()\") # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_venting_exclusion = [ \"venting_description_1\" , ] optional_if_venting_exclusion = [ \"ogi_performed\" , \"ogi_date\" , \"ogi_result\" , \"method21_performed\" , \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , \"venting_description_2\" , \"initial_mitigation_plan\" , \"equipment_at_source\" , \"equipment_other_description\" , \"component_at_source\" , \"component_other_description\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , \"additional_notes\" , ] venting_exclusion_test = self . venting_exclusion . data == \"Yes\" # logger.debug(f\"\\n\\t{venting_exclusion_test=}, {self.venting_exclusion_test.data=}\") change_validators_on_test ( self , venting_exclusion_test , required_if_venting_exclusion , optional_if_venting_exclusion ) required_if_ogi_performed = [ \"ogi_date\" , \"ogi_result\" , ] ogi_test = self . ogi_performed . data == \"Yes\" change_validators_on_test ( self , ogi_test , required_if_ogi_performed ) required_if_method21_performed = [ \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , ] method21_test = self . method21_performed . data == \"Yes\" change_validators_on_test ( self , method21_test , required_if_method21_performed ) required_if_venting_on_inspection = [ \"venting_description_2\" , ] venting2_test = False if self . ogi_result . data in self . venting_responses or self . method21_result . data in self . venting_responses : venting2_test = True change_validators_on_test ( self , venting2_test , required_if_venting_on_inspection ) required_if_unintentional = [ \"initial_mitigation_plan\" , \"equipment_at_source\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , ] unintentional_test = False if self . ogi_result . data in self . unintentional_leak or self . method21_result . data in self . unintentional_leak : unintentional_test = True change_validators_on_test ( self , unintentional_test , required_if_unintentional ) required_if_equipment_other = [ \"equipment_other_description\" , ] equipment_other_test = self . equipment_at_source . data == \"Other\" change_validators_on_test ( self , equipment_other_test , required_if_equipment_other ) required_if_component_other = [ \"component_other_description\" , ] component_other_test = self . component_at_source . data == \"Other\" change_validators_on_test ( self , component_other_test , required_if_component_other ) determine_contingent_fields () Adjust validators based on user selections that imply exclusions or optional behavior. Affects validation logic such as 95669.1(b)(1) exclusions where OGI inspection is not required. Skipping downstream fields when \"No leak was detected\" is selected. Making \"Other\" explanations required only if \"Other\" is selected. Notes Should be called before validation to sync rules with input state. Venting-related exclusions may need careful ordering to preserve business logic. Returns: None \u2013 None. Source code in arb\\portal\\wtf_oil_and_gas.py 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 def determine_contingent_fields ( self ) -> None : \"\"\" Adjust validators based on user selections that imply exclusions or optional behavior. Affects validation logic such as: - 95669.1(b)(1) exclusions where OGI inspection is not required. - Skipping downstream fields when \"No leak was detected\" is selected. - Making \"Other\" explanations required only if \"Other\" is selected. Notes: - Should be called before validation to sync rules with input state. - Venting-related exclusions may need careful ordering to preserve business logic. Returns: None. \"\"\" # logger.debug(f\"In determine_contingent_fields()\") # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_venting_exclusion = [ \"venting_description_1\" , ] optional_if_venting_exclusion = [ \"ogi_performed\" , \"ogi_date\" , \"ogi_result\" , \"method21_performed\" , \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , \"venting_description_2\" , \"initial_mitigation_plan\" , \"equipment_at_source\" , \"equipment_other_description\" , \"component_at_source\" , \"component_other_description\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , \"additional_notes\" , ] venting_exclusion_test = self . venting_exclusion . data == \"Yes\" # logger.debug(f\"\\n\\t{venting_exclusion_test=}, {self.venting_exclusion_test.data=}\") change_validators_on_test ( self , venting_exclusion_test , required_if_venting_exclusion , optional_if_venting_exclusion ) required_if_ogi_performed = [ \"ogi_date\" , \"ogi_result\" , ] ogi_test = self . ogi_performed . data == \"Yes\" change_validators_on_test ( self , ogi_test , required_if_ogi_performed ) required_if_method21_performed = [ \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , ] method21_test = self . method21_performed . data == \"Yes\" change_validators_on_test ( self , method21_test , required_if_method21_performed ) required_if_venting_on_inspection = [ \"venting_description_2\" , ] venting2_test = False if self . ogi_result . data in self . venting_responses or self . method21_result . data in self . venting_responses : venting2_test = True change_validators_on_test ( self , venting2_test , required_if_venting_on_inspection ) required_if_unintentional = [ \"initial_mitigation_plan\" , \"equipment_at_source\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , ] unintentional_test = False if self . ogi_result . data in self . unintentional_leak or self . method21_result . data in self . unintentional_leak : unintentional_test = True change_validators_on_test ( self , unintentional_test , required_if_unintentional ) required_if_equipment_other = [ \"equipment_other_description\" , ] equipment_other_test = self . equipment_at_source . data == \"Other\" change_validators_on_test ( self , equipment_other_test , required_if_equipment_other ) required_if_component_other = [ \"component_other_description\" , ] component_other_test = self . component_at_source . data == \"Other\" change_validators_on_test ( self , component_other_test , required_if_component_other ) update_contingent_selectors () Update dropdown field options based on dependent selector fields. Dynamically replaces .choices for contingent fields depending on parent selections. Uses the Globals.drop_downs_contingent structure for Oil & Gas to determine appropriate mappings. Examples: not implemented yet Returns: None \u2013 None Source code in arb\\portal\\wtf_oil_and_gas.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def update_contingent_selectors ( self ) -> None : \"\"\" Update dropdown field options based on dependent selector fields. Dynamically replaces `.choices` for contingent fields depending on parent selections. Uses the `Globals.drop_downs_contingent` structure for Oil & Gas to determine appropriate mappings. Examples: - not implemented yet Returns: None \"\"\" validate ( extra_validators = None ) Override the default WTForms validation logic with cross-field rules specific to Oil & Gas reporting. Invokes determine_contingent_fields() to update validators before validation. super().validate() to apply all field and form-level validations. Custom checks include Required fields based on mitigation status or inspection outcomes. Logical enforcement of conditional relationships between fields. Parameters: extra_validators ( dict , default: None ) \u2013 Additional validators provided at runtime. Returns: bool ( bool ) \u2013 True if form passes all validation rules, otherwise False. Source code in arb\\portal\\wtf_oil_and_gas.py 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 def validate ( self , extra_validators = None ) -> bool : \"\"\" Override the default WTForms validation logic with cross-field rules specific to Oil & Gas reporting. Invokes: - `determine_contingent_fields()` to update validators before validation. - `super().validate()` to apply all field and form-level validations. Custom checks include: - Required fields based on mitigation status or inspection outcomes. - Logical enforcement of conditional relationships between fields. Args: extra_validators (dict, optional): Additional validators provided at runtime. Returns: bool: True if form passes all validation rules, otherwise False. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () ################################################################################################### # Set selectors with values not in their choices list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each fields individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(\"in the validator before super\") super_return = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### if self . observation_timestamp . data and self . ogi_date . data : if self . observation_timestamp . data > self . ogi_date . data : self . ogi_date . errors . append ( \"Initial OGI timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . method21_date . data : if self . observation_timestamp . data > self . method21_date . data : self . method21_date . errors . append ( \"Initial Method 21 timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . repair_timestamp . data : if self . observation_timestamp . data > self . repair_timestamp . data : self . method21_date . errors . append ( \"Repair timestamp must be after the plume observation timestamp\" ) if self . venting_exclusion and self . ogi_result . data : if self . venting_exclusion . data == \"Yes\" : if self . ogi_result . data in [ \"Unintentional-leak\" ]: self . ogi_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with OGI.\" ) if self . venting_exclusion and self . method21_result . data : if self . venting_exclusion . data == \"Yes\" : if self . method21_result . data in [ \"Unintentional-leak\" ]: self . method21_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with Method 21.\" ) if self . ogi_result . data in self . unintentional_leak : if self . method21_performed . data != \"Yes\" : self . method21_performed . errors . append ( \"If a leak was detected via OGI, Method 21 must be performed.\" ) if self . ogi_performed . data == \"No\" : if self . ogi_date . data : self . ogi_date . errors . append ( \"Can't have an OGI inspection date if OGI was not performed\" ) # print(f\"{self.ogi_result.data=}\") if self . ogi_result . data != PLEASE_SELECT : if self . ogi_result . data != \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Can't have an OGI result if OGI was not performed\" ) if self . method21_performed . data == \"No\" : if self . method21_date . data : self . method21_date . errors . append ( \"Can't have an Method 21 inspection date if Method 21 was not performed\" ) if self . initial_leak_concentration . data : self . initial_leak_concentration . errors . append ( \"Can't have an Method 21 concentration if Method 21 was not performed\" ) # print(f\"{self.method21_result.data=}\") if self . method21_result . data != PLEASE_SELECT : if self . method21_result . data != \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Can't have an Method 21 result if Method 21 was not performed\" ) if self . venting_exclusion . data == \"No\" and self . ogi_performed . data == \"No\" and self . method21_performed . data == \"No\" : self . method21_performed . errors . append ( \"If you do not claim a venting exclusion, Method 21 or OGI must be performed.\" ) # todo (consider) - you could also remove the option for not applicable rather than the following two tests if self . ogi_performed . data == \"Yes\" : if self . ogi_result . data == \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Invalid response given your Q8 answer\" ) if self . method21_performed . data == \"Yes\" : if self . method21_result . data == \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Invalid response given your Q11 answer\" ) ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields to a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) logger . debug ( f \"after validate(): { self . errors =} \" ) return form_valid","title":"arb.portal.wtf_oil_and_gas"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arbportalwtf_oil_and_gas","text":"Oil & Gas Feedback Form (WTForms) for the ARB Feedback Portal. Defines the OGFeedback class, a complex feedback form used for collecting structured data about methane emission incidents in the oil and gas sector. The form logic mirrors the official O&G spreadsheet and includes conditional field validation, dynamic dropdown dependencies, and timestamp-based consistency checks.","title":"arb.portal.wtf_oil_and_gas"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas--key-features","text":"Enforces correct response flows based on regulatory logic (e.g., 95669.1(b)(1) exclusions). Includes geospatial validation and timestamp sequencing checks. Cross-field validation logic implemented in validate() . Supports conditional validation with custom helpers like change_validators_on_test() .","title":"Key Features:"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas--usage","text":"form = OGFeedback() form.process(request.form) if form.validate_on_submit(): process_feedback_data(form.data)","title":"Usage:"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas--notes","text":"Fields such as id_incidence are read-only and display-only. Contingent dropdowns are updated via update_contingent_selectors() . Cross-dependencies (e.g., OGI required if no venting exclusion) are enforced dynamically.","title":"Notes:"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback","text":"Bases: FlaskForm WTForms class for collecting feedback on Oil & Gas methane emissions. This form models the structure of the O&G feedback spreadsheet and enforces regulatory logic outlined in California methane rules (e.g., 95669.1). Sections include metadata, inspection information, emissions details, mitigation actions, and contact data. Core Features Uses standard WTForms field types, with conditionally required fields. Dropdowns update dynamically based on user selections. Includes geospatial coordinates and timestamp logic. Implements cross-field validation for inspection results and mitigation status. Used by The web-based feedback form in the ARB Feedback Portal. Routes such as og_incidence_create and incidence_update . Notes Sector-specific contingent dropdowns are handled via Globals. Validators are adjusted at runtime depending on the selected conditions. Source code in arb\\portal\\wtf_oil_and_gas.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 class OGFeedback ( FlaskForm ): \"\"\" WTForms class for collecting feedback on Oil & Gas methane emissions. This form models the structure of the O&G feedback spreadsheet and enforces regulatory logic outlined in California methane rules (e.g., 95669.1). Sections include metadata, inspection information, emissions details, mitigation actions, and contact data. Core Features: - Uses standard WTForms field types, with conditionally required fields. - Dropdowns update dynamically based on user selections. - Includes geospatial coordinates and timestamp logic. - Implements cross-field validation for inspection results and mitigation status. Used by: - The web-based feedback form in the ARB Feedback Portal. - Routes such as `og_incidence_create` and `incidence_update`. Notes: - Sector-specific contingent dropdowns are handled via Globals. - Validators are adjusted at runtime depending on the selected conditions. \"\"\" # venting through inspection (not through the 95669.1(b)(1) exclusion) venting_responses = [ \"Venting-construction/maintenance\" , \"Venting-routine\" , ] # These are considered leaks that require mitigation unintentional_leak = [ \"Unintentional-leak\" , \"Unintentional-non-component\" , ] # Section 3 # This field is read-only and displayed for context only. It should not be edited or submitted. label = \"1. Incidence/Emission ID\" id_incidence = IntegerField ( label , validators = [ Optional ()], render_kw = { \"readonly\" : True } ) label = \"2. Plume ID(s)\" id_plume = IntegerField ( label = label , validators = [ InputRequired (), NumberRange ( min = 1 , message = \"Plume ID must be a positive integer\" )], ) # REFERENCES plumes (id_plume) label = \"3. Plume Observation Timestamp(s)\" observation_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"4. Plume CARB Estimated Latitude\" lat_carb = DecimalField ( label = label , places = GPS_RESOLUTION , validators = [ InputRequired (), NumberRange ( ** LATITUDE_VALIDATION )], ) label = \"5. Plume CARB Estimated Longitude\" long_carb = DecimalField ( label = label , places = GPS_RESOLUTION , validators = [ InputRequired (), NumberRange ( ** LONGITUDE_VALIDATION )], ) label = \"6. CARB Message ID\" id_message = StringField ( label = label , validators = [ Optional ()], ) # Section 4 label = \"Q1. Facility Name\" facility_name = StringField ( label = label , validators = [ InputRequired ()], ) label = \"Q2. Facility's Cal e-GGRT ARB ID (if known)\" id_arb_eggrt = StringField ( label = label , validators = [ Optional ()], ) label = \"Q3. Contact Name\" contact_name = StringField ( label = label , validators = [ InputRequired ()], ) # contact_phone = StringField(label=\"Contact Phone\", validators=[InputRequired()]) label = \"Q4. Contact Phone Number\" message = \"Invalid phone number. Phone number must be in format '(123) 456-7890' or '(123) 456-7890 x1234567'.\" contact_phone = StringField ( label = label , validators = [ InputRequired (), Regexp ( regex = r \"^\\(\\d {3} \\) \\d {3} -\\d {4} ( x\\d{1,7})?$\" , message = message ) ], ) label = \"Q5. Contact Email Address\" contact_email = EmailField ( label = label , validators = [ InputRequired (), Email ()], ) # Section 5 label = ( f \"Q6. Was the plume a result of activity-based venting that is being reported \" f \"per section 95669.1(b)(1) of the Oil and Gas Methane Regulation?\" ) venting_exclusion = SelectField ( label = label , choices = Globals . drop_downs [ \"venting_exclusion\" ], validators = [ InputRequired ()], ) label = ( f \"Q7. If you answered 'Yes' to Q6, please provide a brief summary of the source of the venting \" f \"defined by Regulation 95669.1(b)(1) and why the venting occurred.\" ) message = \"If provided, a description must be at least 30 characters.\" venting_description_1 = TextAreaField ( label = label , validators = [ InputRequired (), Length ( min = 30 , message = message )], ) # Section 6 label = \"Q8. Was an OGI inspection performed?\" ogi_performed = SelectField ( label = label , choices = Globals . drop_downs [ \"ogi_performed\" ], validators = [ InputRequired ()], ) label = \"Q9. If you answered 'Yes' to Q8, what date and time was the OGI inspection performed?\" ogi_date = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"Q10. If you answered 'Yes' to Q8, what type of source was found using OGI?\" ogi_result = SelectField ( label = label , choices = Globals . drop_downs [ \"ogi_result\" ], validators = [ InputRequired ()], ) label = \"Q11. Was a Method 21 inspection performed?\" method21_performed = SelectField ( label = label , choices = Globals . drop_downs [ \"method21_performed\" ], validators = [ InputRequired ()], ) label = \"Q12. If you answered 'Yes' to Q11, what date and time was the Method 21 inspection performed?\" method21_date = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = \"Q13. If you answered 'Yes' to Q11, what type of source was found using Method 21?\" method21_result = SelectField ( label = label , choices = Globals . drop_downs [ \"method21_result\" ], validators = [ InputRequired ()], ) label = \"Q14. If you answered 'Yes' to Q11, what was the initial leak concentration in ppmv (if applicable)?\" initial_leak_concentration = FloatField ( label = label , validators = [ InputRequired ()], ) label = ( f \"Q15. If you answered 'Venting' to Q10 or Q13, please provide a brief summary of the source \" f \"of the venting discovered during the ground inspection and why the venting occurred.\" ) venting_description_2 = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = ( f \"Q16. If you answered a 'Unintentional-leak' or 'Unintentional-non-component' to Q10 or Q13, \" f \"please provide a description of your initial mitigation plan.\" ) initial_mitigation_plan = TextAreaField ( label = label , validators = [ InputRequired ()], ) # Section 7 label = f \"Q17. What type of equipment is at the source of the emissions?\" equipment_at_source = SelectField ( label = label , choices = Globals . drop_downs [ \"equipment_at_source\" ], validators = [ InputRequired (), ], ) label = \"Q18. If you answered 'Other' for Q17, please provide an additional description of the equipment.\" equipment_other_description = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = f \"Q19. If your source is a component, what type of component is at the source of the emissions?\" component_at_source = SelectField ( label = label , choices = Globals . drop_downs [ \"component_at_source\" ], validators = [], ) label = \"Q20. If you answered 'Other' for Q19, please provide an additional description of the component.\" component_other_description = TextAreaField ( label = label , validators = [ InputRequired ()], ) label = f \"Q21. Repair/mitigation completion date & time (if applicable).\" repair_timestamp = DateTimeLocalField ( label = label , validators = [ InputRequired ()], format = HTML_LOCAL_TIME_FORMAT , ) label = f \"Q22. Final repair concentration in ppmv (if applicable).\" final_repair_concentration = FloatField ( label = label , validators = [ InputRequired ()], ) label = f \"Q23. Repair/Mitigation actions taken (if applicable).\" repair_description = StringField ( label = label , validators = [ InputRequired ()], ) # Section 8 label = f \"Q24. Additional notes or comments.\" additional_notes = TextAreaField ( label = label , validators = [], ) label = \"1. CARB internal notes\" carb_notes = TextAreaField ( label = label , validators = [], ) def update_contingent_selectors ( self ) -> None : \"\"\" Update dropdown field options based on dependent selector fields. Dynamically replaces `.choices` for contingent fields depending on parent selections. Uses the `Globals.drop_downs_contingent` structure for Oil & Gas to determine appropriate mappings. Examples: - not implemented yet Returns: None \"\"\" pass def validate ( self , extra_validators = None ) -> bool : \"\"\" Override the default WTForms validation logic with cross-field rules specific to Oil & Gas reporting. Invokes: - `determine_contingent_fields()` to update validators before validation. - `super().validate()` to apply all field and form-level validations. Custom checks include: - Required fields based on mitigation status or inspection outcomes. - Logical enforcement of conditional relationships between fields. Args: extra_validators (dict, optional): Additional validators provided at runtime. Returns: bool: True if form passes all validation rules, otherwise False. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () ################################################################################################### # Set selectors with values not in their choices list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each fields individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(\"in the validator before super\") super_return = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### if self . observation_timestamp . data and self . ogi_date . data : if self . observation_timestamp . data > self . ogi_date . data : self . ogi_date . errors . append ( \"Initial OGI timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . method21_date . data : if self . observation_timestamp . data > self . method21_date . data : self . method21_date . errors . append ( \"Initial Method 21 timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . repair_timestamp . data : if self . observation_timestamp . data > self . repair_timestamp . data : self . method21_date . errors . append ( \"Repair timestamp must be after the plume observation timestamp\" ) if self . venting_exclusion and self . ogi_result . data : if self . venting_exclusion . data == \"Yes\" : if self . ogi_result . data in [ \"Unintentional-leak\" ]: self . ogi_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with OGI.\" ) if self . venting_exclusion and self . method21_result . data : if self . venting_exclusion . data == \"Yes\" : if self . method21_result . data in [ \"Unintentional-leak\" ]: self . method21_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with Method 21.\" ) if self . ogi_result . data in self . unintentional_leak : if self . method21_performed . data != \"Yes\" : self . method21_performed . errors . append ( \"If a leak was detected via OGI, Method 21 must be performed.\" ) if self . ogi_performed . data == \"No\" : if self . ogi_date . data : self . ogi_date . errors . append ( \"Can't have an OGI inspection date if OGI was not performed\" ) # print(f\"{self.ogi_result.data=}\") if self . ogi_result . data != PLEASE_SELECT : if self . ogi_result . data != \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Can't have an OGI result if OGI was not performed\" ) if self . method21_performed . data == \"No\" : if self . method21_date . data : self . method21_date . errors . append ( \"Can't have an Method 21 inspection date if Method 21 was not performed\" ) if self . initial_leak_concentration . data : self . initial_leak_concentration . errors . append ( \"Can't have an Method 21 concentration if Method 21 was not performed\" ) # print(f\"{self.method21_result.data=}\") if self . method21_result . data != PLEASE_SELECT : if self . method21_result . data != \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Can't have an Method 21 result if Method 21 was not performed\" ) if self . venting_exclusion . data == \"No\" and self . ogi_performed . data == \"No\" and self . method21_performed . data == \"No\" : self . method21_performed . errors . append ( \"If you do not claim a venting exclusion, Method 21 or OGI must be performed.\" ) # todo (consider) - you could also remove the option for not applicable rather than the following two tests if self . ogi_performed . data == \"Yes\" : if self . ogi_result . data == \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Invalid response given your Q8 answer\" ) if self . method21_performed . data == \"Yes\" : if self . method21_result . data == \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Invalid response given your Q11 answer\" ) ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields to a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) logger . debug ( f \"after validate(): { self . errors =} \" ) return form_valid def determine_contingent_fields ( self ) -> None : \"\"\" Adjust validators based on user selections that imply exclusions or optional behavior. Affects validation logic such as: - 95669.1(b)(1) exclusions where OGI inspection is not required. - Skipping downstream fields when \"No leak was detected\" is selected. - Making \"Other\" explanations required only if \"Other\" is selected. Notes: - Should be called before validation to sync rules with input state. - Venting-related exclusions may need careful ordering to preserve business logic. Returns: None. \"\"\" # logger.debug(f\"In determine_contingent_fields()\") # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_venting_exclusion = [ \"venting_description_1\" , ] optional_if_venting_exclusion = [ \"ogi_performed\" , \"ogi_date\" , \"ogi_result\" , \"method21_performed\" , \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , \"venting_description_2\" , \"initial_mitigation_plan\" , \"equipment_at_source\" , \"equipment_other_description\" , \"component_at_source\" , \"component_other_description\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , \"additional_notes\" , ] venting_exclusion_test = self . venting_exclusion . data == \"Yes\" # logger.debug(f\"\\n\\t{venting_exclusion_test=}, {self.venting_exclusion_test.data=}\") change_validators_on_test ( self , venting_exclusion_test , required_if_venting_exclusion , optional_if_venting_exclusion ) required_if_ogi_performed = [ \"ogi_date\" , \"ogi_result\" , ] ogi_test = self . ogi_performed . data == \"Yes\" change_validators_on_test ( self , ogi_test , required_if_ogi_performed ) required_if_method21_performed = [ \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , ] method21_test = self . method21_performed . data == \"Yes\" change_validators_on_test ( self , method21_test , required_if_method21_performed ) required_if_venting_on_inspection = [ \"venting_description_2\" , ] venting2_test = False if self . ogi_result . data in self . venting_responses or self . method21_result . data in self . venting_responses : venting2_test = True change_validators_on_test ( self , venting2_test , required_if_venting_on_inspection ) required_if_unintentional = [ \"initial_mitigation_plan\" , \"equipment_at_source\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , ] unintentional_test = False if self . ogi_result . data in self . unintentional_leak or self . method21_result . data in self . unintentional_leak : unintentional_test = True change_validators_on_test ( self , unintentional_test , required_if_unintentional ) required_if_equipment_other = [ \"equipment_other_description\" , ] equipment_other_test = self . equipment_at_source . data == \"Other\" change_validators_on_test ( self , equipment_other_test , required_if_equipment_other ) required_if_component_other = [ \"component_other_description\" , ] component_other_test = self . component_at_source . data == \"Other\" change_validators_on_test ( self , component_other_test , required_if_component_other )","title":"OGFeedback"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback.determine_contingent_fields","text":"Adjust validators based on user selections that imply exclusions or optional behavior. Affects validation logic such as 95669.1(b)(1) exclusions where OGI inspection is not required. Skipping downstream fields when \"No leak was detected\" is selected. Making \"Other\" explanations required only if \"Other\" is selected. Notes Should be called before validation to sync rules with input state. Venting-related exclusions may need careful ordering to preserve business logic. Returns: None \u2013 None. Source code in arb\\portal\\wtf_oil_and_gas.py 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 def determine_contingent_fields ( self ) -> None : \"\"\" Adjust validators based on user selections that imply exclusions or optional behavior. Affects validation logic such as: - 95669.1(b)(1) exclusions where OGI inspection is not required. - Skipping downstream fields when \"No leak was detected\" is selected. - Making \"Other\" explanations required only if \"Other\" is selected. Notes: - Should be called before validation to sync rules with input state. - Venting-related exclusions may need careful ordering to preserve business logic. Returns: None. \"\"\" # logger.debug(f\"In determine_contingent_fields()\") # If a venting exclusion is claimed, then a venting description is required and many fields become optional required_if_venting_exclusion = [ \"venting_description_1\" , ] optional_if_venting_exclusion = [ \"ogi_performed\" , \"ogi_date\" , \"ogi_result\" , \"method21_performed\" , \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , \"venting_description_2\" , \"initial_mitigation_plan\" , \"equipment_at_source\" , \"equipment_other_description\" , \"component_at_source\" , \"component_other_description\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , \"additional_notes\" , ] venting_exclusion_test = self . venting_exclusion . data == \"Yes\" # logger.debug(f\"\\n\\t{venting_exclusion_test=}, {self.venting_exclusion_test.data=}\") change_validators_on_test ( self , venting_exclusion_test , required_if_venting_exclusion , optional_if_venting_exclusion ) required_if_ogi_performed = [ \"ogi_date\" , \"ogi_result\" , ] ogi_test = self . ogi_performed . data == \"Yes\" change_validators_on_test ( self , ogi_test , required_if_ogi_performed ) required_if_method21_performed = [ \"method21_date\" , \"method21_result\" , \"initial_leak_concentration\" , ] method21_test = self . method21_performed . data == \"Yes\" change_validators_on_test ( self , method21_test , required_if_method21_performed ) required_if_venting_on_inspection = [ \"venting_description_2\" , ] venting2_test = False if self . ogi_result . data in self . venting_responses or self . method21_result . data in self . venting_responses : venting2_test = True change_validators_on_test ( self , venting2_test , required_if_venting_on_inspection ) required_if_unintentional = [ \"initial_mitigation_plan\" , \"equipment_at_source\" , \"repair_timestamp\" , \"final_repair_concentration\" , \"repair_description\" , ] unintentional_test = False if self . ogi_result . data in self . unintentional_leak or self . method21_result . data in self . unintentional_leak : unintentional_test = True change_validators_on_test ( self , unintentional_test , required_if_unintentional ) required_if_equipment_other = [ \"equipment_other_description\" , ] equipment_other_test = self . equipment_at_source . data == \"Other\" change_validators_on_test ( self , equipment_other_test , required_if_equipment_other ) required_if_component_other = [ \"component_other_description\" , ] component_other_test = self . component_at_source . data == \"Other\" change_validators_on_test ( self , component_other_test , required_if_component_other )","title":"determine_contingent_fields"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback.update_contingent_selectors","text":"Update dropdown field options based on dependent selector fields. Dynamically replaces .choices for contingent fields depending on parent selections. Uses the Globals.drop_downs_contingent structure for Oil & Gas to determine appropriate mappings. Examples: not implemented yet Returns: None \u2013 None Source code in arb\\portal\\wtf_oil_and_gas.py 298 299 300 301 302 303 304 305 306 307 308 309 310 311 def update_contingent_selectors ( self ) -> None : \"\"\" Update dropdown field options based on dependent selector fields. Dynamically replaces `.choices` for contingent fields depending on parent selections. Uses the `Globals.drop_downs_contingent` structure for Oil & Gas to determine appropriate mappings. Examples: - not implemented yet Returns: None \"\"\"","title":"update_contingent_selectors"},{"location":"reference/arb/portal/wtf_oil_and_gas/#arb.portal.wtf_oil_and_gas.OGFeedback.validate","text":"Override the default WTForms validation logic with cross-field rules specific to Oil & Gas reporting. Invokes determine_contingent_fields() to update validators before validation. super().validate() to apply all field and form-level validations. Custom checks include Required fields based on mitigation status or inspection outcomes. Logical enforcement of conditional relationships between fields. Parameters: extra_validators ( dict , default: None ) \u2013 Additional validators provided at runtime. Returns: bool ( bool ) \u2013 True if form passes all validation rules, otherwise False. Source code in arb\\portal\\wtf_oil_and_gas.py 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 def validate ( self , extra_validators = None ) -> bool : \"\"\" Override the default WTForms validation logic with cross-field rules specific to Oil & Gas reporting. Invokes: - `determine_contingent_fields()` to update validators before validation. - `super().validate()` to apply all field and form-level validations. Custom checks include: - Required fields based on mitigation status or inspection outcomes. - Logical enforcement of conditional relationships between fields. Args: extra_validators (dict, optional): Additional validators provided at runtime. Returns: bool: True if form passes all validation rules, otherwise False. \"\"\" logger . debug ( f \"validate() called.\" ) form_fields = get_wtforms_fields ( self ) # Dictionary to replace standard WTForm messages with alternative message error_message_replacement_dict = { \"Not a valid float value.\" : \"Not a valid numeric value.\" } ################################################################################################### # Add, Remove, or Modify validation at a field level here before the super is called (for example) ################################################################################################### self . determine_contingent_fields () ################################################################################################### # Set selectors with values not in their choices list to \"Please Select\" ################################################################################################### for field_name in form_fields : field = getattr ( self , field_name ) logger . debug ( f \"field_name: { field_name } , { type ( field . data ) =} , { field . data =} , { type ( field . raw_data ) =} \" ) if isinstance ( field , SelectField ): ensure_field_choice ( field_name , field ) ################################################################################################### # call the super to perform each fields individual validation (which saves to form.errors) # This will create the form.errors dictionary. If there are form_errors they will be in the None key. # The form_errors will not affect if validate returns True/False, only the fields are considered. ################################################################################################### # logger.debug(\"in the validator before super\") super_return = super () . validate ( extra_validators = extra_validators ) ################################################################################################### # Validating selectors explicitly ensures the same number of errors on GETS and POSTS for the same data ################################################################################################### validate_selectors ( self , PLEASE_SELECT ) ################################################################################################### # Perform any field level validation where one field is cross-referenced to another # The error will be associated with one of the fields ################################################################################################### if self . observation_timestamp . data and self . ogi_date . data : if self . observation_timestamp . data > self . ogi_date . data : self . ogi_date . errors . append ( \"Initial OGI timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . method21_date . data : if self . observation_timestamp . data > self . method21_date . data : self . method21_date . errors . append ( \"Initial Method 21 timestamp must be after the plume observation timestamp\" ) if self . observation_timestamp . data and self . repair_timestamp . data : if self . observation_timestamp . data > self . repair_timestamp . data : self . method21_date . errors . append ( \"Repair timestamp must be after the plume observation timestamp\" ) if self . venting_exclusion and self . ogi_result . data : if self . venting_exclusion . data == \"Yes\" : if self . ogi_result . data in [ \"Unintentional-leak\" ]: self . ogi_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with OGI.\" ) if self . venting_exclusion and self . method21_result . data : if self . venting_exclusion . data == \"Yes\" : if self . method21_result . data in [ \"Unintentional-leak\" ]: self . method21_result . errors . append ( \"If you claim a venting exclusion, you can't also have a leak detected with Method 21.\" ) if self . ogi_result . data in self . unintentional_leak : if self . method21_performed . data != \"Yes\" : self . method21_performed . errors . append ( \"If a leak was detected via OGI, Method 21 must be performed.\" ) if self . ogi_performed . data == \"No\" : if self . ogi_date . data : self . ogi_date . errors . append ( \"Can't have an OGI inspection date if OGI was not performed\" ) # print(f\"{self.ogi_result.data=}\") if self . ogi_result . data != PLEASE_SELECT : if self . ogi_result . data != \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Can't have an OGI result if OGI was not performed\" ) if self . method21_performed . data == \"No\" : if self . method21_date . data : self . method21_date . errors . append ( \"Can't have an Method 21 inspection date if Method 21 was not performed\" ) if self . initial_leak_concentration . data : self . initial_leak_concentration . errors . append ( \"Can't have an Method 21 concentration if Method 21 was not performed\" ) # print(f\"{self.method21_result.data=}\") if self . method21_result . data != PLEASE_SELECT : if self . method21_result . data != \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Can't have an Method 21 result if Method 21 was not performed\" ) if self . venting_exclusion . data == \"No\" and self . ogi_performed . data == \"No\" and self . method21_performed . data == \"No\" : self . method21_performed . errors . append ( \"If you do not claim a venting exclusion, Method 21 or OGI must be performed.\" ) # todo (consider) - you could also remove the option for not applicable rather than the following two tests if self . ogi_performed . data == \"Yes\" : if self . ogi_result . data == \"Not applicable as OGI was not performed\" : self . ogi_result . errors . append ( \"Invalid response given your Q8 answer\" ) if self . method21_performed . data == \"Yes\" : if self . method21_result . data == \"Not applicable as Method 21 was not performed\" : self . method21_result . errors . append ( \"Invalid response given your Q11 answer\" ) ################################################################################################### # perform any form level validation and append it to the form_errors property # This may not be useful, but if you want to have form level errors appear at the top of the error # header, put the logic here. ################################################################################################### # self.form_errors.append(\"I'm a form level error #1\") # self.form_errors.append(\"I'm a form level error #2\") ################################################################################################### # Search and replace the error messages associated with input fields to a custom message # For instance, the default 'float' error is changed because a typical user will not know what a # float value is (they will be more comfortable with the word 'numeric') ################################################################################################### for field in form_fields : field_errors = getattr ( self , field ) . errors replace_list_occurrences ( field_errors , error_message_replacement_dict ) ################################################################################################### # Current logic to determine if form is valid the error dict must be empty. # #Consider other approaches ################################################################################################### form_valid = not bool ( self . errors ) logger . debug ( f \"after validate(): { self . errors =} \" ) return form_valid","title":"validate"},{"location":"reference/arb/portal/wtf_upload/","text":"arb.portal.wtf_upload WTForms-based upload form for the ARB Feedback Portal. Defines a minimal form used to upload Excel files via the web interface. Typically used in the /upload route. Fields: file: Accepts .xls or .xlsx files only. submit: Triggers form submission. Notes: Leverages Flask-WTF integration with Bootstrap-compatible rendering. Additional validation for file size or filename may be added externally. UploadForm Bases: FlaskForm WTForm for uploading Excel or JSON files via the ARB Feedback Portal. Fields file (FileField): Upload field for selecting a .xls or .xlsx file. submit (SubmitField): Form button to initiate upload. Notes Uses Flask-WTF and integrates with Bootstrap templates. File extension restrictions enforced via FileAllowed . Form is rendered in the Upload UI at /upload . Source code in arb\\portal\\wtf_upload.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class UploadForm ( FlaskForm ): \"\"\" WTForm for uploading Excel or JSON files via the ARB Feedback Portal. Fields: file (FileField): Upload field for selecting a `.xls` or `.xlsx` file. submit (SubmitField): Form button to initiate upload. Notes: - Uses Flask-WTF and integrates with Bootstrap templates. - File extension restrictions enforced via `FileAllowed`. - Form is rendered in the Upload UI at `/upload`. \"\"\" file = FileField ( \"Choose Excel File\" , validators = [ DataRequired (), FileAllowed ([ 'xls' , 'xlsx' ], 'Excel files only!' )] ) submit = SubmitField ( \"Upload\" )","title":"arb.portal.wtf_upload"},{"location":"reference/arb/portal/wtf_upload/#arbportalwtf_upload","text":"WTForms-based upload form for the ARB Feedback Portal. Defines a minimal form used to upload Excel files via the web interface. Typically used in the /upload route.","title":"arb.portal.wtf_upload"},{"location":"reference/arb/portal/wtf_upload/#arb.portal.wtf_upload--fields","text":"file: Accepts .xls or .xlsx files only. submit: Triggers form submission.","title":"Fields:"},{"location":"reference/arb/portal/wtf_upload/#arb.portal.wtf_upload--notes","text":"Leverages Flask-WTF integration with Bootstrap-compatible rendering. Additional validation for file size or filename may be added externally.","title":"Notes:"},{"location":"reference/arb/portal/wtf_upload/#arb.portal.wtf_upload.UploadForm","text":"Bases: FlaskForm WTForm for uploading Excel or JSON files via the ARB Feedback Portal. Fields file (FileField): Upload field for selecting a .xls or .xlsx file. submit (SubmitField): Form button to initiate upload. Notes Uses Flask-WTF and integrates with Bootstrap templates. File extension restrictions enforced via FileAllowed . Form is rendered in the Upload UI at /upload . Source code in arb\\portal\\wtf_upload.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class UploadForm ( FlaskForm ): \"\"\" WTForm for uploading Excel or JSON files via the ARB Feedback Portal. Fields: file (FileField): Upload field for selecting a `.xls` or `.xlsx` file. submit (SubmitField): Form button to initiate upload. Notes: - Uses Flask-WTF and integrates with Bootstrap templates. - File extension restrictions enforced via `FileAllowed`. - Form is rendered in the Upload UI at `/upload`. \"\"\" file = FileField ( \"Choose Excel File\" , validators = [ DataRequired (), FileAllowed ([ 'xls' , 'xlsx' ], 'Excel files only!' )] ) submit = SubmitField ( \"Upload\" )","title":"UploadForm"},{"location":"reference/arb/portal/config/settings/","text":"arb.portal.config.settings Environment-specific configuration classes for the Flask application. Defines base and derived configuration classes used by the ARB portal. Each config class inherits from BaseConfig and may override environment-specific values. Usage from config.settings import DevelopmentConfig, ProductionConfig, TestingConfig Notes Static and environment-derived values belong here. Runtime-dependent settings (platform, CLI, etc.) should go in startup/runtime_info.py . BaseConfig Base configuration shared across all environments. Attributes: POSTGRES_DB_URI ( str ) \u2013 Default PostgreSQL URI if DATABASE_URI is unset. SQLALCHEMY_ENGINE_OPTIONS ( dict ) \u2013 Connection settings for SQLAlchemy. SECRET_KEY ( str ) \u2013 Flask session key. SQLALCHEMY_DATABASE_URI ( str ) \u2013 Final URI used by the app. SQLALCHEMY_TRACK_MODIFICATIONS ( bool ) \u2013 SQLAlchemy event system flag. EXPLAIN_TEMPLATE_LOADING ( bool ) \u2013 Whether to trace template resolution errors. WTF_CSRF_ENABLED ( bool ) \u2013 Cross-site request forgery protection toggle. LOG_LEVEL ( str ) \u2013 Default logging level. TIMEZONE ( str ) \u2013 Target timezone for timestamp formatting. FAST_LOAD ( bool ) \u2013 Enables performance optimizations at startup. Source code in arb\\portal\\config\\settings.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class BaseConfig : \"\"\" Base configuration shared across all environments. Attributes: POSTGRES_DB_URI (str): Default PostgreSQL URI if DATABASE_URI is unset. SQLALCHEMY_ENGINE_OPTIONS (dict): Connection settings for SQLAlchemy. SECRET_KEY (str): Flask session key. SQLALCHEMY_DATABASE_URI (str): Final URI used by the app. SQLALCHEMY_TRACK_MODIFICATIONS (bool): SQLAlchemy event system flag. EXPLAIN_TEMPLATE_LOADING (bool): Whether to trace template resolution errors. WTF_CSRF_ENABLED (bool): Cross-site request forgery protection toggle. LOG_LEVEL (str): Default logging level. TIMEZONE (str): Target timezone for timestamp formatting. FAST_LOAD (bool): Enables performance optimizations at startup. \"\"\" POSTGRES_DB_URI = ( 'postgresql+psycopg2://methane:methaneCH4@prj-bus-methane-aurora-postgresql-instance-1' '.cdae8kkz3fpi.us-west-2.rds.amazonaws.com/plumetracker' ) SQLALCHEMY_ENGINE_OPTIONS = { 'connect_args' : { # 'options': '-c search_path=satellite_tracker_demo1,public -c timezone=UTC' # practice schema 'options' : '-c search_path=satellite_tracker_new,public -c timezone=UTC' # dan's live schema } } SECRET_KEY = os . environ . get ( 'SECRET_KEY' ) or 'secret-key-goes-here' SQLALCHEMY_DATABASE_URI = os . environ . get ( 'DATABASE_URI' ) or POSTGRES_DB_URI SQLALCHEMY_TRACK_MODIFICATIONS = False # When enabled, Flask will log detailed information about templating files # consider setting to True if you're getting TemplateNotFound errors. EXPLAIN_TEMPLATE_LOADING = False # Recommended setting for most use cases. WTF_CSRF_ENABLED = True LOG_LEVEL = \"INFO\" TIMEZONE = \"America/Los_Angeles\" # --------------------------------------------------------------------- # Get other relevant environmental variables here and commandline flags here # for example: set FAST_LOAD=true # --------------------------------------------------------------------- FAST_LOAD = False # flask does not allow for custom arguments so the next block is commented out # if \"--fast-load\" in sys.argv: # print(f\"--fast-load detected in CLI arguments\") # FAST_LOAD = True if os . getenv ( \"FAST_LOAD\" ) == \"true\" : logger . info ( f \"FAST_LOAD detected in CLI arguments\" ) FAST_LOAD = True logger . info ( f \" { FAST_LOAD = } \" ) DevelopmentConfig Bases: BaseConfig Configuration for local development. Attributes: DEBUG ( bool ) \u2013 Enables debug mode. FLASK_ENV ( str ) \u2013 Flask environment indicator. LOG_LEVEL ( str ) \u2013 Logging level (default: \"DEBUG\"). Source code in arb\\portal\\config\\settings.py 78 79 80 81 82 83 84 85 86 87 88 89 90 class DevelopmentConfig ( BaseConfig ): \"\"\" Configuration for local development. Attributes: DEBUG (bool): Enables debug mode. FLASK_ENV (str): Flask environment indicator. LOG_LEVEL (str): Logging level (default: \"DEBUG\"). \"\"\" DEBUG = True FLASK_ENV = \"development\" # EXPLAIN_TEMPLATE_LOADING = True LOG_LEVEL = \"DEBUG\" ProductionConfig Bases: BaseConfig Configuration for deployed production environments. Attributes: DEBUG ( bool ) \u2013 Disables debug features. FLASK_ENV ( str ) \u2013 Environment label for Flask runtime. WTF_CSRF_ENABLED ( bool ) \u2013 Enables CSRF protection. LOG_LEVEL ( str ) \u2013 Logging level (default: \"INFO\"). Source code in arb\\portal\\config\\settings.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class ProductionConfig ( BaseConfig ): \"\"\" Configuration for deployed production environments. Attributes: DEBUG (bool): Disables debug features. FLASK_ENV (str): Environment label for Flask runtime. WTF_CSRF_ENABLED (bool): Enables CSRF protection. LOG_LEVEL (str): Logging level (default: \"INFO\"). \"\"\" DEBUG = False FLASK_ENV = \"production\" WTF_CSRF_ENABLED = True LOG_LEVEL = \"INFO\" TestingConfig Bases: BaseConfig Configuration for isolated testing environments. Attributes: TESTING ( bool ) \u2013 Enables Flask test mode. DEBUG ( bool ) \u2013 Enables debug logging. FLASK_ENV ( str ) \u2013 Flask environment label. WTF_CSRF_ENABLED ( bool ) \u2013 Disables CSRF for test convenience. LOG_LEVEL ( str ) \u2013 Logging level (default: \"WARNING\"). Source code in arb\\portal\\config\\settings.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class TestingConfig ( BaseConfig ): \"\"\" Configuration for isolated testing environments. Attributes: TESTING (bool): Enables Flask test mode. DEBUG (bool): Enables debug logging. FLASK_ENV (str): Flask environment label. WTF_CSRF_ENABLED (bool): Disables CSRF for test convenience. LOG_LEVEL (str): Logging level (default: \"WARNING\"). \"\"\" TESTING = True DEBUG = True FLASK_ENV = \"testing\" WTF_CSRF_ENABLED = False LOG_LEVEL = \"WARNING\"","title":"arb.portal.config.settings"},{"location":"reference/arb/portal/config/settings/#arbportalconfigsettings","text":"Environment-specific configuration classes for the Flask application. Defines base and derived configuration classes used by the ARB portal. Each config class inherits from BaseConfig and may override environment-specific values. Usage from config.settings import DevelopmentConfig, ProductionConfig, TestingConfig Notes Static and environment-derived values belong here. Runtime-dependent settings (platform, CLI, etc.) should go in startup/runtime_info.py .","title":"arb.portal.config.settings"},{"location":"reference/arb/portal/config/settings/#arb.portal.config.settings.BaseConfig","text":"Base configuration shared across all environments. Attributes: POSTGRES_DB_URI ( str ) \u2013 Default PostgreSQL URI if DATABASE_URI is unset. SQLALCHEMY_ENGINE_OPTIONS ( dict ) \u2013 Connection settings for SQLAlchemy. SECRET_KEY ( str ) \u2013 Flask session key. SQLALCHEMY_DATABASE_URI ( str ) \u2013 Final URI used by the app. SQLALCHEMY_TRACK_MODIFICATIONS ( bool ) \u2013 SQLAlchemy event system flag. EXPLAIN_TEMPLATE_LOADING ( bool ) \u2013 Whether to trace template resolution errors. WTF_CSRF_ENABLED ( bool ) \u2013 Cross-site request forgery protection toggle. LOG_LEVEL ( str ) \u2013 Default logging level. TIMEZONE ( str ) \u2013 Target timezone for timestamp formatting. FAST_LOAD ( bool ) \u2013 Enables performance optimizations at startup. Source code in arb\\portal\\config\\settings.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 class BaseConfig : \"\"\" Base configuration shared across all environments. Attributes: POSTGRES_DB_URI (str): Default PostgreSQL URI if DATABASE_URI is unset. SQLALCHEMY_ENGINE_OPTIONS (dict): Connection settings for SQLAlchemy. SECRET_KEY (str): Flask session key. SQLALCHEMY_DATABASE_URI (str): Final URI used by the app. SQLALCHEMY_TRACK_MODIFICATIONS (bool): SQLAlchemy event system flag. EXPLAIN_TEMPLATE_LOADING (bool): Whether to trace template resolution errors. WTF_CSRF_ENABLED (bool): Cross-site request forgery protection toggle. LOG_LEVEL (str): Default logging level. TIMEZONE (str): Target timezone for timestamp formatting. FAST_LOAD (bool): Enables performance optimizations at startup. \"\"\" POSTGRES_DB_URI = ( 'postgresql+psycopg2://methane:methaneCH4@prj-bus-methane-aurora-postgresql-instance-1' '.cdae8kkz3fpi.us-west-2.rds.amazonaws.com/plumetracker' ) SQLALCHEMY_ENGINE_OPTIONS = { 'connect_args' : { # 'options': '-c search_path=satellite_tracker_demo1,public -c timezone=UTC' # practice schema 'options' : '-c search_path=satellite_tracker_new,public -c timezone=UTC' # dan's live schema } } SECRET_KEY = os . environ . get ( 'SECRET_KEY' ) or 'secret-key-goes-here' SQLALCHEMY_DATABASE_URI = os . environ . get ( 'DATABASE_URI' ) or POSTGRES_DB_URI SQLALCHEMY_TRACK_MODIFICATIONS = False # When enabled, Flask will log detailed information about templating files # consider setting to True if you're getting TemplateNotFound errors. EXPLAIN_TEMPLATE_LOADING = False # Recommended setting for most use cases. WTF_CSRF_ENABLED = True LOG_LEVEL = \"INFO\" TIMEZONE = \"America/Los_Angeles\" # --------------------------------------------------------------------- # Get other relevant environmental variables here and commandline flags here # for example: set FAST_LOAD=true # --------------------------------------------------------------------- FAST_LOAD = False # flask does not allow for custom arguments so the next block is commented out # if \"--fast-load\" in sys.argv: # print(f\"--fast-load detected in CLI arguments\") # FAST_LOAD = True if os . getenv ( \"FAST_LOAD\" ) == \"true\" : logger . info ( f \"FAST_LOAD detected in CLI arguments\" ) FAST_LOAD = True logger . info ( f \" { FAST_LOAD = } \" )","title":"BaseConfig"},{"location":"reference/arb/portal/config/settings/#arb.portal.config.settings.DevelopmentConfig","text":"Bases: BaseConfig Configuration for local development. Attributes: DEBUG ( bool ) \u2013 Enables debug mode. FLASK_ENV ( str ) \u2013 Flask environment indicator. LOG_LEVEL ( str ) \u2013 Logging level (default: \"DEBUG\"). Source code in arb\\portal\\config\\settings.py 78 79 80 81 82 83 84 85 86 87 88 89 90 class DevelopmentConfig ( BaseConfig ): \"\"\" Configuration for local development. Attributes: DEBUG (bool): Enables debug mode. FLASK_ENV (str): Flask environment indicator. LOG_LEVEL (str): Logging level (default: \"DEBUG\"). \"\"\" DEBUG = True FLASK_ENV = \"development\" # EXPLAIN_TEMPLATE_LOADING = True LOG_LEVEL = \"DEBUG\"","title":"DevelopmentConfig"},{"location":"reference/arb/portal/config/settings/#arb.portal.config.settings.ProductionConfig","text":"Bases: BaseConfig Configuration for deployed production environments. Attributes: DEBUG ( bool ) \u2013 Disables debug features. FLASK_ENV ( str ) \u2013 Environment label for Flask runtime. WTF_CSRF_ENABLED ( bool ) \u2013 Enables CSRF protection. LOG_LEVEL ( str ) \u2013 Logging level (default: \"INFO\"). Source code in arb\\portal\\config\\settings.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 class ProductionConfig ( BaseConfig ): \"\"\" Configuration for deployed production environments. Attributes: DEBUG (bool): Disables debug features. FLASK_ENV (str): Environment label for Flask runtime. WTF_CSRF_ENABLED (bool): Enables CSRF protection. LOG_LEVEL (str): Logging level (default: \"INFO\"). \"\"\" DEBUG = False FLASK_ENV = \"production\" WTF_CSRF_ENABLED = True LOG_LEVEL = \"INFO\"","title":"ProductionConfig"},{"location":"reference/arb/portal/config/settings/#arb.portal.config.settings.TestingConfig","text":"Bases: BaseConfig Configuration for isolated testing environments. Attributes: TESTING ( bool ) \u2013 Enables Flask test mode. DEBUG ( bool ) \u2013 Enables debug logging. FLASK_ENV ( str ) \u2013 Flask environment label. WTF_CSRF_ENABLED ( bool ) \u2013 Disables CSRF for test convenience. LOG_LEVEL ( str ) \u2013 Logging level (default: \"WARNING\"). Source code in arb\\portal\\config\\settings.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 class TestingConfig ( BaseConfig ): \"\"\" Configuration for isolated testing environments. Attributes: TESTING (bool): Enables Flask test mode. DEBUG (bool): Enables debug logging. FLASK_ENV (str): Flask environment label. WTF_CSRF_ENABLED (bool): Disables CSRF for test convenience. LOG_LEVEL (str): Logging level (default: \"WARNING\"). \"\"\" TESTING = True DEBUG = True FLASK_ENV = \"testing\" WTF_CSRF_ENABLED = False LOG_LEVEL = \"WARNING\"","title":"TestingConfig"},{"location":"reference/arb/portal/startup/db/","text":"arb.portal.startup.db Database initialization and reflection routines for the ARB Feedback Portal. These functions are intended to be called during Flask app startup (from create_app() ) to configure SQLAlchemy metadata, initialize models, and create missing tables. Usage from startup.db import reflect_database, db_initialize_and_create Notes SQLAlchemy models must be explicitly imported to register before table creation. Logging is enabled throughout to trace database state and startup flow. db_create () Create all tables defined in SQLAlchemy metadata if they don\u2019t exist. Skips creation if FAST_LOAD=True is set in the app config. Returns: None \u2013 None Logs Warn: If creation is skipped due to FAST_LOAD Info: When table creation begins Debug: After schema creation completes Source code in arb\\portal\\startup\\db.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def db_create () -> None : \"\"\" Create all tables defined in SQLAlchemy metadata if they don\u2019t exist. Skips creation if `FAST_LOAD=True` is set in the app config. Returns: None Logs: - Warn: If creation is skipped due to FAST_LOAD - Info: When table creation begins - Debug: After schema creation completes \"\"\" if current_app . config . get ( \"FAST_LOAD\" , False ) is True : logger . warning ( \"Skipping table creation for FAST_LOAD=True.\" ) return logger . info ( \"Creating all missing tables.\" ) db . create_all () logger . debug ( \"Database schema created.\" ) db_initialize () Import and register SQLAlchemy ORM models. This ensures model classes are registered before calling db.create_all() . Notes Import must be executed (even if unused) to register models. Example import arb.portal.sqla_models as models Source code in arb\\portal\\startup\\db.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def db_initialize () -> None : \"\"\" Import and register SQLAlchemy ORM models. This ensures model classes are registered before calling `db.create_all()`. Notes: - Import must be executed (even if unused) to register models. Example: import arb.portal.sqla_models as models \"\"\" logger . info ( \"Initializing database models.\" ) # Add model registration below # noinspection PyUnresolvedReferences import arb.portal.sqla_models as models db_initialize_and_create () Register models and create missing tables in one call. Combines db_initialize() and db_create() for convenience. Returns: None \u2013 None Logs Info: Upon successful database initialization Source code in arb\\portal\\startup\\db.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def db_initialize_and_create () -> None : \"\"\" Register models and create missing tables in one call. Combines `db_initialize()` and `db_create()` for convenience. Returns: None Logs: - Info: Upon successful database initialization \"\"\" db_initialize () db_create () logger . info ( \"Database initialized and tables ensured.\" ) reflect_database () Reflect the existing database into SQLAlchemy metadata. This enables access to existing tables even without defined ORM models. Returns: None \u2013 None Logs Info: Start of reflection Debug: Completion of reflection Source code in arb\\portal\\startup\\db.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def reflect_database () -> None : \"\"\" Reflect the existing database into SQLAlchemy metadata. This enables access to existing tables even without defined ORM models. Returns: None Logs: - Info: Start of reflection - Debug: Completion of reflection \"\"\" logger . info ( \"Reflecting database metadata.\" ) db . metadata . reflect ( bind = db . engine ) logger . debug ( \"Reflection complete.\" )","title":"arb.portal.startup.db"},{"location":"reference/arb/portal/startup/db/#arbportalstartupdb","text":"Database initialization and reflection routines for the ARB Feedback Portal. These functions are intended to be called during Flask app startup (from create_app() ) to configure SQLAlchemy metadata, initialize models, and create missing tables. Usage from startup.db import reflect_database, db_initialize_and_create Notes SQLAlchemy models must be explicitly imported to register before table creation. Logging is enabled throughout to trace database state and startup flow.","title":"arb.portal.startup.db"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.db_create","text":"Create all tables defined in SQLAlchemy metadata if they don\u2019t exist. Skips creation if FAST_LOAD=True is set in the app config. Returns: None \u2013 None Logs Warn: If creation is skipped due to FAST_LOAD Info: When table creation begins Debug: After schema creation completes Source code in arb\\portal\\startup\\db.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def db_create () -> None : \"\"\" Create all tables defined in SQLAlchemy metadata if they don\u2019t exist. Skips creation if `FAST_LOAD=True` is set in the app config. Returns: None Logs: - Warn: If creation is skipped due to FAST_LOAD - Info: When table creation begins - Debug: After schema creation completes \"\"\" if current_app . config . get ( \"FAST_LOAD\" , False ) is True : logger . warning ( \"Skipping table creation for FAST_LOAD=True.\" ) return logger . info ( \"Creating all missing tables.\" ) db . create_all () logger . debug ( \"Database schema created.\" )","title":"db_create"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.db_initialize","text":"Import and register SQLAlchemy ORM models. This ensures model classes are registered before calling db.create_all() . Notes Import must be executed (even if unused) to register models. Example import arb.portal.sqla_models as models Source code in arb\\portal\\startup\\db.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def db_initialize () -> None : \"\"\" Import and register SQLAlchemy ORM models. This ensures model classes are registered before calling `db.create_all()`. Notes: - Import must be executed (even if unused) to register models. Example: import arb.portal.sqla_models as models \"\"\" logger . info ( \"Initializing database models.\" ) # Add model registration below # noinspection PyUnresolvedReferences import arb.portal.sqla_models as models","title":"db_initialize"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.db_initialize_and_create","text":"Register models and create missing tables in one call. Combines db_initialize() and db_create() for convenience. Returns: None \u2013 None Logs Info: Upon successful database initialization Source code in arb\\portal\\startup\\db.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def db_initialize_and_create () -> None : \"\"\" Register models and create missing tables in one call. Combines `db_initialize()` and `db_create()` for convenience. Returns: None Logs: - Info: Upon successful database initialization \"\"\" db_initialize () db_create () logger . info ( \"Database initialized and tables ensured.\" )","title":"db_initialize_and_create"},{"location":"reference/arb/portal/startup/db/#arb.portal.startup.db.reflect_database","text":"Reflect the existing database into SQLAlchemy metadata. This enables access to existing tables even without defined ORM models. Returns: None \u2013 None Logs Info: Start of reflection Debug: Completion of reflection Source code in arb\\portal\\startup\\db.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def reflect_database () -> None : \"\"\" Reflect the existing database into SQLAlchemy metadata. This enables access to existing tables even without defined ORM models. Returns: None Logs: - Info: Start of reflection - Debug: Completion of reflection \"\"\" logger . info ( \"Reflecting database metadata.\" ) db . metadata . reflect ( bind = db . engine ) logger . debug ( \"Reflection complete.\" )","title":"reflect_database"},{"location":"reference/arb/portal/startup/flask/","text":"arb.portal.startup.flask Flask-specific application setup utilities for the ARB Feedback Portal. This module configures Flask app behavior, including: - Jinja2 environment customization - Upload limits and paths - Flask logger settings - Custom template filters and globals Should be invoked during application factory setup: Example from startup.flask import configure_flask_app app = Flask( name ) configure_flask_app(app) configure_flask_app ( app ) Apply global configuration to the Flask app instance. Parameters: app ( Flask ) \u2013 The Flask application to configure. Configures Jinja2 environment: Enables strict mode for undefined variables Trims and left-strips whitespace blocks Registers custom filters and timezone globals Upload settings: Sets UPLOAD_FOLDER to the shared upload path Limits MAX_CONTENT_LENGTH to 16MB Logger: Applies LOG_LEVEL from app config Disables Werkzeug color log markup Source code in arb\\portal\\startup\\flask.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def configure_flask_app ( app : Flask ) -> None : \"\"\" Apply global configuration to the Flask app instance. Args: app (Flask): The Flask application to configure. Configures: - Jinja2 environment: * Enables strict mode for undefined variables * Trims and left-strips whitespace blocks * Registers custom filters and timezone globals - Upload settings: * Sets `UPLOAD_FOLDER` to the shared upload path * Limits `MAX_CONTENT_LENGTH` to 16MB - Logger: * Applies `LOG_LEVEL` from app config * Disables Werkzeug color log markup \"\"\" logger . debug ( \"configure_flask_app() called\" ) app . jinja_env . globals [ \"app_name\" ] = \"CARB Feedback Portal\" # ------------------------------------------------------------------------- # Logging Configuration # ------------------------------------------------------------------------- logger . setLevel ( app . config . get ( \"LOG_LEVEL\" , \"INFO\" )) # Logging: Turn off color coding (avoids special terminal characters in log file) werkzeug . serving . _log_add_style = False # ------------------------------------------------------------------------- # Upload Configuration # ------------------------------------------------------------------------- app . config [ 'UPLOAD_FOLDER' ] = UPLOAD_PATH app . config [ 'MAX_CONTENT_LENGTH' ] = 16 * 1024 * 1024 # 16MB max upload # ------------------------------------------------------------------------- # Jinja Configuration # ------------------------------------------------------------------------- app . jinja_env . undefined = StrictUndefined # Jinja: Trim whitespace before/after {{ }} text injection app . jinja_env . trim_blocks = True app . jinja_env . lstrip_blocks = True # Jinja: custom filters for debugging and string manipulation app . jinja_env . filters [ 'debug' ] = diag_recursive # todo - make sure these datetime filters work in light of the use of native and UTC timestamps app . jinja_env . filters [ 'date_to_string' ] = date_to_string app . jinja_env . filters [ 'repr_datetime_to_string' ] = repr_datetime_to_string app . jinja_env . filters [ 'args_to_string' ] = args_to_string # Jinja: expose Python ZoneInfo class to templates for local time conversion app . jinja_env . globals [ \"california_tz\" ] = ZoneInfo ( \"America/Los_Angeles\" ) logger . debug ( \"Flask Jinja2 globals and logging initialized.\" )","title":"arb.portal.startup.flask"},{"location":"reference/arb/portal/startup/flask/#arbportalstartupflask","text":"Flask-specific application setup utilities for the ARB Feedback Portal. This module configures Flask app behavior, including: - Jinja2 environment customization - Upload limits and paths - Flask logger settings - Custom template filters and globals Should be invoked during application factory setup: Example from startup.flask import configure_flask_app app = Flask( name ) configure_flask_app(app)","title":"arb.portal.startup.flask"},{"location":"reference/arb/portal/startup/flask/#arb.portal.startup.flask.configure_flask_app","text":"Apply global configuration to the Flask app instance. Parameters: app ( Flask ) \u2013 The Flask application to configure. Configures Jinja2 environment: Enables strict mode for undefined variables Trims and left-strips whitespace blocks Registers custom filters and timezone globals Upload settings: Sets UPLOAD_FOLDER to the shared upload path Limits MAX_CONTENT_LENGTH to 16MB Logger: Applies LOG_LEVEL from app config Disables Werkzeug color log markup Source code in arb\\portal\\startup\\flask.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def configure_flask_app ( app : Flask ) -> None : \"\"\" Apply global configuration to the Flask app instance. Args: app (Flask): The Flask application to configure. Configures: - Jinja2 environment: * Enables strict mode for undefined variables * Trims and left-strips whitespace blocks * Registers custom filters and timezone globals - Upload settings: * Sets `UPLOAD_FOLDER` to the shared upload path * Limits `MAX_CONTENT_LENGTH` to 16MB - Logger: * Applies `LOG_LEVEL` from app config * Disables Werkzeug color log markup \"\"\" logger . debug ( \"configure_flask_app() called\" ) app . jinja_env . globals [ \"app_name\" ] = \"CARB Feedback Portal\" # ------------------------------------------------------------------------- # Logging Configuration # ------------------------------------------------------------------------- logger . setLevel ( app . config . get ( \"LOG_LEVEL\" , \"INFO\" )) # Logging: Turn off color coding (avoids special terminal characters in log file) werkzeug . serving . _log_add_style = False # ------------------------------------------------------------------------- # Upload Configuration # ------------------------------------------------------------------------- app . config [ 'UPLOAD_FOLDER' ] = UPLOAD_PATH app . config [ 'MAX_CONTENT_LENGTH' ] = 16 * 1024 * 1024 # 16MB max upload # ------------------------------------------------------------------------- # Jinja Configuration # ------------------------------------------------------------------------- app . jinja_env . undefined = StrictUndefined # Jinja: Trim whitespace before/after {{ }} text injection app . jinja_env . trim_blocks = True app . jinja_env . lstrip_blocks = True # Jinja: custom filters for debugging and string manipulation app . jinja_env . filters [ 'debug' ] = diag_recursive # todo - make sure these datetime filters work in light of the use of native and UTC timestamps app . jinja_env . filters [ 'date_to_string' ] = date_to_string app . jinja_env . filters [ 'repr_datetime_to_string' ] = repr_datetime_to_string app . jinja_env . filters [ 'args_to_string' ] = args_to_string # Jinja: expose Python ZoneInfo class to templates for local time conversion app . jinja_env . globals [ \"california_tz\" ] = ZoneInfo ( \"America/Los_Angeles\" ) logger . debug ( \"Flask Jinja2 globals and logging initialized.\" )","title":"configure_flask_app"},{"location":"reference/arb/portal/startup/runtime_info/","text":"arb.portal.startup.runtime_info Provides runtime metadata and dynamic paths for the application. This module defines Project root and key directories (uploads, logs, static) Operating system detection (Windows, Linux, macOS) Platform-level info useful for conditional behavior Diagnostic tools for runtime environment inspection Example from startup.runtime_info import ( PROJECT_ROOT, UPLOAD_PATH, LOG_DIR, IS_WINDOWS, IS_LINUX, IS_MAC, print_runtime_diagnostics ) Notes The project root directory is assumed to be named \"feedback_portal\". If the app is run from: feedback_portal/source/production/arb/wsgi.py then directory resolution is: Path( file ).resolve().parents[0] \u2192 .../arb Path( file ).resolve().parents[1] \u2192 .../production Path( file ).resolve().parents[2] \u2192 .../source Path( file ).resolve().parents[3] \u2192 .../feedback_portal print_runtime_diagnostics () Print and log detected runtime paths and platform flags for debugging. Outputs Platform name and OS flags Resolved project root path Paths for uploads, logs, and static assets Returns: None \u2013 None Source code in arb\\portal\\startup\\runtime_info.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def print_runtime_diagnostics () -> None : \"\"\" Print and log detected runtime paths and platform flags for debugging. Outputs: - Platform name and OS flags - Resolved project root path - Paths for uploads, logs, and static assets Returns: None \"\"\" logger . info ( f \" { 'PLATFORM' : <20 } = { PLATFORM } \" ) logger . info ( f \" { 'IS_WINDOWS' : <20 } = { IS_WINDOWS } \" ) logger . info ( f \" { 'IS_LINUX' : <20 } = { IS_LINUX } \" ) logger . info ( f \" { 'IS_MAC' : <20 } = { IS_MAC } \" ) logger . info ( f \" { 'PROJECT_ROOT' : <20 } = { PROJECT_ROOT } \" ) logger . info ( f \" { 'UPLOAD_PATH' : <20 } = { UPLOAD_PATH } \" ) logger . info ( f \" { 'LOG_DIR' : <20 } = { LOG_DIR } \" ) logger . info ( f \" { 'STATIC_DIR' : <20 } = { STATIC_DIR } \" )","title":"arb.portal.startup.runtime_info"},{"location":"reference/arb/portal/startup/runtime_info/#arbportalstartupruntime_info","text":"Provides runtime metadata and dynamic paths for the application. This module defines Project root and key directories (uploads, logs, static) Operating system detection (Windows, Linux, macOS) Platform-level info useful for conditional behavior Diagnostic tools for runtime environment inspection Example from startup.runtime_info import ( PROJECT_ROOT, UPLOAD_PATH, LOG_DIR, IS_WINDOWS, IS_LINUX, IS_MAC, print_runtime_diagnostics ) Notes The project root directory is assumed to be named \"feedback_portal\". If the app is run from: feedback_portal/source/production/arb/wsgi.py then directory resolution is: Path( file ).resolve().parents[0] \u2192 .../arb Path( file ).resolve().parents[1] \u2192 .../production Path( file ).resolve().parents[2] \u2192 .../source Path( file ).resolve().parents[3] \u2192 .../feedback_portal","title":"arb.portal.startup.runtime_info"},{"location":"reference/arb/portal/startup/runtime_info/#arb.portal.startup.runtime_info.print_runtime_diagnostics","text":"Print and log detected runtime paths and platform flags for debugging. Outputs Platform name and OS flags Resolved project root path Paths for uploads, logs, and static assets Returns: None \u2013 None Source code in arb\\portal\\startup\\runtime_info.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def print_runtime_diagnostics () -> None : \"\"\" Print and log detected runtime paths and platform flags for debugging. Outputs: - Platform name and OS flags - Resolved project root path - Paths for uploads, logs, and static assets Returns: None \"\"\" logger . info ( f \" { 'PLATFORM' : <20 } = { PLATFORM } \" ) logger . info ( f \" { 'IS_WINDOWS' : <20 } = { IS_WINDOWS } \" ) logger . info ( f \" { 'IS_LINUX' : <20 } = { IS_LINUX } \" ) logger . info ( f \" { 'IS_MAC' : <20 } = { IS_MAC } \" ) logger . info ( f \" { 'PROJECT_ROOT' : <20 } = { PROJECT_ROOT } \" ) logger . info ( f \" { 'UPLOAD_PATH' : <20 } = { UPLOAD_PATH } \" ) logger . info ( f \" { 'LOG_DIR' : <20 } = { LOG_DIR } \" ) logger . info ( f \" { 'STATIC_DIR' : <20 } = { STATIC_DIR } \" )","title":"print_runtime_diagnostics"},{"location":"reference/arb/utils/constants/","text":"arb.utils.constants Shared constants for general utility modules. These constants are designed to remain immutable and serve as application-wide placeholders or configuration defaults. PLEASE_SELECT = 'Please Select' module-attribute str: Placeholder value used in dropdown selectors to indicate a required user selection.","title":"arb.utils.constants"},{"location":"reference/arb/utils/constants/#arbutilsconstants","text":"Shared constants for general utility modules. These constants are designed to remain immutable and serve as application-wide placeholders or configuration defaults.","title":"arb.utils.constants"},{"location":"reference/arb/utils/constants/#arb.utils.constants.PLEASE_SELECT","text":"str: Placeholder value used in dropdown selectors to indicate a required user selection.","title":"PLEASE_SELECT"},{"location":"reference/arb/utils/database/","text":"arb.utils.database Miscellaneous database utilities. Includes helpers for dropping tables, executing SQL scripts, auto-reflecting base metadata, and bulk cleansing of JSON fields across database rows. Intended for use in migrations, diagnostics, and administrative scripts. Requires SQLAlchemy (for model and metadata operations) A db object and an automapped or declarative base from the Flask app Functions: Name Description - db_drop_all Drop all database tables - execute_sql_script Run external SQL script files - get_reflected_base Return a SQLAlchemy automap base - cleanse_misc_json Strip \"Please Select\" values from misc_json fields cleanse_misc_json ( db , base , table_name , json_column_name = 'misc_json' , remove_value = 'Please Select' , dry_run = False ) Remove key/value pairs in a JSON column where value == remove_value . Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance. base ( AutomapBase ) \u2013 Declarative or automap base. table_name ( str ) \u2013 Table name to target (e.g., 'incidences'). json_column_name ( str , default: 'misc_json' ) \u2013 Column name to scan (default: \"misc_json\"). remove_value ( str , default: 'Please Select' ) \u2013 Value to match for deletion (default: \"Please Select\"). dry_run ( bool , default: False ) \u2013 If True, logs changes but rolls back. Raises: ValueError \u2013 If table or column cannot be found or mapped. RuntimeError \u2013 On failure to commit or query. Source code in arb\\utils\\database.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def cleanse_misc_json ( db : SQLAlchemy , base : AutomapBase , table_name : str , json_column_name : str = \"misc_json\" , remove_value : str = \"Please Select\" , dry_run : bool = False ) -> None : \"\"\" Remove key/value pairs in a JSON column where value == `remove_value`. Args: db (SQLAlchemy): SQLAlchemy instance. base (AutomapBase): Declarative or automap base. table_name (str): Table name to target (e.g., 'incidences'). json_column_name (str): Column name to scan (default: \"misc_json\"). remove_value (str): Value to match for deletion (default: \"Please Select\"). dry_run (bool): If True, logs changes but rolls back. Raises: ValueError: If table or column cannot be found or mapped. RuntimeError: On failure to commit or query. \"\"\" from arb.utils.sql_alchemy import get_class_from_table_name model_cls = get_class_from_table_name ( base , table_name ) if model_cls is None : raise ValueError ( f \"Table ' { table_name } ' not found or not mapped.\" ) if not hasattr ( model_cls , json_column_name ): raise ValueError ( f \"Column ' { json_column_name } ' not found on model for table ' { table_name } '.\" ) try : rows = db . session . query ( model_cls ) . all () count_total = len ( rows ) count_modified = 0 for row in rows : json_data = getattr ( row , json_column_name ) or {} if not isinstance ( json_data , dict ): continue filtered = { k : v for k , v in json_data . items () if v != remove_value } if filtered != json_data : setattr ( row , json_column_name , filtered ) from sqlalchemy.orm.attributes import flag_modified flag_modified ( row , json_column_name ) count_modified += 1 if dry_run : logger . info ( f \"[Dry Run] { count_modified } of { count_total } rows would be modified.\" ) db . session . rollback () else : db . session . commit () logger . info ( f \"[Committed] { count_modified } of { count_total } rows modified.\" ) except Exception as e : db . session . rollback () raise RuntimeError ( f \"Error during cleansing: { e } \" ) db_drop_all ( flask_app , db ) Drop all database tables from the configured database. Parameters: flask_app ( Flask ) \u2013 The Flask application object. db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. Warning This is irreversible \u2014 all tables will be deleted. Source code in arb\\utils\\database.py 34 35 36 37 38 39 40 41 42 43 44 45 46 def db_drop_all ( flask_app : Flask , db : SQLAlchemy ) -> None : \"\"\" Drop all database tables from the configured database. Args: flask_app (Flask): The Flask application object. db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. Warning: This is irreversible \u2014 all tables will be deleted. \"\"\" logger . debug ( \"dropping all database tables\" ) execute_sql_script ( script_path = None , connection = None ) Execute a SQL script using a provided or default SQLite connection. Parameters: script_path ( str | Path | None , default: None ) \u2013 Path to the .sql script. Defaults to ../sql_scripts/script_01.sql . connection ( Connection | None , default: None ) \u2013 SQLite connection (defaults to sqlite3.connect('app.db') if None). Source code in arb\\utils\\database.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def execute_sql_script ( script_path : str | Path = None , connection : sqlite3 . Connection | None = None ) -> None : \"\"\" Execute a SQL script using a provided or default SQLite connection. Args: script_path (str | Path | None): Path to the `.sql` script. Defaults to `../sql_scripts/script_01.sql`. connection (sqlite3.Connection | None): SQLite connection (defaults to `sqlite3.connect('app.db')` if None). \"\"\" logger . debug ( f \"execute_sql_script() called with { script_path =} , { connection =} \" ) if script_path is None : script_path = '../sql_scripts/script_01.sql' if connection is None : connection = sqlite3 . connect ( 'app.db' ) with open ( script_path ) as f : connection . executescript ( f . read ()) connection . commit () connection . close () get_reflected_base ( db ) Return a SQLAlchemy automap base using the existing metadata (no re-reflection). Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance with metadata. Returns: AutomapBase ( AutomapBase ) \u2013 Reflected base class. Source code in arb\\utils\\database.py 76 77 78 79 80 81 82 83 84 85 86 87 88 def get_reflected_base ( db : SQLAlchemy ) -> AutomapBase : \"\"\" Return a SQLAlchemy automap base using the existing metadata (no re-reflection). Args: db (SQLAlchemy): SQLAlchemy instance with metadata. Returns: AutomapBase: Reflected base class. \"\"\" Base = automap_base ( metadata = db . metadata ) # reuse metadata! Base . prepare ( db . engine , reflect = False ) # no extra reflection return Base","title":"arb.utils.database"},{"location":"reference/arb/utils/database/#arbutilsdatabase","text":"Miscellaneous database utilities. Includes helpers for dropping tables, executing SQL scripts, auto-reflecting base metadata, and bulk cleansing of JSON fields across database rows. Intended for use in migrations, diagnostics, and administrative scripts. Requires SQLAlchemy (for model and metadata operations) A db object and an automapped or declarative base from the Flask app Functions: Name Description - db_drop_all Drop all database tables - execute_sql_script Run external SQL script files - get_reflected_base Return a SQLAlchemy automap base - cleanse_misc_json Strip \"Please Select\" values from misc_json fields","title":"arb.utils.database"},{"location":"reference/arb/utils/database/#arb.utils.database.cleanse_misc_json","text":"Remove key/value pairs in a JSON column where value == remove_value . Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance. base ( AutomapBase ) \u2013 Declarative or automap base. table_name ( str ) \u2013 Table name to target (e.g., 'incidences'). json_column_name ( str , default: 'misc_json' ) \u2013 Column name to scan (default: \"misc_json\"). remove_value ( str , default: 'Please Select' ) \u2013 Value to match for deletion (default: \"Please Select\"). dry_run ( bool , default: False ) \u2013 If True, logs changes but rolls back. Raises: ValueError \u2013 If table or column cannot be found or mapped. RuntimeError \u2013 On failure to commit or query. Source code in arb\\utils\\database.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def cleanse_misc_json ( db : SQLAlchemy , base : AutomapBase , table_name : str , json_column_name : str = \"misc_json\" , remove_value : str = \"Please Select\" , dry_run : bool = False ) -> None : \"\"\" Remove key/value pairs in a JSON column where value == `remove_value`. Args: db (SQLAlchemy): SQLAlchemy instance. base (AutomapBase): Declarative or automap base. table_name (str): Table name to target (e.g., 'incidences'). json_column_name (str): Column name to scan (default: \"misc_json\"). remove_value (str): Value to match for deletion (default: \"Please Select\"). dry_run (bool): If True, logs changes but rolls back. Raises: ValueError: If table or column cannot be found or mapped. RuntimeError: On failure to commit or query. \"\"\" from arb.utils.sql_alchemy import get_class_from_table_name model_cls = get_class_from_table_name ( base , table_name ) if model_cls is None : raise ValueError ( f \"Table ' { table_name } ' not found or not mapped.\" ) if not hasattr ( model_cls , json_column_name ): raise ValueError ( f \"Column ' { json_column_name } ' not found on model for table ' { table_name } '.\" ) try : rows = db . session . query ( model_cls ) . all () count_total = len ( rows ) count_modified = 0 for row in rows : json_data = getattr ( row , json_column_name ) or {} if not isinstance ( json_data , dict ): continue filtered = { k : v for k , v in json_data . items () if v != remove_value } if filtered != json_data : setattr ( row , json_column_name , filtered ) from sqlalchemy.orm.attributes import flag_modified flag_modified ( row , json_column_name ) count_modified += 1 if dry_run : logger . info ( f \"[Dry Run] { count_modified } of { count_total } rows would be modified.\" ) db . session . rollback () else : db . session . commit () logger . info ( f \"[Committed] { count_modified } of { count_total } rows modified.\" ) except Exception as e : db . session . rollback () raise RuntimeError ( f \"Error during cleansing: { e } \" )","title":"cleanse_misc_json"},{"location":"reference/arb/utils/database/#arb.utils.database.db_drop_all","text":"Drop all database tables from the configured database. Parameters: flask_app ( Flask ) \u2013 The Flask application object. db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. Warning This is irreversible \u2014 all tables will be deleted. Source code in arb\\utils\\database.py 34 35 36 37 38 39 40 41 42 43 44 45 46 def db_drop_all ( flask_app : Flask , db : SQLAlchemy ) -> None : \"\"\" Drop all database tables from the configured database. Args: flask_app (Flask): The Flask application object. db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. Warning: This is irreversible \u2014 all tables will be deleted. \"\"\" logger . debug ( \"dropping all database tables\" )","title":"db_drop_all"},{"location":"reference/arb/utils/database/#arb.utils.database.execute_sql_script","text":"Execute a SQL script using a provided or default SQLite connection. Parameters: script_path ( str | Path | None , default: None ) \u2013 Path to the .sql script. Defaults to ../sql_scripts/script_01.sql . connection ( Connection | None , default: None ) \u2013 SQLite connection (defaults to sqlite3.connect('app.db') if None). Source code in arb\\utils\\database.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def execute_sql_script ( script_path : str | Path = None , connection : sqlite3 . Connection | None = None ) -> None : \"\"\" Execute a SQL script using a provided or default SQLite connection. Args: script_path (str | Path | None): Path to the `.sql` script. Defaults to `../sql_scripts/script_01.sql`. connection (sqlite3.Connection | None): SQLite connection (defaults to `sqlite3.connect('app.db')` if None). \"\"\" logger . debug ( f \"execute_sql_script() called with { script_path =} , { connection =} \" ) if script_path is None : script_path = '../sql_scripts/script_01.sql' if connection is None : connection = sqlite3 . connect ( 'app.db' ) with open ( script_path ) as f : connection . executescript ( f . read ()) connection . commit () connection . close ()","title":"execute_sql_script"},{"location":"reference/arb/utils/database/#arb.utils.database.get_reflected_base","text":"Return a SQLAlchemy automap base using the existing metadata (no re-reflection). Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance with metadata. Returns: AutomapBase ( AutomapBase ) \u2013 Reflected base class. Source code in arb\\utils\\database.py 76 77 78 79 80 81 82 83 84 85 86 87 88 def get_reflected_base ( db : SQLAlchemy ) -> AutomapBase : \"\"\" Return a SQLAlchemy automap base using the existing metadata (no re-reflection). Args: db (SQLAlchemy): SQLAlchemy instance with metadata. Returns: AutomapBase: Reflected base class. \"\"\" Base = automap_base ( metadata = db . metadata ) # reuse metadata! Base . prepare ( db . engine , reflect = False ) # no extra reflection return Base","title":"get_reflected_base"},{"location":"reference/arb/utils/date_and_time/","text":"arb.utils.date_and_time Datetime parsing and timezone utilities for ISO 8601, UTC/Pacific conversion, repr-format recovery, and recursive datetime transformation in nested structures. Features: - ISO 8601 validation and parsing (via dateutil ) - Conversion between UTC and naive Pacific time (Los Angeles) - Safe handling of repr-formatted datetime strings (e.g., \"datetime.datetime(...)\") - Recursive datetime transformations within nested dicts/lists/sets/tuples Timezone policy: - UTC_TZ and PACIFIC_TZ are globally defined using zoneinfo.ZoneInfo - Naive timestamps are only assumed to be UTC if explicitly configured via arguments ca_naive_to_utc_datetime ( dt ) Convert a naive Pacific Time datetime to a UTC-aware datetime. Parameters: dt ( datetime ) \u2013 A naive datetime. Returns: datetime ( datetime ) \u2013 A UTC-aware datetime. Raises: ValueError \u2013 If the input is not naive (i.e., has timezone info). Source code in arb\\utils\\date_and_time.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def ca_naive_to_utc_datetime ( dt : datetime ) -> datetime : \"\"\" Convert a naive Pacific Time datetime to a UTC-aware datetime. Args: dt (datetime): A naive datetime. Returns: datetime: A UTC-aware datetime. Raises: ValueError: If the input is not naive (i.e., has timezone info). \"\"\" if dt . tzinfo is not None : raise ValueError ( f \"Expected naive datetime, got { dt !r} \" ) return dt . replace ( tzinfo = PACIFIC_TZ ) . astimezone ( UTC_TZ ) convert_ca_naive_datetimes_to_utc ( data ) Recursively convert all naive Pacific Time datetimes in a nested structure to UTC-aware datetimes. Parameters: data ( object ) \u2013 A nested structure (e.g., dict, list, tuple). Returns: object ( object ) \u2013 The same structure with datetime values converted to UTC-aware format. Source code in arb\\utils\\date_and_time.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def convert_ca_naive_datetimes_to_utc ( data : object ) -> object : \"\"\" Recursively convert all naive Pacific Time datetimes in a nested structure to UTC-aware datetimes. Args: data (object): A nested structure (e.g., dict, list, tuple). Returns: object: The same structure with datetime values converted to UTC-aware format. \"\"\" if isinstance ( data , datetime ): return ca_naive_to_utc_datetime ( data ) elif isinstance ( data , Mapping ): return { convert_ca_naive_datetimes_to_utc ( k ): convert_ca_naive_datetimes_to_utc ( v ) for k , v in data . items ()} elif isinstance ( data , list ): return [ convert_ca_naive_datetimes_to_utc ( i ) for i in data ] elif isinstance ( data , tuple ): return tuple ( convert_ca_naive_datetimes_to_utc ( i ) for i in data ) elif isinstance ( data , set ): return { convert_ca_naive_datetimes_to_utc ( i ) for i in data } return data convert_datetimes_to_ca_naive ( data , assume_naive_is_utc = False , utc_strict = True ) Recursively convert all datetime objects in a nested structure to naive Pacific Time. Parameters: data ( object ) \u2013 A structure that may include datetime values (dict, list, etc.). assume_naive_is_utc ( bool , default: False ) \u2013 Whether to treat naive datetimes as UTC. utc_strict ( bool , default: True ) \u2013 Whether to enforce that input datetimes are explicitly UTC. Returns: object ( object ) \u2013 A structure of the same shape, with datetime values converted to naive Pacific. Examples: >>> from datetime import datetime >>> from zoneinfo import ZoneInfo >>> nested = { ... datetime ( 2025 , 4 , 23 , 15 , 0 , tzinfo = ZoneInfo ( \"UTC\" )): [ ... { \"created\" : datetime ( 2025 , 4 , 23 , 18 , 0 )}, ... ( datetime ( 2025 , 4 , 23 , 20 , 0 , tzinfo = ZoneInfo ( \"UTC\" )),) ... ] >>> convert_datetimes_to_ca_naive ( nested ) { datetime.datetime(2025, 4, 23, 8, 0): [ {\"created\": datetime.datetime(2025, 4, 23, 11, 0)}, (datetime.datetime(2025, 4, 23, 13, 0),) ] } Source code in arb\\utils\\date_and_time.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def convert_datetimes_to_ca_naive ( data : object , assume_naive_is_utc : bool = False , utc_strict : bool = True ) -> object : \"\"\" Recursively convert all datetime objects in a nested structure to naive Pacific Time. Args: data (object): A structure that may include datetime values (dict, list, etc.). assume_naive_is_utc (bool): Whether to treat naive datetimes as UTC. utc_strict (bool): Whether to enforce that input datetimes are explicitly UTC. Returns: object: A structure of the same shape, with datetime values converted to naive Pacific. Examples: >>> from datetime import datetime >>> from zoneinfo import ZoneInfo >>> nested = { ... datetime(2025, 4, 23, 15, 0, tzinfo=ZoneInfo(\"UTC\")): [ ... {\"created\": datetime(2025, 4, 23, 18, 0)}, ... (datetime(2025, 4, 23, 20, 0, tzinfo=ZoneInfo(\"UTC\")),) ... ] >>> convert_datetimes_to_ca_naive(nested) { datetime.datetime(2025, 4, 23, 8, 0): [ {\"created\": datetime.datetime(2025, 4, 23, 11, 0)}, (datetime.datetime(2025, 4, 23, 13, 0),) ] } \"\"\" if isinstance ( data , datetime ): return datetime_to_ca_naive ( data , assume_naive_is_utc , utc_strict ) elif isinstance ( data , Mapping ): return { convert_datetimes_to_ca_naive ( k , assume_naive_is_utc , utc_strict ): convert_datetimes_to_ca_naive ( v , assume_naive_is_utc , utc_strict ) for k , v in data . items () } elif isinstance ( data , list ): return [ convert_datetimes_to_ca_naive ( i , assume_naive_is_utc , utc_strict ) for i in data ] elif isinstance ( data , tuple ): return tuple ( convert_datetimes_to_ca_naive ( i , assume_naive_is_utc , utc_strict ) for i in data ) elif isinstance ( data , set ): return { convert_datetimes_to_ca_naive ( i , assume_naive_is_utc , utc_strict ) for i in data } return data date_to_string ( dt , str_format = '%Y-%m- %d T%H:%M' ) Convert a datetime object to a formatted string. Parameters: dt ( datetime | None ) \u2013 A datetime object or None. str_format ( str , default: '%Y-%m-%dT%H:%M' ) \u2013 Format string used with strftime . Returns: str | None \u2013 str | None: The formatted string if input is not None; otherwise, None. Raises: TypeError \u2013 If x is not a datetime object. Source code in arb\\utils\\date_and_time.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def date_to_string ( dt : datetime | None , str_format : str = \"%Y-%m- %d T%H:%M\" ) -> str | None : \"\"\" Convert a datetime object to a formatted string. Args: dt (datetime | None): A datetime object or None. str_format (str): Format string used with `strftime`. Returns: str | None: The formatted string if input is not None; otherwise, None. Raises: TypeError: If `x` is not a datetime object. \"\"\" if dt is None : return None if not isinstance ( dt , datetime ): raise TypeError ( f 'x must be datetime.datetime or None. { dt =} ' ) return dt . strftime ( str_format ) datetime_to_ca_naive ( dt , assume_naive_is_utc = False , utc_strict = True ) Convert a datetime (UTC or naive) to naive Pacific Time. Parameters: dt ( datetime ) \u2013 The datetime to convert. assume_naive_is_utc ( bool , default: False ) \u2013 If True, interpret naive datetime as UTC. utc_strict ( bool , default: True ) \u2013 If True, raise an error unless datetime is explicitly UTC. Returns: datetime ( datetime ) \u2013 A naive Pacific Time datetime. Raises: ValueError \u2013 If datetime is naive and assume_naive_is_utc is False, or if UTC check fails under utc_strict . Source code in arb\\utils\\date_and_time.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def datetime_to_ca_naive ( dt : datetime , assume_naive_is_utc : bool = False , utc_strict : bool = True ) -> datetime : \"\"\" Convert a datetime (UTC or naive) to naive Pacific Time. Args: dt (datetime): The datetime to convert. assume_naive_is_utc (bool): If True, interpret naive datetime as UTC. utc_strict (bool): If True, raise an error unless datetime is explicitly UTC. Returns: datetime: A naive Pacific Time datetime. Raises: ValueError: If datetime is naive and `assume_naive_is_utc` is False, or if UTC check fails under `utc_strict`. \"\"\" if dt . tzinfo is None : if assume_naive_is_utc : logger . warning ( \"Assuming UTC for naive datetime.\" ) dt = dt . replace ( tzinfo = UTC_TZ ) else : raise ValueError ( \"Naive datetime provided without assume_naive_is_utc=True\" ) elif utc_strict and dt . tzinfo != UTC_TZ : raise ValueError ( \"datetime must be UTC when utc_strict=True\" ) return dt . astimezone ( PACIFIC_TZ ) . replace ( tzinfo = None ) is_datetime_naive ( dt ) Check whether a datetime object is naive (lacks timezone info). Parameters: dt ( datetime ) \u2013 A datetime instance. Returns: bool ( bool ) \u2013 True if the datetime is naive (tzinfo is None or utcoffset is None); False otherwise. Examples: >>> is_datetime_naive ( datetime . now ()) True >>> from zoneinfo import ZoneInfo >>> is_datetime_naive ( datetime . now ( tz = ZoneInfo ( \"UTC\" ))) False Source code in arb\\utils\\date_and_time.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 def is_datetime_naive ( dt : datetime ) -> bool : \"\"\" Check whether a datetime object is naive (lacks timezone info). Args: dt (datetime): A datetime instance. Returns: bool: True if the datetime is naive (tzinfo is None or utcoffset is None); False otherwise. Examples: >>> is_datetime_naive(datetime.now()) True >>> from zoneinfo import ZoneInfo >>> is_datetime_naive(datetime.now(tz=ZoneInfo(\"UTC\"))) False \"\"\" return dt . tzinfo is None or dt . tzinfo . utcoffset ( dt ) is None is_iso8601 ( string ) Determine whether a string is a valid ISO 8601 datetime. Parameters: string ( str ) \u2013 The string to validate. Returns: bool ( bool ) \u2013 True if the string conforms to ISO 8601 datetime format, False otherwise. Notes Uses dateutil.parser.isoparse for strict ISO 8601 parsing. ISO 8601 requires a full date (YYYY-MM-DD) at minimum. Time-only strings are invalid. This does not validate durations, ordinal dates, or week dates. Examples: Valid cases (returns True): - \"2024-12-31\" - \"2024-12-31T23:59:59\" - \"2024-12-31T23:59:59Z\" # UTC/Zulu time - \"2024-12-31T23:59:59+00:00\" # Timezone offset - \"2024-12-31T23:59:59.123456\" # Microsecond precision - \"2024-12-31T23:59:59.123+02:00\" # Fractional seconds with TZ Invalid cases (returns False): - \"12/31/2024\" # US-style date format - \"2024-31-12\" # Invalid format (day and month swapped) - \"2024\" # Year only, not a complete date or datetime - \"P1Y2M\" # ISO 8601 duration, not a datetime - \"Just some text\" # Clearly not a date - \"2024-12-31 23:59:59\" # Space instead of 'T' (technically invalid ISO 8601) # Time-only values \u2014 invalid because ISO 8601 requires at least a full date - \"23:59\" # Just time (no date) - \"23:59:59\" # Time only, still invalid - \"T23:59:59Z\" # Prefixed with 'T' but still no date - \"23:59:59+00:00\" # Time with timezone, but missing a date - \"T23:59:59.123456\" # Microsecond time, still not valid without date Notes ISO 8601 datetimes require at least a full date (YYYY-MM-DD). Time-only strings are not valid by themselves. This does not validate ISO 8601 durations, ordinal dates, or week dates. This routine strictly uses dateutil.parser.isoparse , which rejects incomplete formats like time-only values. Source code in arb\\utils\\date_and_time.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def is_iso8601 ( string : str ) -> bool : \"\"\" Determine whether a string is a valid ISO 8601 datetime. Args: string (str): The string to validate. Returns: bool: True if the string conforms to ISO 8601 datetime format, False otherwise. Notes: - Uses `dateutil.parser.isoparse` for strict ISO 8601 parsing. - ISO 8601 requires a full date (YYYY-MM-DD) at minimum. Time-only strings are invalid. - This does not validate durations, ordinal dates, or week dates. Examples: Valid cases (returns True): - \"2024-12-31\" - \"2024-12-31T23:59:59\" - \"2024-12-31T23:59:59Z\" # UTC/Zulu time - \"2024-12-31T23:59:59+00:00\" # Timezone offset - \"2024-12-31T23:59:59.123456\" # Microsecond precision - \"2024-12-31T23:59:59.123+02:00\" # Fractional seconds with TZ Invalid cases (returns False): - \"12/31/2024\" # US-style date format - \"2024-31-12\" # Invalid format (day and month swapped) - \"2024\" # Year only, not a complete date or datetime - \"P1Y2M\" # ISO 8601 duration, not a datetime - \"Just some text\" # Clearly not a date - \"2024-12-31 23:59:59\" # Space instead of 'T' (technically invalid ISO 8601) # Time-only values \u2014 invalid because ISO 8601 requires at least a full date - \"23:59\" # Just time (no date) - \"23:59:59\" # Time only, still invalid - \"T23:59:59Z\" # Prefixed with 'T' but still no date - \"23:59:59+00:00\" # Time with timezone, but missing a date - \"T23:59:59.123456\" # Microsecond time, still not valid without date Notes: - ISO 8601 datetimes require at least a full date (YYYY-MM-DD). Time-only strings are not valid by themselves. - This does not validate ISO 8601 durations, ordinal dates, or week dates. - This routine strictly uses `dateutil.parser.isoparse`, which rejects incomplete formats like time-only values. \"\"\" try : parser . isoparse ( string ) return True except ValueError : return False iso8601_to_utc_dt ( iso_string , error_on_missing_tz = True ) Parse an ISO 8601 string and return a timezone-aware datetime object in UTC. This function uses dateutil.parser.isoparse() for robust ISO 8601 parsing, including support for variations like 'Z' (UTC shorthand) and missing separators. Parameters: iso_string ( str ) \u2013 An ISO 8601-formatted datetime string. error_on_missing_tz ( bool , default: True ) \u2013 If True, raises a ValueError when the string has no timezone info. Returns: datetime \u2013 A timezone-aware datetime object in UTC. Raises: ValueError \u2013 If the input string is invalid or lacks timezone info and error_on_missing_tz is True. Examples: >>> iso8601_to_utc_dt ( \"2025-04-20T14:30:00+00:00\" ) datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\")) >>> iso8601_to_utc_dt ( \"2025-04-20T14:30:00Z\" ) datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\")) >>> iso8601_to_utc_dt ( \"2025-04-20 14:30:00\" , error_on_missing_tz = False ) datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\")) >>> iso8601_to_utc_dt ( \"2025-04-20T14:30:00Z\" ) datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo('UTC')) Source code in arb\\utils\\date_and_time.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def iso8601_to_utc_dt ( iso_string : str , error_on_missing_tz : bool = True ) -> datetime : \"\"\" Parse an ISO 8601 string and return a timezone-aware datetime object in UTC. This function uses `dateutil.parser.isoparse()` for robust ISO 8601 parsing, including support for variations like 'Z' (UTC shorthand) and missing separators. Args: iso_string: An ISO 8601-formatted datetime string. error_on_missing_tz: If True, raises a ValueError when the string has no timezone info. Returns: A timezone-aware datetime object in UTC. Raises: ValueError: If the input string is invalid or lacks timezone info and `error_on_missing_tz` is True. Examples: >>> iso8601_to_utc_dt(\"2025-04-20T14:30:00+00:00\") datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\")) >>> iso8601_to_utc_dt(\"2025-04-20T14:30:00Z\") datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\")) >>> iso8601_to_utc_dt(\"2025-04-20 14:30:00\", error_on_missing_tz=False) datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\")) >>> iso8601_to_utc_dt(\"2025-04-20T14:30:00Z\") datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo('UTC')) \"\"\" try : dt = parser . isoparse ( iso_string ) except ( ValueError , TypeError ) as e : raise ValueError ( f \"Invalid ISO 8601 datetime string: ' { iso_string } '\" ) from e if dt . tzinfo is None : if error_on_missing_tz : raise ValueError ( \"Missing timezone info in ISO 8601 string.\" ) logger . warning ( f \"Assuming UTC for naive ISO string: { iso_string } \" ) dt = dt . replace ( tzinfo = UTC_TZ ) return dt . astimezone ( UTC_TZ ) parse_unknown_datetime ( date_str ) Attempt to parse an arbitrary string into a datetime using dateutil.parser.parse() . Parameters: date_str ( str ) \u2013 A date or datetime string in an unknown format. Returns: datetime | None \u2013 datetime | None: Parsed datetime object if successful; otherwise, None. Raises: None \u2013 Gracefully handles errors internally. Notes This method is lenient and accepts a wide range of human-readable formats. Returns None if the string is empty, not a string, or unparseable. Source code in arb\\utils\\date_and_time.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 def parse_unknown_datetime ( date_str : str ) -> datetime | None : \"\"\" Attempt to parse an arbitrary string into a datetime using `dateutil.parser.parse()`. Args: date_str (str): A date or datetime string in an unknown format. Returns: datetime | None: Parsed datetime object if successful; otherwise, None. Raises: None: Gracefully handles errors internally. Notes: - This method is lenient and accepts a wide range of human-readable formats. - Returns None if the string is empty, not a string, or unparseable. \"\"\" if not isinstance ( date_str , str ) or not date_str : return None try : return parser . parse ( date_str ) except ( ValueError , TypeError ): return None repr_datetime_to_string ( datetime_string ) Convert a repr-formatted datetime string into an ISO 8601 string. Parameters: datetime_string ( str | None ) \u2013 A string in the format \"datetime.datetime(...)\" Returns: str | None \u2013 str | None: ISO 8601 string or None if parsing fails or input is None. Source code in arb\\utils\\date_and_time.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def repr_datetime_to_string ( datetime_string : str | None ) -> str | None : \"\"\" Convert a repr-formatted datetime string into an ISO 8601 string. Args: datetime_string (str | None): A string in the format \"datetime.datetime(...)\" Returns: str | None: ISO 8601 string or None if parsing fails or input is None. \"\"\" if datetime_string is None : return None dt = str_to_datetime ( datetime_string ) return dt . isoformat () if dt else None run_diagnostics () Run a series of diagnostic operations to verify correctness of datetime utilities. Demonstrates ISO 8601 parsing to UTC Conversion to naive Pacific time Round-trip UTC conversion Recursive conversion in nested data structures Returns: None \u2013 None Source code in arb\\utils\\date_and_time.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 def run_diagnostics () -> None : \"\"\" Run a series of diagnostic operations to verify correctness of datetime utilities. Demonstrates: - ISO 8601 parsing to UTC - Conversion to naive Pacific time - Round-trip UTC conversion - Recursive conversion in nested data structures Returns: None \"\"\" from pprint import pprint print ( \"Running diagnostics on datetime utilities... \\n \" ) sample_iso = \"2025-05-05T10:00:00Z\" dt = iso8601_to_utc_dt ( sample_iso ) print ( \"Parsed ISO 8601 to UTC:\" , dt ) naive_pacific = datetime_to_ca_naive ( dt ) print ( \"Converted to naive Pacific:\" , naive_pacific ) round_trip = ca_naive_to_utc_datetime ( naive_pacific ) print ( \"Round-trip back to UTC:\" , round_trip ) nested_data = { dt : [ { \"created\" : dt }, ( dt ,) ] } pacific_data = convert_datetimes_to_ca_naive ( nested_data ) print ( \" \\n Converted nested UTC datetimes to Pacific:\" ) pprint ( pacific_data ) restored = convert_ca_naive_datetimes_to_utc ( pacific_data ) print ( \" \\n Restored nested Pacific datetimes to UTC:\" ) pprint ( restored ) str_to_datetime ( datetime_str ) Convert a repr() -formatted string into a datetime object. Parameters: datetime_str ( str ) \u2013 Example: \"datetime.datetime(2024, 11, 15, 14, 30, 45)\" Returns: datetime | None \u2013 datetime | None: Parsed datetime object if matched; otherwise, None. Notes This reverses repr(datetime) stringification for debugging recovery. Source code in arb\\utils\\date_and_time.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def str_to_datetime ( datetime_str : str ) -> datetime | None : \"\"\" Convert a `repr()`-formatted string into a datetime object. Args: datetime_str (str): Example: \"datetime.datetime(2024, 11, 15, 14, 30, 45)\" Returns: datetime | None: Parsed datetime object if matched; otherwise, None. Notes: This reverses `repr(datetime)` stringification for debugging recovery. \"\"\" match = re . search ( r \"datetime\\.datetime\\(([^)]+)\\)\" , datetime_str ) if match : date_parts = list ( map ( int , match . group ( 1 ) . split ( \", \" ))) return datetime ( * date_parts ) return None","title":"arb.utils.date_and_time"},{"location":"reference/arb/utils/date_and_time/#arbutilsdate_and_time","text":"Datetime parsing and timezone utilities for ISO 8601, UTC/Pacific conversion, repr-format recovery, and recursive datetime transformation in nested structures. Features: - ISO 8601 validation and parsing (via dateutil ) - Conversion between UTC and naive Pacific time (Los Angeles) - Safe handling of repr-formatted datetime strings (e.g., \"datetime.datetime(...)\") - Recursive datetime transformations within nested dicts/lists/sets/tuples Timezone policy: - UTC_TZ and PACIFIC_TZ are globally defined using zoneinfo.ZoneInfo - Naive timestamps are only assumed to be UTC if explicitly configured via arguments","title":"arb.utils.date_and_time"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.ca_naive_to_utc_datetime","text":"Convert a naive Pacific Time datetime to a UTC-aware datetime. Parameters: dt ( datetime ) \u2013 A naive datetime. Returns: datetime ( datetime ) \u2013 A UTC-aware datetime. Raises: ValueError \u2013 If the input is not naive (i.e., has timezone info). Source code in arb\\utils\\date_and_time.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def ca_naive_to_utc_datetime ( dt : datetime ) -> datetime : \"\"\" Convert a naive Pacific Time datetime to a UTC-aware datetime. Args: dt (datetime): A naive datetime. Returns: datetime: A UTC-aware datetime. Raises: ValueError: If the input is not naive (i.e., has timezone info). \"\"\" if dt . tzinfo is not None : raise ValueError ( f \"Expected naive datetime, got { dt !r} \" ) return dt . replace ( tzinfo = PACIFIC_TZ ) . astimezone ( UTC_TZ )","title":"ca_naive_to_utc_datetime"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.convert_ca_naive_datetimes_to_utc","text":"Recursively convert all naive Pacific Time datetimes in a nested structure to UTC-aware datetimes. Parameters: data ( object ) \u2013 A nested structure (e.g., dict, list, tuple). Returns: object ( object ) \u2013 The same structure with datetime values converted to UTC-aware format. Source code in arb\\utils\\date_and_time.py 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def convert_ca_naive_datetimes_to_utc ( data : object ) -> object : \"\"\" Recursively convert all naive Pacific Time datetimes in a nested structure to UTC-aware datetimes. Args: data (object): A nested structure (e.g., dict, list, tuple). Returns: object: The same structure with datetime values converted to UTC-aware format. \"\"\" if isinstance ( data , datetime ): return ca_naive_to_utc_datetime ( data ) elif isinstance ( data , Mapping ): return { convert_ca_naive_datetimes_to_utc ( k ): convert_ca_naive_datetimes_to_utc ( v ) for k , v in data . items ()} elif isinstance ( data , list ): return [ convert_ca_naive_datetimes_to_utc ( i ) for i in data ] elif isinstance ( data , tuple ): return tuple ( convert_ca_naive_datetimes_to_utc ( i ) for i in data ) elif isinstance ( data , set ): return { convert_ca_naive_datetimes_to_utc ( i ) for i in data } return data","title":"convert_ca_naive_datetimes_to_utc"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.convert_datetimes_to_ca_naive","text":"Recursively convert all datetime objects in a nested structure to naive Pacific Time. Parameters: data ( object ) \u2013 A structure that may include datetime values (dict, list, etc.). assume_naive_is_utc ( bool , default: False ) \u2013 Whether to treat naive datetimes as UTC. utc_strict ( bool , default: True ) \u2013 Whether to enforce that input datetimes are explicitly UTC. Returns: object ( object ) \u2013 A structure of the same shape, with datetime values converted to naive Pacific. Examples: >>> from datetime import datetime >>> from zoneinfo import ZoneInfo >>> nested = { ... datetime ( 2025 , 4 , 23 , 15 , 0 , tzinfo = ZoneInfo ( \"UTC\" )): [ ... { \"created\" : datetime ( 2025 , 4 , 23 , 18 , 0 )}, ... ( datetime ( 2025 , 4 , 23 , 20 , 0 , tzinfo = ZoneInfo ( \"UTC\" )),) ... ] >>> convert_datetimes_to_ca_naive ( nested ) { datetime.datetime(2025, 4, 23, 8, 0): [ {\"created\": datetime.datetime(2025, 4, 23, 11, 0)}, (datetime.datetime(2025, 4, 23, 13, 0),) ] } Source code in arb\\utils\\date_and_time.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def convert_datetimes_to_ca_naive ( data : object , assume_naive_is_utc : bool = False , utc_strict : bool = True ) -> object : \"\"\" Recursively convert all datetime objects in a nested structure to naive Pacific Time. Args: data (object): A structure that may include datetime values (dict, list, etc.). assume_naive_is_utc (bool): Whether to treat naive datetimes as UTC. utc_strict (bool): Whether to enforce that input datetimes are explicitly UTC. Returns: object: A structure of the same shape, with datetime values converted to naive Pacific. Examples: >>> from datetime import datetime >>> from zoneinfo import ZoneInfo >>> nested = { ... datetime(2025, 4, 23, 15, 0, tzinfo=ZoneInfo(\"UTC\")): [ ... {\"created\": datetime(2025, 4, 23, 18, 0)}, ... (datetime(2025, 4, 23, 20, 0, tzinfo=ZoneInfo(\"UTC\")),) ... ] >>> convert_datetimes_to_ca_naive(nested) { datetime.datetime(2025, 4, 23, 8, 0): [ {\"created\": datetime.datetime(2025, 4, 23, 11, 0)}, (datetime.datetime(2025, 4, 23, 13, 0),) ] } \"\"\" if isinstance ( data , datetime ): return datetime_to_ca_naive ( data , assume_naive_is_utc , utc_strict ) elif isinstance ( data , Mapping ): return { convert_datetimes_to_ca_naive ( k , assume_naive_is_utc , utc_strict ): convert_datetimes_to_ca_naive ( v , assume_naive_is_utc , utc_strict ) for k , v in data . items () } elif isinstance ( data , list ): return [ convert_datetimes_to_ca_naive ( i , assume_naive_is_utc , utc_strict ) for i in data ] elif isinstance ( data , tuple ): return tuple ( convert_datetimes_to_ca_naive ( i , assume_naive_is_utc , utc_strict ) for i in data ) elif isinstance ( data , set ): return { convert_datetimes_to_ca_naive ( i , assume_naive_is_utc , utc_strict ) for i in data } return data","title":"convert_datetimes_to_ca_naive"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.date_to_string","text":"Convert a datetime object to a formatted string. Parameters: dt ( datetime | None ) \u2013 A datetime object or None. str_format ( str , default: '%Y-%m-%dT%H:%M' ) \u2013 Format string used with strftime . Returns: str | None \u2013 str | None: The formatted string if input is not None; otherwise, None. Raises: TypeError \u2013 If x is not a datetime object. Source code in arb\\utils\\date_and_time.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def date_to_string ( dt : datetime | None , str_format : str = \"%Y-%m- %d T%H:%M\" ) -> str | None : \"\"\" Convert a datetime object to a formatted string. Args: dt (datetime | None): A datetime object or None. str_format (str): Format string used with `strftime`. Returns: str | None: The formatted string if input is not None; otherwise, None. Raises: TypeError: If `x` is not a datetime object. \"\"\" if dt is None : return None if not isinstance ( dt , datetime ): raise TypeError ( f 'x must be datetime.datetime or None. { dt =} ' ) return dt . strftime ( str_format )","title":"date_to_string"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.datetime_to_ca_naive","text":"Convert a datetime (UTC or naive) to naive Pacific Time. Parameters: dt ( datetime ) \u2013 The datetime to convert. assume_naive_is_utc ( bool , default: False ) \u2013 If True, interpret naive datetime as UTC. utc_strict ( bool , default: True ) \u2013 If True, raise an error unless datetime is explicitly UTC. Returns: datetime ( datetime ) \u2013 A naive Pacific Time datetime. Raises: ValueError \u2013 If datetime is naive and assume_naive_is_utc is False, or if UTC check fails under utc_strict . Source code in arb\\utils\\date_and_time.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 def datetime_to_ca_naive ( dt : datetime , assume_naive_is_utc : bool = False , utc_strict : bool = True ) -> datetime : \"\"\" Convert a datetime (UTC or naive) to naive Pacific Time. Args: dt (datetime): The datetime to convert. assume_naive_is_utc (bool): If True, interpret naive datetime as UTC. utc_strict (bool): If True, raise an error unless datetime is explicitly UTC. Returns: datetime: A naive Pacific Time datetime. Raises: ValueError: If datetime is naive and `assume_naive_is_utc` is False, or if UTC check fails under `utc_strict`. \"\"\" if dt . tzinfo is None : if assume_naive_is_utc : logger . warning ( \"Assuming UTC for naive datetime.\" ) dt = dt . replace ( tzinfo = UTC_TZ ) else : raise ValueError ( \"Naive datetime provided without assume_naive_is_utc=True\" ) elif utc_strict and dt . tzinfo != UTC_TZ : raise ValueError ( \"datetime must be UTC when utc_strict=True\" ) return dt . astimezone ( PACIFIC_TZ ) . replace ( tzinfo = None )","title":"datetime_to_ca_naive"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.is_datetime_naive","text":"Check whether a datetime object is naive (lacks timezone info). Parameters: dt ( datetime ) \u2013 A datetime instance. Returns: bool ( bool ) \u2013 True if the datetime is naive (tzinfo is None or utcoffset is None); False otherwise. Examples: >>> is_datetime_naive ( datetime . now ()) True >>> from zoneinfo import ZoneInfo >>> is_datetime_naive ( datetime . now ( tz = ZoneInfo ( \"UTC\" ))) False Source code in arb\\utils\\date_and_time.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 def is_datetime_naive ( dt : datetime ) -> bool : \"\"\" Check whether a datetime object is naive (lacks timezone info). Args: dt (datetime): A datetime instance. Returns: bool: True if the datetime is naive (tzinfo is None or utcoffset is None); False otherwise. Examples: >>> is_datetime_naive(datetime.now()) True >>> from zoneinfo import ZoneInfo >>> is_datetime_naive(datetime.now(tz=ZoneInfo(\"UTC\"))) False \"\"\" return dt . tzinfo is None or dt . tzinfo . utcoffset ( dt ) is None","title":"is_datetime_naive"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.is_iso8601","text":"Determine whether a string is a valid ISO 8601 datetime. Parameters: string ( str ) \u2013 The string to validate. Returns: bool ( bool ) \u2013 True if the string conforms to ISO 8601 datetime format, False otherwise. Notes Uses dateutil.parser.isoparse for strict ISO 8601 parsing. ISO 8601 requires a full date (YYYY-MM-DD) at minimum. Time-only strings are invalid. This does not validate durations, ordinal dates, or week dates. Examples: Valid cases (returns True): - \"2024-12-31\" - \"2024-12-31T23:59:59\" - \"2024-12-31T23:59:59Z\" # UTC/Zulu time - \"2024-12-31T23:59:59+00:00\" # Timezone offset - \"2024-12-31T23:59:59.123456\" # Microsecond precision - \"2024-12-31T23:59:59.123+02:00\" # Fractional seconds with TZ Invalid cases (returns False): - \"12/31/2024\" # US-style date format - \"2024-31-12\" # Invalid format (day and month swapped) - \"2024\" # Year only, not a complete date or datetime - \"P1Y2M\" # ISO 8601 duration, not a datetime - \"Just some text\" # Clearly not a date - \"2024-12-31 23:59:59\" # Space instead of 'T' (technically invalid ISO 8601) # Time-only values \u2014 invalid because ISO 8601 requires at least a full date - \"23:59\" # Just time (no date) - \"23:59:59\" # Time only, still invalid - \"T23:59:59Z\" # Prefixed with 'T' but still no date - \"23:59:59+00:00\" # Time with timezone, but missing a date - \"T23:59:59.123456\" # Microsecond time, still not valid without date Notes ISO 8601 datetimes require at least a full date (YYYY-MM-DD). Time-only strings are not valid by themselves. This does not validate ISO 8601 durations, ordinal dates, or week dates. This routine strictly uses dateutil.parser.isoparse , which rejects incomplete formats like time-only values. Source code in arb\\utils\\date_and_time.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def is_iso8601 ( string : str ) -> bool : \"\"\" Determine whether a string is a valid ISO 8601 datetime. Args: string (str): The string to validate. Returns: bool: True if the string conforms to ISO 8601 datetime format, False otherwise. Notes: - Uses `dateutil.parser.isoparse` for strict ISO 8601 parsing. - ISO 8601 requires a full date (YYYY-MM-DD) at minimum. Time-only strings are invalid. - This does not validate durations, ordinal dates, or week dates. Examples: Valid cases (returns True): - \"2024-12-31\" - \"2024-12-31T23:59:59\" - \"2024-12-31T23:59:59Z\" # UTC/Zulu time - \"2024-12-31T23:59:59+00:00\" # Timezone offset - \"2024-12-31T23:59:59.123456\" # Microsecond precision - \"2024-12-31T23:59:59.123+02:00\" # Fractional seconds with TZ Invalid cases (returns False): - \"12/31/2024\" # US-style date format - \"2024-31-12\" # Invalid format (day and month swapped) - \"2024\" # Year only, not a complete date or datetime - \"P1Y2M\" # ISO 8601 duration, not a datetime - \"Just some text\" # Clearly not a date - \"2024-12-31 23:59:59\" # Space instead of 'T' (technically invalid ISO 8601) # Time-only values \u2014 invalid because ISO 8601 requires at least a full date - \"23:59\" # Just time (no date) - \"23:59:59\" # Time only, still invalid - \"T23:59:59Z\" # Prefixed with 'T' but still no date - \"23:59:59+00:00\" # Time with timezone, but missing a date - \"T23:59:59.123456\" # Microsecond time, still not valid without date Notes: - ISO 8601 datetimes require at least a full date (YYYY-MM-DD). Time-only strings are not valid by themselves. - This does not validate ISO 8601 durations, ordinal dates, or week dates. - This routine strictly uses `dateutil.parser.isoparse`, which rejects incomplete formats like time-only values. \"\"\" try : parser . isoparse ( string ) return True except ValueError : return False","title":"is_iso8601"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.iso8601_to_utc_dt","text":"Parse an ISO 8601 string and return a timezone-aware datetime object in UTC. This function uses dateutil.parser.isoparse() for robust ISO 8601 parsing, including support for variations like 'Z' (UTC shorthand) and missing separators. Parameters: iso_string ( str ) \u2013 An ISO 8601-formatted datetime string. error_on_missing_tz ( bool , default: True ) \u2013 If True, raises a ValueError when the string has no timezone info. Returns: datetime \u2013 A timezone-aware datetime object in UTC. Raises: ValueError \u2013 If the input string is invalid or lacks timezone info and error_on_missing_tz is True. Examples: >>> iso8601_to_utc_dt ( \"2025-04-20T14:30:00+00:00\" ) datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\")) >>> iso8601_to_utc_dt ( \"2025-04-20T14:30:00Z\" ) datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\")) >>> iso8601_to_utc_dt ( \"2025-04-20 14:30:00\" , error_on_missing_tz = False ) datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\")) >>> iso8601_to_utc_dt ( \"2025-04-20T14:30:00Z\" ) datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo('UTC')) Source code in arb\\utils\\date_and_time.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def iso8601_to_utc_dt ( iso_string : str , error_on_missing_tz : bool = True ) -> datetime : \"\"\" Parse an ISO 8601 string and return a timezone-aware datetime object in UTC. This function uses `dateutil.parser.isoparse()` for robust ISO 8601 parsing, including support for variations like 'Z' (UTC shorthand) and missing separators. Args: iso_string: An ISO 8601-formatted datetime string. error_on_missing_tz: If True, raises a ValueError when the string has no timezone info. Returns: A timezone-aware datetime object in UTC. Raises: ValueError: If the input string is invalid or lacks timezone info and `error_on_missing_tz` is True. Examples: >>> iso8601_to_utc_dt(\"2025-04-20T14:30:00+00:00\") datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\")) >>> iso8601_to_utc_dt(\"2025-04-20T14:30:00Z\") datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\")) >>> iso8601_to_utc_dt(\"2025-04-20 14:30:00\", error_on_missing_tz=False) datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo(\"UTC\")) >>> iso8601_to_utc_dt(\"2025-04-20T14:30:00Z\") datetime.datetime(2025, 4, 20, 14, 30, tzinfo=zoneinfo.ZoneInfo('UTC')) \"\"\" try : dt = parser . isoparse ( iso_string ) except ( ValueError , TypeError ) as e : raise ValueError ( f \"Invalid ISO 8601 datetime string: ' { iso_string } '\" ) from e if dt . tzinfo is None : if error_on_missing_tz : raise ValueError ( \"Missing timezone info in ISO 8601 string.\" ) logger . warning ( f \"Assuming UTC for naive ISO string: { iso_string } \" ) dt = dt . replace ( tzinfo = UTC_TZ ) return dt . astimezone ( UTC_TZ )","title":"iso8601_to_utc_dt"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.parse_unknown_datetime","text":"Attempt to parse an arbitrary string into a datetime using dateutil.parser.parse() . Parameters: date_str ( str ) \u2013 A date or datetime string in an unknown format. Returns: datetime | None \u2013 datetime | None: Parsed datetime object if successful; otherwise, None. Raises: None \u2013 Gracefully handles errors internally. Notes This method is lenient and accepts a wide range of human-readable formats. Returns None if the string is empty, not a string, or unparseable. Source code in arb\\utils\\date_and_time.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 def parse_unknown_datetime ( date_str : str ) -> datetime | None : \"\"\" Attempt to parse an arbitrary string into a datetime using `dateutil.parser.parse()`. Args: date_str (str): A date or datetime string in an unknown format. Returns: datetime | None: Parsed datetime object if successful; otherwise, None. Raises: None: Gracefully handles errors internally. Notes: - This method is lenient and accepts a wide range of human-readable formats. - Returns None if the string is empty, not a string, or unparseable. \"\"\" if not isinstance ( date_str , str ) or not date_str : return None try : return parser . parse ( date_str ) except ( ValueError , TypeError ): return None","title":"parse_unknown_datetime"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.repr_datetime_to_string","text":"Convert a repr-formatted datetime string into an ISO 8601 string. Parameters: datetime_string ( str | None ) \u2013 A string in the format \"datetime.datetime(...)\" Returns: str | None \u2013 str | None: ISO 8601 string or None if parsing fails or input is None. Source code in arb\\utils\\date_and_time.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def repr_datetime_to_string ( datetime_string : str | None ) -> str | None : \"\"\" Convert a repr-formatted datetime string into an ISO 8601 string. Args: datetime_string (str | None): A string in the format \"datetime.datetime(...)\" Returns: str | None: ISO 8601 string or None if parsing fails or input is None. \"\"\" if datetime_string is None : return None dt = str_to_datetime ( datetime_string ) return dt . isoformat () if dt else None","title":"repr_datetime_to_string"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.run_diagnostics","text":"Run a series of diagnostic operations to verify correctness of datetime utilities. Demonstrates ISO 8601 parsing to UTC Conversion to naive Pacific time Round-trip UTC conversion Recursive conversion in nested data structures Returns: None \u2013 None Source code in arb\\utils\\date_and_time.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 def run_diagnostics () -> None : \"\"\" Run a series of diagnostic operations to verify correctness of datetime utilities. Demonstrates: - ISO 8601 parsing to UTC - Conversion to naive Pacific time - Round-trip UTC conversion - Recursive conversion in nested data structures Returns: None \"\"\" from pprint import pprint print ( \"Running diagnostics on datetime utilities... \\n \" ) sample_iso = \"2025-05-05T10:00:00Z\" dt = iso8601_to_utc_dt ( sample_iso ) print ( \"Parsed ISO 8601 to UTC:\" , dt ) naive_pacific = datetime_to_ca_naive ( dt ) print ( \"Converted to naive Pacific:\" , naive_pacific ) round_trip = ca_naive_to_utc_datetime ( naive_pacific ) print ( \"Round-trip back to UTC:\" , round_trip ) nested_data = { dt : [ { \"created\" : dt }, ( dt ,) ] } pacific_data = convert_datetimes_to_ca_naive ( nested_data ) print ( \" \\n Converted nested UTC datetimes to Pacific:\" ) pprint ( pacific_data ) restored = convert_ca_naive_datetimes_to_utc ( pacific_data ) print ( \" \\n Restored nested Pacific datetimes to UTC:\" ) pprint ( restored )","title":"run_diagnostics"},{"location":"reference/arb/utils/date_and_time/#arb.utils.date_and_time.str_to_datetime","text":"Convert a repr() -formatted string into a datetime object. Parameters: datetime_str ( str ) \u2013 Example: \"datetime.datetime(2024, 11, 15, 14, 30, 45)\" Returns: datetime | None \u2013 datetime | None: Parsed datetime object if matched; otherwise, None. Notes This reverses repr(datetime) stringification for debugging recovery. Source code in arb\\utils\\date_and_time.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def str_to_datetime ( datetime_str : str ) -> datetime | None : \"\"\" Convert a `repr()`-formatted string into a datetime object. Args: datetime_str (str): Example: \"datetime.datetime(2024, 11, 15, 14, 30, 45)\" Returns: datetime | None: Parsed datetime object if matched; otherwise, None. Notes: This reverses `repr(datetime)` stringification for debugging recovery. \"\"\" match = re . search ( r \"datetime\\.datetime\\(([^)]+)\\)\" , datetime_str ) if match : date_parts = list ( map ( int , match . group ( 1 ) . split ( \", \" ))) return datetime ( * date_parts ) return None","title":"str_to_datetime"},{"location":"reference/arb/utils/diagnostics/","text":"arb.utils.diagnostics Diagnostic utilities for inspecting and logging Python objects. Provides: - Object introspection for development/debugging - Attribute/value logging (including hidden and callable members) - Dictionary comparisons and formatting - Recursive HTML-safe rendering of complex data structures - Integration with Flask for developer-oriented diagnostics Intended primarily for use in debug environments, template rendering, or ad-hoc inspection of application state during development. compare_dicts ( dict1 , dict2 , dict1_name = None , dict2_name = None ) Compare two dictionaries and log differences in keys and values. Parameters: dict1 ( dict ) \u2013 First dictionary. dict2 ( dict ) \u2013 Second dictionary. dict1_name ( str | None , default: None ) \u2013 Optional label for first dictionary in logs. dict2_name ( str | None , default: None ) \u2013 Optional label for second dictionary in logs. Returns: bool ( bool ) \u2013 True if dictionaries are equivalent; False otherwise. Examples: >>> dict1 = { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } >>> dict2 = { \"a\" : 1 , \"b\" : 4 , \"d\" : 5 } >>> compare_dicts ( dict1 , dict2 ) False Source code in arb\\utils\\diagnostics.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def compare_dicts ( dict1 : dict , dict2 : dict , dict1_name : str | None = None , dict2_name : str | None = None ) -> bool : \"\"\" Compare two dictionaries and log differences in keys and values. Args: dict1 (dict): First dictionary. dict2 (dict): Second dictionary. dict1_name (str | None): Optional label for first dictionary in logs. dict2_name (str | None): Optional label for second dictionary in logs. Returns: bool: True if dictionaries are equivalent; False otherwise. Examples: >>> dict1 = {\"a\": 1, \"b\": 2, \"c\": 3} >>> dict2 = {\"a\": 1, \"b\": 4, \"d\": 5} >>> compare_dicts(dict1, dict2) False \"\"\" dict1_name = dict1_name or \"dict_1\" dict2_name = dict2_name or \"dict_2\" logger . debug ( f \"compare_dicts called to compare { dict1_name } with { dict2_name } \" ) keys_in_dict1_not_in_dict2 = set ( dict1 ) - set ( dict2 ) keys_in_dict2_not_in_dict1 = set ( dict2 ) - set ( dict1 ) differing_values = { key : ( dict1 [ key ], dict2 [ key ]) for key in dict1 . keys () & dict2 . keys () if dict1 [ key ] != dict2 [ key ] } if keys_in_dict1_not_in_dict2 or keys_in_dict2_not_in_dict1 or differing_values : logger . debug ( \"Key differences:\" ) if keys_in_dict1_not_in_dict2 : logger . debug ( f \"- In { dict1_name } but not in { dict2_name } : { sorted ( keys_in_dict1_not_in_dict2 ) } \" ) if keys_in_dict2_not_in_dict1 : logger . debug ( f \"- In { dict2_name } but not in { dict1_name } : { sorted ( keys_in_dict2_not_in_dict1 ) } \" ) if differing_values : logger . debug ( \"Value differences:\" ) for key , ( v1 , v2 ) in dict ( sorted ( differing_values . items ())) . items (): logger . debug ( f \"- Key: ' { key } ', { dict1_name } : { v1 } , { dict2_name } : { v2 } \" ) return False return True diag_recursive ( x , depth = 0 , index = 0 ) Recursively log the structure and values of a nested iterable. Parameters: x ( object ) \u2013 Input object to inspect. depth ( int , default: 0 ) \u2013 Current recursion depth. index ( int , default: 0 ) \u2013 Index at the current level (if applicable). Returns: None \u2013 None Notes Strings are treated as non-iterables. Source code in arb\\utils\\diagnostics.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def diag_recursive ( x : object , depth : int = 0 , index : int = 0 ) -> None : \"\"\" Recursively log the structure and values of a nested iterable. Args: x (object): Input object to inspect. depth (int): Current recursion depth. index (int): Index at the current level (if applicable). Returns: None Notes: Strings are treated as non-iterables. \"\"\" indent = ' ' * 3 * depth if depth == 0 : logger . debug ( f \"diag_recursive diagnostics called \\n { '-' * 120 } \" ) logger . debug ( f \"Type: { type ( x ) } , Value: { x } \" ) else : logger . debug ( f \" { indent } Depth: { depth } , Index: { index } , Type: { type ( x ) } , Value: { x } \" ) if not isinstance ( x , str ): try : for i , y in enumerate ( x ): diag_recursive ( y , depth + 1 , index = i ) except TypeError : pass dict_to_str ( x , depth = 0 ) Convert a dictionary to a pretty-printed multiline string. Parameters: x ( dict ) \u2013 Dictionary to convert. depth ( int , default: 0 ) \u2013 Current indentation depth for nested dictionaries. Returns: str ( str ) \u2013 String representation of dictionary with indentation. Examples: >>> d = { \"a\" : 1 , \"b\" : { \"c\" : 2 }} >>> print ( dict_to_str ( d )) a: 1 b: c: 2 Source code in arb\\utils\\diagnostics.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def dict_to_str ( x : dict , depth : int = 0 ) -> str : \"\"\" Convert a dictionary to a pretty-printed multiline string. Args: x (dict): Dictionary to convert. depth (int): Current indentation depth for nested dictionaries. Returns: str: String representation of dictionary with indentation. Examples: >>> d = {\"a\": 1, \"b\": {\"c\": 2}} >>> print(dict_to_str(d)) a: 1 b: c: 2 \"\"\" msg = \"\" indent = ' ' * 3 * depth for k , v in x . items (): msg += f \" { indent }{ k } : \\n \" if isinstance ( v , dict ): msg += dict_to_str ( v , depth = depth + 1 ) else : msg += f \" { indent }{ v } \\n \" return msg get_changed_fields ( new_dict , old_dict ) Extract fields from new_dict that differ from old_dict. Parameters: new_dict ( dict ) \u2013 New/updated values (e.g., from a form). old_dict ( dict ) \u2013 Old/reference values (e.g., from model JSON). Returns: dict ( dict ) \u2013 Keys with values that have changed. Notes: - Only keys present in new_dict are considered. This prevents unrelated fields from being overwritten when merging partial form data into a larger stored structure. Source code in arb\\utils\\diagnostics.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def get_changed_fields ( new_dict : dict , old_dict : dict ) -> dict : \"\"\" Extract fields from new_dict that differ from old_dict. Args: new_dict (dict): New/updated values (e.g., from a form). old_dict (dict): Old/reference values (e.g., from model JSON). Returns: dict: Keys with values that have changed. Notes: - Only keys present in new_dict are considered. This prevents unrelated fields from being overwritten when merging partial form data into a larger stored structure. \"\"\" changes = {} for key in new_dict : if new_dict [ key ] != old_dict . get ( key ): changes [ key ] = new_dict [ key ] return changes list_differences ( iterable_01 , iterable_02 , iterable_01_name = 'List 1' , iterable_02_name = 'List 2' , print_warning = False ) Identify differences between two iterable objects (list or dict). Parameters: iterable_01 ( list | dict ) \u2013 First iterable object to compare. iterable_02 ( list | dict ) \u2013 Second iterable object to compare. iterable_01_name ( str , default: 'List 1' ) \u2013 Label for the first iterable in log messages. iterable_02_name ( str , default: 'List 2' ) \u2013 Label for the second iterable in log messages. print_warning ( bool , default: False ) \u2013 If True, logs warnings for non-overlapping items. Returns: tuple [ list , list ] \u2013 tuple[list, list]: - Items in iterable_01 but not in iterable_02 - Items in iterable_02 but not in iterable_01 Examples: >>> list_differences([\"a\", \"b\"], [\"b\", \"c\"]) ([\"a\"], [\"c\"]) Source code in arb\\utils\\diagnostics.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def list_differences ( iterable_01 : list | dict , iterable_02 : list | dict , iterable_01_name : str = \"List 1\" , iterable_02_name : str = \"List 2\" , print_warning : bool = False ) -> tuple [ list , list ]: \"\"\" Identify differences between two iterable objects (list or dict). Args: iterable_01 (list | dict): First iterable object to compare. iterable_02 (list | dict): Second iterable object to compare. iterable_01_name (str): Label for the first iterable in log messages. iterable_02_name (str): Label for the second iterable in log messages. print_warning (bool): If True, logs warnings for non-overlapping items. Returns: tuple[list, list]: - Items in `iterable_01` but not in `iterable_02` - Items in `iterable_02` but not in `iterable_01` Examples: >>> list_differences([\"a\", \"b\"], [\"b\", \"c\"]) ([\"a\"], [\"c\"]) \"\"\" in_iterable_1_only = [ x for x in iterable_01 if x not in iterable_02 ] in_iterable_2_only = [ x for x in iterable_02 if x not in iterable_01 ] if print_warning : if in_iterable_1_only : logger . warning ( f \"Warning: { iterable_01_name } has { len ( in_iterable_1_only ) } item(s) not in { iterable_02_name } : \\t { in_iterable_1_only } \" ) if in_iterable_2_only : logger . warning ( f \"Warning: { iterable_02_name } has { len ( in_iterable_2_only ) } item(s) not in { iterable_01_name } : \\t { in_iterable_2_only } \" ) return in_iterable_1_only , in_iterable_2_only obj_diagnostics ( obj , include_hidden = False , include_functions = False , message = None ) Log detailed diagnostics about an object's attributes and values. Parameters: obj ( object ) \u2013 The object to inspect. include_hidden ( bool , default: False ) \u2013 Whether to include private attributes (starting with _ ). include_functions ( bool , default: False ) \u2013 Whether to include methods or callable attributes. message ( str | None , default: None ) \u2013 Optional prefix message to label the diagnostic output. Returns: None \u2013 None Example obj_diagnostics(my_object, include_hidden=True, include_functions=True) Source code in arb\\utils\\diagnostics.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def obj_diagnostics ( obj : object , include_hidden : bool = False , include_functions : bool = False , message : str | None = None ) -> None : \"\"\" Log detailed diagnostics about an object's attributes and values. Args: obj (object): The object to inspect. include_hidden (bool): Whether to include private attributes (starting with `_`). include_functions (bool): Whether to include methods or callable attributes. message (str | None): Optional prefix message to label the diagnostic output. Returns: None Example: >>> obj_diagnostics(my_object, include_hidden=True, include_functions=True) \"\"\" logger . debug ( f \"Diagnostics for: { obj } \" ) if message : logger . debug ( f \" { message =} \" ) for attr_name in dir ( obj ): attr_value = getattr ( obj , attr_name ) if not attr_name . startswith ( '_' ) or include_hidden : if callable ( attr_value ): if include_functions : logger . debug ( f \" { attr_name } (): is function\" ) else : logger . debug ( f \" { attr_name } { type ( attr_value ) } : \\t { attr_value } \" ) obj_to_html ( obj ) Convert any Python object to a formatted HTML string for Jinja rendering. Parameters: obj ( object ) \u2013 A Python object suitable for pprint . Returns: str ( str ) \u2013 HTML string wrapped in tags that is safe for use with |safe in templates. Notes The HTML content must be marked |safe in the template to avoid escaping. Example (in Jinja): {% if result is defined %} {{ result|safe }} {% endif %} Source code in arb\\utils\\diagnostics.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def obj_to_html ( obj : object ) -> str : \"\"\" Convert any Python object to a formatted HTML string for Jinja rendering. Args: obj (object): A Python object suitable for `pprint`. Returns: str: HTML string wrapped in <pre> tags that is safe for use with `|safe` in templates. Notes: The HTML content must be marked `|safe` in the template to avoid escaping. Example (in Jinja): {% if result is defined %} {{ result|safe }} {% endif %} \"\"\" pp = pprint . PrettyPrinter ( indent = 4 , width = 200 ) formatted_data = pp . pformat ( obj ) soup = BeautifulSoup ( \"<pre></pre>\" , \"html.parser\" ) soup . pre . string = formatted_data return soup . prettify () run_diagnostics () Run example-based tests for diagnostic functions in this module. Logs example output for each function. Source code in arb\\utils\\diagnostics.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def run_diagnostics () -> None : \"\"\" Run example-based tests for diagnostic functions in this module. Logs example output for each function. \"\"\" logger . debug ( \"Running diagnostics on arb.utils.diagnostics module\" ) # obj_diagnostics class TestClass : def __init__ ( self ): self . x = 42 self . y = \"hello\" def greet ( self ): return \"hi\" obj = TestClass () obj_diagnostics ( obj , include_hidden = False , include_functions = True ) # list_differences a = [ \"x\" , \"y\" , \"z\" ] b = [ \"x\" , \"z\" , \"w\" ] list_differences ( a , b , \"List A\" , \"List B\" , print_warning = True ) # diag_recursive diag_recursive ([[ 1 , 2 ], [ 3 , [ 4 , 5 ]]]) # dict_to_str nested_dict = { \"a\" : 1 , \"b\" : { \"c\" : 2 , \"d\" : { \"e\" : 3 }}} logger . debug ( \"dict_to_str output: \\t \" + dict_to_str ( nested_dict )) # obj_to_html html_result = obj_to_html ( nested_dict ) logger . debug ( \"HTML representation of object (truncated): \\t \" + html_result [: 300 ]) # compare_dicts d1 = { \"x\" : 1 , \"y\" : 2 , \"z\" : 3 } d2 = { \"x\" : 1 , \"y\" : 99 , \"w\" : 0 } compare_dicts ( d1 , d2 , \"First Dict\" , \"Second Dict\" )","title":"arb.utils.diagnostics"},{"location":"reference/arb/utils/diagnostics/#arbutilsdiagnostics","text":"Diagnostic utilities for inspecting and logging Python objects. Provides: - Object introspection for development/debugging - Attribute/value logging (including hidden and callable members) - Dictionary comparisons and formatting - Recursive HTML-safe rendering of complex data structures - Integration with Flask for developer-oriented diagnostics Intended primarily for use in debug environments, template rendering, or ad-hoc inspection of application state during development.","title":"arb.utils.diagnostics"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.compare_dicts","text":"Compare two dictionaries and log differences in keys and values. Parameters: dict1 ( dict ) \u2013 First dictionary. dict2 ( dict ) \u2013 Second dictionary. dict1_name ( str | None , default: None ) \u2013 Optional label for first dictionary in logs. dict2_name ( str | None , default: None ) \u2013 Optional label for second dictionary in logs. Returns: bool ( bool ) \u2013 True if dictionaries are equivalent; False otherwise. Examples: >>> dict1 = { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } >>> dict2 = { \"a\" : 1 , \"b\" : 4 , \"d\" : 5 } >>> compare_dicts ( dict1 , dict2 ) False Source code in arb\\utils\\diagnostics.py 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def compare_dicts ( dict1 : dict , dict2 : dict , dict1_name : str | None = None , dict2_name : str | None = None ) -> bool : \"\"\" Compare two dictionaries and log differences in keys and values. Args: dict1 (dict): First dictionary. dict2 (dict): Second dictionary. dict1_name (str | None): Optional label for first dictionary in logs. dict2_name (str | None): Optional label for second dictionary in logs. Returns: bool: True if dictionaries are equivalent; False otherwise. Examples: >>> dict1 = {\"a\": 1, \"b\": 2, \"c\": 3} >>> dict2 = {\"a\": 1, \"b\": 4, \"d\": 5} >>> compare_dicts(dict1, dict2) False \"\"\" dict1_name = dict1_name or \"dict_1\" dict2_name = dict2_name or \"dict_2\" logger . debug ( f \"compare_dicts called to compare { dict1_name } with { dict2_name } \" ) keys_in_dict1_not_in_dict2 = set ( dict1 ) - set ( dict2 ) keys_in_dict2_not_in_dict1 = set ( dict2 ) - set ( dict1 ) differing_values = { key : ( dict1 [ key ], dict2 [ key ]) for key in dict1 . keys () & dict2 . keys () if dict1 [ key ] != dict2 [ key ] } if keys_in_dict1_not_in_dict2 or keys_in_dict2_not_in_dict1 or differing_values : logger . debug ( \"Key differences:\" ) if keys_in_dict1_not_in_dict2 : logger . debug ( f \"- In { dict1_name } but not in { dict2_name } : { sorted ( keys_in_dict1_not_in_dict2 ) } \" ) if keys_in_dict2_not_in_dict1 : logger . debug ( f \"- In { dict2_name } but not in { dict1_name } : { sorted ( keys_in_dict2_not_in_dict1 ) } \" ) if differing_values : logger . debug ( \"Value differences:\" ) for key , ( v1 , v2 ) in dict ( sorted ( differing_values . items ())) . items (): logger . debug ( f \"- Key: ' { key } ', { dict1_name } : { v1 } , { dict2_name } : { v2 } \" ) return False return True","title":"compare_dicts"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.diag_recursive","text":"Recursively log the structure and values of a nested iterable. Parameters: x ( object ) \u2013 Input object to inspect. depth ( int , default: 0 ) \u2013 Current recursion depth. index ( int , default: 0 ) \u2013 Index at the current level (if applicable). Returns: None \u2013 None Notes Strings are treated as non-iterables. Source code in arb\\utils\\diagnostics.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def diag_recursive ( x : object , depth : int = 0 , index : int = 0 ) -> None : \"\"\" Recursively log the structure and values of a nested iterable. Args: x (object): Input object to inspect. depth (int): Current recursion depth. index (int): Index at the current level (if applicable). Returns: None Notes: Strings are treated as non-iterables. \"\"\" indent = ' ' * 3 * depth if depth == 0 : logger . debug ( f \"diag_recursive diagnostics called \\n { '-' * 120 } \" ) logger . debug ( f \"Type: { type ( x ) } , Value: { x } \" ) else : logger . debug ( f \" { indent } Depth: { depth } , Index: { index } , Type: { type ( x ) } , Value: { x } \" ) if not isinstance ( x , str ): try : for i , y in enumerate ( x ): diag_recursive ( y , depth + 1 , index = i ) except TypeError : pass","title":"diag_recursive"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.dict_to_str","text":"Convert a dictionary to a pretty-printed multiline string. Parameters: x ( dict ) \u2013 Dictionary to convert. depth ( int , default: 0 ) \u2013 Current indentation depth for nested dictionaries. Returns: str ( str ) \u2013 String representation of dictionary with indentation. Examples: >>> d = { \"a\" : 1 , \"b\" : { \"c\" : 2 }} >>> print ( dict_to_str ( d )) a: 1 b: c: 2 Source code in arb\\utils\\diagnostics.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def dict_to_str ( x : dict , depth : int = 0 ) -> str : \"\"\" Convert a dictionary to a pretty-printed multiline string. Args: x (dict): Dictionary to convert. depth (int): Current indentation depth for nested dictionaries. Returns: str: String representation of dictionary with indentation. Examples: >>> d = {\"a\": 1, \"b\": {\"c\": 2}} >>> print(dict_to_str(d)) a: 1 b: c: 2 \"\"\" msg = \"\" indent = ' ' * 3 * depth for k , v in x . items (): msg += f \" { indent }{ k } : \\n \" if isinstance ( v , dict ): msg += dict_to_str ( v , depth = depth + 1 ) else : msg += f \" { indent }{ v } \\n \" return msg","title":"dict_to_str"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.get_changed_fields","text":"Extract fields from new_dict that differ from old_dict. Parameters: new_dict ( dict ) \u2013 New/updated values (e.g., from a form). old_dict ( dict ) \u2013 Old/reference values (e.g., from model JSON). Returns: dict ( dict ) \u2013 Keys with values that have changed. Notes: - Only keys present in new_dict are considered. This prevents unrelated fields from being overwritten when merging partial form data into a larger stored structure. Source code in arb\\utils\\diagnostics.py 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 def get_changed_fields ( new_dict : dict , old_dict : dict ) -> dict : \"\"\" Extract fields from new_dict that differ from old_dict. Args: new_dict (dict): New/updated values (e.g., from a form). old_dict (dict): Old/reference values (e.g., from model JSON). Returns: dict: Keys with values that have changed. Notes: - Only keys present in new_dict are considered. This prevents unrelated fields from being overwritten when merging partial form data into a larger stored structure. \"\"\" changes = {} for key in new_dict : if new_dict [ key ] != old_dict . get ( key ): changes [ key ] = new_dict [ key ] return changes","title":"get_changed_fields"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.list_differences","text":"Identify differences between two iterable objects (list or dict). Parameters: iterable_01 ( list | dict ) \u2013 First iterable object to compare. iterable_02 ( list | dict ) \u2013 Second iterable object to compare. iterable_01_name ( str , default: 'List 1' ) \u2013 Label for the first iterable in log messages. iterable_02_name ( str , default: 'List 2' ) \u2013 Label for the second iterable in log messages. print_warning ( bool , default: False ) \u2013 If True, logs warnings for non-overlapping items. Returns: tuple [ list , list ] \u2013 tuple[list, list]: - Items in iterable_01 but not in iterable_02 - Items in iterable_02 but not in iterable_01 Examples: >>> list_differences([\"a\", \"b\"], [\"b\", \"c\"]) ([\"a\"], [\"c\"]) Source code in arb\\utils\\diagnostics.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def list_differences ( iterable_01 : list | dict , iterable_02 : list | dict , iterable_01_name : str = \"List 1\" , iterable_02_name : str = \"List 2\" , print_warning : bool = False ) -> tuple [ list , list ]: \"\"\" Identify differences between two iterable objects (list or dict). Args: iterable_01 (list | dict): First iterable object to compare. iterable_02 (list | dict): Second iterable object to compare. iterable_01_name (str): Label for the first iterable in log messages. iterable_02_name (str): Label for the second iterable in log messages. print_warning (bool): If True, logs warnings for non-overlapping items. Returns: tuple[list, list]: - Items in `iterable_01` but not in `iterable_02` - Items in `iterable_02` but not in `iterable_01` Examples: >>> list_differences([\"a\", \"b\"], [\"b\", \"c\"]) ([\"a\"], [\"c\"]) \"\"\" in_iterable_1_only = [ x for x in iterable_01 if x not in iterable_02 ] in_iterable_2_only = [ x for x in iterable_02 if x not in iterable_01 ] if print_warning : if in_iterable_1_only : logger . warning ( f \"Warning: { iterable_01_name } has { len ( in_iterable_1_only ) } item(s) not in { iterable_02_name } : \\t { in_iterable_1_only } \" ) if in_iterable_2_only : logger . warning ( f \"Warning: { iterable_02_name } has { len ( in_iterable_2_only ) } item(s) not in { iterable_01_name } : \\t { in_iterable_2_only } \" ) return in_iterable_1_only , in_iterable_2_only","title":"list_differences"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.obj_diagnostics","text":"Log detailed diagnostics about an object's attributes and values. Parameters: obj ( object ) \u2013 The object to inspect. include_hidden ( bool , default: False ) \u2013 Whether to include private attributes (starting with _ ). include_functions ( bool , default: False ) \u2013 Whether to include methods or callable attributes. message ( str | None , default: None ) \u2013 Optional prefix message to label the diagnostic output. Returns: None \u2013 None Example obj_diagnostics(my_object, include_hidden=True, include_functions=True) Source code in arb\\utils\\diagnostics.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def obj_diagnostics ( obj : object , include_hidden : bool = False , include_functions : bool = False , message : str | None = None ) -> None : \"\"\" Log detailed diagnostics about an object's attributes and values. Args: obj (object): The object to inspect. include_hidden (bool): Whether to include private attributes (starting with `_`). include_functions (bool): Whether to include methods or callable attributes. message (str | None): Optional prefix message to label the diagnostic output. Returns: None Example: >>> obj_diagnostics(my_object, include_hidden=True, include_functions=True) \"\"\" logger . debug ( f \"Diagnostics for: { obj } \" ) if message : logger . debug ( f \" { message =} \" ) for attr_name in dir ( obj ): attr_value = getattr ( obj , attr_name ) if not attr_name . startswith ( '_' ) or include_hidden : if callable ( attr_value ): if include_functions : logger . debug ( f \" { attr_name } (): is function\" ) else : logger . debug ( f \" { attr_name } { type ( attr_value ) } : \\t { attr_value } \" )","title":"obj_diagnostics"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.obj_to_html","text":"Convert any Python object to a formatted HTML string for Jinja rendering. Parameters: obj ( object ) \u2013 A Python object suitable for pprint . Returns: str ( str ) \u2013 HTML string wrapped in tags that is safe for use with |safe in templates. Notes The HTML content must be marked |safe in the template to avoid escaping. Example (in Jinja): {% if result is defined %} {{ result|safe }} {% endif %} Source code in arb\\utils\\diagnostics.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def obj_to_html ( obj : object ) -> str : \"\"\" Convert any Python object to a formatted HTML string for Jinja rendering. Args: obj (object): A Python object suitable for `pprint`. Returns: str: HTML string wrapped in <pre> tags that is safe for use with `|safe` in templates. Notes: The HTML content must be marked `|safe` in the template to avoid escaping. Example (in Jinja): {% if result is defined %} {{ result|safe }} {% endif %} \"\"\" pp = pprint . PrettyPrinter ( indent = 4 , width = 200 ) formatted_data = pp . pformat ( obj ) soup = BeautifulSoup ( \"<pre></pre>\" , \"html.parser\" ) soup . pre . string = formatted_data return soup . prettify ()","title":"obj_to_html"},{"location":"reference/arb/utils/diagnostics/#arb.utils.diagnostics.run_diagnostics","text":"Run example-based tests for diagnostic functions in this module. Logs example output for each function. Source code in arb\\utils\\diagnostics.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 def run_diagnostics () -> None : \"\"\" Run example-based tests for diagnostic functions in this module. Logs example output for each function. \"\"\" logger . debug ( \"Running diagnostics on arb.utils.diagnostics module\" ) # obj_diagnostics class TestClass : def __init__ ( self ): self . x = 42 self . y = \"hello\" def greet ( self ): return \"hi\" obj = TestClass () obj_diagnostics ( obj , include_hidden = False , include_functions = True ) # list_differences a = [ \"x\" , \"y\" , \"z\" ] b = [ \"x\" , \"z\" , \"w\" ] list_differences ( a , b , \"List A\" , \"List B\" , print_warning = True ) # diag_recursive diag_recursive ([[ 1 , 2 ], [ 3 , [ 4 , 5 ]]]) # dict_to_str nested_dict = { \"a\" : 1 , \"b\" : { \"c\" : 2 , \"d\" : { \"e\" : 3 }}} logger . debug ( \"dict_to_str output: \\t \" + dict_to_str ( nested_dict )) # obj_to_html html_result = obj_to_html ( nested_dict ) logger . debug ( \"HTML representation of object (truncated): \\t \" + html_result [: 300 ]) # compare_dicts d1 = { \"x\" : 1 , \"y\" : 2 , \"z\" : 3 } d2 = { \"x\" : 1 , \"y\" : 99 , \"w\" : 0 } compare_dicts ( d1 , d2 , \"First Dict\" , \"Second Dict\" )","title":"run_diagnostics"},{"location":"reference/arb/utils/file_io/","text":"arb.utils.file_io Utility functions for file and path handling, including directory creation, secure file name generation, and project root resolution. Features: - Ensures parent and target directories exist - Generates secure, timestamped file names using UTC - Dynamically resolves the project root based on directory structure - Includes diagnostics for local testing and validation Notes: - Uses werkzeug.utils.secure_filename to sanitize input filenames - Timestamps are formatted in UTC using the DATETIME_WITH_SECONDS pattern Potential Future Upgrades: - Add support for Windows-specific path edge cases, if needed. - Expand run_diagnostics to perform write/delete tests in a sandbox directory. ProjectRootNotFoundError Bases: ValueError Raised when no matching project root can be determined from the provided file path and candidate folder sequences. Source code in arb\\utils\\file_io.py 100 101 102 103 104 105 class ProjectRootNotFoundError ( ValueError ): \"\"\" Raised when no matching project root can be determined from the provided file path and candidate folder sequences. \"\"\" pass ensure_dir_exists ( dir_path ) Ensure that the specified directory exists, creating it if necessary. Parameters: dir_path ( str | Path ) \u2013 Path to the directory. Raises: ValueError \u2013 If the path exists but is not a directory. Returns: None \u2013 None Example ensure_dir_exists(\"logs/output\") Source code in arb\\utils\\file_io.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def ensure_dir_exists ( dir_path : str | Path ) -> None : \"\"\" Ensure that the specified directory exists, creating it if necessary. Args: dir_path (str | Path): Path to the directory. Raises: ValueError: If the path exists but is not a directory. Returns: None Example: >>> ensure_dir_exists(\"logs/output\") \"\"\" logger . debug ( f \"ensure_dir_exists() called for: { dir_path =} \" ) dir_path = Path ( dir_path ) if dir_path . exists () and not dir_path . is_dir (): raise ValueError ( f \"The path { dir_path } exists and is not a directory.\" ) dir_path . mkdir ( parents = True , exist_ok = True ) ensure_parent_dirs ( file_name ) Ensure that the parent directories for a given file path exist. Parameters: file_name ( str | Path ) \u2013 The full path to a file. Parent folders will be created if needed. Returns: None \u2013 None Example: >>> ensure_parent_dirs(\"/tmp/some/deep/file.txt\") >>> ensure_parent_dirs(\"local_file.txt\") Source code in arb\\utils\\file_io.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def ensure_parent_dirs ( file_name : str | Path ) -> None : \"\"\" Ensure that the parent directories for a given file path exist. Args: file_name (str | Path): The full path to a file. Parent folders will be created if needed. Returns: None Example: >>> ensure_parent_dirs(\"/tmp/some/deep/file.txt\") >>> ensure_parent_dirs(\"local_file.txt\") \"\"\" logger . debug ( f \"ensure_parent_dirs() called for: { file_name =} \" ) file_path = Path ( file_name ) file_path . parent . mkdir ( parents = True , exist_ok = True ) get_project_root_dir ( file , match_parts ) Traverse up the directory tree from a file path to locate the root of a known structure. Parameters: file ( str | Path ) \u2013 The starting file path, typically __file__ . match_parts ( list [ str ] ) \u2013 Folder names expected in the path, ordered from root to leaf. Returns: Path ( Path ) \u2013 Path to the top of the matched folder chain. Raises: ValueError \u2013 If no matching structure is found in the parent hierarchy. Passing Example If file = \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\" and match_parts = [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"] , then: \u2192 match found at /Users/tony/dev/ feedback_portal /source/production/arb/portal \u2192 returns: Path(\"/Users/tony/dev/feedback_portal\") Failing Example If the file path is unrelated (e.g., \"/tmp/random_file.py\"), the function will raise a ValueError. Source code in arb\\utils\\file_io.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def get_project_root_dir ( file : str | Path , match_parts : list [ str ]) -> Path : \"\"\" Traverse up the directory tree from a file path to locate the root of a known structure. Args: file (str | Path): The starting file path, typically `__file__`. match_parts (list[str]): Folder names expected in the path, ordered from root to leaf. Returns: Path: Path to the top of the matched folder chain. Raises: ValueError: If no matching structure is found in the parent hierarchy. Passing Example: If `file = \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\"` and `match_parts = [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"]`, then: \u2192 match found at /Users/tony/dev/**feedback_portal**/source/production/arb/portal \u2192 returns: Path(\"/Users/tony/dev/feedback_portal\") Failing Example: If the file path is unrelated (e.g., \"/tmp/random_file.py\"), the function will raise a ValueError. \"\"\" path = Path ( file ) . resolve () match_len = len ( match_parts ) current = path while current != current . parent : if list ( current . parts [ - match_len :]) == match_parts : return Path ( * current . parts [: len ( current . parts ) - match_len + 1 ]) current = current . parent raise ValueError ( f \"Could not locate project root using match_parts= { match_parts } from path= { path } \" ) get_secure_timestamped_file_name ( directory , file_name ) Generate a sanitized file name in the given directory, appending a UTC timestamp. Parameters: directory ( str | Path ) \u2013 Target directory where the file will be saved. file_name ( str ) \u2013 Proposed name for the file, possibly unsafe. Returns: Path ( Path ) \u2013 The full secure, timestamped file path. Example get_secure_timestamped_file_name(\"/tmp\", \"user report.xlsx\") Path(\"/home/user/tmp/user_report_ts_2025-05-05T12-30-00Z.xlsx\") Source code in arb\\utils\\file_io.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def get_secure_timestamped_file_name ( directory : str | Path , file_name : str ) -> Path : \"\"\" Generate a sanitized file name in the given directory, appending a UTC timestamp. Args: directory (str | Path): Target directory where the file will be saved. file_name (str): Proposed name for the file, possibly unsafe. Returns: Path: The full secure, timestamped file path. Example: >>> get_secure_timestamped_file_name(\"/tmp\", \"user report.xlsx\") Path(\"/home/user/tmp/user_report_ts_2025-05-05T12-30-00Z.xlsx\") \"\"\" file_name_clean = secure_filename ( file_name ) full_path = Path . home () / directory / file_name_clean timestamp = datetime . now ( ZoneInfo ( \"UTC\" )) . strftime ( DATETIME_WITH_SECONDS ) new_name = f \" { full_path . stem } _ts_ { timestamp }{ full_path . suffix } \" return full_path . with_name ( new_name ) resolve_project_root ( file_path , candidate_structures = None ) Attempt to locate the project root directory using known folder sequences. Parameters: file_path ( str | Path ) \u2013 The file path to begin traversal from (typically __file__ ). candidate_structures ( list [ list [ str ]] | None , default: None ) \u2013 List of folder name sequences to match. Returns: Path ( Path ) \u2013 Path to the root of the matched folder chain. Raises: ProjectRootNotFoundError \u2013 If no matching sequence is found. Example resolve_project_root( file ) Path(\"/Users/tony/dev/feedback_portal\") Source code in arb\\utils\\file_io.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def resolve_project_root ( file_path : str | Path , candidate_structures : list [ list [ str ]] | None = None ) -> Path : \"\"\" Attempt to locate the project root directory using known folder sequences. Args: file_path (str | Path): The file path to begin traversal from (typically `__file__`). candidate_structures (list[list[str]] | None): List of folder name sequences to match. Returns: Path: Path to the root of the matched folder chain. Raises: ProjectRootNotFoundError: If no matching sequence is found. Example: >>> resolve_project_root(__file__) Path(\"/Users/tony/dev/feedback_portal\") \"\"\" if candidate_structures is None : candidate_structures = [ [ 'feedback_portal' , 'source' , 'production' , 'arb' , 'utils' , 'excel' ], [ 'feedback_portal' , 'source' , 'production' , 'arb' , 'portal' ], ] errors = [] for structure in candidate_structures : try : root = get_project_root_dir ( file_path , structure ) logger . debug ( f \" { root =} , based on structure { structure =} \" ) return root except ValueError as e : errors . append ( f \" { structure } : { e } \" ) raise ProjectRootNotFoundError ( \"Unable to determine project root. Tried the following structures: \\n \" + \" \\n \" . join ( errors ) ) run_diagnostics () Run a series of checks to validate directory creation, secure filename generation, and project root resolution logic. Returns: None \u2013 None Source code in arb\\utils\\file_io.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def run_diagnostics () -> None : \"\"\" Run a series of checks to validate directory creation, secure filename generation, and project root resolution logic. Returns: None \"\"\" import tempfile print ( \"Running diagnostics...\" ) # Test ensure_dir_exists test_dir = Path ( tempfile . gettempdir ()) / \"arb_test_nested/subdir\" ensure_dir_exists ( test_dir ) assert test_dir . exists () and test_dir . is_dir () # Test ensure_parent_dirs test_file = test_dir / \"test_file.txt\" ensure_parent_dirs ( test_file ) assert test_file . parent . exists () # Test secure file name generation secured_path = get_secure_timestamped_file_name ( test_dir , \"My Unsafe Report.xlsx\" ) print ( f \"Generated secure file: { secured_path } \" ) assert secured_path . name . startswith ( \"My_Unsafe_Report_ts_\" ) # Test project root resolution try : project_root = resolve_project_root ( __file__ ) print ( f \"Resolved project root: { project_root } \" ) assert project_root . exists () except ProjectRootNotFoundError as e : print ( \"WARNING: Could not resolve project root. Skipping root validation.\" ) print ( \"All diagnostics completed successfully.\" )","title":"arb.utils.file_io"},{"location":"reference/arb/utils/file_io/#arbutilsfile_io","text":"Utility functions for file and path handling, including directory creation, secure file name generation, and project root resolution. Features: - Ensures parent and target directories exist - Generates secure, timestamped file names using UTC - Dynamically resolves the project root based on directory structure - Includes diagnostics for local testing and validation Notes: - Uses werkzeug.utils.secure_filename to sanitize input filenames - Timestamps are formatted in UTC using the DATETIME_WITH_SECONDS pattern Potential Future Upgrades: - Add support for Windows-specific path edge cases, if needed. - Expand run_diagnostics to perform write/delete tests in a sandbox directory.","title":"arb.utils.file_io"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.ProjectRootNotFoundError","text":"Bases: ValueError Raised when no matching project root can be determined from the provided file path and candidate folder sequences. Source code in arb\\utils\\file_io.py 100 101 102 103 104 105 class ProjectRootNotFoundError ( ValueError ): \"\"\" Raised when no matching project root can be determined from the provided file path and candidate folder sequences. \"\"\" pass","title":"ProjectRootNotFoundError"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.ensure_dir_exists","text":"Ensure that the specified directory exists, creating it if necessary. Parameters: dir_path ( str | Path ) \u2013 Path to the directory. Raises: ValueError \u2013 If the path exists but is not a directory. Returns: None \u2013 None Example ensure_dir_exists(\"logs/output\") Source code in arb\\utils\\file_io.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def ensure_dir_exists ( dir_path : str | Path ) -> None : \"\"\" Ensure that the specified directory exists, creating it if necessary. Args: dir_path (str | Path): Path to the directory. Raises: ValueError: If the path exists but is not a directory. Returns: None Example: >>> ensure_dir_exists(\"logs/output\") \"\"\" logger . debug ( f \"ensure_dir_exists() called for: { dir_path =} \" ) dir_path = Path ( dir_path ) if dir_path . exists () and not dir_path . is_dir (): raise ValueError ( f \"The path { dir_path } exists and is not a directory.\" ) dir_path . mkdir ( parents = True , exist_ok = True )","title":"ensure_dir_exists"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.ensure_parent_dirs","text":"Ensure that the parent directories for a given file path exist. Parameters: file_name ( str | Path ) \u2013 The full path to a file. Parent folders will be created if needed. Returns: None \u2013 None Example: >>> ensure_parent_dirs(\"/tmp/some/deep/file.txt\") >>> ensure_parent_dirs(\"local_file.txt\") Source code in arb\\utils\\file_io.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 def ensure_parent_dirs ( file_name : str | Path ) -> None : \"\"\" Ensure that the parent directories for a given file path exist. Args: file_name (str | Path): The full path to a file. Parent folders will be created if needed. Returns: None Example: >>> ensure_parent_dirs(\"/tmp/some/deep/file.txt\") >>> ensure_parent_dirs(\"local_file.txt\") \"\"\" logger . debug ( f \"ensure_parent_dirs() called for: { file_name =} \" ) file_path = Path ( file_name ) file_path . parent . mkdir ( parents = True , exist_ok = True )","title":"ensure_parent_dirs"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.get_project_root_dir","text":"Traverse up the directory tree from a file path to locate the root of a known structure. Parameters: file ( str | Path ) \u2013 The starting file path, typically __file__ . match_parts ( list [ str ] ) \u2013 Folder names expected in the path, ordered from root to leaf. Returns: Path ( Path ) \u2013 Path to the top of the matched folder chain. Raises: ValueError \u2013 If no matching structure is found in the parent hierarchy. Passing Example If file = \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\" and match_parts = [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"] , then: \u2192 match found at /Users/tony/dev/ feedback_portal /source/production/arb/portal \u2192 returns: Path(\"/Users/tony/dev/feedback_portal\") Failing Example If the file path is unrelated (e.g., \"/tmp/random_file.py\"), the function will raise a ValueError. Source code in arb\\utils\\file_io.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def get_project_root_dir ( file : str | Path , match_parts : list [ str ]) -> Path : \"\"\" Traverse up the directory tree from a file path to locate the root of a known structure. Args: file (str | Path): The starting file path, typically `__file__`. match_parts (list[str]): Folder names expected in the path, ordered from root to leaf. Returns: Path: Path to the top of the matched folder chain. Raises: ValueError: If no matching structure is found in the parent hierarchy. Passing Example: If `file = \"/Users/tony/dev/feedback_portal/source/production/arb/portal/config.py\"` and `match_parts = [\"feedback_portal\", \"source\", \"production\", \"arb\", \"portal\"]`, then: \u2192 match found at /Users/tony/dev/**feedback_portal**/source/production/arb/portal \u2192 returns: Path(\"/Users/tony/dev/feedback_portal\") Failing Example: If the file path is unrelated (e.g., \"/tmp/random_file.py\"), the function will raise a ValueError. \"\"\" path = Path ( file ) . resolve () match_len = len ( match_parts ) current = path while current != current . parent : if list ( current . parts [ - match_len :]) == match_parts : return Path ( * current . parts [: len ( current . parts ) - match_len + 1 ]) current = current . parent raise ValueError ( f \"Could not locate project root using match_parts= { match_parts } from path= { path } \" )","title":"get_project_root_dir"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.get_secure_timestamped_file_name","text":"Generate a sanitized file name in the given directory, appending a UTC timestamp. Parameters: directory ( str | Path ) \u2013 Target directory where the file will be saved. file_name ( str ) \u2013 Proposed name for the file, possibly unsafe. Returns: Path ( Path ) \u2013 The full secure, timestamped file path. Example get_secure_timestamped_file_name(\"/tmp\", \"user report.xlsx\") Path(\"/home/user/tmp/user_report_ts_2025-05-05T12-30-00Z.xlsx\") Source code in arb\\utils\\file_io.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def get_secure_timestamped_file_name ( directory : str | Path , file_name : str ) -> Path : \"\"\" Generate a sanitized file name in the given directory, appending a UTC timestamp. Args: directory (str | Path): Target directory where the file will be saved. file_name (str): Proposed name for the file, possibly unsafe. Returns: Path: The full secure, timestamped file path. Example: >>> get_secure_timestamped_file_name(\"/tmp\", \"user report.xlsx\") Path(\"/home/user/tmp/user_report_ts_2025-05-05T12-30-00Z.xlsx\") \"\"\" file_name_clean = secure_filename ( file_name ) full_path = Path . home () / directory / file_name_clean timestamp = datetime . now ( ZoneInfo ( \"UTC\" )) . strftime ( DATETIME_WITH_SECONDS ) new_name = f \" { full_path . stem } _ts_ { timestamp }{ full_path . suffix } \" return full_path . with_name ( new_name )","title":"get_secure_timestamped_file_name"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.resolve_project_root","text":"Attempt to locate the project root directory using known folder sequences. Parameters: file_path ( str | Path ) \u2013 The file path to begin traversal from (typically __file__ ). candidate_structures ( list [ list [ str ]] | None , default: None ) \u2013 List of folder name sequences to match. Returns: Path ( Path ) \u2013 Path to the root of the matched folder chain. Raises: ProjectRootNotFoundError \u2013 If no matching sequence is found. Example resolve_project_root( file ) Path(\"/Users/tony/dev/feedback_portal\") Source code in arb\\utils\\file_io.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def resolve_project_root ( file_path : str | Path , candidate_structures : list [ list [ str ]] | None = None ) -> Path : \"\"\" Attempt to locate the project root directory using known folder sequences. Args: file_path (str | Path): The file path to begin traversal from (typically `__file__`). candidate_structures (list[list[str]] | None): List of folder name sequences to match. Returns: Path: Path to the root of the matched folder chain. Raises: ProjectRootNotFoundError: If no matching sequence is found. Example: >>> resolve_project_root(__file__) Path(\"/Users/tony/dev/feedback_portal\") \"\"\" if candidate_structures is None : candidate_structures = [ [ 'feedback_portal' , 'source' , 'production' , 'arb' , 'utils' , 'excel' ], [ 'feedback_portal' , 'source' , 'production' , 'arb' , 'portal' ], ] errors = [] for structure in candidate_structures : try : root = get_project_root_dir ( file_path , structure ) logger . debug ( f \" { root =} , based on structure { structure =} \" ) return root except ValueError as e : errors . append ( f \" { structure } : { e } \" ) raise ProjectRootNotFoundError ( \"Unable to determine project root. Tried the following structures: \\n \" + \" \\n \" . join ( errors ) )","title":"resolve_project_root"},{"location":"reference/arb/utils/file_io/#arb.utils.file_io.run_diagnostics","text":"Run a series of checks to validate directory creation, secure filename generation, and project root resolution logic. Returns: None \u2013 None Source code in arb\\utils\\file_io.py 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def run_diagnostics () -> None : \"\"\" Run a series of checks to validate directory creation, secure filename generation, and project root resolution logic. Returns: None \"\"\" import tempfile print ( \"Running diagnostics...\" ) # Test ensure_dir_exists test_dir = Path ( tempfile . gettempdir ()) / \"arb_test_nested/subdir\" ensure_dir_exists ( test_dir ) assert test_dir . exists () and test_dir . is_dir () # Test ensure_parent_dirs test_file = test_dir / \"test_file.txt\" ensure_parent_dirs ( test_file ) assert test_file . parent . exists () # Test secure file name generation secured_path = get_secure_timestamped_file_name ( test_dir , \"My Unsafe Report.xlsx\" ) print ( f \"Generated secure file: { secured_path } \" ) assert secured_path . name . startswith ( \"My_Unsafe_Report_ts_\" ) # Test project root resolution try : project_root = resolve_project_root ( __file__ ) print ( f \"Resolved project root: { project_root } \" ) assert project_root . exists () except ProjectRootNotFoundError as e : print ( \"WARNING: Could not resolve project root. Skipping root validation.\" ) print ( \"All diagnostics completed successfully.\" )","title":"run_diagnostics"},{"location":"reference/arb/utils/json/","text":"arb.utils.json Module for JSON-related utility functions and classes. Includes Custom serialization for datetime and decimal objects Metadata support for enhanced JSON files File-based diagnostics and JSON comparison utilities WTForms integration for form data extraction and casting Version 1.0.0 Notes Designed for structured JSON handling across ARB portal utilities. Emphasizes ISO 8601 datetime formats and consistent value type casting. Supports \"Pacific Time naive\" conversion via ZoneInfo-aware logic. add_metadata_to_json ( file_name_in , file_name_out = None ) Add metadata to an existing JSON file or overwrite it in-place. Parameters: file_name_in ( str | Path ) \u2013 Input JSON file path. file_name_out ( str | Path | None , default: None ) \u2013 Output file path. If None, overwrites input. Example: >>> add_metadata_to_json(\"schema.json\") Source code in arb\\utils\\json.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def add_metadata_to_json ( file_name_in : str | pathlib . Path , file_name_out : str | pathlib . Path | None = None ) -> None : \"\"\" Add metadata to an existing JSON file or overwrite it in-place. Args: file_name_in (str | Path): Input JSON file path. file_name_out (str | Path | None): Output file path. If None, overwrites input. Example: >>> add_metadata_to_json(\"schema.json\") \"\"\" logger . debug ( f \"add_metadata_to_json() called with { file_name_in =} , { file_name_out =} \" ) if file_name_out is None : file_name_out = file_name_in data = json_load ( file_name_in ) json_save_with_meta ( file_name_out , data = data ) cast_model_value ( value , value_type , convert_time_to_ca = False ) Cast a stringified JSON value into a Python object of the expected type. Parameters: value ( str ) \u2013 Input value to cast. value_type ( type ) \u2013 Python type to cast to. convert_time_to_ca ( bool , default: False ) \u2013 If True, convert UTC to California naive datetime. Returns: object ( object ) \u2013 Value converted to the target Python type. Raises: ValueError \u2013 If the value cannot be cast to the given type. Source code in arb\\utils\\json.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def cast_model_value ( value : str , value_type : type , convert_time_to_ca : bool = False ) -> object : \"\"\" Cast a stringified JSON value into a Python object of the expected type. Args: value (str): Input value to cast. value_type (type): Python type to cast to. convert_time_to_ca (bool): If True, convert UTC to California naive datetime. Returns: object: Value converted to the target Python type. Raises: ValueError: If the value cannot be cast to the given type. \"\"\" try : if value_type == str : # No need to cast a string return value elif value_type in [ bool , int , float ]: return value_type ( value ) elif value_type == datetime . datetime : dt = iso8601_to_utc_dt ( value ) return datetime_to_ca_naive ( dt ) if convert_time_to_ca else dt elif value_type == decimal . Decimal : return decimal . Decimal ( value ) else : raise ValueError ( f \"Unsupported type for casting: { value_type } \" ) except Exception as e : raise ValueError ( f \"Failed to cast { value !r} to { value_type } : { e } \" ) compare_json_files ( file_name_1 , file_name_2 ) Compare the contents of two JSON files including metadata and values. Parameters: file_name_1 ( str | Path ) \u2013 Path to the first file. file_name_2 ( str | Path ) \u2013 Path to the second file. Logs Differences or matches are logged at debug level. Example compare_json_files(\"old.json\", \"new.json\") Source code in arb\\utils\\json.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def compare_json_files ( file_name_1 : str | pathlib . Path , file_name_2 : str | pathlib . Path ) -> None : \"\"\" Compare the contents of two JSON files including metadata and values. Args: file_name_1 (str | Path): Path to the first file. file_name_2 (str | Path): Path to the second file. Logs: Differences or matches are logged at debug level. Example: >>> compare_json_files(\"old.json\", \"new.json\") \"\"\" logger . debug ( f \"compare_json_files() comparing { file_name_1 } and { file_name_2 } \" ) data_1 , meta_1 = json_load_with_meta ( file_name_1 ) data_2 , meta_2 = json_load_with_meta ( file_name_2 ) logger . debug ( \"Comparing metadata\" ) if compare_dicts ( meta_1 , meta_2 , \"metadata_01\" , \"metadata_02\" ) is True : logger . debug ( \"Metadata are equivalent\" ) else : logger . debug ( \"Metadata differ\" ) logger . debug ( \"Comparing data\" ) if compare_dicts ( data_1 , data_2 , \"data_01\" , \"data_02\" ) is True : logger . debug ( \"Data are equivalent\" ) else : logger . debug ( \"Data differ\" ) deserialize_dict ( input_dict , type_map , convert_time_to_ca = False ) Parameters: input_dict ( dict ) \u2013 Dictionary of raw values. type_map ( dict [ str , type ] ) \u2013 Field-to-type mapping for deserialization. convert_time_to_ca ( bool , default: False ) \u2013 If True, converts datetime to CA time. Returns: dict ( dict ) \u2013 Fully deserialized dictionary. Raises: TypeError \u2013 If any key is not a string. ValueError \u2013 If value casting fails for a key. Source code in arb\\utils\\json.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 def deserialize_dict ( input_dict : dict , type_map : dict [ str , type ], convert_time_to_ca : bool = False ) -> dict : \"\"\" Args: input_dict (dict): Dictionary of raw values. type_map (dict[str, type]): Field-to-type mapping for deserialization. convert_time_to_ca (bool): If True, converts datetime to CA time. Returns: dict: Fully deserialized dictionary. Raises: TypeError: If any key is not a string. ValueError: If value casting fails for a key. \"\"\" result = {} for key , value in input_dict . items (): if not isinstance ( key , str ): raise TypeError ( f \"All keys must be strings. Invalid key: { key } ( { type ( key ) } )\" ) if key in type_map and value is not None : result [ key ] = cast_model_value ( value , type_map [ key ], convert_time_to_ca ) else : result [ key ] = value return result json_deserializer ( obj ) Custom JSON deserializer for class/type representations created by json_serializer . Parameters: obj ( dict ) \u2013 Dictionary object from JSON with special tags for known types. Returns: object ( object ) \u2013 Reconstructed Python object. Example json.loads(json_string, object_hook=json_deserializer) Source code in arb\\utils\\json.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def json_deserializer ( obj : dict ) -> object : \"\"\" Custom JSON deserializer for class/type representations created by `json_serializer`. Args: obj (dict): Dictionary object from JSON with special tags for known types. Returns: object: Reconstructed Python object. Example: >>> json.loads(json_string, object_hook=json_deserializer) \"\"\" new_obj = obj if \"__class__\" in obj : # logger.debug(f\"{obj['__class__']=} detected in object deserializer.\") if obj [ \"__class__\" ] == \"str\" : new_obj = str elif obj [ \"__class__\" ] == \"int\" : new_obj = int elif obj [ \"__class__\" ] == \"float\" : new_obj = float elif obj [ \"__class__\" ] == \"bool\" : new_obj = bool elif obj [ \"__class__\" ] == \"datetime\" : new_obj = datetime . datetime else : raise TypeError ( f \"Object of type { type ( obj ) . __name__ } is not JSON deserializable\" ) elif \"__type__\" in obj : type_tag = obj [ \"__type__\" ] if type_tag == \"datetime.datetime\" : new_obj = datetime . datetime . fromisoformat ( obj [ \"value\" ]) elif type_tag == \"decimal.Decimal\" : new_obj = decimal . Decimal ( obj [ \"value\" ]) else : logger . debug ( f \"No known conversion type for type { obj [ '__type__' ] } \" ) # logger.debug(f\"deserializer() returning type= {type(new_obj)}, new_obj= {new_obj}\") return new_obj json_load ( file_path , json_options = None ) Load and deserialize data from a JSON file. Parameters: file_path ( str | Path ) \u2013 Path to the JSON file. json_options ( dict | None , default: None ) \u2013 Optional options passed to json.load . Returns: object ( object ) \u2013 Deserialized Python object. Example: >>> json_load(\"data.json\") Notes encoding=\"utf-8-sig\" will remove (if present) [BOM (Byte Order Mark)] for UTF-8 (\ufeff) if it appears as a special marker at the beginning of some UTF-8 encoded files. This marker can cause JSON decoding errors if not handled properly. Source code in arb\\utils\\json.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def json_load ( file_path : str | pathlib . Path , json_options : dict | None = None ) -> object : \"\"\" Load and deserialize data from a JSON file. Args: file_path (str | Path): Path to the JSON file. json_options (dict | None): Optional options passed to `json.load`. Returns: object: Deserialized Python object. Example: >>> json_load(\"data.json\") Notes: - encoding=\"utf-8-sig\" will remove (if present) [BOM (Byte Order Mark)] for UTF-8 (\\uFEFF) if it appears as a special marker at the beginning of some UTF-8 encoded files. This marker can cause JSON decoding errors if not handled properly. \"\"\" logger . debug ( f \"json_load() called with { file_path =} , { json_options =} \" ) if json_options is None : json_options = { \"object_hook\" : json_deserializer } with open ( file_path , \"r\" , encoding = \"utf-8-sig\" ) as f : return json . load ( f , ** json_options ) json_load_with_meta ( file_path , json_options = None ) Load a JSON file and return both data and metadata if present. Parameters: file_path ( str | Path ) \u2013 Path to the JSON file. json_options ( dict | None , default: None ) \u2013 Optional options passed to json.load . Returns: tuple ( tuple [ object , dict ] ) \u2013 object: Deserialized data in \" data \" (or full file if not present). dict: Deserialized Metadata in \" metadata \" (or empty if not present). Example data, meta = json_load_with_meta(\"example.json\") Notes If the json file is a dictionary with a key data , then the data and metadata (if possible) will be extracted. Otherwise, the data is assumed to be the whole JSON file with no metadata. Source code in arb\\utils\\json.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def json_load_with_meta ( file_path : str | pathlib . Path , json_options : dict | None = None ) -> tuple [ object , dict ]: \"\"\" Load a JSON file and return both data and metadata if present. Args: file_path (str | Path): Path to the JSON file. json_options (dict | None): Optional options passed to `json.load`. Returns: tuple: - object: Deserialized data in \"_data_\" (or full file if not present). - dict: Deserialized Metadata in \"_metadata_\" (or empty if not present). Example: >>> data, meta = json_load_with_meta(\"example.json\") Notes: If the json file is a dictionary with a key _data_, then the _data_ and _metadata_ (if possible) will be extracted. Otherwise, the data is assumed to be the whole JSON file with no metadata. \"\"\" logger . debug ( f \"json_load_with_meta() called with { file_path =} , { json_options =} \" ) all_data = json_load ( file_path , json_options = json_options ) if isinstance ( all_data , dict ) and \"_data_\" in all_data : return all_data [ \"_data_\" ], all_data . get ( \"_metadata_\" , {}) return all_data , {} json_save ( file_path , data , json_options = None ) Save a Python object to a JSON file with optional serialization settings. Parameters: file_path ( str | Path ) \u2013 Path to write the JSON file. data ( object ) \u2013 Data to serialize and write. json_options ( dict | None , default: None ) \u2013 Options to pass to json.dump . Example: >>> json_save(\"output.json\", {\"x\": decimal.Decimal(\"1.23\")}) Source code in arb\\utils\\json.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def json_save ( file_path : str | pathlib . Path , data : object , json_options : dict | None = None ) -> None : \"\"\" Save a Python object to a JSON file with optional serialization settings. Args: file_path (str | Path): Path to write the JSON file. data (object): Data to serialize and write. json_options (dict | None): Options to pass to `json.dump`. Example: >>> json_save(\"output.json\", {\"x\": decimal.Decimal(\"1.23\")}) \"\"\" logger . debug ( f \"json_save() called with { file_path =} , { json_options =} , { data =} \" ) if json_options is None : json_options = { \"default\" : json_serializer , \"indent\" : 4 } with open ( file_path , \"w\" , encoding = \"utf-8\" ) as f : json . dump ( data , f , ** json_options ) logger . debug ( f \"JSON saved to file: ' { file_path } '.\" ) json_save_with_meta ( file_path , data , metadata = None , json_options = None ) Save data with metadata to a JSON file under special keys. Parameters: file_path ( str | Path ) \u2013 Output JSON file path. data ( object ) \u2013 Primary data to store under \" data \". metadata ( dict | None , default: None ) \u2013 Optional metadata under \" metadata \". json_options ( dict | None , default: None ) \u2013 Options for json.dump . Example: >>> json_save_with_meta(\"log.json\", {\"key\": \"value\"}, {\"source\": \"generated\"}) Source code in arb\\utils\\json.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def json_save_with_meta ( file_path : str | pathlib . Path , data : object , metadata : dict | None = None , json_options : dict | None = None ) -> None : \"\"\" Save data with metadata to a JSON file under special keys. Args: file_path (str | Path): Output JSON file path. data (object): Primary data to store under \"_data_\". metadata (dict | None): Optional metadata under \"_metadata_\". json_options (dict | None): Options for `json.dump`. Example: >>> json_save_with_meta(\"log.json\", {\"key\": \"value\"}, {\"source\": \"generated\"}) \"\"\" logger . debug ( f \"json_save_with_meta() called with { file_path =} , { json_options =} , { metadata =} , { data =} \" ) if metadata is None : metadata = {} metadata . update ({ \"File created at\" : datetime . datetime . now ( ZoneInfo ( \"UTC\" )) . isoformat (), \"Serialized with\" : \"utils.json.json_save_with_meta\" , \"Deserialize with\" : \"utils.json.json_load_with_meta\" , }) wrapped = { \"_metadata_\" : metadata , \"_data_\" : data , } json_save ( file_path , wrapped , json_options = json_options ) json_serializer ( obj ) Custom JSON serializer for objects not natively serializable by json.dump . Parameters: obj ( object ) \u2013 The object to serialize. Returns: dict ( dict ) \u2013 A JSON-compatible dictionary representation of the object. Raises: TypeError \u2013 If the object type is unsupported. Example json.dumps(datetime.datetime.now(), default=json_serializer) Source code in arb\\utils\\json.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def json_serializer ( obj : object ) -> dict : \"\"\" Custom JSON serializer for objects not natively serializable by `json.dump`. Args: obj (object): The object to serialize. Returns: dict: A JSON-compatible dictionary representation of the object. Raises: TypeError: If the object type is unsupported. Example: >>> json.dumps(datetime.datetime.now(), default=json_serializer) \"\"\" if isinstance ( obj , type ): return { \"__class__\" : obj . __name__ , \"__module__\" : obj . __module__ } elif isinstance ( obj , datetime . datetime ): return { \"__type__\" : \"datetime.datetime\" , \"value\" : obj . isoformat ()} elif isinstance ( obj , decimal . Decimal ): return { \"__type__\" : \"decimal.Decimal\" , \"value\" : str ( obj )} raise TypeError ( f \"Object of type { type ( obj ) . __name__ } is not JSON serializable\" ) make_dict_serializeable ( input_dict , type_map = None , convert_time_to_ca = False ) Transform a dictionary to ensure JSON compatibility of its values. Parameters: input_dict ( dict ) \u2013 Original dictionary to process. type_map ( dict [ str , type ] | None , default: None ) \u2013 Optional field-to-type map for casting. convert_time_to_ca ( bool , default: False ) \u2013 Convert datetimes to CA time before serialization. Returns: dict ( dict ) \u2013 A dictionary with only JSON-serializable values. Raises: TypeError \u2013 If any key is not a string. ValueError \u2013 If value cannot be cast to expected type. Source code in arb\\utils\\json.py 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def make_dict_serializeable ( input_dict : dict , type_map : dict [ str , type ] | None = None , convert_time_to_ca : bool = False ) -> dict : \"\"\" Transform a dictionary to ensure JSON compatibility of its values. Args: input_dict (dict): Original dictionary to process. type_map (dict[str, type] | None): Optional field-to-type map for casting. convert_time_to_ca (bool): Convert datetimes to CA time before serialization. Returns: dict: A dictionary with only JSON-serializable values. Raises: TypeError: If any key is not a string. ValueError: If value cannot be cast to expected type. \"\"\" result = {} for key , value in input_dict . items (): if not isinstance ( key , str ): raise TypeError ( f \"All keys must be strings. Invalid key: { key } ( { type ( key ) } )\" ) if type_map and key in type_map : try : value = safe_cast ( value , type_map [ key ]) except Exception as e : raise ValueError ( f \"Failed to cast key ' { key } ' to { type_map [ key ] } : { e } \" ) if isinstance ( value , datetime . datetime ): if convert_time_to_ca : value = ca_naive_to_utc_datetime ( value ) value = value . isoformat () elif isinstance ( value , decimal . Decimal ): value = float ( value ) result [ key ] = value return result run_diagnostics () Run internal validation for all JSON utilities. Tests Custom serializer/deserializer JSON saving and loading (with and without metadata) Metadata updating File comparison Raises: Exception \u2013 If any test fails. Source code in arb\\utils\\json.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 def run_diagnostics () -> None : \"\"\" Run internal validation for all JSON utilities. Tests: - Custom serializer/deserializer - JSON saving and loading (with and without metadata) - Metadata updating - File comparison Raises: Exception: If any test fails. \"\"\" import tempfile import shutil print ( \"Running diagnostics for JSON utilities...\" ) temp_dir = pathlib . Path ( tempfile . gettempdir ()) / \"json_utils_test\" if temp_dir . exists (): shutil . rmtree ( temp_dir ) temp_dir . mkdir ( parents = True ) try : # Test data data = { \"decimal\" : decimal . Decimal ( \"123.45\" ), \"timestamp\" : datetime . datetime ( 2025 , 5 , 5 , 13 , 30 , 0 , tzinfo = ZoneInfo ( \"UTC\" )), \"nested\" : { \"a\" : 1 , \"b\" : 2 }, } # File paths json_file_1 = temp_dir / \"test_1.json\" json_file_2 = temp_dir / \"test_2.json\" plain_file = temp_dir / \"plain.json\" # Save and load using json_save/json_load json_save ( json_file_1 , data ) loaded_1 = json_load ( json_file_1 ) assert loaded_1 == data , \"Basic save/load failed\" # Save with metadata and reload meta_info = { \"note\" : \"test file\" } json_save_with_meta ( json_file_2 , data , metadata = meta_info ) loaded_data , loaded_meta = json_load_with_meta ( json_file_2 ) assert loaded_data == data , \"Data mismatch in metadata test\" assert \"note\" in loaded_meta , \"Metadata not found\" # Write plain file, with serializer included, then enrich with metadata with open ( plain_file , \"w\" , encoding = \"utf-8\" ) as f : json . dump ( data , f , indent = 2 , default = json_serializer ) add_metadata_to_json ( plain_file ) # Compare metadata-enriched files compare_json_files ( json_file_2 , plain_file ) print ( \"All diagnostics completed successfully.\" ) except Exception as e : print ( f \"Diagnostics failed: { e } \" ) raise wtform_types_and_values ( wtform ) Extract field types and current data values from a WTForm. Parameters: wtform ( FlaskForm ) \u2013 WTForms instance. Returns: tuple ( tuple [ dict [ str , type ], dict [ str , object ]] ) \u2013 dict[str, type]: Field name to type mapping for deserialization. dict[str, object]: Field name to current value mapping (may include 'Please Select'). Source code in arb\\utils\\json.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def wtform_types_and_values ( wtform ) -> tuple [ dict [ str , type ], dict [ str , object ]]: \"\"\" Extract field types and current data values from a WTForm. Args: wtform (FlaskForm): WTForms instance. Returns: tuple: - dict[str, type]: Field name to type mapping for deserialization. - dict[str, object]: Field name to current value mapping (may include 'Please Select'). \"\"\" type_map = {} field_data = {} for name , field in wtform . _fields . items (): value = field . data field_data [ name ] = value # Identify complex field types for type mapping if isinstance ( field , DateTimeField ): type_map [ name ] = datetime . datetime elif isinstance ( field , DecimalField ): type_map [ name ] = decimal . Decimal elif isinstance ( field , BooleanField ): type_map [ name ] = bool elif isinstance ( field , IntegerField ): type_map [ name ] = int elif isinstance ( field , SelectField ): type_map [ name ] = str # 'Please Select' is valid return type_map , field_data","title":"arb.utils.json"},{"location":"reference/arb/utils/json/#arbutilsjson","text":"Module for JSON-related utility functions and classes. Includes Custom serialization for datetime and decimal objects Metadata support for enhanced JSON files File-based diagnostics and JSON comparison utilities WTForms integration for form data extraction and casting Version 1.0.0 Notes Designed for structured JSON handling across ARB portal utilities. Emphasizes ISO 8601 datetime formats and consistent value type casting. Supports \"Pacific Time naive\" conversion via ZoneInfo-aware logic.","title":"arb.utils.json"},{"location":"reference/arb/utils/json/#arb.utils.json.add_metadata_to_json","text":"Add metadata to an existing JSON file or overwrite it in-place. Parameters: file_name_in ( str | Path ) \u2013 Input JSON file path. file_name_out ( str | Path | None , default: None ) \u2013 Output file path. If None, overwrites input. Example: >>> add_metadata_to_json(\"schema.json\") Source code in arb\\utils\\json.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 def add_metadata_to_json ( file_name_in : str | pathlib . Path , file_name_out : str | pathlib . Path | None = None ) -> None : \"\"\" Add metadata to an existing JSON file or overwrite it in-place. Args: file_name_in (str | Path): Input JSON file path. file_name_out (str | Path | None): Output file path. If None, overwrites input. Example: >>> add_metadata_to_json(\"schema.json\") \"\"\" logger . debug ( f \"add_metadata_to_json() called with { file_name_in =} , { file_name_out =} \" ) if file_name_out is None : file_name_out = file_name_in data = json_load ( file_name_in ) json_save_with_meta ( file_name_out , data = data )","title":"add_metadata_to_json"},{"location":"reference/arb/utils/json/#arb.utils.json.cast_model_value","text":"Cast a stringified JSON value into a Python object of the expected type. Parameters: value ( str ) \u2013 Input value to cast. value_type ( type ) \u2013 Python type to cast to. convert_time_to_ca ( bool , default: False ) \u2013 If True, convert UTC to California naive datetime. Returns: object ( object ) \u2013 Value converted to the target Python type. Raises: ValueError \u2013 If the value cannot be cast to the given type. Source code in arb\\utils\\json.py 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 def cast_model_value ( value : str , value_type : type , convert_time_to_ca : bool = False ) -> object : \"\"\" Cast a stringified JSON value into a Python object of the expected type. Args: value (str): Input value to cast. value_type (type): Python type to cast to. convert_time_to_ca (bool): If True, convert UTC to California naive datetime. Returns: object: Value converted to the target Python type. Raises: ValueError: If the value cannot be cast to the given type. \"\"\" try : if value_type == str : # No need to cast a string return value elif value_type in [ bool , int , float ]: return value_type ( value ) elif value_type == datetime . datetime : dt = iso8601_to_utc_dt ( value ) return datetime_to_ca_naive ( dt ) if convert_time_to_ca else dt elif value_type == decimal . Decimal : return decimal . Decimal ( value ) else : raise ValueError ( f \"Unsupported type for casting: { value_type } \" ) except Exception as e : raise ValueError ( f \"Failed to cast { value !r} to { value_type } : { e } \" )","title":"cast_model_value"},{"location":"reference/arb/utils/json/#arb.utils.json.compare_json_files","text":"Compare the contents of two JSON files including metadata and values. Parameters: file_name_1 ( str | Path ) \u2013 Path to the first file. file_name_2 ( str | Path ) \u2013 Path to the second file. Logs Differences or matches are logged at debug level. Example compare_json_files(\"old.json\", \"new.json\") Source code in arb\\utils\\json.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 def compare_json_files ( file_name_1 : str | pathlib . Path , file_name_2 : str | pathlib . Path ) -> None : \"\"\" Compare the contents of two JSON files including metadata and values. Args: file_name_1 (str | Path): Path to the first file. file_name_2 (str | Path): Path to the second file. Logs: Differences or matches are logged at debug level. Example: >>> compare_json_files(\"old.json\", \"new.json\") \"\"\" logger . debug ( f \"compare_json_files() comparing { file_name_1 } and { file_name_2 } \" ) data_1 , meta_1 = json_load_with_meta ( file_name_1 ) data_2 , meta_2 = json_load_with_meta ( file_name_2 ) logger . debug ( \"Comparing metadata\" ) if compare_dicts ( meta_1 , meta_2 , \"metadata_01\" , \"metadata_02\" ) is True : logger . debug ( \"Metadata are equivalent\" ) else : logger . debug ( \"Metadata differ\" ) logger . debug ( \"Comparing data\" ) if compare_dicts ( data_1 , data_2 , \"data_01\" , \"data_02\" ) is True : logger . debug ( \"Data are equivalent\" ) else : logger . debug ( \"Data differ\" )","title":"compare_json_files"},{"location":"reference/arb/utils/json/#arb.utils.json.deserialize_dict","text":"Parameters: input_dict ( dict ) \u2013 Dictionary of raw values. type_map ( dict [ str , type ] ) \u2013 Field-to-type mapping for deserialization. convert_time_to_ca ( bool , default: False ) \u2013 If True, converts datetime to CA time. Returns: dict ( dict ) \u2013 Fully deserialized dictionary. Raises: TypeError \u2013 If any key is not a string. ValueError \u2013 If value casting fails for a key. Source code in arb\\utils\\json.py 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 def deserialize_dict ( input_dict : dict , type_map : dict [ str , type ], convert_time_to_ca : bool = False ) -> dict : \"\"\" Args: input_dict (dict): Dictionary of raw values. type_map (dict[str, type]): Field-to-type mapping for deserialization. convert_time_to_ca (bool): If True, converts datetime to CA time. Returns: dict: Fully deserialized dictionary. Raises: TypeError: If any key is not a string. ValueError: If value casting fails for a key. \"\"\" result = {} for key , value in input_dict . items (): if not isinstance ( key , str ): raise TypeError ( f \"All keys must be strings. Invalid key: { key } ( { type ( key ) } )\" ) if key in type_map and value is not None : result [ key ] = cast_model_value ( value , type_map [ key ], convert_time_to_ca ) else : result [ key ] = value return result","title":"deserialize_dict"},{"location":"reference/arb/utils/json/#arb.utils.json.json_deserializer","text":"Custom JSON deserializer for class/type representations created by json_serializer . Parameters: obj ( dict ) \u2013 Dictionary object from JSON with special tags for known types. Returns: object ( object ) \u2013 Reconstructed Python object. Example json.loads(json_string, object_hook=json_deserializer) Source code in arb\\utils\\json.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def json_deserializer ( obj : dict ) -> object : \"\"\" Custom JSON deserializer for class/type representations created by `json_serializer`. Args: obj (dict): Dictionary object from JSON with special tags for known types. Returns: object: Reconstructed Python object. Example: >>> json.loads(json_string, object_hook=json_deserializer) \"\"\" new_obj = obj if \"__class__\" in obj : # logger.debug(f\"{obj['__class__']=} detected in object deserializer.\") if obj [ \"__class__\" ] == \"str\" : new_obj = str elif obj [ \"__class__\" ] == \"int\" : new_obj = int elif obj [ \"__class__\" ] == \"float\" : new_obj = float elif obj [ \"__class__\" ] == \"bool\" : new_obj = bool elif obj [ \"__class__\" ] == \"datetime\" : new_obj = datetime . datetime else : raise TypeError ( f \"Object of type { type ( obj ) . __name__ } is not JSON deserializable\" ) elif \"__type__\" in obj : type_tag = obj [ \"__type__\" ] if type_tag == \"datetime.datetime\" : new_obj = datetime . datetime . fromisoformat ( obj [ \"value\" ]) elif type_tag == \"decimal.Decimal\" : new_obj = decimal . Decimal ( obj [ \"value\" ]) else : logger . debug ( f \"No known conversion type for type { obj [ '__type__' ] } \" ) # logger.debug(f\"deserializer() returning type= {type(new_obj)}, new_obj= {new_obj}\") return new_obj","title":"json_deserializer"},{"location":"reference/arb/utils/json/#arb.utils.json.json_load","text":"Load and deserialize data from a JSON file. Parameters: file_path ( str | Path ) \u2013 Path to the JSON file. json_options ( dict | None , default: None ) \u2013 Optional options passed to json.load . Returns: object ( object ) \u2013 Deserialized Python object. Example: >>> json_load(\"data.json\") Notes encoding=\"utf-8-sig\" will remove (if present) [BOM (Byte Order Mark)] for UTF-8 (\ufeff) if it appears as a special marker at the beginning of some UTF-8 encoded files. This marker can cause JSON decoding errors if not handled properly. Source code in arb\\utils\\json.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def json_load ( file_path : str | pathlib . Path , json_options : dict | None = None ) -> object : \"\"\" Load and deserialize data from a JSON file. Args: file_path (str | Path): Path to the JSON file. json_options (dict | None): Optional options passed to `json.load`. Returns: object: Deserialized Python object. Example: >>> json_load(\"data.json\") Notes: - encoding=\"utf-8-sig\" will remove (if present) [BOM (Byte Order Mark)] for UTF-8 (\\uFEFF) if it appears as a special marker at the beginning of some UTF-8 encoded files. This marker can cause JSON decoding errors if not handled properly. \"\"\" logger . debug ( f \"json_load() called with { file_path =} , { json_options =} \" ) if json_options is None : json_options = { \"object_hook\" : json_deserializer } with open ( file_path , \"r\" , encoding = \"utf-8-sig\" ) as f : return json . load ( f , ** json_options )","title":"json_load"},{"location":"reference/arb/utils/json/#arb.utils.json.json_load_with_meta","text":"Load a JSON file and return both data and metadata if present. Parameters: file_path ( str | Path ) \u2013 Path to the JSON file. json_options ( dict | None , default: None ) \u2013 Optional options passed to json.load . Returns: tuple ( tuple [ object , dict ] ) \u2013 object: Deserialized data in \" data \" (or full file if not present). dict: Deserialized Metadata in \" metadata \" (or empty if not present). Example data, meta = json_load_with_meta(\"example.json\") Notes If the json file is a dictionary with a key data , then the data and metadata (if possible) will be extracted. Otherwise, the data is assumed to be the whole JSON file with no metadata. Source code in arb\\utils\\json.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 def json_load_with_meta ( file_path : str | pathlib . Path , json_options : dict | None = None ) -> tuple [ object , dict ]: \"\"\" Load a JSON file and return both data and metadata if present. Args: file_path (str | Path): Path to the JSON file. json_options (dict | None): Optional options passed to `json.load`. Returns: tuple: - object: Deserialized data in \"_data_\" (or full file if not present). - dict: Deserialized Metadata in \"_metadata_\" (or empty if not present). Example: >>> data, meta = json_load_with_meta(\"example.json\") Notes: If the json file is a dictionary with a key _data_, then the _data_ and _metadata_ (if possible) will be extracted. Otherwise, the data is assumed to be the whole JSON file with no metadata. \"\"\" logger . debug ( f \"json_load_with_meta() called with { file_path =} , { json_options =} \" ) all_data = json_load ( file_path , json_options = json_options ) if isinstance ( all_data , dict ) and \"_data_\" in all_data : return all_data [ \"_data_\" ], all_data . get ( \"_metadata_\" , {}) return all_data , {}","title":"json_load_with_meta"},{"location":"reference/arb/utils/json/#arb.utils.json.json_save","text":"Save a Python object to a JSON file with optional serialization settings. Parameters: file_path ( str | Path ) \u2013 Path to write the JSON file. data ( object ) \u2013 Data to serialize and write. json_options ( dict | None , default: None ) \u2013 Options to pass to json.dump . Example: >>> json_save(\"output.json\", {\"x\": decimal.Decimal(\"1.23\")}) Source code in arb\\utils\\json.py 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def json_save ( file_path : str | pathlib . Path , data : object , json_options : dict | None = None ) -> None : \"\"\" Save a Python object to a JSON file with optional serialization settings. Args: file_path (str | Path): Path to write the JSON file. data (object): Data to serialize and write. json_options (dict | None): Options to pass to `json.dump`. Example: >>> json_save(\"output.json\", {\"x\": decimal.Decimal(\"1.23\")}) \"\"\" logger . debug ( f \"json_save() called with { file_path =} , { json_options =} , { data =} \" ) if json_options is None : json_options = { \"default\" : json_serializer , \"indent\" : 4 } with open ( file_path , \"w\" , encoding = \"utf-8\" ) as f : json . dump ( data , f , ** json_options ) logger . debug ( f \"JSON saved to file: ' { file_path } '.\" )","title":"json_save"},{"location":"reference/arb/utils/json/#arb.utils.json.json_save_with_meta","text":"Save data with metadata to a JSON file under special keys. Parameters: file_path ( str | Path ) \u2013 Output JSON file path. data ( object ) \u2013 Primary data to store under \" data \". metadata ( dict | None , default: None ) \u2013 Optional metadata under \" metadata \". json_options ( dict | None , default: None ) \u2013 Options for json.dump . Example: >>> json_save_with_meta(\"log.json\", {\"key\": \"value\"}, {\"source\": \"generated\"}) Source code in arb\\utils\\json.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def json_save_with_meta ( file_path : str | pathlib . Path , data : object , metadata : dict | None = None , json_options : dict | None = None ) -> None : \"\"\" Save data with metadata to a JSON file under special keys. Args: file_path (str | Path): Output JSON file path. data (object): Primary data to store under \"_data_\". metadata (dict | None): Optional metadata under \"_metadata_\". json_options (dict | None): Options for `json.dump`. Example: >>> json_save_with_meta(\"log.json\", {\"key\": \"value\"}, {\"source\": \"generated\"}) \"\"\" logger . debug ( f \"json_save_with_meta() called with { file_path =} , { json_options =} , { metadata =} , { data =} \" ) if metadata is None : metadata = {} metadata . update ({ \"File created at\" : datetime . datetime . now ( ZoneInfo ( \"UTC\" )) . isoformat (), \"Serialized with\" : \"utils.json.json_save_with_meta\" , \"Deserialize with\" : \"utils.json.json_load_with_meta\" , }) wrapped = { \"_metadata_\" : metadata , \"_data_\" : data , } json_save ( file_path , wrapped , json_options = json_options )","title":"json_save_with_meta"},{"location":"reference/arb/utils/json/#arb.utils.json.json_serializer","text":"Custom JSON serializer for objects not natively serializable by json.dump . Parameters: obj ( object ) \u2013 The object to serialize. Returns: dict ( dict ) \u2013 A JSON-compatible dictionary representation of the object. Raises: TypeError \u2013 If the object type is unsupported. Example json.dumps(datetime.datetime.now(), default=json_serializer) Source code in arb\\utils\\json.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def json_serializer ( obj : object ) -> dict : \"\"\" Custom JSON serializer for objects not natively serializable by `json.dump`. Args: obj (object): The object to serialize. Returns: dict: A JSON-compatible dictionary representation of the object. Raises: TypeError: If the object type is unsupported. Example: >>> json.dumps(datetime.datetime.now(), default=json_serializer) \"\"\" if isinstance ( obj , type ): return { \"__class__\" : obj . __name__ , \"__module__\" : obj . __module__ } elif isinstance ( obj , datetime . datetime ): return { \"__type__\" : \"datetime.datetime\" , \"value\" : obj . isoformat ()} elif isinstance ( obj , decimal . Decimal ): return { \"__type__\" : \"decimal.Decimal\" , \"value\" : str ( obj )} raise TypeError ( f \"Object of type { type ( obj ) . __name__ } is not JSON serializable\" )","title":"json_serializer"},{"location":"reference/arb/utils/json/#arb.utils.json.make_dict_serializeable","text":"Transform a dictionary to ensure JSON compatibility of its values. Parameters: input_dict ( dict ) \u2013 Original dictionary to process. type_map ( dict [ str , type ] | None , default: None ) \u2013 Optional field-to-type map for casting. convert_time_to_ca ( bool , default: False ) \u2013 Convert datetimes to CA time before serialization. Returns: dict ( dict ) \u2013 A dictionary with only JSON-serializable values. Raises: TypeError \u2013 If any key is not a string. ValueError \u2013 If value cannot be cast to expected type. Source code in arb\\utils\\json.py 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 def make_dict_serializeable ( input_dict : dict , type_map : dict [ str , type ] | None = None , convert_time_to_ca : bool = False ) -> dict : \"\"\" Transform a dictionary to ensure JSON compatibility of its values. Args: input_dict (dict): Original dictionary to process. type_map (dict[str, type] | None): Optional field-to-type map for casting. convert_time_to_ca (bool): Convert datetimes to CA time before serialization. Returns: dict: A dictionary with only JSON-serializable values. Raises: TypeError: If any key is not a string. ValueError: If value cannot be cast to expected type. \"\"\" result = {} for key , value in input_dict . items (): if not isinstance ( key , str ): raise TypeError ( f \"All keys must be strings. Invalid key: { key } ( { type ( key ) } )\" ) if type_map and key in type_map : try : value = safe_cast ( value , type_map [ key ]) except Exception as e : raise ValueError ( f \"Failed to cast key ' { key } ' to { type_map [ key ] } : { e } \" ) if isinstance ( value , datetime . datetime ): if convert_time_to_ca : value = ca_naive_to_utc_datetime ( value ) value = value . isoformat () elif isinstance ( value , decimal . Decimal ): value = float ( value ) result [ key ] = value return result","title":"make_dict_serializeable"},{"location":"reference/arb/utils/json/#arb.utils.json.run_diagnostics","text":"Run internal validation for all JSON utilities. Tests Custom serializer/deserializer JSON saving and loading (with and without metadata) Metadata updating File comparison Raises: Exception \u2013 If any test fails. Source code in arb\\utils\\json.py 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 def run_diagnostics () -> None : \"\"\" Run internal validation for all JSON utilities. Tests: - Custom serializer/deserializer - JSON saving and loading (with and without metadata) - Metadata updating - File comparison Raises: Exception: If any test fails. \"\"\" import tempfile import shutil print ( \"Running diagnostics for JSON utilities...\" ) temp_dir = pathlib . Path ( tempfile . gettempdir ()) / \"json_utils_test\" if temp_dir . exists (): shutil . rmtree ( temp_dir ) temp_dir . mkdir ( parents = True ) try : # Test data data = { \"decimal\" : decimal . Decimal ( \"123.45\" ), \"timestamp\" : datetime . datetime ( 2025 , 5 , 5 , 13 , 30 , 0 , tzinfo = ZoneInfo ( \"UTC\" )), \"nested\" : { \"a\" : 1 , \"b\" : 2 }, } # File paths json_file_1 = temp_dir / \"test_1.json\" json_file_2 = temp_dir / \"test_2.json\" plain_file = temp_dir / \"plain.json\" # Save and load using json_save/json_load json_save ( json_file_1 , data ) loaded_1 = json_load ( json_file_1 ) assert loaded_1 == data , \"Basic save/load failed\" # Save with metadata and reload meta_info = { \"note\" : \"test file\" } json_save_with_meta ( json_file_2 , data , metadata = meta_info ) loaded_data , loaded_meta = json_load_with_meta ( json_file_2 ) assert loaded_data == data , \"Data mismatch in metadata test\" assert \"note\" in loaded_meta , \"Metadata not found\" # Write plain file, with serializer included, then enrich with metadata with open ( plain_file , \"w\" , encoding = \"utf-8\" ) as f : json . dump ( data , f , indent = 2 , default = json_serializer ) add_metadata_to_json ( plain_file ) # Compare metadata-enriched files compare_json_files ( json_file_2 , plain_file ) print ( \"All diagnostics completed successfully.\" ) except Exception as e : print ( f \"Diagnostics failed: { e } \" ) raise","title":"run_diagnostics"},{"location":"reference/arb/utils/json/#arb.utils.json.wtform_types_and_values","text":"Extract field types and current data values from a WTForm. Parameters: wtform ( FlaskForm ) \u2013 WTForms instance. Returns: tuple ( tuple [ dict [ str , type ], dict [ str , object ]] ) \u2013 dict[str, type]: Field name to type mapping for deserialization. dict[str, object]: Field name to current value mapping (may include 'Please Select'). Source code in arb\\utils\\json.py 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 def wtform_types_and_values ( wtform ) -> tuple [ dict [ str , type ], dict [ str , object ]]: \"\"\" Extract field types and current data values from a WTForm. Args: wtform (FlaskForm): WTForms instance. Returns: tuple: - dict[str, type]: Field name to type mapping for deserialization. - dict[str, object]: Field name to current value mapping (may include 'Please Select'). \"\"\" type_map = {} field_data = {} for name , field in wtform . _fields . items (): value = field . data field_data [ name ] = value # Identify complex field types for type mapping if isinstance ( field , DateTimeField ): type_map [ name ] = datetime . datetime elif isinstance ( field , DecimalField ): type_map [ name ] = decimal . Decimal elif isinstance ( field , BooleanField ): type_map [ name ] = bool elif isinstance ( field , IntegerField ): type_map [ name ] = int elif isinstance ( field , SelectField ): type_map [ name ] = str # 'Please Select' is valid return type_map , field_data","title":"wtform_types_and_values"},{"location":"reference/arb/utils/log_util/","text":"arb.utils.log_util log_util.py Logging utilities to trace function calls and parameter values across the application. This module provides two main tools for logging function arguments: 1. log_function_parameters(): Logs the name and arguments of the current function. 2. log_parameters(): A decorator that logs all arguments of decorated functions. It also includes a logging filter, FlaskUserContextFilter, to inject the current Flask user into all log records when inside a request context. Features Logs arguments from both positional and keyword inputs. Automatically derives the correct logger based on caller/module context. Supports optional printing to stdout for real-time debugging. Integrates Flask g.user context when available, aiding request traceability. Intended Use Diagnostic tracing and observability in Flask applications. Debugging individual functions without modifying logic. Enhancing structured logging with contextual request user information. Dependencies Python standard library (inspect, logging) Flask (optional, for FlaskUserContextFilter) Version 1.0.0 Example Usage: import arb.utils.log_util as log_util logging.basicConfig(level=logging.DEBUG) @log_parameters(print_to_console=True) def greet(name, lang=\"en\"): return f\"Hello {name} [{lang}]\" def example(): log_function_parameters(print_to_console=True) greet(\"Alice\", lang=\"fr\") example() Output: greet(name='Alice', lang='fr') example() Recommendations: Use log_parameters for consistent tracing of function calls across modules. Use log_function_parameters for temporary instrumentation or detailed inline debugging. This module is safe to import in any Python project and requires only the standard library. FlaskUserContextFilter Bases: Filter Logging filter that injects Flask's g.user into log records, if available. This allows log formats to include the active Flask user for traceability. Adds record.user (str): User identifier from Flask's request context, or \"n/a\" if unavailable. Source code in arb\\utils\\log_util.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class FlaskUserContextFilter ( logging . Filter ): \"\"\" Logging filter that injects Flask's `g.user` into log records, if available. This allows log formats to include the active Flask user for traceability. Adds: record.user (str): User identifier from Flask's request context, or \"n/a\" if unavailable. \"\"\" def filter ( self , record ): if has_request_context () and hasattr ( g , \"user\" ): record . user = g . user else : record . user = \"n/a\" return True log_function_parameters ( logger = None , print_to_console = False ) Log the current function's name and arguments using debug-level logging. Parameters: logger ( Logger | None , default: None ) \u2013 Optional logger. If None, derives one from caller's module. print_to_console ( bool , default: False ) \u2013 If True, also prints the message to stdout. Example def example(a, b=2): log_function_parameters() example(1) Logs: example(a=1, b=2) Source code in arb\\utils\\log_util.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def log_function_parameters ( logger : logging . Logger | None = None , print_to_console : bool = False ) -> None : \"\"\" Log the current function's name and arguments using debug-level logging. Args: logger (logging.Logger | None): Optional logger. If None, derives one from caller's module. print_to_console (bool): If True, also prints the message to stdout. Example: >>> def example(a, b=2): log_function_parameters() >>> example(1) # Logs: example(a=1, b=2) \"\"\" frame = inspect . currentframe () . f_back func_name = frame . f_code . co_name if logger is None : module_name = frame . f_globals . get ( \"__name__\" , \"default_logger\" ) logger = logging . getLogger ( module_name ) args_info = inspect . getargvalues ( frame ) params = [] for arg in args_info . args : value = args_info . locals . get ( arg ) params . append ( f \" { arg } = { value !r} \" ) if args_info . varargs : value = args_info . locals . get ( args_info . varargs ) params . append ( f \" { args_info . varargs } = { value !r} \" ) if args_info . keywords : kwargs = args_info . locals . get ( args_info . keywords , {}) for k , v in kwargs . items (): params . append ( f \" { k } = { v !r} \" ) log_line = f \" { func_name } ( { ', ' . join ( params ) } )\" logger . debug ( log_line ) if print_to_console : print ( log_line ) log_parameters ( logger = None , print_to_console = False ) Decorator to log all arguments passed to a function upon each invocation. Parameters: logger ( Logger | None , default: None ) \u2013 Optional logger instance. Defaults to caller's module logger. print_to_console ( bool , default: False ) \u2013 If True, also prints the log message to stdout. Returns: Callable ( Callable ) \u2013 A decorator that logs parameter values each time the function is called. Example @log_parameters(print_to_console=True) def greet(name, lang=\"en\"): return f\"Hello {name} [{lang}]\" Source code in arb\\utils\\log_util.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def log_parameters ( logger : logging . Logger | None = None , print_to_console : bool = False ) -> Callable : \"\"\" Decorator to log all arguments passed to a function upon each invocation. Args: logger (logging.Logger | None): Optional logger instance. Defaults to caller's module logger. print_to_console (bool): If True, also prints the log message to stdout. Returns: Callable: A decorator that logs parameter values each time the function is called. Example: >>> @log_parameters(print_to_console=True) >>> def greet(name, lang=\"en\"): >>> return f\"Hello {name} [{lang}]\" \"\"\" def decorator ( func : Callable ) -> Callable : @wraps ( func ) def wrapper ( * args , ** kwargs ): nonlocal logger if logger is None : logger = logging . getLogger ( func . __module__ ) bound = inspect . signature ( func ) . bind ( * args , ** kwargs ) bound . apply_defaults () param_str = \", \" . join ( f \" { k } = { v !r} \" for k , v in bound . arguments . items ()) log_line = f \" { func . __name__ } ( { param_str } )\" logger . debug ( log_line ) if print_to_console : print ( log_line ) return func ( * args , ** kwargs ) return wrapper return decorator","title":"arb.utils.log_util"},{"location":"reference/arb/utils/log_util/#arbutilslog_util","text":"log_util.py Logging utilities to trace function calls and parameter values across the application. This module provides two main tools for logging function arguments: 1. log_function_parameters(): Logs the name and arguments of the current function. 2. log_parameters(): A decorator that logs all arguments of decorated functions. It also includes a logging filter, FlaskUserContextFilter, to inject the current Flask user into all log records when inside a request context. Features Logs arguments from both positional and keyword inputs. Automatically derives the correct logger based on caller/module context. Supports optional printing to stdout for real-time debugging. Integrates Flask g.user context when available, aiding request traceability. Intended Use Diagnostic tracing and observability in Flask applications. Debugging individual functions without modifying logic. Enhancing structured logging with contextual request user information. Dependencies Python standard library (inspect, logging) Flask (optional, for FlaskUserContextFilter) Version 1.0.0","title":"arb.utils.log_util"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util--example-usage","text":"import arb.utils.log_util as log_util logging.basicConfig(level=logging.DEBUG) @log_parameters(print_to_console=True) def greet(name, lang=\"en\"): return f\"Hello {name} [{lang}]\" def example(): log_function_parameters(print_to_console=True) greet(\"Alice\", lang=\"fr\") example()","title":"Example Usage:"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util--output","text":"greet(name='Alice', lang='fr') example()","title":"Output:"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util--recommendations","text":"Use log_parameters for consistent tracing of function calls across modules. Use log_function_parameters for temporary instrumentation or detailed inline debugging. This module is safe to import in any Python project and requires only the standard library.","title":"Recommendations:"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util.FlaskUserContextFilter","text":"Bases: Filter Logging filter that injects Flask's g.user into log records, if available. This allows log formats to include the active Flask user for traceability. Adds record.user (str): User identifier from Flask's request context, or \"n/a\" if unavailable. Source code in arb\\utils\\log_util.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 class FlaskUserContextFilter ( logging . Filter ): \"\"\" Logging filter that injects Flask's `g.user` into log records, if available. This allows log formats to include the active Flask user for traceability. Adds: record.user (str): User identifier from Flask's request context, or \"n/a\" if unavailable. \"\"\" def filter ( self , record ): if has_request_context () and hasattr ( g , \"user\" ): record . user = g . user else : record . user = \"n/a\" return True","title":"FlaskUserContextFilter"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util.log_function_parameters","text":"Log the current function's name and arguments using debug-level logging. Parameters: logger ( Logger | None , default: None ) \u2013 Optional logger. If None, derives one from caller's module. print_to_console ( bool , default: False ) \u2013 If True, also prints the message to stdout. Example def example(a, b=2): log_function_parameters() example(1)","title":"log_function_parameters"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util.log_function_parameters--logs-examplea1-b2","text":"Source code in arb\\utils\\log_util.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def log_function_parameters ( logger : logging . Logger | None = None , print_to_console : bool = False ) -> None : \"\"\" Log the current function's name and arguments using debug-level logging. Args: logger (logging.Logger | None): Optional logger. If None, derives one from caller's module. print_to_console (bool): If True, also prints the message to stdout. Example: >>> def example(a, b=2): log_function_parameters() >>> example(1) # Logs: example(a=1, b=2) \"\"\" frame = inspect . currentframe () . f_back func_name = frame . f_code . co_name if logger is None : module_name = frame . f_globals . get ( \"__name__\" , \"default_logger\" ) logger = logging . getLogger ( module_name ) args_info = inspect . getargvalues ( frame ) params = [] for arg in args_info . args : value = args_info . locals . get ( arg ) params . append ( f \" { arg } = { value !r} \" ) if args_info . varargs : value = args_info . locals . get ( args_info . varargs ) params . append ( f \" { args_info . varargs } = { value !r} \" ) if args_info . keywords : kwargs = args_info . locals . get ( args_info . keywords , {}) for k , v in kwargs . items (): params . append ( f \" { k } = { v !r} \" ) log_line = f \" { func_name } ( { ', ' . join ( params ) } )\" logger . debug ( log_line ) if print_to_console : print ( log_line )","title":"Logs: example(a=1, b=2)"},{"location":"reference/arb/utils/log_util/#arb.utils.log_util.log_parameters","text":"Decorator to log all arguments passed to a function upon each invocation. Parameters: logger ( Logger | None , default: None ) \u2013 Optional logger instance. Defaults to caller's module logger. print_to_console ( bool , default: False ) \u2013 If True, also prints the log message to stdout. Returns: Callable ( Callable ) \u2013 A decorator that logs parameter values each time the function is called. Example @log_parameters(print_to_console=True) def greet(name, lang=\"en\"): return f\"Hello {name} [{lang}]\" Source code in arb\\utils\\log_util.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def log_parameters ( logger : logging . Logger | None = None , print_to_console : bool = False ) -> Callable : \"\"\" Decorator to log all arguments passed to a function upon each invocation. Args: logger (logging.Logger | None): Optional logger instance. Defaults to caller's module logger. print_to_console (bool): If True, also prints the log message to stdout. Returns: Callable: A decorator that logs parameter values each time the function is called. Example: >>> @log_parameters(print_to_console=True) >>> def greet(name, lang=\"en\"): >>> return f\"Hello {name} [{lang}]\" \"\"\" def decorator ( func : Callable ) -> Callable : @wraps ( func ) def wrapper ( * args , ** kwargs ): nonlocal logger if logger is None : logger = logging . getLogger ( func . __module__ ) bound = inspect . signature ( func ) . bind ( * args , ** kwargs ) bound . apply_defaults () param_str = \", \" . join ( f \" { k } = { v !r} \" for k , v in bound . arguments . items ()) log_line = f \" { func . __name__ } ( { param_str } )\" logger . debug ( log_line ) if print_to_console : print ( log_line ) return func ( * args , ** kwargs ) return wrapper return decorator","title":"log_parameters"},{"location":"reference/arb/utils/misc/","text":"arb.utils.misc misc.py Miscellaneous utility functions for common tasks including dictionary traversal, default injection, argument formatting, exception logging, and safe type casting. Functions included get_nested_value: Safely access deeply nested values in a dictionary. ensure_key_value_pair: Add missing keys to sub-dictionaries using defaults. replace_list_occurrences: Modify list elements in-place based on a mapping. args_to_string: Format argument lists into padded strings. log_error: Log full exception tracebacks and re-raise. safe_cast: Convert values to expected types if needed. run_diagnostics: Test suite for all utilities. Intended Use Shared helpers for Flask or CLI-based Python applications. Improves code reuse and diagnostic traceability. Dependencies Python standard library Logging provided by arb.__get_logger Version 1.0.0 TODO: - Consider converting log_error into a structured 500 error response for Flask apps args_to_string ( args ) Convert a list or tuple of arguments into a single space-separated string with padding. Parameters: args ( list | tuple | None ) \u2013 Arguments to convert. Returns: str ( str ) \u2013 Space-separated string representation. Example args_to_string([\"--debug\", \"--log\", \"file.txt\"]) ' --debug --log file.txt ' Source code in arb\\utils\\misc.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def args_to_string ( args : list | tuple | None ) -> str : \"\"\" Convert a list or tuple of arguments into a single space-separated string with padding. Args: args (list | tuple | None): Arguments to convert. Returns: str: Space-separated string representation. Example: >>> args_to_string([\"--debug\", \"--log\", \"file.txt\"]) ' --debug --log file.txt ' \"\"\" if not args : return '' else : args = [ str ( arg ) for arg in args ] return_string = \" \" + \" \" . join ( args ) + \" \" return return_string ensure_key_value_pair ( dict_ , default_dict , sub_key ) Ensure each sub-dictionary in dict_ has a given key, populating it from default_dict if missing. Parameters: dict_ ( dict [ str , dict ] ) \u2013 A dictionary whose values are sub-dictionaries. default_dict ( dict ) \u2013 A lookup dictionary to supply missing key-value pairs. sub_key ( str ) \u2013 The key that must exist in each sub-dictionary. Raises: TypeError \u2013 If the sub_key is missing, and no fallback is found in default_dict. Example dict_ = {\"a\": {\"x\": 1}, \"b\": {\"x\": 2}, \"c\": {}} defaults = {\"c\": 99} ensure_key_value_pair(dict_, defaults, \"x\") dict_[\"c\"][\"x\"] 99 Source code in arb\\utils\\misc.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def ensure_key_value_pair ( dict_ : dict [ str , dict ], default_dict : dict , sub_key : str ) -> None : \"\"\" Ensure each sub-dictionary in dict_ has a given key, populating it from default_dict if missing. Args: dict_ (dict[str, dict]): A dictionary whose values are sub-dictionaries. default_dict (dict): A lookup dictionary to supply missing key-value pairs. sub_key (str): The key that must exist in each sub-dictionary. Raises: TypeError: If the sub_key is missing, and no fallback is found in default_dict. Example: >>> dict_ = {\"a\": {\"x\": 1}, \"b\": {\"x\": 2}, \"c\": {}} >>> defaults = {\"c\": 99} >>> ensure_key_value_pair(dict_, defaults, \"x\") >>> dict_[\"c\"][\"x\"] 99 \"\"\" for key , sub_dict in dict_ . items (): logger . debug ( f \" { key =} , { sub_dict =} \" ) if sub_key not in sub_dict : if key in default_dict : sub_dict [ sub_key ] = default_dict [ key ] else : raise TypeError ( f \" { sub_key } is not present in sub dictionary for key ' { key } ' \" f \"and no default provided in default_dict\" ) get_nested_value ( nested_dict , keys ) Retrieve a value from a nested dictionary using a key path. Parameters: nested_dict ( dict ) \u2013 The dictionary to search. keys ( list | tuple | str ) \u2013 A sequence of keys to traverse the dictionary, or a single key. Returns: object ( object ) \u2013 The value found at the specified key path. Raises: KeyError \u2013 If a key is missing at any level. TypeError \u2013 If a non-dictionary value is encountered mid-traversal. Examples: >>> data = { \"a\" : { \"b\" : { \"c\" : 42 }}, \"x\" : 99 } >>> get_nested_value ( data , ( \"a\" , \"b\" , \"c\" )) 42 >>> get_nested_value ( data , \"x\" ) 99 Source code in arb\\utils\\misc.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_nested_value ( nested_dict : dict , keys : list | tuple | str ) -> object : \"\"\" Retrieve a value from a nested dictionary using a key path. Args: nested_dict (dict): The dictionary to search. keys (list | tuple | str): A sequence of keys to traverse the dictionary, or a single key. Returns: object: The value found at the specified key path. Raises: KeyError: If a key is missing at any level. TypeError: If a non-dictionary value is encountered mid-traversal. Examples: >>> data = {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99} >>> get_nested_value(data, (\"a\", \"b\", \"c\")) 42 >>> get_nested_value(data, \"x\") 99 \"\"\" if not isinstance ( keys , ( list , tuple )): # Single key case if keys not in nested_dict : raise KeyError ( f \"Key ' { keys } ' not found in the dictionary\" ) return nested_dict [ keys ] current = nested_dict for key in keys : if not isinstance ( current , dict ): raise TypeError ( f \"Expected a dictionary at key ' { key } ', found { type ( current ) . __name__ } \" ) if key not in current : raise KeyError ( f \"Key ' { key } ' not found in the dictionary\" ) current = current [ key ] return current log_error ( e ) Log an exception and its stack trace, then re-raise the exception. Parameters: e ( Exception ) \u2013 The exception to log. Raises: Exception \u2013 Always re-raises the input exception after logging. Notes Outputs full traceback to logger. Re-raises the original exception. Useful during development or structured exception monitoring. TODO Consider wrapping this in Flask to render a 500 error page instead. Source code in arb\\utils\\misc.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def log_error ( e : Exception ) -> None : \"\"\" Log an exception and its stack trace, then re-raise the exception. Args: e (Exception): The exception to log. Raises: Exception: Always re-raises the input exception after logging. Notes: - Outputs full traceback to logger. - Re-raises the original exception. - Useful during development or structured exception monitoring. TODO: Consider wrapping this in Flask to render a 500 error page instead. \"\"\" logger . error ( e , exc_info = True ) stack = traceback . extract_stack () logger . error ( stack ) raise e replace_list_occurrences ( list_ , lookup_dict ) Replace elements of a list in-place using a lookup dictionary. Parameters: list_ ( list ) \u2013 The list whose elements may be replaced. lookup_dict ( dict ) \u2013 A dictionary mapping old values to new values. Example values = [\"cat\", \"dog\", \"bird\"] lookup = {\"dog\": \"puppy\", \"bird\": \"parrot\"} replace_list_occurrences(values, lookup) values ['cat', 'puppy', 'parrot'] Source code in arb\\utils\\misc.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def replace_list_occurrences ( list_ : list , lookup_dict : dict ) -> None : \"\"\" Replace elements of a list in-place using a lookup dictionary. Args: list_ (list): The list whose elements may be replaced. lookup_dict (dict): A dictionary mapping old values to new values. Example: >>> values = [\"cat\", \"dog\", \"bird\"] >>> lookup = {\"dog\": \"puppy\", \"bird\": \"parrot\"} >>> replace_list_occurrences(values, lookup) >>> values ['cat', 'puppy', 'parrot'] \"\"\" for i in range ( len ( list_ )): if list_ [ i ] in lookup_dict : list_ [ i ] = lookup_dict [ list_ [ i ]] run_diagnostics () Run diagnostics to validate functionality of misc.py utilities. This includes Nested dictionary access Default key/value injection into sub-dictionaries In-place replacement of list values Argument string formatting Error logging (non-raising test only) Example run_diagnostics() Source code in arb\\utils\\misc.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def run_diagnostics () -> None : \"\"\" Run diagnostics to validate functionality of misc.py utilities. This includes: - Nested dictionary access - Default key/value injection into sub-dictionaries - In-place replacement of list values - Argument string formatting - Error logging (non-raising test only) Example: >>> run_diagnostics() \"\"\" print ( \"Running diagnostics for misc.py utilities...\" ) # --- Test get_nested_value --- test_dict = { \"a\" : { \"b\" : { \"c\" : 42 }}, \"x\" : 99 } assert get_nested_value ( test_dict , ( \"a\" , \"b\" , \"c\" )) == 42 , \"Nested dict access failed\" assert get_nested_value ( test_dict , \"x\" ) == 99 , \"Single key access failed\" # --- Test ensure_key_value_pair --- dict_with_sub = { \"apple\" : { \"color\" : \"red\" }, \"banana\" : {}} defaults = { \"banana\" : \"yellow\" } ensure_key_value_pair ( dict_with_sub , defaults , \"color\" ) assert dict_with_sub [ \"banana\" ][ \"color\" ] == \"yellow\" , \"Default insertion failed\" # --- Test replace_list_occurrences --- items = [ \"dog\" , \"cat\" , \"parrot\" ] replace_list_occurrences ( items , { \"dog\" : \"puppy\" , \"parrot\" : \"bird\" }) assert items == [ \"puppy\" , \"cat\" , \"bird\" ], \"List replacement failed\" # --- Test args_to_string --- assert args_to_string ([ \"--a\" , \"--b\" , \"value\" ]) == \" --a --b value \" , \"args_to_string failed\" assert args_to_string ( None ) == \"\" , \"args_to_string empty case failed\" # --- Test log_error (without raising) --- try : try : raise ValueError ( \"Test exception for log_error\" ) except Exception as e : # Simulate logging only \u2014 comment out re-raise logger . error ( e , exc_info = True ) except Exception : assert False , \"log_error should not re-raise during diagnostics\" print ( \"All diagnostics passed.\" ) safe_cast ( value , expected_type ) Cast a value to the expected type only if it's not already of that type. Parameters: value ( Any ) \u2013 The value to check and potentially cast. expected_type ( type ) \u2013 The target Python type to cast to. Returns: object ( object ) \u2013 The original or casted value. Raises: ValueError \u2013 If the cast fails or is inappropriate for the type. Source code in arb\\utils\\misc.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def safe_cast ( value , expected_type : type ) -> object : \"\"\" Cast a value to the expected type only if it's not already of that type. Args: value (Any): The value to check and potentially cast. expected_type (type): The target Python type to cast to. Returns: object: The original or casted value. Raises: ValueError: If the cast fails or is inappropriate for the type. \"\"\" try : if not isinstance ( value , expected_type ): value = expected_type ( value ) return value except Exception as e : raise ValueError ( f \"Failed to cast value { value !r} to { expected_type } : { e } \" )","title":"arb.utils.misc"},{"location":"reference/arb/utils/misc/#arbutilsmisc","text":"misc.py Miscellaneous utility functions for common tasks including dictionary traversal, default injection, argument formatting, exception logging, and safe type casting. Functions included get_nested_value: Safely access deeply nested values in a dictionary. ensure_key_value_pair: Add missing keys to sub-dictionaries using defaults. replace_list_occurrences: Modify list elements in-place based on a mapping. args_to_string: Format argument lists into padded strings. log_error: Log full exception tracebacks and re-raise. safe_cast: Convert values to expected types if needed. run_diagnostics: Test suite for all utilities. Intended Use Shared helpers for Flask or CLI-based Python applications. Improves code reuse and diagnostic traceability. Dependencies Python standard library Logging provided by arb.__get_logger Version 1.0.0 TODO: - Consider converting log_error into a structured 500 error response for Flask apps","title":"arb.utils.misc"},{"location":"reference/arb/utils/misc/#arb.utils.misc.args_to_string","text":"Convert a list or tuple of arguments into a single space-separated string with padding. Parameters: args ( list | tuple | None ) \u2013 Arguments to convert. Returns: str ( str ) \u2013 Space-separated string representation. Example args_to_string([\"--debug\", \"--log\", \"file.txt\"]) ' --debug --log file.txt ' Source code in arb\\utils\\misc.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def args_to_string ( args : list | tuple | None ) -> str : \"\"\" Convert a list or tuple of arguments into a single space-separated string with padding. Args: args (list | tuple | None): Arguments to convert. Returns: str: Space-separated string representation. Example: >>> args_to_string([\"--debug\", \"--log\", \"file.txt\"]) ' --debug --log file.txt ' \"\"\" if not args : return '' else : args = [ str ( arg ) for arg in args ] return_string = \" \" + \" \" . join ( args ) + \" \" return return_string","title":"args_to_string"},{"location":"reference/arb/utils/misc/#arb.utils.misc.ensure_key_value_pair","text":"Ensure each sub-dictionary in dict_ has a given key, populating it from default_dict if missing. Parameters: dict_ ( dict [ str , dict ] ) \u2013 A dictionary whose values are sub-dictionaries. default_dict ( dict ) \u2013 A lookup dictionary to supply missing key-value pairs. sub_key ( str ) \u2013 The key that must exist in each sub-dictionary. Raises: TypeError \u2013 If the sub_key is missing, and no fallback is found in default_dict. Example dict_ = {\"a\": {\"x\": 1}, \"b\": {\"x\": 2}, \"c\": {}} defaults = {\"c\": 99} ensure_key_value_pair(dict_, defaults, \"x\") dict_[\"c\"][\"x\"] 99 Source code in arb\\utils\\misc.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def ensure_key_value_pair ( dict_ : dict [ str , dict ], default_dict : dict , sub_key : str ) -> None : \"\"\" Ensure each sub-dictionary in dict_ has a given key, populating it from default_dict if missing. Args: dict_ (dict[str, dict]): A dictionary whose values are sub-dictionaries. default_dict (dict): A lookup dictionary to supply missing key-value pairs. sub_key (str): The key that must exist in each sub-dictionary. Raises: TypeError: If the sub_key is missing, and no fallback is found in default_dict. Example: >>> dict_ = {\"a\": {\"x\": 1}, \"b\": {\"x\": 2}, \"c\": {}} >>> defaults = {\"c\": 99} >>> ensure_key_value_pair(dict_, defaults, \"x\") >>> dict_[\"c\"][\"x\"] 99 \"\"\" for key , sub_dict in dict_ . items (): logger . debug ( f \" { key =} , { sub_dict =} \" ) if sub_key not in sub_dict : if key in default_dict : sub_dict [ sub_key ] = default_dict [ key ] else : raise TypeError ( f \" { sub_key } is not present in sub dictionary for key ' { key } ' \" f \"and no default provided in default_dict\" )","title":"ensure_key_value_pair"},{"location":"reference/arb/utils/misc/#arb.utils.misc.get_nested_value","text":"Retrieve a value from a nested dictionary using a key path. Parameters: nested_dict ( dict ) \u2013 The dictionary to search. keys ( list | tuple | str ) \u2013 A sequence of keys to traverse the dictionary, or a single key. Returns: object ( object ) \u2013 The value found at the specified key path. Raises: KeyError \u2013 If a key is missing at any level. TypeError \u2013 If a non-dictionary value is encountered mid-traversal. Examples: >>> data = { \"a\" : { \"b\" : { \"c\" : 42 }}, \"x\" : 99 } >>> get_nested_value ( data , ( \"a\" , \"b\" , \"c\" )) 42 >>> get_nested_value ( data , \"x\" ) 99 Source code in arb\\utils\\misc.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def get_nested_value ( nested_dict : dict , keys : list | tuple | str ) -> object : \"\"\" Retrieve a value from a nested dictionary using a key path. Args: nested_dict (dict): The dictionary to search. keys (list | tuple | str): A sequence of keys to traverse the dictionary, or a single key. Returns: object: The value found at the specified key path. Raises: KeyError: If a key is missing at any level. TypeError: If a non-dictionary value is encountered mid-traversal. Examples: >>> data = {\"a\": {\"b\": {\"c\": 42}}, \"x\": 99} >>> get_nested_value(data, (\"a\", \"b\", \"c\")) 42 >>> get_nested_value(data, \"x\") 99 \"\"\" if not isinstance ( keys , ( list , tuple )): # Single key case if keys not in nested_dict : raise KeyError ( f \"Key ' { keys } ' not found in the dictionary\" ) return nested_dict [ keys ] current = nested_dict for key in keys : if not isinstance ( current , dict ): raise TypeError ( f \"Expected a dictionary at key ' { key } ', found { type ( current ) . __name__ } \" ) if key not in current : raise KeyError ( f \"Key ' { key } ' not found in the dictionary\" ) current = current [ key ] return current","title":"get_nested_value"},{"location":"reference/arb/utils/misc/#arb.utils.misc.log_error","text":"Log an exception and its stack trace, then re-raise the exception. Parameters: e ( Exception ) \u2013 The exception to log. Raises: Exception \u2013 Always re-raises the input exception after logging. Notes Outputs full traceback to logger. Re-raises the original exception. Useful during development or structured exception monitoring. TODO Consider wrapping this in Flask to render a 500 error page instead. Source code in arb\\utils\\misc.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def log_error ( e : Exception ) -> None : \"\"\" Log an exception and its stack trace, then re-raise the exception. Args: e (Exception): The exception to log. Raises: Exception: Always re-raises the input exception after logging. Notes: - Outputs full traceback to logger. - Re-raises the original exception. - Useful during development or structured exception monitoring. TODO: Consider wrapping this in Flask to render a 500 error page instead. \"\"\" logger . error ( e , exc_info = True ) stack = traceback . extract_stack () logger . error ( stack ) raise e","title":"log_error"},{"location":"reference/arb/utils/misc/#arb.utils.misc.replace_list_occurrences","text":"Replace elements of a list in-place using a lookup dictionary. Parameters: list_ ( list ) \u2013 The list whose elements may be replaced. lookup_dict ( dict ) \u2013 A dictionary mapping old values to new values. Example values = [\"cat\", \"dog\", \"bird\"] lookup = {\"dog\": \"puppy\", \"bird\": \"parrot\"} replace_list_occurrences(values, lookup) values ['cat', 'puppy', 'parrot'] Source code in arb\\utils\\misc.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def replace_list_occurrences ( list_ : list , lookup_dict : dict ) -> None : \"\"\" Replace elements of a list in-place using a lookup dictionary. Args: list_ (list): The list whose elements may be replaced. lookup_dict (dict): A dictionary mapping old values to new values. Example: >>> values = [\"cat\", \"dog\", \"bird\"] >>> lookup = {\"dog\": \"puppy\", \"bird\": \"parrot\"} >>> replace_list_occurrences(values, lookup) >>> values ['cat', 'puppy', 'parrot'] \"\"\" for i in range ( len ( list_ )): if list_ [ i ] in lookup_dict : list_ [ i ] = lookup_dict [ list_ [ i ]]","title":"replace_list_occurrences"},{"location":"reference/arb/utils/misc/#arb.utils.misc.run_diagnostics","text":"Run diagnostics to validate functionality of misc.py utilities. This includes Nested dictionary access Default key/value injection into sub-dictionaries In-place replacement of list values Argument string formatting Error logging (non-raising test only) Example run_diagnostics() Source code in arb\\utils\\misc.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def run_diagnostics () -> None : \"\"\" Run diagnostics to validate functionality of misc.py utilities. This includes: - Nested dictionary access - Default key/value injection into sub-dictionaries - In-place replacement of list values - Argument string formatting - Error logging (non-raising test only) Example: >>> run_diagnostics() \"\"\" print ( \"Running diagnostics for misc.py utilities...\" ) # --- Test get_nested_value --- test_dict = { \"a\" : { \"b\" : { \"c\" : 42 }}, \"x\" : 99 } assert get_nested_value ( test_dict , ( \"a\" , \"b\" , \"c\" )) == 42 , \"Nested dict access failed\" assert get_nested_value ( test_dict , \"x\" ) == 99 , \"Single key access failed\" # --- Test ensure_key_value_pair --- dict_with_sub = { \"apple\" : { \"color\" : \"red\" }, \"banana\" : {}} defaults = { \"banana\" : \"yellow\" } ensure_key_value_pair ( dict_with_sub , defaults , \"color\" ) assert dict_with_sub [ \"banana\" ][ \"color\" ] == \"yellow\" , \"Default insertion failed\" # --- Test replace_list_occurrences --- items = [ \"dog\" , \"cat\" , \"parrot\" ] replace_list_occurrences ( items , { \"dog\" : \"puppy\" , \"parrot\" : \"bird\" }) assert items == [ \"puppy\" , \"cat\" , \"bird\" ], \"List replacement failed\" # --- Test args_to_string --- assert args_to_string ([ \"--a\" , \"--b\" , \"value\" ]) == \" --a --b value \" , \"args_to_string failed\" assert args_to_string ( None ) == \"\" , \"args_to_string empty case failed\" # --- Test log_error (without raising) --- try : try : raise ValueError ( \"Test exception for log_error\" ) except Exception as e : # Simulate logging only \u2014 comment out re-raise logger . error ( e , exc_info = True ) except Exception : assert False , \"log_error should not re-raise during diagnostics\" print ( \"All diagnostics passed.\" )","title":"run_diagnostics"},{"location":"reference/arb/utils/misc/#arb.utils.misc.safe_cast","text":"Cast a value to the expected type only if it's not already of that type. Parameters: value ( Any ) \u2013 The value to check and potentially cast. expected_type ( type ) \u2013 The target Python type to cast to. Returns: object ( object ) \u2013 The original or casted value. Raises: ValueError \u2013 If the cast fails or is inappropriate for the type. Source code in arb\\utils\\misc.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def safe_cast ( value , expected_type : type ) -> object : \"\"\" Cast a value to the expected type only if it's not already of that type. Args: value (Any): The value to check and potentially cast. expected_type (type): The target Python type to cast to. Returns: object: The original or casted value. Raises: ValueError: If the cast fails or is inappropriate for the type. \"\"\" try : if not isinstance ( value , expected_type ): value = expected_type ( value ) return value except Exception as e : raise ValueError ( f \"Failed to cast value { value !r} to { expected_type } : { e } \" )","title":"safe_cast"},{"location":"reference/arb/utils/sql_alchemy/","text":"arb.utils.sql_alchemy SQLAlchemy utility functions for Flask applications using declarative or automap base models. This module provides introspection, diagnostics, and model-row operations for SQLAlchemy-based Flask apps. It supports both declarative and automapped models. Included Utilities: Model diagnostics ( sa_model_diagnostics ) Field and column type inspection ( get_sa_fields , get_sa_column_types ) Full automap type mapping ( get_sa_automap_types ) Dictionary conversions ( sa_model_to_dict , sa_model_dict_compare ) Table-to-dict exports ( table_to_list ) Table/class lookups ( get_class_from_table_name ) Row fetch and sort utilities ( get_rows_by_table_name ) Model add/delete with logging ( add_commit_and_log_model , delete_commit_and_log_model ) Foreign key traversal ( get_foreign_value ) PostgreSQL sequence inspection ( find_auto_increment_value ) JSON column loader ( load_model_json_column ) Type Hints: db (SQLAlchemy) : Flask-SQLAlchemy database instance with .session and .engine. base (DeclarativeMeta) : Declarative or automapped SQLAlchemy base (e.g., via automap_base()). model (DeclarativeMeta) : SQLAlchemy ORM model class or instance. session (Session) : SQLAlchemy session object. Usage Notes: Supports PostgreSQL features like sequence inspection via pg_get_serial_sequence . Logging is integrated for debugging and auditing. Compatible with Python 3.10+ syntax (PEP 604 union types). Version 1.0.0 add_commit_and_log_model ( db , model_row , comment = '' , model_before = None ) Add or update a model instance, log changes, and commit. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. model_row ( DeclarativeMeta ) \u2013 ORM model instance. comment ( str , default: '' ) \u2013 Optional log comment. model_before ( dict | None , default: None ) \u2013 Optional snapshot before changes. Source code in arb\\utils\\sql_alchemy.py 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def add_commit_and_log_model ( db : SQLAlchemy , model_row : DeclarativeMeta , comment : str = \"\" , model_before : dict | None = None ) -> None : \"\"\" Add or update a model instance, log changes, and commit. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. model_row (DeclarativeMeta): ORM model instance. comment (str): Optional log comment. model_before (dict | None): Optional snapshot before changes. \"\"\" # todo (update) - use the payload routine apply_json_patch_and_log if model_before : logger . info ( f \"Before commit { comment =} : { model_before } \" ) try : db . session . add ( model_row ) db . session . commit () model_after = sa_model_to_dict ( model_row ) logger . info ( f \"After commit: { model_after } \" ) if model_before : changes = sa_model_dict_compare ( model_before , model_after ) logger . info ( f \"Changed values: { changes } \" ) except Exception as e : log_error ( e ) delete_commit_and_log_model ( db , model_row , comment = '' ) Delete a model instance, log it, and commit the change. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. model_row ( DeclarativeMeta ) \u2013 ORM model instance. comment ( str , default: '' ) \u2013 Optional log comment. Source code in arb\\utils\\sql_alchemy.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 def delete_commit_and_log_model ( db : SQLAlchemy , model_row : DeclarativeMeta , comment : str = \"\" ) -> None : \"\"\" Delete a model instance, log it, and commit the change. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. model_row (DeclarativeMeta): ORM model instance. comment (str): Optional log comment. \"\"\" # todo (update) - use the payload routine apply_json_patch_and_log and or some way to track change logger . info ( f \"Deleting model { comment =} : { sa_model_to_dict ( model_row ) } \" ) try : db . session . delete ( model_row ) db . session . commit () except Exception as e : log_error ( e ) find_auto_increment_value ( db , table_name , column_name ) Find the next auto-increment value for a table column (PostgreSQL only). Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. table_name ( str ) \u2013 Table name. column_name ( str ) \u2013 Column name. Returns: str ( str ) \u2013 Human-readable summary of the next sequence value. Source code in arb\\utils\\sql_alchemy.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 def find_auto_increment_value ( db : SQLAlchemy , table_name : str , column_name : str ) -> str : \"\"\" Find the next auto-increment value for a table column (PostgreSQL only). Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. table_name (str): Table name. column_name (str): Column name. Returns: str: Human-readable summary of the next sequence value. \"\"\" with db . engine . connect () as connection : sql_seq = f \"SELECT pg_get_serial_sequence(' { table_name } ', ' { column_name } ')\" sequence_name = connection . execute ( text ( sql_seq )) . scalar () sql_nextval = f \"SELECT nextval(' { sequence_name } ')\" next_val = connection . execute ( text ( sql_nextval )) . scalar () return f \"Table ' { table_name } ' column ' { column_name } ' sequence ' { sequence_name } ' next value is ' { next_val } '\" get_class_from_table_name ( base , table_name ) Retrieves the mapped class for a given table name. Parameters: base ( DeclarativeMeta ) \u2013 SQLAlchemy declarative base. table_name ( str ) \u2013 Database table name. Returns: DeclarativeMeta | None \u2013 DeclarativeMeta | None: Mapped SQLAlchemy ORM class, or None if not found. Notes: - To access the class mapped to a specific table name in SQLAlchemy without directly using _class_registry, you can use the Base.metadata object, which stores information about all mapped tables. get_class_from_table_name seems to fail from gpt refactor, so i kept my original code here. it failed because my old test of is not None was changed to if - be on the lookout for other subtle bugs like this Source code in arb\\utils\\sql_alchemy.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def get_class_from_table_name ( base : DeclarativeMeta | None , table_name : str ) -> DeclarativeMeta | None : \"\"\" Retrieves the mapped class for a given table name. Args: base (DeclarativeMeta): SQLAlchemy declarative base. table_name (str): Database table name. Returns: DeclarativeMeta | None: Mapped SQLAlchemy ORM class, or None if not found. Notes: - To access the class mapped to a specific table name in SQLAlchemy without directly using _class_registry, you can use the Base.metadata object, which stores information about all mapped tables. - get_class_from_table_name seems to fail from gpt refactor, so i kept my original code here. - it failed because my old test of is not None was changed to if - be on the lookout for other subtle bugs like this \"\"\" try : # Look up the table in metadata and find its mapped class table = base . metadata . tables . get ( table_name ) if table is not None : for mapper in base . registry . mappers : if mapper . local_table == table : return mapper . class_ return None except Exception as e : msg = f \"Exception occurred when trying to get table named { table_name } \" logger . error ( msg , exc_info = True ) logger . error ( f \"exception info: { e } \" ) return None get_foreign_value ( db , base , primary_table_name , foreign_table_name , primary_table_fk_name , foreign_table_column_name , primary_table_pk_value , primary_table_pk_name = None , foreign_table_pk_name = None ) Resolve a foreign key reference and return its value. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. base ( DeclarativeMeta ) \u2013 Declarative base. primary_table_name ( str ) \u2013 Table with FK. foreign_table_name ( str ) \u2013 Table with desired value. primary_table_fk_name ( str ) \u2013 FK field name. foreign_table_column_name ( str ) \u2013 Target field name in foreign table. primary_table_pk_value ( int ) \u2013 PK value of row in primary table. primary_table_pk_name ( str | None , default: None ) \u2013 Optional PK field override. foreign_table_pk_name ( str | None , default: None ) \u2013 Optional PK override in foreign table. Returns: str | None \u2013 str | None: Foreign value if found, else None. Source code in arb\\utils\\sql_alchemy.py 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def get_foreign_value ( db : SQLAlchemy , base : DeclarativeMeta , primary_table_name : str , foreign_table_name : str , primary_table_fk_name : str , foreign_table_column_name : str , primary_table_pk_value : int , primary_table_pk_name : str | None = None , foreign_table_pk_name : str | None = None ) -> str | None : \"\"\" Resolve a foreign key reference and return its value. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. base (DeclarativeMeta): Declarative base. primary_table_name (str): Table with FK. foreign_table_name (str): Table with desired value. primary_table_fk_name (str): FK field name. foreign_table_column_name (str): Target field name in foreign table. primary_table_pk_value (int): PK value of row in primary table. primary_table_pk_name (str | None): Optional PK field override. foreign_table_pk_name (str | None): Optional PK override in foreign table. Returns: str | None: Foreign value if found, else None. \"\"\" logger . debug ( f \"Looking up foreign value: { locals () } \" ) result = None primary_table = get_class_from_table_name ( base , primary_table_name ) foreign_table = get_class_from_table_name ( base , foreign_table_name ) if primary_table_pk_name : pk_column = getattr ( primary_table , primary_table_pk_name ) primary_row = db . session . query ( primary_table ) . filter ( pk_column == primary_table_pk_value ) . first () else : primary_row = db . session . query ( primary_table ) . get ( primary_table_pk_value ) if primary_row : fk_value = getattr ( primary_row , primary_table_fk_name ) if foreign_table_pk_name : fk_column = getattr ( foreign_table , foreign_table_pk_name ) foreign_row = db . session . query ( foreign_table ) . filter ( fk_column == fk_value ) . first () else : foreign_row = db . session . query ( foreign_table ) . get ( fk_value ) if foreign_row : result = getattr ( foreign_row , foreign_table_column_name ) logger . debug ( f \"Foreign key result: { result } \" ) return result get_rows_by_table_name ( db , base , table_name , colum_name_pk = None , ascending = True ) Retrieve all rows from a table, optionally sorted by a column. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy db object. base ( DeclarativeMeta ) \u2013 Declarative base. table_name ( str ) \u2013 Table name. colum_name_pk ( str | None , default: None ) \u2013 Column to sort by. ascending ( bool , default: True ) \u2013 Sort order. Returns: list \u2013 list[DeclarativeMeta]: List of ORM model instances. Source code in arb\\utils\\sql_alchemy.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 def get_rows_by_table_name ( db : SQLAlchemy , base : DeclarativeMeta , table_name : str , colum_name_pk : str | None = None , ascending : bool = True ) -> list : \"\"\" Retrieve all rows from a table, optionally sorted by a column. Args: db: SQLAlchemy db object. base (DeclarativeMeta): Declarative base. table_name (str): Table name. colum_name_pk (str | None): Column to sort by. ascending (bool): Sort order. Returns: list[DeclarativeMeta]: List of ORM model instances. \"\"\" table = get_class_from_table_name ( base , table_name ) logger . info ( f \" { type ( table ) =} \" ) query = db . session . query ( table ) if colum_name_pk : column = getattr ( table , colum_name_pk ) query = query . order_by ( column if ascending else desc ( column )) rows = query . all () logger . debug ( f \"Query result: { type ( rows ) =} \" ) return rows get_sa_automap_types ( engine , base ) Return column type metadata for all mapped classes. Parameters: engine ( Engine ) \u2013 SQLAlchemy engine instance. base ( DeclarativeMeta ) \u2013 Automap base prepared with reflected metadata. Returns: dict ( dict ) \u2013 Nested mapping: table -> column -> type category. The dict is database table names, columns names, and datatypes with the structure: result[table_name][column_name][kind] = type where: kind can be 'database_type', 'sqlalchemy_type', or 'python_type' Notes base likely created with: base = automap_base() base.prepare(db.engine, reflect=True) Source code in arb\\utils\\sql_alchemy.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def get_sa_automap_types ( engine : Engine , base : DeclarativeMeta ) -> dict : \"\"\" Return column type metadata for all mapped classes. Args: engine (Engine): SQLAlchemy engine instance. base (DeclarativeMeta): Automap base prepared with reflected metadata. Returns: dict: Nested mapping: table -> column -> type category. The dict is database table names, columns names, and datatypes with the structure: result[table_name][column_name][kind] = type where: kind can be 'database_type', 'sqlalchemy_type', or 'python_type' Notes: - base likely created with: base = automap_base() base.prepare(db.engine, reflect=True) \"\"\" logger . debug ( f \"calling get_sa_automap_types()\" ) result = {} inspector = inspect ( engine ) # Loop through all the mapped classes (tables) # print(f\"{type(base)=}\") # print(f\"{type(base.classes)=}\") for class_name , mapped_class in base . classes . items (): # print(f\"Table: {class_name}\") result [ class_name ] = {} # Get the table columns columns = mapped_class . __table__ . columns # Loop through columns to get types for column in columns : # print(f\"Column: {column.name}\") result [ class_name ][ column . name ] = {} db_type = None sa_type = None py_type = None # Database (SQL) column type db_column_type = inspector . get_columns ( class_name ) for col in db_column_type : if col [ 'name' ] == column . name : db_type = col [ 'type' ] # print(f\" Database type (SQL): {db_type}\") # SQLAlchemy type sa_type = type ( db_type ) . __name__ # print(f\" SQLAlchemy type: {sa_type}\") # Python type try : py_type = column . type . python_type except Exception as e : msg = f \" { column . name } is of type: { sa_type } that is not implemented in python. Setting python type to None.\" logger . warning ( msg ) logger . warning ( e ) # print(f\" Python type: {py_type}\") result [ class_name ][ column . name ][ \"python_type\" ] = py_type result [ class_name ][ column . name ][ \"database_type\" ] = db_type result [ class_name ][ column . name ][ \"sqlalchemy_type\" ] = sa_type logger . debug ( f \"returning from get_sa_automap_types()\" ) return result get_sa_column_types ( model , is_instance = False ) Return a mapping of each column to its SQLAlchemy and Python types. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance or class. is_instance ( bool , default: False ) \u2013 True if model is an instance, False if a class. Returns: dict ( dict ) \u2013 Mapping from column names to a dict with 'sqlalchemy_type' and 'python_type'. Example get_sa_column_types(User) { 'id': {'sa_type': 'Integer', 'py_type': 'int'}, 'email': {'sa_type': 'String', 'py_type': 'str'} } Source code in arb\\utils\\sql_alchemy.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def get_sa_column_types ( model : DeclarativeMeta , is_instance : bool = False ) -> dict : \"\"\" Return a mapping of each column to its SQLAlchemy and Python types. Args: model (DeclarativeMeta): SQLAlchemy model instance or class. is_instance (bool): True if `model` is an instance, False if a class. Returns: dict: Mapping from column names to a dict with 'sqlalchemy_type' and 'python_type'. Example: >>> get_sa_column_types(User) { 'id': {'sa_type': 'Integer', 'py_type': 'int'}, 'email': {'sa_type': 'String', 'py_type': 'str'} } \"\"\" # Get the table inspector for the model if is_instance : inspector = inspect ( type ( model )) else : inspector = inspect ( model ) logger . debug ( f \" \\t { model =} \" ) logger . debug ( f \" \\t { inspector =} \" ) columns_info = {} for column in inspector . columns : col_name = column . name try : columns_info [ col_name ] = { 'sqlalchemy_type' : column . type , 'python_type' : column . type . python_type } except Exception as e : logger . warning ( f \" { col_name } has unsupported Python type.\" ) logger . warning ( e ) columns_info [ col_name ] = { 'sqlalchemy_type' : column . type , 'python_type' : None } raise # Re-raises the current exception with original traceback - comment out if you don't to warn rather than fail return columns_info get_sa_fields ( model ) Get a sorted list of column names for a SQLAlchemy model. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance or class. Returns: list [ str ] \u2013 list[str]: Alphabetically sorted list of column attribute names. Example get_sa_fields(User) ['email', 'id', 'name'] Source code in arb\\utils\\sql_alchemy.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def get_sa_fields ( model : DeclarativeMeta ) -> list [ str ]: \"\"\" Get a sorted list of column names for a SQLAlchemy model. Args: model (DeclarativeMeta): SQLAlchemy model instance or class. Returns: list[str]: Alphabetically sorted list of column attribute names. Example: >>> get_sa_fields(User) ['email', 'id', 'name'] \"\"\" inst = inspect ( model ) model_fields = [ c_attr . key for c_attr in inst . mapper . column_attrs ] model_fields . sort () return model_fields get_table_row_and_column ( db , base , table_name , column_name , id_ ) Fetch a row and column value given table name and primary key. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. base ( DeclarativeMeta ) \u2013 Declarative base. table_name ( str ) \u2013 Table name. column_name ( str ) \u2013 Column of interest. id_ ( int ) \u2013 Primary key value. Returns: tuple | None \u2013 tuple | None: (row, value) if found, else (None, None). Source code in arb\\utils\\sql_alchemy.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 def get_table_row_and_column ( db : SQLAlchemy , base : DeclarativeMeta , table_name : str , column_name : str , id_ : int ) -> tuple | None : \"\"\" Fetch a row and column value given table name and primary key. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. base (DeclarativeMeta): Declarative base. table_name (str): Table name. column_name (str): Column of interest. id_ (int): Primary key value. Returns: tuple | None: (row, value) if found, else (None, None). \"\"\" logger . debug ( f \"Getting { column_name } from { table_name } where pk= { id_ } \" ) column_value = None table = get_class_from_table_name ( base , table_name ) row = db . session . query ( table ) . get ( id_ ) if row : column_value = getattr ( row , column_name ) logger . debug ( f \" { row =} , { column_value =} \" ) return row , column_value load_model_json_column ( model , column_name ) Safely extract and normalize a JSON dictionary from a model's column. This helper ensures that the value stored in a model's JSON column is returned as a Python dictionary, regardless of whether it's stored as a JSON string or native dict in the database. If the value is a malformed JSON string, a warning is logged and an empty dict is returned. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy ORM model instance. column_name ( str ) \u2013 Name of the attribute on the model (e.g., 'misc_json'). Returns: dict ( dict ) \u2013 Parsed dictionary from the JSON column. Defaults to {} on failure or None. Source code in arb\\utils\\sql_alchemy.py 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 def load_model_json_column ( model : DeclarativeMeta , column_name : str ) -> dict : \"\"\" Safely extract and normalize a JSON dictionary from a model's column. This helper ensures that the value stored in a model's JSON column is returned as a Python dictionary, regardless of whether it's stored as a JSON string or native dict in the database. If the value is a malformed JSON string, a warning is logged and an empty dict is returned. Args: model (DeclarativeMeta): SQLAlchemy ORM model instance. column_name (str): Name of the attribute on the model (e.g., 'misc_json'). Returns: dict: Parsed dictionary from the JSON column. Defaults to {} on failure or None. \"\"\" raw_value = getattr ( model , column_name ) if isinstance ( raw_value , str ): try : return json . loads ( raw_value ) except json . JSONDecodeError : logger . warning ( f \"Corrupt JSON found in { column_name } , resetting to empty dict.\" ) return {} elif raw_value is None : return {} elif isinstance ( raw_value , dict ): return raw_value else : raise TypeError ( f \"Expected str, dict, or None for { column_name } , got { type ( raw_value ) . __name__ } \" ) run_diagnostics ( db , base , session ) Run diagnostics to validate SQLAlchemy utilities in this module. This function performs Model diagnostics Type inspection Conversion to dictionary Change detection Table row extraction Primary and foreign key access Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy db object. base ( DeclarativeMeta ) \u2013 Declarative or automap base. session ( Session ) \u2013 Active SQLAlchemy session. Example run_diagnostics(db, base, db.session) Source code in arb\\utils\\sql_alchemy.py 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 def run_diagnostics ( db : SQLAlchemy , base : DeclarativeMeta , session : Session ) -> None : \"\"\" Run diagnostics to validate SQLAlchemy utilities in this module. This function performs: - Model diagnostics - Type inspection - Conversion to dictionary - Change detection - Table row extraction - Primary and foreign key access Args: db: SQLAlchemy db object. base (DeclarativeMeta): Declarative or automap base. session (Session): Active SQLAlchemy session. Example: >>> run_diagnostics(db, base, db.session) \"\"\" print ( \"Running diagnostics for sqlalchemy_util.py...\" ) # Use first available mapped class class_names = list ( base . classes . keys ()) assert class_names , \"No mapped tables found\" test_table = class_names [ 0 ] cls = base . classes [ test_table ] row = session . query ( cls ) . first () assert row , f \"No rows in table ' { test_table } '\" print ( f \"Using table: { test_table } \" ) # Field listing fields = get_sa_fields ( row ) print ( f \"Fields: { fields } \" ) # Type info types = get_sa_column_types ( row ) print ( f \"Column types: { types } \" ) # Dict conversion and comparison d1 = sa_model_to_dict ( row ) d2 = d1 . copy () d2 [ fields [ 0 ]] = \"__CHANGED__\" changes = sa_model_dict_compare ( d1 , d2 ) assert fields [ 0 ] in changes , \"Change detection failed\" # Table to list all_rows = table_to_list ( base , session , test_table ) assert isinstance ( all_rows , list ) and all_rows , \"table_to_list failed\" # Try auto-increment value if PK exists pk_column = fields [ 0 ] if hasattr ( cls , pk_column ): result = find_auto_increment_value ( db , test_table , pk_column ) print ( f \"Auto-increment: { result } \" ) print ( \"All diagnostics completed successfully.\" ) sa_model_diagnostics ( model , comment = '' ) Log diagnostic details about a SQLAlchemy model instance. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance. comment ( str , default: '' ) \u2013 Optional comment header for log output. Returns: None \u2013 None Example sa_model_diagnostics(user, comment=\"Inspecting User\") Source code in arb\\utils\\sql_alchemy.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def sa_model_diagnostics ( model : DeclarativeMeta , comment : str = \"\" ) -> None : \"\"\" Log diagnostic details about a SQLAlchemy model instance. Args: model (DeclarativeMeta): SQLAlchemy model instance. comment (str): Optional comment header for log output. Returns: None Example: >>> sa_model_diagnostics(user, comment=\"Inspecting User\") \"\"\" logger . debug ( f \"Diagnostics for model of type { type ( model ) =} \" ) if comment : logger . debug ( f \" { comment } \" ) logger . debug ( f \" { model =} \" ) fields = get_sa_fields ( model ) for key in fields : value = getattr ( model , key ) logger . debug ( f \" { key } { type ( value ) } = ( { value } )\" ) sa_model_dict_compare ( model_before , model_after ) Compare two model dictionaries and return changed fields. Parameters: model_before ( dict ) \u2013 Original state. model_after ( dict ) \u2013 New state. Returns: dict ( dict ) \u2013 Changed fields and their new values. Source code in arb\\utils\\sql_alchemy.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def sa_model_dict_compare ( model_before : dict , model_after : dict ) -> dict : \"\"\" Compare two model dictionaries and return changed fields. Args: model_before (dict): Original state. model_after (dict): New state. Returns: dict: Changed fields and their new values. \"\"\" changes = {} for field in model_after : if field not in model_before or model_before [ field ] != model_after [ field ]: changes [ field ] = model_after [ field ] return changes sa_model_to_dict ( model ) Convert a SQLAlchemy model to a Python dictionary. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model. Returns: dict ( dict ) \u2013 Dictionary with column names as keys and values from the model. Source code in arb\\utils\\sql_alchemy.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def sa_model_to_dict ( model : DeclarativeMeta ) -> dict : \"\"\" Convert a SQLAlchemy model to a Python dictionary. Args: model (DeclarativeMeta): SQLAlchemy model. Returns: dict: Dictionary with column names as keys and values from the model. \"\"\" model_as_dict = {} fields = get_sa_fields ( model ) for field in fields : value = getattr ( model , field ) model_as_dict [ field ] = value return model_as_dict table_to_list ( base , session , table_name ) Convert all rows of a mapped table to a list of dicts. Parameters: base ( DeclarativeMeta ) \u2013 Automap base. session ( Session ) \u2013 SQLAlchemy session. table_name ( str ) \u2013 Table name to query. Returns: list [ dict ] \u2013 list[dict]: List of row dictionaries. Source code in arb\\utils\\sql_alchemy.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def table_to_list ( base : DeclarativeMeta , session : Session , table_name : str ) -> list [ dict ]: \"\"\" Convert all rows of a mapped table to a list of dicts. Args: base (DeclarativeMeta): Automap base. session (Session): SQLAlchemy session. table_name (str): Table name to query. Returns: list[dict]: List of row dictionaries. \"\"\" result = [] table = base . classes . get ( table_name ) if table : logger . debug ( f \"Selecting data from: { table_name } \" ) rows = session . query ( table ) . all () col_names = table . __table__ . columns . keys () for row in rows : row_data = { col : getattr ( row , col ) for col in col_names } result . append ( row_data ) else : logger . warning ( f \"Table ' { table_name } ' not found in metadata.\" ) return result","title":"arb.utils.sql_alchemy"},{"location":"reference/arb/utils/sql_alchemy/#arbutilssql_alchemy","text":"SQLAlchemy utility functions for Flask applications using declarative or automap base models. This module provides introspection, diagnostics, and model-row operations for SQLAlchemy-based Flask apps. It supports both declarative and automapped models.","title":"arb.utils.sql_alchemy"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy--included-utilities","text":"Model diagnostics ( sa_model_diagnostics ) Field and column type inspection ( get_sa_fields , get_sa_column_types ) Full automap type mapping ( get_sa_automap_types ) Dictionary conversions ( sa_model_to_dict , sa_model_dict_compare ) Table-to-dict exports ( table_to_list ) Table/class lookups ( get_class_from_table_name ) Row fetch and sort utilities ( get_rows_by_table_name ) Model add/delete with logging ( add_commit_and_log_model , delete_commit_and_log_model ) Foreign key traversal ( get_foreign_value ) PostgreSQL sequence inspection ( find_auto_increment_value ) JSON column loader ( load_model_json_column )","title":"Included Utilities:"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy--type-hints","text":"db (SQLAlchemy) : Flask-SQLAlchemy database instance with .session and .engine. base (DeclarativeMeta) : Declarative or automapped SQLAlchemy base (e.g., via automap_base()). model (DeclarativeMeta) : SQLAlchemy ORM model class or instance. session (Session) : SQLAlchemy session object.","title":"Type Hints:"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy--usage-notes","text":"Supports PostgreSQL features like sequence inspection via pg_get_serial_sequence . Logging is integrated for debugging and auditing. Compatible with Python 3.10+ syntax (PEP 604 union types). Version 1.0.0","title":"Usage Notes:"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.add_commit_and_log_model","text":"Add or update a model instance, log changes, and commit. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. model_row ( DeclarativeMeta ) \u2013 ORM model instance. comment ( str , default: '' ) \u2013 Optional log comment. model_before ( dict | None , default: None ) \u2013 Optional snapshot before changes. Source code in arb\\utils\\sql_alchemy.py 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 def add_commit_and_log_model ( db : SQLAlchemy , model_row : DeclarativeMeta , comment : str = \"\" , model_before : dict | None = None ) -> None : \"\"\" Add or update a model instance, log changes, and commit. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. model_row (DeclarativeMeta): ORM model instance. comment (str): Optional log comment. model_before (dict | None): Optional snapshot before changes. \"\"\" # todo (update) - use the payload routine apply_json_patch_and_log if model_before : logger . info ( f \"Before commit { comment =} : { model_before } \" ) try : db . session . add ( model_row ) db . session . commit () model_after = sa_model_to_dict ( model_row ) logger . info ( f \"After commit: { model_after } \" ) if model_before : changes = sa_model_dict_compare ( model_before , model_after ) logger . info ( f \"Changed values: { changes } \" ) except Exception as e : log_error ( e )","title":"add_commit_and_log_model"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.delete_commit_and_log_model","text":"Delete a model instance, log it, and commit the change. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. model_row ( DeclarativeMeta ) \u2013 ORM model instance. comment ( str , default: '' ) \u2013 Optional log comment. Source code in arb\\utils\\sql_alchemy.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 def delete_commit_and_log_model ( db : SQLAlchemy , model_row : DeclarativeMeta , comment : str = \"\" ) -> None : \"\"\" Delete a model instance, log it, and commit the change. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. model_row (DeclarativeMeta): ORM model instance. comment (str): Optional log comment. \"\"\" # todo (update) - use the payload routine apply_json_patch_and_log and or some way to track change logger . info ( f \"Deleting model { comment =} : { sa_model_to_dict ( model_row ) } \" ) try : db . session . delete ( model_row ) db . session . commit () except Exception as e : log_error ( e )","title":"delete_commit_and_log_model"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.find_auto_increment_value","text":"Find the next auto-increment value for a table column (PostgreSQL only). Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. table_name ( str ) \u2013 Table name. column_name ( str ) \u2013 Column name. Returns: str ( str ) \u2013 Human-readable summary of the next sequence value. Source code in arb\\utils\\sql_alchemy.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 def find_auto_increment_value ( db : SQLAlchemy , table_name : str , column_name : str ) -> str : \"\"\" Find the next auto-increment value for a table column (PostgreSQL only). Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. table_name (str): Table name. column_name (str): Column name. Returns: str: Human-readable summary of the next sequence value. \"\"\" with db . engine . connect () as connection : sql_seq = f \"SELECT pg_get_serial_sequence(' { table_name } ', ' { column_name } ')\" sequence_name = connection . execute ( text ( sql_seq )) . scalar () sql_nextval = f \"SELECT nextval(' { sequence_name } ')\" next_val = connection . execute ( text ( sql_nextval )) . scalar () return f \"Table ' { table_name } ' column ' { column_name } ' sequence ' { sequence_name } ' next value is ' { next_val } '\"","title":"find_auto_increment_value"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_class_from_table_name","text":"Retrieves the mapped class for a given table name. Parameters: base ( DeclarativeMeta ) \u2013 SQLAlchemy declarative base. table_name ( str ) \u2013 Database table name. Returns: DeclarativeMeta | None \u2013 DeclarativeMeta | None: Mapped SQLAlchemy ORM class, or None if not found. Notes: - To access the class mapped to a specific table name in SQLAlchemy without directly using _class_registry, you can use the Base.metadata object, which stores information about all mapped tables. get_class_from_table_name seems to fail from gpt refactor, so i kept my original code here. it failed because my old test of is not None was changed to if - be on the lookout for other subtle bugs like this Source code in arb\\utils\\sql_alchemy.py 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def get_class_from_table_name ( base : DeclarativeMeta | None , table_name : str ) -> DeclarativeMeta | None : \"\"\" Retrieves the mapped class for a given table name. Args: base (DeclarativeMeta): SQLAlchemy declarative base. table_name (str): Database table name. Returns: DeclarativeMeta | None: Mapped SQLAlchemy ORM class, or None if not found. Notes: - To access the class mapped to a specific table name in SQLAlchemy without directly using _class_registry, you can use the Base.metadata object, which stores information about all mapped tables. - get_class_from_table_name seems to fail from gpt refactor, so i kept my original code here. - it failed because my old test of is not None was changed to if - be on the lookout for other subtle bugs like this \"\"\" try : # Look up the table in metadata and find its mapped class table = base . metadata . tables . get ( table_name ) if table is not None : for mapper in base . registry . mappers : if mapper . local_table == table : return mapper . class_ return None except Exception as e : msg = f \"Exception occurred when trying to get table named { table_name } \" logger . error ( msg , exc_info = True ) logger . error ( f \"exception info: { e } \" ) return None","title":"get_class_from_table_name"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_foreign_value","text":"Resolve a foreign key reference and return its value. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. base ( DeclarativeMeta ) \u2013 Declarative base. primary_table_name ( str ) \u2013 Table with FK. foreign_table_name ( str ) \u2013 Table with desired value. primary_table_fk_name ( str ) \u2013 FK field name. foreign_table_column_name ( str ) \u2013 Target field name in foreign table. primary_table_pk_value ( int ) \u2013 PK value of row in primary table. primary_table_pk_name ( str | None , default: None ) \u2013 Optional PK field override. foreign_table_pk_name ( str | None , default: None ) \u2013 Optional PK override in foreign table. Returns: str | None \u2013 str | None: Foreign value if found, else None. Source code in arb\\utils\\sql_alchemy.py 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 def get_foreign_value ( db : SQLAlchemy , base : DeclarativeMeta , primary_table_name : str , foreign_table_name : str , primary_table_fk_name : str , foreign_table_column_name : str , primary_table_pk_value : int , primary_table_pk_name : str | None = None , foreign_table_pk_name : str | None = None ) -> str | None : \"\"\" Resolve a foreign key reference and return its value. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. base (DeclarativeMeta): Declarative base. primary_table_name (str): Table with FK. foreign_table_name (str): Table with desired value. primary_table_fk_name (str): FK field name. foreign_table_column_name (str): Target field name in foreign table. primary_table_pk_value (int): PK value of row in primary table. primary_table_pk_name (str | None): Optional PK field override. foreign_table_pk_name (str | None): Optional PK override in foreign table. Returns: str | None: Foreign value if found, else None. \"\"\" logger . debug ( f \"Looking up foreign value: { locals () } \" ) result = None primary_table = get_class_from_table_name ( base , primary_table_name ) foreign_table = get_class_from_table_name ( base , foreign_table_name ) if primary_table_pk_name : pk_column = getattr ( primary_table , primary_table_pk_name ) primary_row = db . session . query ( primary_table ) . filter ( pk_column == primary_table_pk_value ) . first () else : primary_row = db . session . query ( primary_table ) . get ( primary_table_pk_value ) if primary_row : fk_value = getattr ( primary_row , primary_table_fk_name ) if foreign_table_pk_name : fk_column = getattr ( foreign_table , foreign_table_pk_name ) foreign_row = db . session . query ( foreign_table ) . filter ( fk_column == fk_value ) . first () else : foreign_row = db . session . query ( foreign_table ) . get ( fk_value ) if foreign_row : result = getattr ( foreign_row , foreign_table_column_name ) logger . debug ( f \"Foreign key result: { result } \" ) return result","title":"get_foreign_value"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_rows_by_table_name","text":"Retrieve all rows from a table, optionally sorted by a column. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy db object. base ( DeclarativeMeta ) \u2013 Declarative base. table_name ( str ) \u2013 Table name. colum_name_pk ( str | None , default: None ) \u2013 Column to sort by. ascending ( bool , default: True ) \u2013 Sort order. Returns: list \u2013 list[DeclarativeMeta]: List of ORM model instances. Source code in arb\\utils\\sql_alchemy.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 def get_rows_by_table_name ( db : SQLAlchemy , base : DeclarativeMeta , table_name : str , colum_name_pk : str | None = None , ascending : bool = True ) -> list : \"\"\" Retrieve all rows from a table, optionally sorted by a column. Args: db: SQLAlchemy db object. base (DeclarativeMeta): Declarative base. table_name (str): Table name. colum_name_pk (str | None): Column to sort by. ascending (bool): Sort order. Returns: list[DeclarativeMeta]: List of ORM model instances. \"\"\" table = get_class_from_table_name ( base , table_name ) logger . info ( f \" { type ( table ) =} \" ) query = db . session . query ( table ) if colum_name_pk : column = getattr ( table , colum_name_pk ) query = query . order_by ( column if ascending else desc ( column )) rows = query . all () logger . debug ( f \"Query result: { type ( rows ) =} \" ) return rows","title":"get_rows_by_table_name"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_sa_automap_types","text":"Return column type metadata for all mapped classes. Parameters: engine ( Engine ) \u2013 SQLAlchemy engine instance. base ( DeclarativeMeta ) \u2013 Automap base prepared with reflected metadata. Returns: dict ( dict ) \u2013 Nested mapping: table -> column -> type category. The dict is database table names, columns names, and datatypes with the structure: result[table_name][column_name][kind] = type where: kind can be 'database_type', 'sqlalchemy_type', or 'python_type' Notes base likely created with: base = automap_base() base.prepare(db.engine, reflect=True) Source code in arb\\utils\\sql_alchemy.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def get_sa_automap_types ( engine : Engine , base : DeclarativeMeta ) -> dict : \"\"\" Return column type metadata for all mapped classes. Args: engine (Engine): SQLAlchemy engine instance. base (DeclarativeMeta): Automap base prepared with reflected metadata. Returns: dict: Nested mapping: table -> column -> type category. The dict is database table names, columns names, and datatypes with the structure: result[table_name][column_name][kind] = type where: kind can be 'database_type', 'sqlalchemy_type', or 'python_type' Notes: - base likely created with: base = automap_base() base.prepare(db.engine, reflect=True) \"\"\" logger . debug ( f \"calling get_sa_automap_types()\" ) result = {} inspector = inspect ( engine ) # Loop through all the mapped classes (tables) # print(f\"{type(base)=}\") # print(f\"{type(base.classes)=}\") for class_name , mapped_class in base . classes . items (): # print(f\"Table: {class_name}\") result [ class_name ] = {} # Get the table columns columns = mapped_class . __table__ . columns # Loop through columns to get types for column in columns : # print(f\"Column: {column.name}\") result [ class_name ][ column . name ] = {} db_type = None sa_type = None py_type = None # Database (SQL) column type db_column_type = inspector . get_columns ( class_name ) for col in db_column_type : if col [ 'name' ] == column . name : db_type = col [ 'type' ] # print(f\" Database type (SQL): {db_type}\") # SQLAlchemy type sa_type = type ( db_type ) . __name__ # print(f\" SQLAlchemy type: {sa_type}\") # Python type try : py_type = column . type . python_type except Exception as e : msg = f \" { column . name } is of type: { sa_type } that is not implemented in python. Setting python type to None.\" logger . warning ( msg ) logger . warning ( e ) # print(f\" Python type: {py_type}\") result [ class_name ][ column . name ][ \"python_type\" ] = py_type result [ class_name ][ column . name ][ \"database_type\" ] = db_type result [ class_name ][ column . name ][ \"sqlalchemy_type\" ] = sa_type logger . debug ( f \"returning from get_sa_automap_types()\" ) return result","title":"get_sa_automap_types"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_sa_column_types","text":"Return a mapping of each column to its SQLAlchemy and Python types. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance or class. is_instance ( bool , default: False ) \u2013 True if model is an instance, False if a class. Returns: dict ( dict ) \u2013 Mapping from column names to a dict with 'sqlalchemy_type' and 'python_type'. Example get_sa_column_types(User) { 'id': {'sa_type': 'Integer', 'py_type': 'int'}, 'email': {'sa_type': 'String', 'py_type': 'str'} } Source code in arb\\utils\\sql_alchemy.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 def get_sa_column_types ( model : DeclarativeMeta , is_instance : bool = False ) -> dict : \"\"\" Return a mapping of each column to its SQLAlchemy and Python types. Args: model (DeclarativeMeta): SQLAlchemy model instance or class. is_instance (bool): True if `model` is an instance, False if a class. Returns: dict: Mapping from column names to a dict with 'sqlalchemy_type' and 'python_type'. Example: >>> get_sa_column_types(User) { 'id': {'sa_type': 'Integer', 'py_type': 'int'}, 'email': {'sa_type': 'String', 'py_type': 'str'} } \"\"\" # Get the table inspector for the model if is_instance : inspector = inspect ( type ( model )) else : inspector = inspect ( model ) logger . debug ( f \" \\t { model =} \" ) logger . debug ( f \" \\t { inspector =} \" ) columns_info = {} for column in inspector . columns : col_name = column . name try : columns_info [ col_name ] = { 'sqlalchemy_type' : column . type , 'python_type' : column . type . python_type } except Exception as e : logger . warning ( f \" { col_name } has unsupported Python type.\" ) logger . warning ( e ) columns_info [ col_name ] = { 'sqlalchemy_type' : column . type , 'python_type' : None } raise # Re-raises the current exception with original traceback - comment out if you don't to warn rather than fail return columns_info","title":"get_sa_column_types"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_sa_fields","text":"Get a sorted list of column names for a SQLAlchemy model. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance or class. Returns: list [ str ] \u2013 list[str]: Alphabetically sorted list of column attribute names. Example get_sa_fields(User) ['email', 'id', 'name'] Source code in arb\\utils\\sql_alchemy.py 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def get_sa_fields ( model : DeclarativeMeta ) -> list [ str ]: \"\"\" Get a sorted list of column names for a SQLAlchemy model. Args: model (DeclarativeMeta): SQLAlchemy model instance or class. Returns: list[str]: Alphabetically sorted list of column attribute names. Example: >>> get_sa_fields(User) ['email', 'id', 'name'] \"\"\" inst = inspect ( model ) model_fields = [ c_attr . key for c_attr in inst . mapper . column_attrs ] model_fields . sort () return model_fields","title":"get_sa_fields"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.get_table_row_and_column","text":"Fetch a row and column value given table name and primary key. Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy instance bound to the Flask app. base ( DeclarativeMeta ) \u2013 Declarative base. table_name ( str ) \u2013 Table name. column_name ( str ) \u2013 Column of interest. id_ ( int ) \u2013 Primary key value. Returns: tuple | None \u2013 tuple | None: (row, value) if found, else (None, None). Source code in arb\\utils\\sql_alchemy.py 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 def get_table_row_and_column ( db : SQLAlchemy , base : DeclarativeMeta , table_name : str , column_name : str , id_ : int ) -> tuple | None : \"\"\" Fetch a row and column value given table name and primary key. Args: db (SQLAlchemy): SQLAlchemy instance bound to the Flask app. base (DeclarativeMeta): Declarative base. table_name (str): Table name. column_name (str): Column of interest. id_ (int): Primary key value. Returns: tuple | None: (row, value) if found, else (None, None). \"\"\" logger . debug ( f \"Getting { column_name } from { table_name } where pk= { id_ } \" ) column_value = None table = get_class_from_table_name ( base , table_name ) row = db . session . query ( table ) . get ( id_ ) if row : column_value = getattr ( row , column_name ) logger . debug ( f \" { row =} , { column_value =} \" ) return row , column_value","title":"get_table_row_and_column"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.load_model_json_column","text":"Safely extract and normalize a JSON dictionary from a model's column. This helper ensures that the value stored in a model's JSON column is returned as a Python dictionary, regardless of whether it's stored as a JSON string or native dict in the database. If the value is a malformed JSON string, a warning is logged and an empty dict is returned. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy ORM model instance. column_name ( str ) \u2013 Name of the attribute on the model (e.g., 'misc_json'). Returns: dict ( dict ) \u2013 Parsed dictionary from the JSON column. Defaults to {} on failure or None. Source code in arb\\utils\\sql_alchemy.py 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 def load_model_json_column ( model : DeclarativeMeta , column_name : str ) -> dict : \"\"\" Safely extract and normalize a JSON dictionary from a model's column. This helper ensures that the value stored in a model's JSON column is returned as a Python dictionary, regardless of whether it's stored as a JSON string or native dict in the database. If the value is a malformed JSON string, a warning is logged and an empty dict is returned. Args: model (DeclarativeMeta): SQLAlchemy ORM model instance. column_name (str): Name of the attribute on the model (e.g., 'misc_json'). Returns: dict: Parsed dictionary from the JSON column. Defaults to {} on failure or None. \"\"\" raw_value = getattr ( model , column_name ) if isinstance ( raw_value , str ): try : return json . loads ( raw_value ) except json . JSONDecodeError : logger . warning ( f \"Corrupt JSON found in { column_name } , resetting to empty dict.\" ) return {} elif raw_value is None : return {} elif isinstance ( raw_value , dict ): return raw_value else : raise TypeError ( f \"Expected str, dict, or None for { column_name } , got { type ( raw_value ) . __name__ } \" )","title":"load_model_json_column"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.run_diagnostics","text":"Run diagnostics to validate SQLAlchemy utilities in this module. This function performs Model diagnostics Type inspection Conversion to dictionary Change detection Table row extraction Primary and foreign key access Parameters: db ( SQLAlchemy ) \u2013 SQLAlchemy db object. base ( DeclarativeMeta ) \u2013 Declarative or automap base. session ( Session ) \u2013 Active SQLAlchemy session. Example run_diagnostics(db, base, db.session) Source code in arb\\utils\\sql_alchemy.py 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 def run_diagnostics ( db : SQLAlchemy , base : DeclarativeMeta , session : Session ) -> None : \"\"\" Run diagnostics to validate SQLAlchemy utilities in this module. This function performs: - Model diagnostics - Type inspection - Conversion to dictionary - Change detection - Table row extraction - Primary and foreign key access Args: db: SQLAlchemy db object. base (DeclarativeMeta): Declarative or automap base. session (Session): Active SQLAlchemy session. Example: >>> run_diagnostics(db, base, db.session) \"\"\" print ( \"Running diagnostics for sqlalchemy_util.py...\" ) # Use first available mapped class class_names = list ( base . classes . keys ()) assert class_names , \"No mapped tables found\" test_table = class_names [ 0 ] cls = base . classes [ test_table ] row = session . query ( cls ) . first () assert row , f \"No rows in table ' { test_table } '\" print ( f \"Using table: { test_table } \" ) # Field listing fields = get_sa_fields ( row ) print ( f \"Fields: { fields } \" ) # Type info types = get_sa_column_types ( row ) print ( f \"Column types: { types } \" ) # Dict conversion and comparison d1 = sa_model_to_dict ( row ) d2 = d1 . copy () d2 [ fields [ 0 ]] = \"__CHANGED__\" changes = sa_model_dict_compare ( d1 , d2 ) assert fields [ 0 ] in changes , \"Change detection failed\" # Table to list all_rows = table_to_list ( base , session , test_table ) assert isinstance ( all_rows , list ) and all_rows , \"table_to_list failed\" # Try auto-increment value if PK exists pk_column = fields [ 0 ] if hasattr ( cls , pk_column ): result = find_auto_increment_value ( db , test_table , pk_column ) print ( f \"Auto-increment: { result } \" ) print ( \"All diagnostics completed successfully.\" )","title":"run_diagnostics"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.sa_model_diagnostics","text":"Log diagnostic details about a SQLAlchemy model instance. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance. comment ( str , default: '' ) \u2013 Optional comment header for log output. Returns: None \u2013 None Example sa_model_diagnostics(user, comment=\"Inspecting User\") Source code in arb\\utils\\sql_alchemy.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def sa_model_diagnostics ( model : DeclarativeMeta , comment : str = \"\" ) -> None : \"\"\" Log diagnostic details about a SQLAlchemy model instance. Args: model (DeclarativeMeta): SQLAlchemy model instance. comment (str): Optional comment header for log output. Returns: None Example: >>> sa_model_diagnostics(user, comment=\"Inspecting User\") \"\"\" logger . debug ( f \"Diagnostics for model of type { type ( model ) =} \" ) if comment : logger . debug ( f \" { comment } \" ) logger . debug ( f \" { model =} \" ) fields = get_sa_fields ( model ) for key in fields : value = getattr ( model , key ) logger . debug ( f \" { key } { type ( value ) } = ( { value } )\" )","title":"sa_model_diagnostics"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.sa_model_dict_compare","text":"Compare two model dictionaries and return changed fields. Parameters: model_before ( dict ) \u2013 Original state. model_after ( dict ) \u2013 New state. Returns: dict ( dict ) \u2013 Changed fields and their new values. Source code in arb\\utils\\sql_alchemy.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 def sa_model_dict_compare ( model_before : dict , model_after : dict ) -> dict : \"\"\" Compare two model dictionaries and return changed fields. Args: model_before (dict): Original state. model_after (dict): New state. Returns: dict: Changed fields and their new values. \"\"\" changes = {} for field in model_after : if field not in model_before or model_before [ field ] != model_after [ field ]: changes [ field ] = model_after [ field ] return changes","title":"sa_model_dict_compare"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.sa_model_to_dict","text":"Convert a SQLAlchemy model to a Python dictionary. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model. Returns: dict ( dict ) \u2013 Dictionary with column names as keys and values from the model. Source code in arb\\utils\\sql_alchemy.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def sa_model_to_dict ( model : DeclarativeMeta ) -> dict : \"\"\" Convert a SQLAlchemy model to a Python dictionary. Args: model (DeclarativeMeta): SQLAlchemy model. Returns: dict: Dictionary with column names as keys and values from the model. \"\"\" model_as_dict = {} fields = get_sa_fields ( model ) for field in fields : value = getattr ( model , field ) model_as_dict [ field ] = value return model_as_dict","title":"sa_model_to_dict"},{"location":"reference/arb/utils/sql_alchemy/#arb.utils.sql_alchemy.table_to_list","text":"Convert all rows of a mapped table to a list of dicts. Parameters: base ( DeclarativeMeta ) \u2013 Automap base. session ( Session ) \u2013 SQLAlchemy session. table_name ( str ) \u2013 Table name to query. Returns: list [ dict ] \u2013 list[dict]: List of row dictionaries. Source code in arb\\utils\\sql_alchemy.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 def table_to_list ( base : DeclarativeMeta , session : Session , table_name : str ) -> list [ dict ]: \"\"\" Convert all rows of a mapped table to a list of dicts. Args: base (DeclarativeMeta): Automap base. session (Session): SQLAlchemy session. table_name (str): Table name to query. Returns: list[dict]: List of row dictionaries. \"\"\" result = [] table = base . classes . get ( table_name ) if table : logger . debug ( f \"Selecting data from: { table_name } \" ) rows = session . query ( table ) . all () col_names = table . __table__ . columns . keys () for row in rows : row_data = { col : getattr ( row , col ) for col in col_names } result . append ( row_data ) else : logger . warning ( f \"Table ' { table_name } ' not found in metadata.\" ) return result","title":"table_to_list"},{"location":"reference/arb/utils/web_html/","text":"arb.utils.web_html HTML and WTForms utility functions for form handling and file uploads. This module provides helper functions for Uploading user files with sanitized names Generating WTForms-compatible selector lists Managing triple tuples for dynamic dropdown metadata Notes Avoids circular imports by not depending on other utility modules. Other utility modules (e.g., Excel, DB) may safely import this one. Adds \"Please Select\" logic to dropdowns using arb.utils.constants . Example Usage from arb.utils.web_html import upload_single_file, selector_list_to_tuples ensure_placeholder_option ( tuple_list , item = PLEASE_SELECT , item_dict = { 'disabled' : True }, ensure_first = True ) Ensure a placeholder entry is present in the tuple list. This function ensures that a specified \"placeholder\" option (typically used to prompt users to select a value, such as \"Please Select\") exists in the given list of selector options. If the placeholder is not present, it is inserted at the top. If it exists but is not the first item, it is optionally moved to the first position. Parameters: tuple_list ( list [ tuple [ str , str , dict ]] ) \u2013 Original selector list. item ( str , default: PLEASE_SELECT ) \u2013 Value for the placeholder. Default is \"Please Select\". item_dict ( dict , default: {'disabled': True} ) \u2013 Metadata for the placeholder. Default disables the option. ensure_first ( bool , default: True ) \u2013 If True, move placeholder to top if found elsewhere. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Updated tuple list with ensured placeholder. Example ensure_placeholder_option([(\"A\", \"A\", {})]) [('Please Select', 'Please Select', {'disabled': True}), ('A', 'A', {})] Source code in arb\\utils\\web_html.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def ensure_placeholder_option ( tuple_list : list [ tuple [ str , str , dict ]], item : str = PLEASE_SELECT , item_dict : dict = { \"disabled\" : True }, ensure_first : bool = True ) -> list [ tuple [ str , str , dict ]]: \"\"\" Ensure a placeholder entry is present in the tuple list. This function ensures that a specified \"placeholder\" option (typically used to prompt users to select a value, such as \"Please Select\") exists in the given list of selector options. If the placeholder is not present, it is inserted at the top. If it exists but is not the first item, it is optionally moved to the first position. Args: tuple_list (list[tuple[str, str, dict]]): Original selector list. item (str): Value for the placeholder. Default is \"Please Select\". item_dict (dict): Metadata for the placeholder. Default disables the option. ensure_first (bool): If True, move placeholder to top if found elsewhere. Returns: list[tuple[str, str, dict]]: Updated tuple list with ensured placeholder. Example: >>> ensure_placeholder_option([(\"A\", \"A\", {})]) [('Please Select', 'Please Select', {'disabled': True}), ('A', 'A', {})] \"\"\" if item is None : item = PLEASE_SELECT if item_dict is None : item_dict = { \"disabled\" : True } placeholder = ( item , item , item_dict ) # Find the index of any existing placeholder (based on value match) # Explanation: # - `enumerate(tuple_list)` produces (index, tuple) pairs. # - `t[0] == item` checks if the first element (value) matches the placeholder value. # - `next(...)` returns the index of the first match, or `None` if no match is found. index = next (( i for i , t in enumerate ( tuple_list ) if t [ 0 ] == item ), None ) if index is None : # Placeholder not found; insert it at the beginning of the list. return [ placeholder ] + tuple_list elif ensure_first and index != 0 : # Placeholder found but not in first position and `ensure_first` is True. # Move it to the front while preserving the order of the rest. reordered = [ t for i , t in enumerate ( tuple_list ) if i != index ] return [ tuple_list [ index ]] + reordered # Placeholder exists and is already in the correct position; return unchanged. return tuple_list list_to_triple_tuple ( values ) Convert a list of strings into WTForms triple tuples. Each tuple contains (value, label, metadata). Parameters: values ( list [ str ] ) \u2013 List of form options. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Triple tuples for WTForms SelectField. Example list_to_triple_tuple([\"A\", \"B\"]) [('A', 'A', {}), ('B', 'B', {})] Source code in arb\\utils\\web_html.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def list_to_triple_tuple ( values : list [ str ]) -> list [ tuple [ str , str , dict ]]: \"\"\" Convert a list of strings into WTForms triple tuples. Each tuple contains (value, label, metadata). Args: values (list[str]): List of form options. Returns: list[tuple[str, str, dict]]: Triple tuples for WTForms SelectField. Example: >>> list_to_triple_tuple([\"A\", \"B\"]) [('A', 'A', {}), ('B', 'B', {})] \"\"\" return [( v , v , {}) for v in values ] remove_items ( tuple_list , remove_items ) Remove one or more values from a tuple list by matching the first element. Parameters: tuple_list ( list [ tuple [ str , str , dict ]] ) \u2013 Selector tuples. remove_items ( str | list [ str ] ) \u2013 One or more values to remove by key match. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Filtered list excluding the removed values. Example remove_items([(\"A\", \"A\", {}), (\"B\", \"B\", {})], \"B\") [('A', 'A', {})] Source code in arb\\utils\\web_html.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def remove_items ( tuple_list : list [ tuple [ str , str , dict ]], remove_items : str | list [ str ] ) -> list [ tuple [ str , str , dict ]]: \"\"\" Remove one or more values from a tuple list by matching the first element. Args: tuple_list (list[tuple[str, str, dict]]): Selector tuples. remove_items (str | list[str]): One or more values to remove by key match. Returns: list[tuple[str, str, dict]]: Filtered list excluding the removed values. Example: >>> remove_items([(\"A\", \"A\", {}), (\"B\", \"B\", {})], \"B\") [('A', 'A', {})] \"\"\" remove_set = { remove_items } if isinstance ( remove_items , str ) else set ( remove_items ) return [ t for t in tuple_list if t [ 0 ] not in remove_set ] run_diagnostics () Run assertions to validate selector utility behavior. Tests Conversion of string lists to selector tuples Tuple updating with metadata Placeholder insertion Value removal from selector lists Dict transformation to tuple selectors Returns: None \u2013 None Example run_diagnostics() Source code in arb\\utils\\web_html.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def run_diagnostics () -> None : \"\"\" Run assertions to validate selector utility behavior. Tests: - Conversion of string lists to selector tuples - Tuple updating with metadata - Placeholder insertion - Value removal from selector lists - Dict transformation to tuple selectors Returns: None Example: >>> run_diagnostics() \"\"\" print ( \"Running diagnostics for web_html.py...\" ) test_values = [ \"A\" , \"B\" , \"C\" ] # Test selector_list_to_tuples selector = selector_list_to_tuples ( test_values ) assert selector [ 0 ][ 0 ] == PLEASE_SELECT assert ( \"A\" , \"A\" ) in selector # Test list_to_triple_tuple triple = list_to_triple_tuple ([ \"X\" , \"Y\" ]) assert triple == [( \"X\" , \"X\" , {}), ( \"Y\" , \"Y\" , {})] # Test update_triple_tuple_dict updated = update_triple_tuple_dict ( triple , [ \"Y\" ], { \"selected\" : True }) assert updated [ 1 ][ 2 ] . get ( \"selected\" ) is True # Test update_selector_dict test_dict = { \"colors\" : [ \"red\" , \"green\" ]} updated_dict = update_selector_dict ( test_dict ) assert PLEASE_SELECT in [ x [ 0 ] for x in updated_dict [ \"colors\" ]] # Test ensure_placeholder_option reordered = ensure_placeholder_option ([( \"X\" , \"X\" , {})]) assert reordered [ 0 ][ 0 ] == PLEASE_SELECT # Test remove_items cleaned = remove_items ( triple , \"X\" ) assert all ( t [ 0 ] != \"X\" for t in cleaned ) print ( \"All selector diagnostics passed.\" ) selector_list_to_tuples ( values ) Convert a list of values into WTForms-compatible dropdown tuples. Adds a disabled \"Please Select\" entry at the top of the list. Parameters: values ( list [ str ] ) \u2013 Dropdown options (excluding \"Please Select\"). Returns: list [ tuple [ str , str ] | tuple [ str , str , dict ]] \u2013 list[tuple[str, str] | tuple[str, str, dict]]: WTForms selector list including a disabled \"Please Select\" entry. Example selector_list_to_tuples([\"Red\", \"Green\"]) [('Please Select', 'Please Select', {'disabled': True}), ('Red', 'Red'), ('Green', 'Green')] Source code in arb\\utils\\web_html.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def selector_list_to_tuples ( values : list [ str ]) -> list [ tuple [ str , str ] | tuple [ str , str , dict ]]: \"\"\" Convert a list of values into WTForms-compatible dropdown tuples. Adds a disabled \"Please Select\" entry at the top of the list. Args: values (list[str]): Dropdown options (excluding \"Please Select\"). Returns: list[tuple[str, str] | tuple[str, str, dict]]: WTForms selector list including a disabled \"Please Select\" entry. Example: >>> selector_list_to_tuples([\"Red\", \"Green\"]) [('Please Select', 'Please Select', {'disabled': True}), ('Red', 'Red'), ('Green', 'Green')] \"\"\" result = [( PLEASE_SELECT , PLEASE_SELECT , { \"disabled\" : True })] result += [( v , v ) for v in values ] return result update_selector_dict ( input_dict ) Convert dictionary of string lists into selector-style tuple lists. Each list is transformed to include a \"Please Select\" disabled option followed by (value, label) tuples. Parameters: input_dict ( dict [ str , list [ str ]] ) \u2013 Dict of dropdown options per field. Returns: dict [ str , list [ tuple [ str , str ] | tuple [ str , str , dict ]]] \u2013 dict[str, list[tuple[str, str] | tuple[str, str, dict]]]: Dict with WTForms-ready selector tuples. Example update_selector_dict({\"colors\": [\"Red\", \"Blue\"]}) { \"colors\": [ (\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"Red\", \"Red\"), (\"Blue\", \"Blue\") ] } Source code in arb\\utils\\web_html.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def update_selector_dict ( input_dict : dict [ str , list [ str ]]) -> dict [ str , list [ tuple [ str , str ] | tuple [ str , str , dict ]]]: \"\"\" Convert dictionary of string lists into selector-style tuple lists. Each list is transformed to include a \"Please Select\" disabled option followed by (value, label) tuples. Args: input_dict (dict[str, list[str]]): Dict of dropdown options per field. Returns: dict[str, list[tuple[str, str] | tuple[str, str, dict]]]: Dict with WTForms-ready selector tuples. Example: >>> update_selector_dict({\"colors\": [\"Red\", \"Blue\"]}) { \"colors\": [ (\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"Red\", \"Red\"), (\"Blue\", \"Blue\") ] } \"\"\" return { key : selector_list_to_tuples ( values ) for key , values in input_dict . items ()} update_triple_tuple_dict ( tuple_list , match_list , match_update_dict , unmatch_update_dict = None ) Update the metadata dict of each WTForms triple tuple based on value match. Parameters: tuple_list ( list [ tuple [ str , str , dict ]] ) \u2013 Existing list of selector tuples. match_list ( list [ str ] ) \u2013 Values to match against. match_update_dict ( dict ) \u2013 Metadata to apply if value is in match_list . unmatch_update_dict ( dict | None , default: None ) \u2013 Metadata to apply otherwise (optional). Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Updated list of selector tuples. Example update_triple_tuple_dict( ... [('A', 'A', {}), ('B', 'B', {})], ... ['A'], ... {'disabled': True}, ... {'class': 'available'} ... ) [('A', 'A', {'disabled': True}), ('B', 'B', {'class': 'available'})] Source code in arb\\utils\\web_html.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def update_triple_tuple_dict ( tuple_list : list [ tuple [ str , str , dict ]], match_list : list [ str ], match_update_dict : dict , unmatch_update_dict : dict | None = None ) -> list [ tuple [ str , str , dict ]]: \"\"\" Update the metadata dict of each WTForms triple tuple based on value match. Args: tuple_list (list[tuple[str, str, dict]]): Existing list of selector tuples. match_list (list[str]): Values to match against. match_update_dict (dict): Metadata to apply if value is in `match_list`. unmatch_update_dict (dict | None): Metadata to apply otherwise (optional). Returns: list[tuple[str, str, dict]]: Updated list of selector tuples. Example: >>> update_triple_tuple_dict( ... [('A', 'A', {}), ('B', 'B', {})], ... ['A'], ... {'disabled': True}, ... {'class': 'available'} ... ) [('A', 'A', {'disabled': True}), ('B', 'B', {'class': 'available'})] \"\"\" if unmatch_update_dict is None : unmatch_update_dict = {} result = [] for key , value , meta in tuple_list : meta . update ( match_update_dict if key in match_list else unmatch_update_dict ) result . append (( key , value , meta )) return result upload_single_file ( upload_dir , request_file ) Save a user-uploaded file to the server using a secure, timestamped filename. Parameters: upload_dir ( str | Path ) \u2013 Directory to save the uploaded file. request_file ( FileStorage ) \u2013 Werkzeug object from request.files['<field>'] . Returns: Path ( Path ) \u2013 Full path to the uploaded file on disk. Raises: OSError \u2013 If the file cannot be written to disk. Example file = request.files['data'] path = upload_single_file(\"/data/uploads\", file) Source code in arb\\utils\\web_html.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def upload_single_file ( upload_dir : str | Path , request_file : FileStorage ) -> Path : \"\"\" Save a user-uploaded file to the server using a secure, timestamped filename. Args: upload_dir (str | Path): Directory to save the uploaded file. request_file (FileStorage): Werkzeug object from `request.files['<field>']`. Returns: Path: Full path to the uploaded file on disk. Raises: OSError: If the file cannot be written to disk. Example: >>> file = request.files['data'] >>> path = upload_single_file(\"/data/uploads\", file) \"\"\" logger . debug ( f \"Attempting to upload { request_file . filename =} \" ) file_name = get_secure_timestamped_file_name ( upload_dir , request_file . filename ) logger . debug ( f \"Upload single file as: { file_name } \" ) request_file . save ( file_name ) return file_name","title":"arb.utils.web_html"},{"location":"reference/arb/utils/web_html/#arbutilsweb_html","text":"HTML and WTForms utility functions for form handling and file uploads. This module provides helper functions for Uploading user files with sanitized names Generating WTForms-compatible selector lists Managing triple tuples for dynamic dropdown metadata Notes Avoids circular imports by not depending on other utility modules. Other utility modules (e.g., Excel, DB) may safely import this one. Adds \"Please Select\" logic to dropdowns using arb.utils.constants . Example Usage from arb.utils.web_html import upload_single_file, selector_list_to_tuples","title":"arb.utils.web_html"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.ensure_placeholder_option","text":"Ensure a placeholder entry is present in the tuple list. This function ensures that a specified \"placeholder\" option (typically used to prompt users to select a value, such as \"Please Select\") exists in the given list of selector options. If the placeholder is not present, it is inserted at the top. If it exists but is not the first item, it is optionally moved to the first position. Parameters: tuple_list ( list [ tuple [ str , str , dict ]] ) \u2013 Original selector list. item ( str , default: PLEASE_SELECT ) \u2013 Value for the placeholder. Default is \"Please Select\". item_dict ( dict , default: {'disabled': True} ) \u2013 Metadata for the placeholder. Default disables the option. ensure_first ( bool , default: True ) \u2013 If True, move placeholder to top if found elsewhere. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Updated tuple list with ensured placeholder. Example ensure_placeholder_option([(\"A\", \"A\", {})]) [('Please Select', 'Please Select', {'disabled': True}), ('A', 'A', {})] Source code in arb\\utils\\web_html.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def ensure_placeholder_option ( tuple_list : list [ tuple [ str , str , dict ]], item : str = PLEASE_SELECT , item_dict : dict = { \"disabled\" : True }, ensure_first : bool = True ) -> list [ tuple [ str , str , dict ]]: \"\"\" Ensure a placeholder entry is present in the tuple list. This function ensures that a specified \"placeholder\" option (typically used to prompt users to select a value, such as \"Please Select\") exists in the given list of selector options. If the placeholder is not present, it is inserted at the top. If it exists but is not the first item, it is optionally moved to the first position. Args: tuple_list (list[tuple[str, str, dict]]): Original selector list. item (str): Value for the placeholder. Default is \"Please Select\". item_dict (dict): Metadata for the placeholder. Default disables the option. ensure_first (bool): If True, move placeholder to top if found elsewhere. Returns: list[tuple[str, str, dict]]: Updated tuple list with ensured placeholder. Example: >>> ensure_placeholder_option([(\"A\", \"A\", {})]) [('Please Select', 'Please Select', {'disabled': True}), ('A', 'A', {})] \"\"\" if item is None : item = PLEASE_SELECT if item_dict is None : item_dict = { \"disabled\" : True } placeholder = ( item , item , item_dict ) # Find the index of any existing placeholder (based on value match) # Explanation: # - `enumerate(tuple_list)` produces (index, tuple) pairs. # - `t[0] == item` checks if the first element (value) matches the placeholder value. # - `next(...)` returns the index of the first match, or `None` if no match is found. index = next (( i for i , t in enumerate ( tuple_list ) if t [ 0 ] == item ), None ) if index is None : # Placeholder not found; insert it at the beginning of the list. return [ placeholder ] + tuple_list elif ensure_first and index != 0 : # Placeholder found but not in first position and `ensure_first` is True. # Move it to the front while preserving the order of the rest. reordered = [ t for i , t in enumerate ( tuple_list ) if i != index ] return [ tuple_list [ index ]] + reordered # Placeholder exists and is already in the correct position; return unchanged. return tuple_list","title":"ensure_placeholder_option"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.list_to_triple_tuple","text":"Convert a list of strings into WTForms triple tuples. Each tuple contains (value, label, metadata). Parameters: values ( list [ str ] ) \u2013 List of form options. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Triple tuples for WTForms SelectField. Example list_to_triple_tuple([\"A\", \"B\"]) [('A', 'A', {}), ('B', 'B', {})] Source code in arb\\utils\\web_html.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def list_to_triple_tuple ( values : list [ str ]) -> list [ tuple [ str , str , dict ]]: \"\"\" Convert a list of strings into WTForms triple tuples. Each tuple contains (value, label, metadata). Args: values (list[str]): List of form options. Returns: list[tuple[str, str, dict]]: Triple tuples for WTForms SelectField. Example: >>> list_to_triple_tuple([\"A\", \"B\"]) [('A', 'A', {}), ('B', 'B', {})] \"\"\" return [( v , v , {}) for v in values ]","title":"list_to_triple_tuple"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.remove_items","text":"Remove one or more values from a tuple list by matching the first element. Parameters: tuple_list ( list [ tuple [ str , str , dict ]] ) \u2013 Selector tuples. remove_items ( str | list [ str ] ) \u2013 One or more values to remove by key match. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Filtered list excluding the removed values. Example remove_items([(\"A\", \"A\", {}), (\"B\", \"B\", {})], \"B\") [('A', 'A', {})] Source code in arb\\utils\\web_html.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 def remove_items ( tuple_list : list [ tuple [ str , str , dict ]], remove_items : str | list [ str ] ) -> list [ tuple [ str , str , dict ]]: \"\"\" Remove one or more values from a tuple list by matching the first element. Args: tuple_list (list[tuple[str, str, dict]]): Selector tuples. remove_items (str | list[str]): One or more values to remove by key match. Returns: list[tuple[str, str, dict]]: Filtered list excluding the removed values. Example: >>> remove_items([(\"A\", \"A\", {}), (\"B\", \"B\", {})], \"B\") [('A', 'A', {})] \"\"\" remove_set = { remove_items } if isinstance ( remove_items , str ) else set ( remove_items ) return [ t for t in tuple_list if t [ 0 ] not in remove_set ]","title":"remove_items"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.run_diagnostics","text":"Run assertions to validate selector utility behavior. Tests Conversion of string lists to selector tuples Tuple updating with metadata Placeholder insertion Value removal from selector lists Dict transformation to tuple selectors Returns: None \u2013 None Example run_diagnostics() Source code in arb\\utils\\web_html.py 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def run_diagnostics () -> None : \"\"\" Run assertions to validate selector utility behavior. Tests: - Conversion of string lists to selector tuples - Tuple updating with metadata - Placeholder insertion - Value removal from selector lists - Dict transformation to tuple selectors Returns: None Example: >>> run_diagnostics() \"\"\" print ( \"Running diagnostics for web_html.py...\" ) test_values = [ \"A\" , \"B\" , \"C\" ] # Test selector_list_to_tuples selector = selector_list_to_tuples ( test_values ) assert selector [ 0 ][ 0 ] == PLEASE_SELECT assert ( \"A\" , \"A\" ) in selector # Test list_to_triple_tuple triple = list_to_triple_tuple ([ \"X\" , \"Y\" ]) assert triple == [( \"X\" , \"X\" , {}), ( \"Y\" , \"Y\" , {})] # Test update_triple_tuple_dict updated = update_triple_tuple_dict ( triple , [ \"Y\" ], { \"selected\" : True }) assert updated [ 1 ][ 2 ] . get ( \"selected\" ) is True # Test update_selector_dict test_dict = { \"colors\" : [ \"red\" , \"green\" ]} updated_dict = update_selector_dict ( test_dict ) assert PLEASE_SELECT in [ x [ 0 ] for x in updated_dict [ \"colors\" ]] # Test ensure_placeholder_option reordered = ensure_placeholder_option ([( \"X\" , \"X\" , {})]) assert reordered [ 0 ][ 0 ] == PLEASE_SELECT # Test remove_items cleaned = remove_items ( triple , \"X\" ) assert all ( t [ 0 ] != \"X\" for t in cleaned ) print ( \"All selector diagnostics passed.\" )","title":"run_diagnostics"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.selector_list_to_tuples","text":"Convert a list of values into WTForms-compatible dropdown tuples. Adds a disabled \"Please Select\" entry at the top of the list. Parameters: values ( list [ str ] ) \u2013 Dropdown options (excluding \"Please Select\"). Returns: list [ tuple [ str , str ] | tuple [ str , str , dict ]] \u2013 list[tuple[str, str] | tuple[str, str, dict]]: WTForms selector list including a disabled \"Please Select\" entry. Example selector_list_to_tuples([\"Red\", \"Green\"]) [('Please Select', 'Please Select', {'disabled': True}), ('Red', 'Red'), ('Green', 'Green')] Source code in arb\\utils\\web_html.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def selector_list_to_tuples ( values : list [ str ]) -> list [ tuple [ str , str ] | tuple [ str , str , dict ]]: \"\"\" Convert a list of values into WTForms-compatible dropdown tuples. Adds a disabled \"Please Select\" entry at the top of the list. Args: values (list[str]): Dropdown options (excluding \"Please Select\"). Returns: list[tuple[str, str] | tuple[str, str, dict]]: WTForms selector list including a disabled \"Please Select\" entry. Example: >>> selector_list_to_tuples([\"Red\", \"Green\"]) [('Please Select', 'Please Select', {'disabled': True}), ('Red', 'Red'), ('Green', 'Green')] \"\"\" result = [( PLEASE_SELECT , PLEASE_SELECT , { \"disabled\" : True })] result += [( v , v ) for v in values ] return result","title":"selector_list_to_tuples"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.update_selector_dict","text":"Convert dictionary of string lists into selector-style tuple lists. Each list is transformed to include a \"Please Select\" disabled option followed by (value, label) tuples. Parameters: input_dict ( dict [ str , list [ str ]] ) \u2013 Dict of dropdown options per field. Returns: dict [ str , list [ tuple [ str , str ] | tuple [ str , str , dict ]]] \u2013 dict[str, list[tuple[str, str] | tuple[str, str, dict]]]: Dict with WTForms-ready selector tuples. Example update_selector_dict({\"colors\": [\"Red\", \"Blue\"]}) { \"colors\": [ (\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"Red\", \"Red\"), (\"Blue\", \"Blue\") ] } Source code in arb\\utils\\web_html.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def update_selector_dict ( input_dict : dict [ str , list [ str ]]) -> dict [ str , list [ tuple [ str , str ] | tuple [ str , str , dict ]]]: \"\"\" Convert dictionary of string lists into selector-style tuple lists. Each list is transformed to include a \"Please Select\" disabled option followed by (value, label) tuples. Args: input_dict (dict[str, list[str]]): Dict of dropdown options per field. Returns: dict[str, list[tuple[str, str] | tuple[str, str, dict]]]: Dict with WTForms-ready selector tuples. Example: >>> update_selector_dict({\"colors\": [\"Red\", \"Blue\"]}) { \"colors\": [ (\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"Red\", \"Red\"), (\"Blue\", \"Blue\") ] } \"\"\" return { key : selector_list_to_tuples ( values ) for key , values in input_dict . items ()}","title":"update_selector_dict"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.update_triple_tuple_dict","text":"Update the metadata dict of each WTForms triple tuple based on value match. Parameters: tuple_list ( list [ tuple [ str , str , dict ]] ) \u2013 Existing list of selector tuples. match_list ( list [ str ] ) \u2013 Values to match against. match_update_dict ( dict ) \u2013 Metadata to apply if value is in match_list . unmatch_update_dict ( dict | None , default: None ) \u2013 Metadata to apply otherwise (optional). Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Updated list of selector tuples. Example update_triple_tuple_dict( ... [('A', 'A', {}), ('B', 'B', {})], ... ['A'], ... {'disabled': True}, ... {'class': 'available'} ... ) [('A', 'A', {'disabled': True}), ('B', 'B', {'class': 'available'})] Source code in arb\\utils\\web_html.py 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def update_triple_tuple_dict ( tuple_list : list [ tuple [ str , str , dict ]], match_list : list [ str ], match_update_dict : dict , unmatch_update_dict : dict | None = None ) -> list [ tuple [ str , str , dict ]]: \"\"\" Update the metadata dict of each WTForms triple tuple based on value match. Args: tuple_list (list[tuple[str, str, dict]]): Existing list of selector tuples. match_list (list[str]): Values to match against. match_update_dict (dict): Metadata to apply if value is in `match_list`. unmatch_update_dict (dict | None): Metadata to apply otherwise (optional). Returns: list[tuple[str, str, dict]]: Updated list of selector tuples. Example: >>> update_triple_tuple_dict( ... [('A', 'A', {}), ('B', 'B', {})], ... ['A'], ... {'disabled': True}, ... {'class': 'available'} ... ) [('A', 'A', {'disabled': True}), ('B', 'B', {'class': 'available'})] \"\"\" if unmatch_update_dict is None : unmatch_update_dict = {} result = [] for key , value , meta in tuple_list : meta . update ( match_update_dict if key in match_list else unmatch_update_dict ) result . append (( key , value , meta )) return result","title":"update_triple_tuple_dict"},{"location":"reference/arb/utils/web_html/#arb.utils.web_html.upload_single_file","text":"Save a user-uploaded file to the server using a secure, timestamped filename. Parameters: upload_dir ( str | Path ) \u2013 Directory to save the uploaded file. request_file ( FileStorage ) \u2013 Werkzeug object from request.files['<field>'] . Returns: Path ( Path ) \u2013 Full path to the uploaded file on disk. Raises: OSError \u2013 If the file cannot be written to disk. Example file = request.files['data'] path = upload_single_file(\"/data/uploads\", file) Source code in arb\\utils\\web_html.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def upload_single_file ( upload_dir : str | Path , request_file : FileStorage ) -> Path : \"\"\" Save a user-uploaded file to the server using a secure, timestamped filename. Args: upload_dir (str | Path): Directory to save the uploaded file. request_file (FileStorage): Werkzeug object from `request.files['<field>']`. Returns: Path: Full path to the uploaded file on disk. Raises: OSError: If the file cannot be written to disk. Example: >>> file = request.files['data'] >>> path = upload_single_file(\"/data/uploads\", file) \"\"\" logger . debug ( f \"Attempting to upload { request_file . filename =} \" ) file_name = get_secure_timestamped_file_name ( upload_dir , request_file . filename ) logger . debug ( f \"Upload single file as: { file_name } \" ) request_file . save ( file_name ) return file_name","title":"upload_single_file"},{"location":"reference/arb/utils/wtf_forms_util/","text":"arb.utils.wtf_forms_util Functions and helper classes to support WTForms models. Notes WTForm model classes should remain adjacent to Flask views (e.g., in wtf_landfill.py ) This module is for shared utilities, validators, and form-to-model conversion logic. IfTruthy WTForms validator: Dynamically switches between InputRequired and Optional. The validator behavior is conditional on another field\u2019s truthiness. Depending on mode , this field becomes required or optional. Parameters: other_field_name ( str ) \u2013 Name of the field to evaluate. falsy_values ( list | None , default: None ) \u2013 Custom falsy value list (defaults to standard values). mode ( str , default: 'required on truthy' ) \u2013 Either 'required on truthy' or 'optional on truthy'. message ( str | None , default: None ) \u2013 Optional validation error message. Raises: TypeError \u2013 If an invalid mode is provided. Example class MyForm(FlaskForm): toggle = BooleanField() extra = StringField(validators=[IfTruthy(\"toggle\", mode=\"required on truthy\")]) Source code in arb\\utils\\wtf_forms_util.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class IfTruthy : \"\"\" WTForms validator: Dynamically switches between InputRequired and Optional. The validator behavior is conditional on another field\u2019s truthiness. Depending on `mode`, this field becomes required or optional. Args: other_field_name (str): Name of the field to evaluate. falsy_values (list | None): Custom falsy value list (defaults to standard values). mode (str): Either 'required on truthy' or 'optional on truthy'. message (str | None): Optional validation error message. Raises: TypeError: If an invalid mode is provided. Example: class MyForm(FlaskForm): toggle = BooleanField() extra = StringField(validators=[IfTruthy(\"toggle\", mode=\"required on truthy\")]) \"\"\" field_flags = ( \"iftruthy\" ,) def __init__ ( self , other_field_name : str , falsy_values : list | None = None , mode : str = 'required on truthy' , message : str | None = None ): self . other_field_name = other_field_name if falsy_values is None : falsy_values = [ False , [], {}, (), '' , '0' , '0.0' , 0 , 0.0 ] self . falsy_values = falsy_values if mode == 'required on truthy' : self . validators = { 'truthy' : InputRequired , 'falsy' : Optional } elif mode == 'optional on truthy' : self . validators = { 'truthy' : Optional , 'falsy' : InputRequired } else : raise TypeError ( f \"Unknown mode: { mode } \" ) self . message = message def __call__ ( self , form , field ): other_field = form [ self . other_field_name ] if other_field is None : raise Exception ( f 'No field named \" { self . other_field_name } \" in form' ) validator_class = self . validators [ 'truthy' ] if other_field . data not in self . falsy_values else self . validators [ 'falsy' ] validator_class ( self . message ) . __call__ ( form , field ) RequiredIfTruthy WTForms validator: Applies InputRequired or Optional based on another field's truthiness. If the referenced field is \"truthy\" (not in a falsy list), then this field is required. If it is \"falsy\", this field becomes optional. Parameters: other_field_name ( str ) \u2013 Name of the field to check. message ( str | None , default: None ) \u2013 Optional custom validation error message. other_field_invalid_values ( list | None , default: None ) \u2013 Values considered falsy (defaults to standard empty/zero/null values). Example class MyForm(FlaskForm): confirm = BooleanField() notes = StringField(validators=[RequiredIfTruthy(\"confirm\")]) Source code in arb\\utils\\wtf_forms_util.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 class RequiredIfTruthy : \"\"\" WTForms validator: Applies InputRequired or Optional based on another field's truthiness. If the referenced field is \"truthy\" (not in a falsy list), then this field is required. If it is \"falsy\", this field becomes optional. Args: other_field_name (str): Name of the field to check. message (str | None): Optional custom validation error message. other_field_invalid_values (list | None): Values considered falsy (defaults to standard empty/zero/null values). Example: class MyForm(FlaskForm): confirm = BooleanField() notes = StringField(validators=[RequiredIfTruthy(\"confirm\")]) \"\"\" field_flags = ( \"requirediftruthy\" ,) def __init__ ( self , other_field_name : str , message : str | None = None , other_field_invalid_values : list | None = None ): self . other_field_name = other_field_name self . message = message if other_field_invalid_values is None : other_field_invalid_values = [ False , [], {}, (), '' , '0' , '0.0' , 0 , 0.0 ] self . other_field_invalid_values = other_field_invalid_values logger . debug ( \"RequiredIfTruthy initialized\" ) def __call__ ( self , form , field ): other_field = form [ self . other_field_name ] if other_field is None : raise Exception ( f 'No field named \" { self . other_field_name } \" in form' ) if other_field . data not in self . other_field_invalid_values : logger . debug ( \"other_field is truthy \u2192 requiring this field\" ) InputRequired ( self . message ) . __call__ ( form , field ) else : logger . debug ( \"other_field is falsy \u2192 allowing this field to be optional\" ) Optional ( self . message ) . __call__ ( form , field ) build_choices ( header , items ) Combine header and dynamic items into a list of triple-tuples for WTForms SelectFields. Parameters: header ( list [ tuple [ str , str , dict ]] ) \u2013 Static options to appear first in the dropdown. items ( list [ str ] ) \u2013 Dynamic option values to convert into (value, label, {}) format. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Combined list of header and generated item tuples. Example build_choices( ... [(\"Please Select\", \"Please Select\", {\"disabled\": True})], ... [\"One\", \"Two\"] ... ) [ (\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"One\", \"One\", {}), (\"Two\", \"Two\", {}) ] Source code in arb\\utils\\wtf_forms_util.py 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 def build_choices ( header : list [ tuple [ str , str , dict ]], items : list [ str ]) -> list [ tuple [ str , str , dict ]]: \"\"\" Combine header and dynamic items into a list of triple-tuples for WTForms SelectFields. Args: header (list[tuple[str, str, dict]]): Static options to appear first in the dropdown. items (list[str]): Dynamic option values to convert into (value, label, {}) format. Returns: list[tuple[str, str, dict]]: Combined list of header and generated item tuples. Example: >>> build_choices( ... [(\"Please Select\", \"Please Select\", {\"disabled\": True})], ... [\"One\", \"Two\"] ... ) [ (\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"One\", \"One\", {}), (\"Two\", \"Two\", {}) ] \"\"\" footer = [( item , item , {}) for item in items ] return header + footer change_validators ( form , field_names_to_change , old_validator , new_validator ) Replace one validator type with another on a list of WTForms fields. Parameters: form ( FlaskForm ) \u2013 WTForms form instance. field_names_to_change ( list [ str ] ) \u2013 List of fields to alter. old_validator ( type ) \u2013 Validator class to remove (e.g., Optional). new_validator ( type ) \u2013 Validator class to add (e.g., InputRequired). Notes The replacement is done in-place on each field's validators list. Useful for dynamically changing required status. Example change_validators(form, [\"name\"], Optional, InputRequired) Source code in arb\\utils\\wtf_forms_util.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def change_validators ( form : FlaskForm , field_names_to_change : list [ str ], old_validator : type , new_validator : type ) -> None : \"\"\" Replace one validator type with another on a list of WTForms fields. Args: form (FlaskForm): WTForms form instance. field_names_to_change (list[str]): List of fields to alter. old_validator (type): Validator class to remove (e.g., Optional). new_validator (type): Validator class to add (e.g., InputRequired). Notes: - The replacement is done in-place on each field's `validators` list. - Useful for dynamically changing required status. Example: >>> change_validators(form, [\"name\"], Optional, InputRequired) \"\"\" field_names = get_wtforms_fields ( form , include_csrf_token = False ) for field_name in field_names : if field_name in field_names_to_change : validators = form [ field_name ] . validators for i , validator in enumerate ( validators ): if isinstance ( validator , old_validator ): validators [ i ] = new_validator () change_validators_on_test ( form , bool_test , required_if_true , optional_if_true = None ) Conditionally switch validators on selected form fields based on a boolean test. If bool_test is True Fields in required_if_true become required (InputRequired). Fields in optional_if_true become optional (Optional). If bool_test is False Fields in required_if_true become optional. Fields in optional_if_true become required. Parameters: form ( FlaskForm ) \u2013 The form to update. bool_test ( bool ) \u2013 If True, required/optional fields are swapped accordingly. required_if_true ( list [ str ] ) \u2013 Field names that become required when bool_test is True. optional_if_true ( list [ str ] | None , default: None ) \u2013 Field names that become optional when bool_test is True. Example change_validators_on_test(form, bool_test=is_active, ... required_if_true=[\"comment\"], ... optional_if_true=[\"note\"]) Source code in arb\\utils\\wtf_forms_util.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def change_validators_on_test ( form : FlaskForm , bool_test : bool , required_if_true : list [ str ], optional_if_true : list [ str ] | None = None ) -> None : \"\"\" Conditionally switch validators on selected form fields based on a boolean test. If bool_test is True: - Fields in required_if_true become required (InputRequired). - Fields in optional_if_true become optional (Optional). If bool_test is False: - Fields in required_if_true become optional. - Fields in optional_if_true become required. Args: form (FlaskForm): The form to update. bool_test (bool): If True, required/optional fields are swapped accordingly. required_if_true (list[str]): Field names that become required when bool_test is True. optional_if_true (list[str] | None): Field names that become optional when bool_test is True. Example: >>> change_validators_on_test(form, bool_test=is_active, ... required_if_true=[\"comment\"], ... optional_if_true=[\"note\"]) \"\"\" if optional_if_true is None : optional_if_true = [] if bool_test : change_validators ( form , field_names_to_change = required_if_true , old_validator = Optional , new_validator = InputRequired , ) change_validators ( form , field_names_to_change = optional_if_true , old_validator = InputRequired , new_validator = Optional , ) else : change_validators ( form , field_names_to_change = required_if_true , old_validator = InputRequired , new_validator = Optional , ) change_validators ( form , field_names_to_change = optional_if_true , old_validator = Optional , new_validator = InputRequired , ) ensure_field_choice ( field_name , field , choices = None ) Ensure a field\u2019s current value is among its valid choices, or reset it to a placeholder. Parameters: field_name ( str ) \u2013 Name of the WTForms field (for logging purposes). field ( Field ) \u2013 WTForms-compatible field (typically a SelectField). choices ( list [ tuple [ str , str ]] | list [ tuple [ str , str , dict ]] | None , default: None ) \u2013 Valid choices to enforce. If None, uses the field's existing choices. Returns: None \u2013 None Notes: - If choices is provided, this function sets field.choices to the new list. - If choices is None, it uses the field's existing .choices . In either case, it validates that the current field.data is among the available options and resets it to \"Please Select\" if not. - Both field.data and field.raw_data are reset to keep form behavior consistent. - Each choice tuple should be in the form: - (value, label), or - (value, label, metadata_dict) - Only the first element ( value ) is used for validation. - Use this with SelectField or similar fields where .choices must be explicitly defined. - The reset value \"Please Select\" should match a placeholder value if one is used in your app. Example ensure_field_choice(\"sector\", form.sector, [(\"oil\", \"Oil & Gas\"), (\"land\", \"Landfill\")]) form.sector.choices [('oil', 'Oil & Gas'), ('land', 'Landfill')] Source code in arb\\utils\\wtf_forms_util.py 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 def ensure_field_choice ( field_name : str , field , choices : list [ tuple [ str , str ] | tuple [ str , str , dict ]] | None = None ) -> None : \"\"\" Ensure a field\u2019s current value is among its valid choices, or reset it to a placeholder. Args: field_name (str): Name of the WTForms field (for logging purposes). field (Field): WTForms-compatible field (typically a SelectField). choices (list[tuple[str, str]] | list[tuple[str, str, dict]] | None): Valid choices to enforce. If None, uses the field's existing choices. Returns: None Notes: - If `choices` is provided, this function sets `field.choices` to the new list. - If `choices` is None, it uses the field's existing `.choices`. In either case, it validates that the current `field.data` is among the available options and resets it to \"Please Select\" if not. - Both `field.data` and `field.raw_data` are reset to keep form behavior consistent. - Each choice tuple should be in the form: - (value, label), or - (value, label, metadata_dict) - Only the first element (`value`) is used for validation. - Use this with SelectField or similar fields where `.choices` must be explicitly defined. - The reset value \"Please Select\" should match a placeholder value if one is used in your app. Example: >>> ensure_field_choice(\"sector\", form.sector, [(\"oil\", \"Oil & Gas\"), (\"land\", \"Landfill\")]) >>> form.sector.choices [('oil', 'Oil & Gas'), ('land', 'Landfill')] \"\"\" if choices is None : # Use existing field choices if none are supplied choices = field . choices else : # Apply a new set of choices to the field field . choices = choices valid_values = { c [ 0 ] for c in choices } if field . data not in valid_values : logger . debug ( f \" { field_name } .data= { field . data !r} not in valid options, resetting to ' { PLEASE_SELECT } '\" ) field . data = PLEASE_SELECT field . raw_data = [ field . data ] format_raw_data ( field , value ) Convert a field value to a format suitable for WTForms .raw_data . Parameters: field ( Field ) \u2013 A WTForms field instance (e.g., DecimalField, DateTimeField). value ( str | int | float | Decimal | datetime | None ) \u2013 The field's data value. Returns: list [ str ] \u2013 list[str]: List of string values to assign to field.raw_data . Raises: ValueError \u2013 If the value type is unsupported. Example format_raw_data(field, Decimal(\"10.5\")) ['10.5'] Source code in arb\\utils\\wtf_forms_util.py 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def format_raw_data ( field : Field , value ) -> list [ str ]: \"\"\" Convert a field value to a format suitable for WTForms `.raw_data`. Args: field (Field): A WTForms field instance (e.g., DecimalField, DateTimeField). value (str | int | float | Decimal | datetime.datetime | None): The field's data value. Returns: list[str]: List of string values to assign to `field.raw_data`. Raises: ValueError: If the value type is unsupported. Example: >>> format_raw_data(field, Decimal(\"10.5\")) ['10.5'] \"\"\" if value is None : return [] elif isinstance ( value , ( str , int , float )): return [ str ( value )] elif isinstance ( value , Decimal ): return [ str ( float ( value ))] # Cast to float before converting to string elif isinstance ( value , datetime . datetime ): return [ value . isoformat ()] else : raise ValueError ( f \"Unsupported type for raw_data: { type ( value ) } with value { value } \" ) get_payloads ( model , wtform , ignore_fields = None ) DEPRECATED: Use wtform_to_model() instead. Extract all field values and changed values from a WTForm. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model with JSON column misc_json . wtform ( FlaskForm ) \u2013 The form to extract values from. ignore_fields ( list [ str ] | None , default: None ) \u2013 List of fields to skip during comparison. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: Tuple of (payload_all, payload_changes) - payload_all: All form fields - payload_changes: Subset of fields with changed values vs. model Notes Performs a naive comparison (==) without deserializing types. Use skip_empty_fields = True to suppress null-like values. Source code in arb\\utils\\wtf_forms_util.py 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 def get_payloads ( model : DeclarativeMeta , wtform : FlaskForm , ignore_fields : list [ str ] | None = None ) -> tuple [ dict , dict ]: \"\"\" DEPRECATED: Use `wtform_to_model()` instead. Extract all field values and changed values from a WTForm. Args: model (DeclarativeMeta): SQLAlchemy model with JSON column `misc_json`. wtform (FlaskForm): The form to extract values from. ignore_fields (list[str] | None): List of fields to skip during comparison. Returns: tuple[dict, dict]: Tuple of (payload_all, payload_changes) - payload_all: All form fields - payload_changes: Subset of fields with changed values vs. model Notes: - Performs a naive comparison (==) without deserializing types. - Use skip_empty_fields = True to suppress null-like values. \"\"\" if ignore_fields is None : ignore_fields = [] skip_empty_fields = False # Yes if you wish to skip blank fields from being updated when feasible payload_all = {} payload_changes = {} model_json_dict = getattr ( model , \"misc_json\" ) or {} logger . debug ( f \" { model_json_dict =} \" ) model_field_names = list ( model_json_dict . keys ()) form_field_names = get_wtforms_fields ( wtform ) list_differences ( model_field_names , form_field_names , iterable_01_name = \"SQLAlchemy Model\" , iterable_02_name = \"WTForm Fields\" , print_warning = False , ) for form_field_name in form_field_names : field = getattr ( wtform , form_field_name ) field_value = field . data model_value = model_json_dict . get ( form_field_name ) if form_field_name in ignore_fields : continue if skip_empty_fields is True : # skipping empty strings if the model is \"\" or None if field_value == \"\" : if model_value in [ None , \"\" ]: continue # Only persist \"Please Select\" if overwriting a meaningful value. if isinstance ( field , SelectField ) and field_value == PLEASE_SELECT : if model_value in [ None , \"\" ]: continue payload_all [ form_field_name ] = field_value # todo (depreciated) - object types are not being seen as equivalent (because they are serialized strings) # need to update logic - check out prep_payload_for_json for uniform approach if model_value != field_value : payload_changes [ form_field_name ] = field_value return payload_all , payload_changes get_wtforms_fields ( form , include_csrf_token = False ) Return the sorted field names associated with a WTForms form. Parameters: form ( FlaskForm ) \u2013 The WTForms form instance. include_csrf_token ( bool , default: False ) \u2013 If True, include 'csrf_token' in the result. Returns: list [ str ] \u2013 list[str]: Alphabetically sorted list of field names in the form. Example get_wtforms_fields(form) ['name', 'sector'] Source code in arb\\utils\\wtf_forms_util.py 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 def get_wtforms_fields ( form : FlaskForm , include_csrf_token : bool = False ) -> list [ str ]: \"\"\" Return the sorted field names associated with a WTForms form. Args: form (FlaskForm): The WTForms form instance. include_csrf_token (bool): If True, include 'csrf_token' in the result. Returns: list[str]: Alphabetically sorted list of field names in the form. Example: >>> get_wtforms_fields(form) ['name', 'sector'] \"\"\" field_names = [ name for name in form . data if include_csrf_token or name != \"csrf_token\" ] field_names . sort () return field_names initialize_drop_downs ( form , default = None ) Set default values for uninitialized WTForms SelectFields. Parameters: form ( FlaskForm ) \u2013 The form containing SelectField fields to be initialized. default ( str | None , default: None ) \u2013 The value to assign to a field if its current value is None. If not provided, uses the application's global placeholder (e.g., \"Please Select\"). Returns: None \u2013 None Example initialize_drop_downs(form, default=\"Please Select\") Notes Fields that already have a value (even a falsy one like an empty string) are not modified. Only fields of type SelectField are affected. This function is typically used after form construction but before rendering or validation. Source code in arb\\utils\\wtf_forms_util.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 def initialize_drop_downs ( form : FlaskForm , default : str = None ) -> None : \"\"\" Set default values for uninitialized WTForms SelectFields. Args: form (FlaskForm): The form containing SelectField fields to be initialized. default (str | None): The value to assign to a field if its current value is None. If not provided, uses the application's global placeholder (e.g., \"Please Select\"). Returns: None Example: >>> initialize_drop_downs(form, default=\"Please Select\") Notes: - Fields that already have a value (even a falsy one like an empty string) are not modified. - Only fields of type `SelectField` are affected. - This function is typically used after form construction but before rendering or validation. \"\"\" if default is None : default = PLEASE_SELECT logger . debug ( \"Initializing drop-downs...\" ) for field in form : if isinstance ( field , SelectField ) and field . data is None : logger . debug ( f \" { field . name } set to default value: { default } \" ) field . data = default min_decimal_precision ( min_digits ) Return a validator for WTForms DecimalField enforcing minimum decimal precision. Parameters: min_digits ( int ) \u2013 Minimum number of digits required after the decimal. Returns: Callable ( Callable ) \u2013 WTForms-compatible validator that raises ValidationError if decimal places are insufficient. Example field = DecimalField(\"Amount\", validators=[min_decimal_precision(2)]) Source code in arb\\utils\\wtf_forms_util.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def min_decimal_precision ( min_digits : int ) -> Callable : \"\"\" Return a validator for WTForms DecimalField enforcing minimum decimal precision. Args: min_digits (int): Minimum number of digits required after the decimal. Returns: Callable: WTForms-compatible validator that raises ValidationError if decimal places are insufficient. Example: >>> field = DecimalField(\"Amount\", validators=[min_decimal_precision(2)]) \"\"\" def _min_decimal_precision ( form , field ): if field . data is None : return try : value_str = str ( field . data ) if '.' in value_str : _ , decimals = value_str . split ( '.' ) if len ( decimals ) < min_digits : raise ValidationError () elif min_digits > 0 : raise ValidationError () except ( ValueError , TypeError ): raise ValidationError ( f \"Field must be a valid numeric value with at least { min_digits } decimal places.\" ) return _min_decimal_precision model_to_wtform ( model , wtform , json_column = 'misc_json' ) Populate a WTForm from a SQLAlchemy model's JSON column. This function loads the model's JSON field (typically 'misc_json') and sets WTForms field .data and .raw_data accordingly. Required for correct rendering and validation of pre-filled forms. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance containing a JSON column. wtform ( FlaskForm ) \u2013 The WTForm instance to populate. json_column ( str , default: 'misc_json' ) \u2013 The attribute name of the JSON column. Defaults to \"misc_json\". Raises: ValueError \u2013 If a datetime value cannot be parsed. TypeError \u2013 If the field type is unsupported. Notes Supports preloading DateTimeField and DecimalField types. Converts ISO8601 UTC \u2192 localized Pacific time. Ignores JSON fields that don\u2019t map to WTForm fields. Source code in arb\\utils\\wtf_forms_util.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 def model_to_wtform ( model : DeclarativeMeta , wtform : FlaskForm , json_column : str = \"misc_json\" ) -> None : \"\"\" Populate a WTForm from a SQLAlchemy model's JSON column. This function loads the model's JSON field (typically 'misc_json') and sets WTForms field `.data` and `.raw_data` accordingly. Required for correct rendering and validation of pre-filled forms. Args: model (DeclarativeMeta): SQLAlchemy model instance containing a JSON column. wtform (FlaskForm): The WTForm instance to populate. json_column (str): The attribute name of the JSON column. Defaults to \"misc_json\". Raises: ValueError: If a datetime value cannot be parsed. TypeError: If the field type is unsupported. Notes: - Supports preloading DateTimeField and DecimalField types. - Converts ISO8601 UTC \u2192 localized Pacific time. - Ignores JSON fields that don\u2019t map to WTForm fields. \"\"\" model_json_dict = getattr ( model , json_column ) logger . debug ( f \"model_to_wtform called with model= { model } , json= { model_json_dict } \" ) # Ensure dict, not str if isinstance ( model_json_dict , str ): try : model_json_dict = json . loads ( model_json_dict ) logger . debug ( \"Parsed JSON string into dict.\" ) except json . JSONDecodeError : logger . warning ( f \"Invalid JSON in model's ' { json_column } ' column.\" ) model_json_dict = {} if model_json_dict is None : model_json_dict = {} if \"id_incidence\" in model_json_dict and model_json_dict [ \"id_incidence\" ] != model . id_incidence : logger . warning ( f \"[model_to_wtform] MISMATCH: model.id_incidence= { model . id_incidence } \" f \"!= misc_json['id_incidence']= { model_json_dict [ 'id_incidence' ] } \" ) form_fields = get_wtforms_fields ( wtform ) model_fields = list ( model_json_dict . keys ()) list_differences ( model_fields , form_fields , iterable_01_name = \"SQLAlchemy Model JSON\" , iterable_02_name = \"WTForm Fields\" , print_warning = False ) # Use utilities to get type map and convert model dict type_map , _ = wtform_types_and_values ( wtform ) parsed_dict = deserialize_dict ( model_json_dict , type_map , convert_time_to_ca = True ) for field_name in form_fields : field = getattr ( wtform , field_name ) model_value = parsed_dict . get ( field_name ) # Set field data and raw_data for proper rendering/validation field . data = model_value field . raw_data = format_raw_data ( field , model_value ) logger . debug ( f \"Set { field_name =} , data= { field . data } , raw_data= { field . raw_data } \" ) prep_payload_for_json ( payload , type_matching_dict = None ) Prepare a payload dictionary for JSON-safe serialization. Parameters: payload ( dict ) \u2013 Key-value updates extracted from a WTForm or other source. type_matching_dict ( dict [ str , type ] | None , default: None ) \u2013 Optional type coercion rules. e.g., {\"id_incidence\": int, \"some_flag\": bool} Returns: dict ( dict ) \u2013 Transformed version of the payload, suitable for use in a model's JSON field. Notes Applies datetime to ISO, Decimal to float Respects \"Please Select\" for placeholders Values in type_matching_dict are explicitly cast to the specified types Source code in arb\\utils\\wtf_forms_util.py 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 def prep_payload_for_json ( payload : dict , type_matching_dict : dict [ str , type ] | None = None ) -> dict : \"\"\" Prepare a payload dictionary for JSON-safe serialization. Args: payload (dict): Key-value updates extracted from a WTForm or other source. type_matching_dict (dict[str, type] | None): Optional type coercion rules. e.g., {\"id_incidence\": int, \"some_flag\": bool} Returns: dict: Transformed version of the payload, suitable for use in a model's JSON field. Notes: - Applies datetime to ISO, Decimal to float - Respects \"Please Select\" for placeholders - Values in `type_matching_dict` are explicitly cast to the specified types \"\"\" type_matching_dict = type_matching_dict or { \"id_incidence\" : int } return make_dict_serializeable ( payload , type_map = type_matching_dict , convert_time_to_ca = True ) remove_validators ( form , field_names , validators_to_remove = None ) Remove specified validators from selected WTForms fields. Parameters: form ( FlaskForm ) \u2013 The WTForms form instance. field_names ( list [ str ] ) \u2013 List of field names to examine and modify. validators_to_remove ( list [ type ] | None , default: None ) \u2013 Validator classes to remove. Defaults to [InputRequired] if not provided. Notes This modifies the validators list in-place and is useful when conditional field requirements apply. Example remove_validators(form, [\"name\", \"email\"], [InputRequired]) Notes Useful when validator logic depends on user input or view context. Not currently in use Source code in arb\\utils\\wtf_forms_util.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def remove_validators ( form : FlaskForm , field_names : list [ str ], validators_to_remove : list [ type ] | None = None ) -> None : \"\"\" Remove specified validators from selected WTForms fields. Args: form (FlaskForm): The WTForms form instance. field_names (list[str]): List of field names to examine and modify. validators_to_remove (list[type] | None): Validator classes to remove. Defaults to [InputRequired] if not provided. Notes: This modifies the validators list in-place and is useful when conditional field requirements apply. Example: >>> remove_validators(form, [\"name\", \"email\"], [InputRequired]) Notes: - Useful when validator logic depends on user input or view context. - Not currently in use \"\"\" if validators_to_remove is None : validators_to_remove = [ InputRequired ] fields = get_wtforms_fields ( form , include_csrf_token = False ) for field in fields : if field in field_names : validators = form [ field ] . validators for validator in validators : for validator_to_remove in validators_to_remove : if isinstance ( validator , validator_to_remove ): # logger.debug(f\"{type(validators)}, {validators=}, {validator=}\") validators . remove ( validator ) run_diagnostics () Run a full diagnostics suite for WTForms utility testing. Simulates a complete form lifecycle Initializes a mock form and model. Transfers model \u2192 form \u2192 model. Adjusts validators dynamically. Simulates invalid user input. Verifies selector enforcement and error tracking. Validates without CSRF enforcement. Returns: None \u2013 None Notes Logs key actions and field state. Does not require a Flask app or real DB connection. Intended for standalone execution and manual inspection. Source code in arb\\utils\\wtf_forms_util.py 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 def run_diagnostics () -> None : \"\"\" Run a full diagnostics suite for WTForms utility testing. Simulates a complete form lifecycle: - Initializes a mock form and model. - Transfers model \u2192 form \u2192 model. - Adjusts validators dynamically. - Simulates invalid user input. - Verifies selector enforcement and error tracking. - Validates without CSRF enforcement. Returns: None Notes: - Logs key actions and field state. - Does not require a Flask app or real DB connection. - Intended for standalone execution and manual inspection. \"\"\" class DummyModel : \"\"\"Mock SQLAlchemy model with JSON-like attribute.\"\"\" def __init__ ( self ): self . misc_json = { \"name\" : \"Alice\" , \"age\" : 30 , \"created_at\" : \"2024-01-01T08:00:00Z\" } class TestForm ( FlaskForm ): \"\"\"Test WTForm.\"\"\" name = SelectField ( 'Name' , choices = [( PLEASE_SELECT , PLEASE_SELECT ), ( \"Alice\" , \"Alice\" ), ( \"Bob\" , \"Bob\" )], validators = [ InputRequired ()]) age = DecimalField ( 'Age' , validators = [ InputRequired ()]) created_at = DateTimeField ( 'Created At' , format = \"%Y-%m- %d T%H:%M\" , validators = [ InputRequired ()]) from werkzeug.datastructures import MultiDict logger . info ( \"Running WTForms diagnostics...\" ) form = TestForm ( formdata = MultiDict ({ \"name\" : \"Alice\" , \"age\" : \"30.00\" , \"created_at\" : \"2024-01-01T00:00\" })) model = DummyModel () # Ensure defaults work initialize_drop_downs ( form ) # Transfer model \u2192 form model_to_wtform ( model , form ) logger . info ( f \"Model \u2192 Form: name= { form . name . data } , age= { form . age . data } , created_at= { form . created_at . data } \" ) # Form \u2192 model (round-trip) wtform_to_model ( model , form ) logger . info ( f \"Updated model.misc_json: { model . misc_json } \" ) # Count errors error_summary = wtf_count_errors ( form , log_errors = True ) logger . info ( f \"Error summary: { error_summary } \" ) # Test validator manipulation logger . info ( \"Testing change_validators...\" ) change_validators ( form , field_names_to_change = [ \"age\" ], old_validator = InputRequired , new_validator = Optional ) for field_name in [ \"name\" , \"age\" , \"created_at\" ]: logger . debug ( f \" { field_name } validators: { form [ field_name ] . validators } \" ) # Test selector validation (simulate bad input) form . name . data = PLEASE_SELECT validate_selectors ( form ) logger . info ( f \"Name field errors after selector validation: { form . name . errors } \" ) # Test CSRF-less validation result = validate_no_csrf ( form ) logger . info ( f \"validate_no_csrf result: { result } , errors: { form . errors } \" ) logger . info ( \"WTForms diagnostics completed successfully.\" ) if __name__ == '__main__' : run_diagnostics () update_model_with_payload ( model , payload , json_field = 'misc_json' , comment = '' ) Apply a JSON-safe payload to a model's JSON column and mark it as changed. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance to update. payload ( dict ) \u2013 Dictionary of updates to apply. json_field ( str , default: 'misc_json' ) \u2013 Name of the model's JSON column (default is \"misc_json\"). comment ( str , default: '' ) \u2013 Optional comment to include with update logging. Notes Calls prep_payload_for_json to ensure data integrity. Uses apply_json_patch_and_log to track and log changes. Deep-copies the existing JSON field to avoid side effects. Source code in arb\\utils\\wtf_forms_util.py 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 def update_model_with_payload ( model : DeclarativeMeta , payload : dict , json_field : str = \"misc_json\" , comment : str = \"\" ) -> None : \"\"\" Apply a JSON-safe payload to a model's JSON column and mark it as changed. Args: model (DeclarativeMeta): SQLAlchemy model instance to update. payload (dict): Dictionary of updates to apply. json_field (str): Name of the model's JSON column (default is \"misc_json\"). comment (str): Optional comment to include with update logging. Notes: - Calls `prep_payload_for_json` to ensure data integrity. - Uses `apply_json_patch_and_log` to track and log changes. - Deep-copies the existing JSON field to avoid side effects. \"\"\" logger . debug ( f \"update_model_with_payload: { model =} , { payload =} \" ) model_json = copy . deepcopy ( getattr ( model , json_field ) or {}) new_payload = prep_payload_for_json ( payload ) model_json . update ( new_payload ) apply_json_patch_and_log ( model , json_field = json_field , updates = model_json , user = \"anonymous\" , comments = comment , ) logger . debug ( f \"Model JSON updated: { getattr ( model , json_field ) =} \" ) validate_no_csrf ( form , extra_validators = None ) Validate a WTForm while skipping CSRF errors (useful for GET-submitted forms). Parameters: form ( FlaskForm ) \u2013 The form to validate. extra_validators ( dict | None , default: None ) \u2013 Optional per-field validators to apply. Returns: bool ( bool ) \u2013 True if the form is valid after removing CSRF errors, otherwise False. Example if validate_no_csrf(form): handle_form() Notes This allows validation to succeed even when CSRF tokens are missing or invalid. It logs before and after validation for debug purposes. Source code in arb\\utils\\wtf_forms_util.py 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 def validate_no_csrf ( form : FlaskForm , extra_validators : dict | None = None ) -> bool : \"\"\" Validate a WTForm while skipping CSRF errors (useful for GET-submitted forms). Args: form (FlaskForm): The form to validate. extra_validators (dict | None): Optional per-field validators to apply. Returns: bool: True if the form is valid after removing CSRF errors, otherwise False. Example: >>> if validate_no_csrf(form): handle_form() Notes: - This allows validation to succeed even when CSRF tokens are missing or invalid. - It logs before and after validation for debug purposes. \"\"\" logger . debug ( f \"validate_no_csrf() called:\" ) form . validate ( extra_validators = extra_validators ) if form . errors and 'csrf_token' in form . errors : del form . errors [ 'csrf_token' ] csrf_field = getattr ( form , 'csrf_token' , None ) if csrf_field : if csrf_field . errors : if 'The CSRF token is missing.' in csrf_field . errors : csrf_field . errors . remove ( 'The CSRF token is missing.' ) form_valid = not bool ( form . errors ) logger . debug ( f \"after validate_no_csrf() called: { form_valid =} , { form . errors =} \" ) return form_valid validate_selectors ( form , default = None ) Append validation errors for SelectFields left at default placeholder values. Parameters: form ( FlaskForm ) \u2013 WTForm instance containing SelectFields. default ( str | None , default: None ) \u2013 Placeholder value to treat as invalid (default: \"Please Select\"). Returns: None \u2013 None Notes Typically used for GET-submitted forms where default values are not caught automatically. Adds \"This field is required.\" error to fields that are InputRequired but still at default. Source code in arb\\utils\\wtf_forms_util.py 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 def validate_selectors ( form : FlaskForm , default : str = None ) -> None : \"\"\" Append validation errors for SelectFields left at default placeholder values. Args: form (FlaskForm): WTForm instance containing SelectFields. default (str | None): Placeholder value to treat as invalid (default: \"Please Select\"). Returns: None Notes: - Typically used for GET-submitted forms where default values are not caught automatically. - Adds \"This field is required.\" error to fields that are InputRequired but still at default. \"\"\" if default is None : default = PLEASE_SELECT for field in form : if isinstance ( field , SelectField ): if field . data is None or field . data == default : for validator in field . validators : if isinstance ( validator , InputRequired ): msg = \"This field is required.\" if msg not in field . errors : field . errors . append ( msg ) wtf_count_errors ( form , log_errors = False ) Count validation errors on a WTForm instance. Parameters: form ( FlaskForm ) \u2013 The form to inspect. log_errors ( bool , default: False ) \u2013 If True, log the form's errors using debug log level. Returns: dict [ str , int ] \u2013 dict[str, int]: Dictionary with error counts: - 'elements_with_errors': number of fields that had one or more errors - 'element_error_count': total number of field-level errors - 'wtf_form_error_count': number of form-level (non-field) errors - 'total_error_count': sum of all error types Notes Ensure form.validate_on_submit() or form.validate() has been called first, or the error counts will be inaccurate. Example error_summary = wtf_count_errors(form) print(error_summary[\"total_error_count\"]) Source code in arb\\utils\\wtf_forms_util.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def wtf_count_errors ( form : FlaskForm , log_errors : bool = False ) -> dict [ str , int ]: \"\"\" Count validation errors on a WTForm instance. Args: form (FlaskForm): The form to inspect. log_errors (bool): If True, log the form's errors using debug log level. Returns: dict[str, int]: Dictionary with error counts: - 'elements_with_errors': number of fields that had one or more errors - 'element_error_count': total number of field-level errors - 'wtf_form_error_count': number of form-level (non-field) errors - 'total_error_count': sum of all error types Notes: Ensure `form.validate_on_submit()` or `form.validate()` has been called first, or the error counts will be inaccurate. Example: >>> error_summary = wtf_count_errors(form) >>> print(error_summary[\"total_error_count\"]) \"\"\" error_count_dict = { 'elements_with_errors' : 0 , 'element_error_count' : 0 , 'wtf_form_error_count' : 0 , 'total_error_count' : 0 , } if log_errors : logger . debug ( f \"Form errors are: { form . errors } \" ) for field , error_list in form . errors . items (): if field is None : error_count_dict [ 'wtf_form_error_count' ] += len ( error_list ) else : error_count_dict [ 'elements_with_errors' ] += 1 error_count_dict [ 'element_error_count' ] += len ( error_list ) error_count_dict [ 'total_error_count' ] = ( error_count_dict [ 'element_error_count' ] + error_count_dict [ 'wtf_form_error_count' ] ) return error_count_dict wtform_to_model ( model , wtform , json_column = 'misc_json' , user = 'anonymous' , comments = '' , ignore_fields = None , type_matching_dict = None ) Extract data from a WTForm and update the model's JSON column. Logs all changes. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance. wtform ( FlaskForm ) \u2013 WTForm with typed Python values. json_column ( str , default: 'misc_json' ) \u2013 JSON column name on the model. user ( str , default: 'anonymous' ) \u2013 Username for logging purposes. comments ( str , default: '' ) \u2013 Optional comment for logging context. ignore_fields ( list [ str ] | None , default: None ) \u2013 Fields to exclude from update. type_matching_dict ( dict [ str , type ] | None , default: None ) \u2013 Optional override for type enforcement. Notes Uses make_dict_serializeable and get_changed_fields to compare values. Delegates to apply_json_patch_and_log to persist and log changes. Source code in arb\\utils\\wtf_forms_util.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 def wtform_to_model ( model : DeclarativeMeta , wtform : FlaskForm , json_column : str = \"misc_json\" , user : str = \"anonymous\" , comments : str = \"\" , ignore_fields : list [ str ] | None = None , type_matching_dict : dict [ str , type ] | None = None ) -> None : \"\"\" Extract data from a WTForm and update the model's JSON column. Logs all changes. Args: model (DeclarativeMeta): SQLAlchemy model instance. wtform (FlaskForm): WTForm with typed Python values. json_column (str): JSON column name on the model. user (str): Username for logging purposes. comments (str): Optional comment for logging context. ignore_fields (list[str] | None): Fields to exclude from update. type_matching_dict (dict[str, type] | None): Optional override for type enforcement. Notes: - Uses make_dict_serializeable and get_changed_fields to compare values. - Delegates to apply_json_patch_and_log to persist and log changes. \"\"\" ignore_fields = set ( ignore_fields or []) payload_all = { field_name : getattr ( wtform , field_name ) . data for field_name in get_wtforms_fields ( wtform ) if field_name not in ignore_fields } # Use manual overrides only \u2014 no type_map from form payload_all = make_dict_serializeable ( payload_all , type_map = type_matching_dict , convert_time_to_ca = True ) existing_json = load_model_json_column ( model , json_column ) existing_serialized = make_dict_serializeable ( existing_json , type_map = type_matching_dict , convert_time_to_ca = True ) payload_changes = get_changed_fields ( payload_all , existing_serialized ) if payload_changes : logger . info ( f \"wtform_to_model payload_changes: { payload_changes } \" ) apply_json_patch_and_log ( model , payload_changes , json_column , user = user , comments = comments ) logger . info ( f \"wtform_to_model payload_all: { payload_all } \" )","title":"arb.utils.wtf_forms_util"},{"location":"reference/arb/utils/wtf_forms_util/#arbutilswtf_forms_util","text":"Functions and helper classes to support WTForms models. Notes WTForm model classes should remain adjacent to Flask views (e.g., in wtf_landfill.py ) This module is for shared utilities, validators, and form-to-model conversion logic.","title":"arb.utils.wtf_forms_util"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.IfTruthy","text":"WTForms validator: Dynamically switches between InputRequired and Optional. The validator behavior is conditional on another field\u2019s truthiness. Depending on mode , this field becomes required or optional. Parameters: other_field_name ( str ) \u2013 Name of the field to evaluate. falsy_values ( list | None , default: None ) \u2013 Custom falsy value list (defaults to standard values). mode ( str , default: 'required on truthy' ) \u2013 Either 'required on truthy' or 'optional on truthy'. message ( str | None , default: None ) \u2013 Optional validation error message. Raises: TypeError \u2013 If an invalid mode is provided. Example class MyForm(FlaskForm): toggle = BooleanField() extra = StringField(validators=[IfTruthy(\"toggle\", mode=\"required on truthy\")]) Source code in arb\\utils\\wtf_forms_util.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 class IfTruthy : \"\"\" WTForms validator: Dynamically switches between InputRequired and Optional. The validator behavior is conditional on another field\u2019s truthiness. Depending on `mode`, this field becomes required or optional. Args: other_field_name (str): Name of the field to evaluate. falsy_values (list | None): Custom falsy value list (defaults to standard values). mode (str): Either 'required on truthy' or 'optional on truthy'. message (str | None): Optional validation error message. Raises: TypeError: If an invalid mode is provided. Example: class MyForm(FlaskForm): toggle = BooleanField() extra = StringField(validators=[IfTruthy(\"toggle\", mode=\"required on truthy\")]) \"\"\" field_flags = ( \"iftruthy\" ,) def __init__ ( self , other_field_name : str , falsy_values : list | None = None , mode : str = 'required on truthy' , message : str | None = None ): self . other_field_name = other_field_name if falsy_values is None : falsy_values = [ False , [], {}, (), '' , '0' , '0.0' , 0 , 0.0 ] self . falsy_values = falsy_values if mode == 'required on truthy' : self . validators = { 'truthy' : InputRequired , 'falsy' : Optional } elif mode == 'optional on truthy' : self . validators = { 'truthy' : Optional , 'falsy' : InputRequired } else : raise TypeError ( f \"Unknown mode: { mode } \" ) self . message = message def __call__ ( self , form , field ): other_field = form [ self . other_field_name ] if other_field is None : raise Exception ( f 'No field named \" { self . other_field_name } \" in form' ) validator_class = self . validators [ 'truthy' ] if other_field . data not in self . falsy_values else self . validators [ 'falsy' ] validator_class ( self . message ) . __call__ ( form , field )","title":"IfTruthy"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.RequiredIfTruthy","text":"WTForms validator: Applies InputRequired or Optional based on another field's truthiness. If the referenced field is \"truthy\" (not in a falsy list), then this field is required. If it is \"falsy\", this field becomes optional. Parameters: other_field_name ( str ) \u2013 Name of the field to check. message ( str | None , default: None ) \u2013 Optional custom validation error message. other_field_invalid_values ( list | None , default: None ) \u2013 Values considered falsy (defaults to standard empty/zero/null values). Example class MyForm(FlaskForm): confirm = BooleanField() notes = StringField(validators=[RequiredIfTruthy(\"confirm\")]) Source code in arb\\utils\\wtf_forms_util.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 class RequiredIfTruthy : \"\"\" WTForms validator: Applies InputRequired or Optional based on another field's truthiness. If the referenced field is \"truthy\" (not in a falsy list), then this field is required. If it is \"falsy\", this field becomes optional. Args: other_field_name (str): Name of the field to check. message (str | None): Optional custom validation error message. other_field_invalid_values (list | None): Values considered falsy (defaults to standard empty/zero/null values). Example: class MyForm(FlaskForm): confirm = BooleanField() notes = StringField(validators=[RequiredIfTruthy(\"confirm\")]) \"\"\" field_flags = ( \"requirediftruthy\" ,) def __init__ ( self , other_field_name : str , message : str | None = None , other_field_invalid_values : list | None = None ): self . other_field_name = other_field_name self . message = message if other_field_invalid_values is None : other_field_invalid_values = [ False , [], {}, (), '' , '0' , '0.0' , 0 , 0.0 ] self . other_field_invalid_values = other_field_invalid_values logger . debug ( \"RequiredIfTruthy initialized\" ) def __call__ ( self , form , field ): other_field = form [ self . other_field_name ] if other_field is None : raise Exception ( f 'No field named \" { self . other_field_name } \" in form' ) if other_field . data not in self . other_field_invalid_values : logger . debug ( \"other_field is truthy \u2192 requiring this field\" ) InputRequired ( self . message ) . __call__ ( form , field ) else : logger . debug ( \"other_field is falsy \u2192 allowing this field to be optional\" ) Optional ( self . message ) . __call__ ( form , field )","title":"RequiredIfTruthy"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.build_choices","text":"Combine header and dynamic items into a list of triple-tuples for WTForms SelectFields. Parameters: header ( list [ tuple [ str , str , dict ]] ) \u2013 Static options to appear first in the dropdown. items ( list [ str ] ) \u2013 Dynamic option values to convert into (value, label, {}) format. Returns: list [ tuple [ str , str , dict ]] \u2013 list[tuple[str, str, dict]]: Combined list of header and generated item tuples. Example build_choices( ... [(\"Please Select\", \"Please Select\", {\"disabled\": True})], ... [\"One\", \"Two\"] ... ) [ (\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"One\", \"One\", {}), (\"Two\", \"Two\", {}) ] Source code in arb\\utils\\wtf_forms_util.py 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 def build_choices ( header : list [ tuple [ str , str , dict ]], items : list [ str ]) -> list [ tuple [ str , str , dict ]]: \"\"\" Combine header and dynamic items into a list of triple-tuples for WTForms SelectFields. Args: header (list[tuple[str, str, dict]]): Static options to appear first in the dropdown. items (list[str]): Dynamic option values to convert into (value, label, {}) format. Returns: list[tuple[str, str, dict]]: Combined list of header and generated item tuples. Example: >>> build_choices( ... [(\"Please Select\", \"Please Select\", {\"disabled\": True})], ... [\"One\", \"Two\"] ... ) [ (\"Please Select\", \"Please Select\", {\"disabled\": True}), (\"One\", \"One\", {}), (\"Two\", \"Two\", {}) ] \"\"\" footer = [( item , item , {}) for item in items ] return header + footer","title":"build_choices"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.change_validators","text":"Replace one validator type with another on a list of WTForms fields. Parameters: form ( FlaskForm ) \u2013 WTForms form instance. field_names_to_change ( list [ str ] ) \u2013 List of fields to alter. old_validator ( type ) \u2013 Validator class to remove (e.g., Optional). new_validator ( type ) \u2013 Validator class to add (e.g., InputRequired). Notes The replacement is done in-place on each field's validators list. Useful for dynamically changing required status. Example change_validators(form, [\"name\"], Optional, InputRequired) Source code in arb\\utils\\wtf_forms_util.py 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def change_validators ( form : FlaskForm , field_names_to_change : list [ str ], old_validator : type , new_validator : type ) -> None : \"\"\" Replace one validator type with another on a list of WTForms fields. Args: form (FlaskForm): WTForms form instance. field_names_to_change (list[str]): List of fields to alter. old_validator (type): Validator class to remove (e.g., Optional). new_validator (type): Validator class to add (e.g., InputRequired). Notes: - The replacement is done in-place on each field's `validators` list. - Useful for dynamically changing required status. Example: >>> change_validators(form, [\"name\"], Optional, InputRequired) \"\"\" field_names = get_wtforms_fields ( form , include_csrf_token = False ) for field_name in field_names : if field_name in field_names_to_change : validators = form [ field_name ] . validators for i , validator in enumerate ( validators ): if isinstance ( validator , old_validator ): validators [ i ] = new_validator ()","title":"change_validators"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.change_validators_on_test","text":"Conditionally switch validators on selected form fields based on a boolean test. If bool_test is True Fields in required_if_true become required (InputRequired). Fields in optional_if_true become optional (Optional). If bool_test is False Fields in required_if_true become optional. Fields in optional_if_true become required. Parameters: form ( FlaskForm ) \u2013 The form to update. bool_test ( bool ) \u2013 If True, required/optional fields are swapped accordingly. required_if_true ( list [ str ] ) \u2013 Field names that become required when bool_test is True. optional_if_true ( list [ str ] | None , default: None ) \u2013 Field names that become optional when bool_test is True. Example change_validators_on_test(form, bool_test=is_active, ... required_if_true=[\"comment\"], ... optional_if_true=[\"note\"]) Source code in arb\\utils\\wtf_forms_util.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 def change_validators_on_test ( form : FlaskForm , bool_test : bool , required_if_true : list [ str ], optional_if_true : list [ str ] | None = None ) -> None : \"\"\" Conditionally switch validators on selected form fields based on a boolean test. If bool_test is True: - Fields in required_if_true become required (InputRequired). - Fields in optional_if_true become optional (Optional). If bool_test is False: - Fields in required_if_true become optional. - Fields in optional_if_true become required. Args: form (FlaskForm): The form to update. bool_test (bool): If True, required/optional fields are swapped accordingly. required_if_true (list[str]): Field names that become required when bool_test is True. optional_if_true (list[str] | None): Field names that become optional when bool_test is True. Example: >>> change_validators_on_test(form, bool_test=is_active, ... required_if_true=[\"comment\"], ... optional_if_true=[\"note\"]) \"\"\" if optional_if_true is None : optional_if_true = [] if bool_test : change_validators ( form , field_names_to_change = required_if_true , old_validator = Optional , new_validator = InputRequired , ) change_validators ( form , field_names_to_change = optional_if_true , old_validator = InputRequired , new_validator = Optional , ) else : change_validators ( form , field_names_to_change = required_if_true , old_validator = InputRequired , new_validator = Optional , ) change_validators ( form , field_names_to_change = optional_if_true , old_validator = Optional , new_validator = InputRequired , )","title":"change_validators_on_test"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.ensure_field_choice","text":"Ensure a field\u2019s current value is among its valid choices, or reset it to a placeholder. Parameters: field_name ( str ) \u2013 Name of the WTForms field (for logging purposes). field ( Field ) \u2013 WTForms-compatible field (typically a SelectField). choices ( list [ tuple [ str , str ]] | list [ tuple [ str , str , dict ]] | None , default: None ) \u2013 Valid choices to enforce. If None, uses the field's existing choices. Returns: None \u2013 None Notes: - If choices is provided, this function sets field.choices to the new list. - If choices is None, it uses the field's existing .choices . In either case, it validates that the current field.data is among the available options and resets it to \"Please Select\" if not. - Both field.data and field.raw_data are reset to keep form behavior consistent. - Each choice tuple should be in the form: - (value, label), or - (value, label, metadata_dict) - Only the first element ( value ) is used for validation. - Use this with SelectField or similar fields where .choices must be explicitly defined. - The reset value \"Please Select\" should match a placeholder value if one is used in your app. Example ensure_field_choice(\"sector\", form.sector, [(\"oil\", \"Oil & Gas\"), (\"land\", \"Landfill\")]) form.sector.choices [('oil', 'Oil & Gas'), ('land', 'Landfill')] Source code in arb\\utils\\wtf_forms_util.py 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 def ensure_field_choice ( field_name : str , field , choices : list [ tuple [ str , str ] | tuple [ str , str , dict ]] | None = None ) -> None : \"\"\" Ensure a field\u2019s current value is among its valid choices, or reset it to a placeholder. Args: field_name (str): Name of the WTForms field (for logging purposes). field (Field): WTForms-compatible field (typically a SelectField). choices (list[tuple[str, str]] | list[tuple[str, str, dict]] | None): Valid choices to enforce. If None, uses the field's existing choices. Returns: None Notes: - If `choices` is provided, this function sets `field.choices` to the new list. - If `choices` is None, it uses the field's existing `.choices`. In either case, it validates that the current `field.data` is among the available options and resets it to \"Please Select\" if not. - Both `field.data` and `field.raw_data` are reset to keep form behavior consistent. - Each choice tuple should be in the form: - (value, label), or - (value, label, metadata_dict) - Only the first element (`value`) is used for validation. - Use this with SelectField or similar fields where `.choices` must be explicitly defined. - The reset value \"Please Select\" should match a placeholder value if one is used in your app. Example: >>> ensure_field_choice(\"sector\", form.sector, [(\"oil\", \"Oil & Gas\"), (\"land\", \"Landfill\")]) >>> form.sector.choices [('oil', 'Oil & Gas'), ('land', 'Landfill')] \"\"\" if choices is None : # Use existing field choices if none are supplied choices = field . choices else : # Apply a new set of choices to the field field . choices = choices valid_values = { c [ 0 ] for c in choices } if field . data not in valid_values : logger . debug ( f \" { field_name } .data= { field . data !r} not in valid options, resetting to ' { PLEASE_SELECT } '\" ) field . data = PLEASE_SELECT field . raw_data = [ field . data ]","title":"ensure_field_choice"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.format_raw_data","text":"Convert a field value to a format suitable for WTForms .raw_data . Parameters: field ( Field ) \u2013 A WTForms field instance (e.g., DecimalField, DateTimeField). value ( str | int | float | Decimal | datetime | None ) \u2013 The field's data value. Returns: list [ str ] \u2013 list[str]: List of string values to assign to field.raw_data . Raises: ValueError \u2013 If the value type is unsupported. Example format_raw_data(field, Decimal(\"10.5\")) ['10.5'] Source code in arb\\utils\\wtf_forms_util.py 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def format_raw_data ( field : Field , value ) -> list [ str ]: \"\"\" Convert a field value to a format suitable for WTForms `.raw_data`. Args: field (Field): A WTForms field instance (e.g., DecimalField, DateTimeField). value (str | int | float | Decimal | datetime.datetime | None): The field's data value. Returns: list[str]: List of string values to assign to `field.raw_data`. Raises: ValueError: If the value type is unsupported. Example: >>> format_raw_data(field, Decimal(\"10.5\")) ['10.5'] \"\"\" if value is None : return [] elif isinstance ( value , ( str , int , float )): return [ str ( value )] elif isinstance ( value , Decimal ): return [ str ( float ( value ))] # Cast to float before converting to string elif isinstance ( value , datetime . datetime ): return [ value . isoformat ()] else : raise ValueError ( f \"Unsupported type for raw_data: { type ( value ) } with value { value } \" )","title":"format_raw_data"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.get_payloads","text":"DEPRECATED: Use wtform_to_model() instead. Extract all field values and changed values from a WTForm. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model with JSON column misc_json . wtform ( FlaskForm ) \u2013 The form to extract values from. ignore_fields ( list [ str ] | None , default: None ) \u2013 List of fields to skip during comparison. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: Tuple of (payload_all, payload_changes) - payload_all: All form fields - payload_changes: Subset of fields with changed values vs. model Notes Performs a naive comparison (==) without deserializing types. Use skip_empty_fields = True to suppress null-like values. Source code in arb\\utils\\wtf_forms_util.py 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 def get_payloads ( model : DeclarativeMeta , wtform : FlaskForm , ignore_fields : list [ str ] | None = None ) -> tuple [ dict , dict ]: \"\"\" DEPRECATED: Use `wtform_to_model()` instead. Extract all field values and changed values from a WTForm. Args: model (DeclarativeMeta): SQLAlchemy model with JSON column `misc_json`. wtform (FlaskForm): The form to extract values from. ignore_fields (list[str] | None): List of fields to skip during comparison. Returns: tuple[dict, dict]: Tuple of (payload_all, payload_changes) - payload_all: All form fields - payload_changes: Subset of fields with changed values vs. model Notes: - Performs a naive comparison (==) without deserializing types. - Use skip_empty_fields = True to suppress null-like values. \"\"\" if ignore_fields is None : ignore_fields = [] skip_empty_fields = False # Yes if you wish to skip blank fields from being updated when feasible payload_all = {} payload_changes = {} model_json_dict = getattr ( model , \"misc_json\" ) or {} logger . debug ( f \" { model_json_dict =} \" ) model_field_names = list ( model_json_dict . keys ()) form_field_names = get_wtforms_fields ( wtform ) list_differences ( model_field_names , form_field_names , iterable_01_name = \"SQLAlchemy Model\" , iterable_02_name = \"WTForm Fields\" , print_warning = False , ) for form_field_name in form_field_names : field = getattr ( wtform , form_field_name ) field_value = field . data model_value = model_json_dict . get ( form_field_name ) if form_field_name in ignore_fields : continue if skip_empty_fields is True : # skipping empty strings if the model is \"\" or None if field_value == \"\" : if model_value in [ None , \"\" ]: continue # Only persist \"Please Select\" if overwriting a meaningful value. if isinstance ( field , SelectField ) and field_value == PLEASE_SELECT : if model_value in [ None , \"\" ]: continue payload_all [ form_field_name ] = field_value # todo (depreciated) - object types are not being seen as equivalent (because they are serialized strings) # need to update logic - check out prep_payload_for_json for uniform approach if model_value != field_value : payload_changes [ form_field_name ] = field_value return payload_all , payload_changes","title":"get_payloads"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.get_wtforms_fields","text":"Return the sorted field names associated with a WTForms form. Parameters: form ( FlaskForm ) \u2013 The WTForms form instance. include_csrf_token ( bool , default: False ) \u2013 If True, include 'csrf_token' in the result. Returns: list [ str ] \u2013 list[str]: Alphabetically sorted list of field names in the form. Example get_wtforms_fields(form) ['name', 'sector'] Source code in arb\\utils\\wtf_forms_util.py 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 def get_wtforms_fields ( form : FlaskForm , include_csrf_token : bool = False ) -> list [ str ]: \"\"\" Return the sorted field names associated with a WTForms form. Args: form (FlaskForm): The WTForms form instance. include_csrf_token (bool): If True, include 'csrf_token' in the result. Returns: list[str]: Alphabetically sorted list of field names in the form. Example: >>> get_wtforms_fields(form) ['name', 'sector'] \"\"\" field_names = [ name for name in form . data if include_csrf_token or name != \"csrf_token\" ] field_names . sort () return field_names","title":"get_wtforms_fields"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.initialize_drop_downs","text":"Set default values for uninitialized WTForms SelectFields. Parameters: form ( FlaskForm ) \u2013 The form containing SelectField fields to be initialized. default ( str | None , default: None ) \u2013 The value to assign to a field if its current value is None. If not provided, uses the application's global placeholder (e.g., \"Please Select\"). Returns: None \u2013 None Example initialize_drop_downs(form, default=\"Please Select\") Notes Fields that already have a value (even a falsy one like an empty string) are not modified. Only fields of type SelectField are affected. This function is typically used after form construction but before rendering or validation. Source code in arb\\utils\\wtf_forms_util.py 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 def initialize_drop_downs ( form : FlaskForm , default : str = None ) -> None : \"\"\" Set default values for uninitialized WTForms SelectFields. Args: form (FlaskForm): The form containing SelectField fields to be initialized. default (str | None): The value to assign to a field if its current value is None. If not provided, uses the application's global placeholder (e.g., \"Please Select\"). Returns: None Example: >>> initialize_drop_downs(form, default=\"Please Select\") Notes: - Fields that already have a value (even a falsy one like an empty string) are not modified. - Only fields of type `SelectField` are affected. - This function is typically used after form construction but before rendering or validation. \"\"\" if default is None : default = PLEASE_SELECT logger . debug ( \"Initializing drop-downs...\" ) for field in form : if isinstance ( field , SelectField ) and field . data is None : logger . debug ( f \" { field . name } set to default value: { default } \" ) field . data = default","title":"initialize_drop_downs"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.min_decimal_precision","text":"Return a validator for WTForms DecimalField enforcing minimum decimal precision. Parameters: min_digits ( int ) \u2013 Minimum number of digits required after the decimal. Returns: Callable ( Callable ) \u2013 WTForms-compatible validator that raises ValidationError if decimal places are insufficient. Example field = DecimalField(\"Amount\", validators=[min_decimal_precision(2)]) Source code in arb\\utils\\wtf_forms_util.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def min_decimal_precision ( min_digits : int ) -> Callable : \"\"\" Return a validator for WTForms DecimalField enforcing minimum decimal precision. Args: min_digits (int): Minimum number of digits required after the decimal. Returns: Callable: WTForms-compatible validator that raises ValidationError if decimal places are insufficient. Example: >>> field = DecimalField(\"Amount\", validators=[min_decimal_precision(2)]) \"\"\" def _min_decimal_precision ( form , field ): if field . data is None : return try : value_str = str ( field . data ) if '.' in value_str : _ , decimals = value_str . split ( '.' ) if len ( decimals ) < min_digits : raise ValidationError () elif min_digits > 0 : raise ValidationError () except ( ValueError , TypeError ): raise ValidationError ( f \"Field must be a valid numeric value with at least { min_digits } decimal places.\" ) return _min_decimal_precision","title":"min_decimal_precision"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.model_to_wtform","text":"Populate a WTForm from a SQLAlchemy model's JSON column. This function loads the model's JSON field (typically 'misc_json') and sets WTForms field .data and .raw_data accordingly. Required for correct rendering and validation of pre-filled forms. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance containing a JSON column. wtform ( FlaskForm ) \u2013 The WTForm instance to populate. json_column ( str , default: 'misc_json' ) \u2013 The attribute name of the JSON column. Defaults to \"misc_json\". Raises: ValueError \u2013 If a datetime value cannot be parsed. TypeError \u2013 If the field type is unsupported. Notes Supports preloading DateTimeField and DecimalField types. Converts ISO8601 UTC \u2192 localized Pacific time. Ignores JSON fields that don\u2019t map to WTForm fields. Source code in arb\\utils\\wtf_forms_util.py 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 def model_to_wtform ( model : DeclarativeMeta , wtform : FlaskForm , json_column : str = \"misc_json\" ) -> None : \"\"\" Populate a WTForm from a SQLAlchemy model's JSON column. This function loads the model's JSON field (typically 'misc_json') and sets WTForms field `.data` and `.raw_data` accordingly. Required for correct rendering and validation of pre-filled forms. Args: model (DeclarativeMeta): SQLAlchemy model instance containing a JSON column. wtform (FlaskForm): The WTForm instance to populate. json_column (str): The attribute name of the JSON column. Defaults to \"misc_json\". Raises: ValueError: If a datetime value cannot be parsed. TypeError: If the field type is unsupported. Notes: - Supports preloading DateTimeField and DecimalField types. - Converts ISO8601 UTC \u2192 localized Pacific time. - Ignores JSON fields that don\u2019t map to WTForm fields. \"\"\" model_json_dict = getattr ( model , json_column ) logger . debug ( f \"model_to_wtform called with model= { model } , json= { model_json_dict } \" ) # Ensure dict, not str if isinstance ( model_json_dict , str ): try : model_json_dict = json . loads ( model_json_dict ) logger . debug ( \"Parsed JSON string into dict.\" ) except json . JSONDecodeError : logger . warning ( f \"Invalid JSON in model's ' { json_column } ' column.\" ) model_json_dict = {} if model_json_dict is None : model_json_dict = {} if \"id_incidence\" in model_json_dict and model_json_dict [ \"id_incidence\" ] != model . id_incidence : logger . warning ( f \"[model_to_wtform] MISMATCH: model.id_incidence= { model . id_incidence } \" f \"!= misc_json['id_incidence']= { model_json_dict [ 'id_incidence' ] } \" ) form_fields = get_wtforms_fields ( wtform ) model_fields = list ( model_json_dict . keys ()) list_differences ( model_fields , form_fields , iterable_01_name = \"SQLAlchemy Model JSON\" , iterable_02_name = \"WTForm Fields\" , print_warning = False ) # Use utilities to get type map and convert model dict type_map , _ = wtform_types_and_values ( wtform ) parsed_dict = deserialize_dict ( model_json_dict , type_map , convert_time_to_ca = True ) for field_name in form_fields : field = getattr ( wtform , field_name ) model_value = parsed_dict . get ( field_name ) # Set field data and raw_data for proper rendering/validation field . data = model_value field . raw_data = format_raw_data ( field , model_value ) logger . debug ( f \"Set { field_name =} , data= { field . data } , raw_data= { field . raw_data } \" )","title":"model_to_wtform"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.prep_payload_for_json","text":"Prepare a payload dictionary for JSON-safe serialization. Parameters: payload ( dict ) \u2013 Key-value updates extracted from a WTForm or other source. type_matching_dict ( dict [ str , type ] | None , default: None ) \u2013 Optional type coercion rules. e.g., {\"id_incidence\": int, \"some_flag\": bool} Returns: dict ( dict ) \u2013 Transformed version of the payload, suitable for use in a model's JSON field. Notes Applies datetime to ISO, Decimal to float Respects \"Please Select\" for placeholders Values in type_matching_dict are explicitly cast to the specified types Source code in arb\\utils\\wtf_forms_util.py 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 def prep_payload_for_json ( payload : dict , type_matching_dict : dict [ str , type ] | None = None ) -> dict : \"\"\" Prepare a payload dictionary for JSON-safe serialization. Args: payload (dict): Key-value updates extracted from a WTForm or other source. type_matching_dict (dict[str, type] | None): Optional type coercion rules. e.g., {\"id_incidence\": int, \"some_flag\": bool} Returns: dict: Transformed version of the payload, suitable for use in a model's JSON field. Notes: - Applies datetime to ISO, Decimal to float - Respects \"Please Select\" for placeholders - Values in `type_matching_dict` are explicitly cast to the specified types \"\"\" type_matching_dict = type_matching_dict or { \"id_incidence\" : int } return make_dict_serializeable ( payload , type_map = type_matching_dict , convert_time_to_ca = True )","title":"prep_payload_for_json"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.remove_validators","text":"Remove specified validators from selected WTForms fields. Parameters: form ( FlaskForm ) \u2013 The WTForms form instance. field_names ( list [ str ] ) \u2013 List of field names to examine and modify. validators_to_remove ( list [ type ] | None , default: None ) \u2013 Validator classes to remove. Defaults to [InputRequired] if not provided. Notes This modifies the validators list in-place and is useful when conditional field requirements apply. Example remove_validators(form, [\"name\", \"email\"], [InputRequired]) Notes Useful when validator logic depends on user input or view context. Not currently in use Source code in arb\\utils\\wtf_forms_util.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def remove_validators ( form : FlaskForm , field_names : list [ str ], validators_to_remove : list [ type ] | None = None ) -> None : \"\"\" Remove specified validators from selected WTForms fields. Args: form (FlaskForm): The WTForms form instance. field_names (list[str]): List of field names to examine and modify. validators_to_remove (list[type] | None): Validator classes to remove. Defaults to [InputRequired] if not provided. Notes: This modifies the validators list in-place and is useful when conditional field requirements apply. Example: >>> remove_validators(form, [\"name\", \"email\"], [InputRequired]) Notes: - Useful when validator logic depends on user input or view context. - Not currently in use \"\"\" if validators_to_remove is None : validators_to_remove = [ InputRequired ] fields = get_wtforms_fields ( form , include_csrf_token = False ) for field in fields : if field in field_names : validators = form [ field ] . validators for validator in validators : for validator_to_remove in validators_to_remove : if isinstance ( validator , validator_to_remove ): # logger.debug(f\"{type(validators)}, {validators=}, {validator=}\") validators . remove ( validator )","title":"remove_validators"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.run_diagnostics","text":"Run a full diagnostics suite for WTForms utility testing. Simulates a complete form lifecycle Initializes a mock form and model. Transfers model \u2192 form \u2192 model. Adjusts validators dynamically. Simulates invalid user input. Verifies selector enforcement and error tracking. Validates without CSRF enforcement. Returns: None \u2013 None Notes Logs key actions and field state. Does not require a Flask app or real DB connection. Intended for standalone execution and manual inspection. Source code in arb\\utils\\wtf_forms_util.py 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 def run_diagnostics () -> None : \"\"\" Run a full diagnostics suite for WTForms utility testing. Simulates a complete form lifecycle: - Initializes a mock form and model. - Transfers model \u2192 form \u2192 model. - Adjusts validators dynamically. - Simulates invalid user input. - Verifies selector enforcement and error tracking. - Validates without CSRF enforcement. Returns: None Notes: - Logs key actions and field state. - Does not require a Flask app or real DB connection. - Intended for standalone execution and manual inspection. \"\"\" class DummyModel : \"\"\"Mock SQLAlchemy model with JSON-like attribute.\"\"\" def __init__ ( self ): self . misc_json = { \"name\" : \"Alice\" , \"age\" : 30 , \"created_at\" : \"2024-01-01T08:00:00Z\" } class TestForm ( FlaskForm ): \"\"\"Test WTForm.\"\"\" name = SelectField ( 'Name' , choices = [( PLEASE_SELECT , PLEASE_SELECT ), ( \"Alice\" , \"Alice\" ), ( \"Bob\" , \"Bob\" )], validators = [ InputRequired ()]) age = DecimalField ( 'Age' , validators = [ InputRequired ()]) created_at = DateTimeField ( 'Created At' , format = \"%Y-%m- %d T%H:%M\" , validators = [ InputRequired ()]) from werkzeug.datastructures import MultiDict logger . info ( \"Running WTForms diagnostics...\" ) form = TestForm ( formdata = MultiDict ({ \"name\" : \"Alice\" , \"age\" : \"30.00\" , \"created_at\" : \"2024-01-01T00:00\" })) model = DummyModel () # Ensure defaults work initialize_drop_downs ( form ) # Transfer model \u2192 form model_to_wtform ( model , form ) logger . info ( f \"Model \u2192 Form: name= { form . name . data } , age= { form . age . data } , created_at= { form . created_at . data } \" ) # Form \u2192 model (round-trip) wtform_to_model ( model , form ) logger . info ( f \"Updated model.misc_json: { model . misc_json } \" ) # Count errors error_summary = wtf_count_errors ( form , log_errors = True ) logger . info ( f \"Error summary: { error_summary } \" ) # Test validator manipulation logger . info ( \"Testing change_validators...\" ) change_validators ( form , field_names_to_change = [ \"age\" ], old_validator = InputRequired , new_validator = Optional ) for field_name in [ \"name\" , \"age\" , \"created_at\" ]: logger . debug ( f \" { field_name } validators: { form [ field_name ] . validators } \" ) # Test selector validation (simulate bad input) form . name . data = PLEASE_SELECT validate_selectors ( form ) logger . info ( f \"Name field errors after selector validation: { form . name . errors } \" ) # Test CSRF-less validation result = validate_no_csrf ( form ) logger . info ( f \"validate_no_csrf result: { result } , errors: { form . errors } \" ) logger . info ( \"WTForms diagnostics completed successfully.\" ) if __name__ == '__main__' : run_diagnostics ()","title":"run_diagnostics"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.update_model_with_payload","text":"Apply a JSON-safe payload to a model's JSON column and mark it as changed. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance to update. payload ( dict ) \u2013 Dictionary of updates to apply. json_field ( str , default: 'misc_json' ) \u2013 Name of the model's JSON column (default is \"misc_json\"). comment ( str , default: '' ) \u2013 Optional comment to include with update logging. Notes Calls prep_payload_for_json to ensure data integrity. Uses apply_json_patch_and_log to track and log changes. Deep-copies the existing JSON field to avoid side effects. Source code in arb\\utils\\wtf_forms_util.py 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 def update_model_with_payload ( model : DeclarativeMeta , payload : dict , json_field : str = \"misc_json\" , comment : str = \"\" ) -> None : \"\"\" Apply a JSON-safe payload to a model's JSON column and mark it as changed. Args: model (DeclarativeMeta): SQLAlchemy model instance to update. payload (dict): Dictionary of updates to apply. json_field (str): Name of the model's JSON column (default is \"misc_json\"). comment (str): Optional comment to include with update logging. Notes: - Calls `prep_payload_for_json` to ensure data integrity. - Uses `apply_json_patch_and_log` to track and log changes. - Deep-copies the existing JSON field to avoid side effects. \"\"\" logger . debug ( f \"update_model_with_payload: { model =} , { payload =} \" ) model_json = copy . deepcopy ( getattr ( model , json_field ) or {}) new_payload = prep_payload_for_json ( payload ) model_json . update ( new_payload ) apply_json_patch_and_log ( model , json_field = json_field , updates = model_json , user = \"anonymous\" , comments = comment , ) logger . debug ( f \"Model JSON updated: { getattr ( model , json_field ) =} \" )","title":"update_model_with_payload"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.validate_no_csrf","text":"Validate a WTForm while skipping CSRF errors (useful for GET-submitted forms). Parameters: form ( FlaskForm ) \u2013 The form to validate. extra_validators ( dict | None , default: None ) \u2013 Optional per-field validators to apply. Returns: bool ( bool ) \u2013 True if the form is valid after removing CSRF errors, otherwise False. Example if validate_no_csrf(form): handle_form() Notes This allows validation to succeed even when CSRF tokens are missing or invalid. It logs before and after validation for debug purposes. Source code in arb\\utils\\wtf_forms_util.py 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 def validate_no_csrf ( form : FlaskForm , extra_validators : dict | None = None ) -> bool : \"\"\" Validate a WTForm while skipping CSRF errors (useful for GET-submitted forms). Args: form (FlaskForm): The form to validate. extra_validators (dict | None): Optional per-field validators to apply. Returns: bool: True if the form is valid after removing CSRF errors, otherwise False. Example: >>> if validate_no_csrf(form): handle_form() Notes: - This allows validation to succeed even when CSRF tokens are missing or invalid. - It logs before and after validation for debug purposes. \"\"\" logger . debug ( f \"validate_no_csrf() called:\" ) form . validate ( extra_validators = extra_validators ) if form . errors and 'csrf_token' in form . errors : del form . errors [ 'csrf_token' ] csrf_field = getattr ( form , 'csrf_token' , None ) if csrf_field : if csrf_field . errors : if 'The CSRF token is missing.' in csrf_field . errors : csrf_field . errors . remove ( 'The CSRF token is missing.' ) form_valid = not bool ( form . errors ) logger . debug ( f \"after validate_no_csrf() called: { form_valid =} , { form . errors =} \" ) return form_valid","title":"validate_no_csrf"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.validate_selectors","text":"Append validation errors for SelectFields left at default placeholder values. Parameters: form ( FlaskForm ) \u2013 WTForm instance containing SelectFields. default ( str | None , default: None ) \u2013 Placeholder value to treat as invalid (default: \"Please Select\"). Returns: None \u2013 None Notes Typically used for GET-submitted forms where default values are not caught automatically. Adds \"This field is required.\" error to fields that are InputRequired but still at default. Source code in arb\\utils\\wtf_forms_util.py 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 def validate_selectors ( form : FlaskForm , default : str = None ) -> None : \"\"\" Append validation errors for SelectFields left at default placeholder values. Args: form (FlaskForm): WTForm instance containing SelectFields. default (str | None): Placeholder value to treat as invalid (default: \"Please Select\"). Returns: None Notes: - Typically used for GET-submitted forms where default values are not caught automatically. - Adds \"This field is required.\" error to fields that are InputRequired but still at default. \"\"\" if default is None : default = PLEASE_SELECT for field in form : if isinstance ( field , SelectField ): if field . data is None or field . data == default : for validator in field . validators : if isinstance ( validator , InputRequired ): msg = \"This field is required.\" if msg not in field . errors : field . errors . append ( msg )","title":"validate_selectors"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.wtf_count_errors","text":"Count validation errors on a WTForm instance. Parameters: form ( FlaskForm ) \u2013 The form to inspect. log_errors ( bool , default: False ) \u2013 If True, log the form's errors using debug log level. Returns: dict [ str , int ] \u2013 dict[str, int]: Dictionary with error counts: - 'elements_with_errors': number of fields that had one or more errors - 'element_error_count': total number of field-level errors - 'wtf_form_error_count': number of form-level (non-field) errors - 'total_error_count': sum of all error types Notes Ensure form.validate_on_submit() or form.validate() has been called first, or the error counts will be inaccurate. Example error_summary = wtf_count_errors(form) print(error_summary[\"total_error_count\"]) Source code in arb\\utils\\wtf_forms_util.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 def wtf_count_errors ( form : FlaskForm , log_errors : bool = False ) -> dict [ str , int ]: \"\"\" Count validation errors on a WTForm instance. Args: form (FlaskForm): The form to inspect. log_errors (bool): If True, log the form's errors using debug log level. Returns: dict[str, int]: Dictionary with error counts: - 'elements_with_errors': number of fields that had one or more errors - 'element_error_count': total number of field-level errors - 'wtf_form_error_count': number of form-level (non-field) errors - 'total_error_count': sum of all error types Notes: Ensure `form.validate_on_submit()` or `form.validate()` has been called first, or the error counts will be inaccurate. Example: >>> error_summary = wtf_count_errors(form) >>> print(error_summary[\"total_error_count\"]) \"\"\" error_count_dict = { 'elements_with_errors' : 0 , 'element_error_count' : 0 , 'wtf_form_error_count' : 0 , 'total_error_count' : 0 , } if log_errors : logger . debug ( f \"Form errors are: { form . errors } \" ) for field , error_list in form . errors . items (): if field is None : error_count_dict [ 'wtf_form_error_count' ] += len ( error_list ) else : error_count_dict [ 'elements_with_errors' ] += 1 error_count_dict [ 'element_error_count' ] += len ( error_list ) error_count_dict [ 'total_error_count' ] = ( error_count_dict [ 'element_error_count' ] + error_count_dict [ 'wtf_form_error_count' ] ) return error_count_dict","title":"wtf_count_errors"},{"location":"reference/arb/utils/wtf_forms_util/#arb.utils.wtf_forms_util.wtform_to_model","text":"Extract data from a WTForm and update the model's JSON column. Logs all changes. Parameters: model ( DeclarativeMeta ) \u2013 SQLAlchemy model instance. wtform ( FlaskForm ) \u2013 WTForm with typed Python values. json_column ( str , default: 'misc_json' ) \u2013 JSON column name on the model. user ( str , default: 'anonymous' ) \u2013 Username for logging purposes. comments ( str , default: '' ) \u2013 Optional comment for logging context. ignore_fields ( list [ str ] | None , default: None ) \u2013 Fields to exclude from update. type_matching_dict ( dict [ str , type ] | None , default: None ) \u2013 Optional override for type enforcement. Notes Uses make_dict_serializeable and get_changed_fields to compare values. Delegates to apply_json_patch_and_log to persist and log changes. Source code in arb\\utils\\wtf_forms_util.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 def wtform_to_model ( model : DeclarativeMeta , wtform : FlaskForm , json_column : str = \"misc_json\" , user : str = \"anonymous\" , comments : str = \"\" , ignore_fields : list [ str ] | None = None , type_matching_dict : dict [ str , type ] | None = None ) -> None : \"\"\" Extract data from a WTForm and update the model's JSON column. Logs all changes. Args: model (DeclarativeMeta): SQLAlchemy model instance. wtform (FlaskForm): WTForm with typed Python values. json_column (str): JSON column name on the model. user (str): Username for logging purposes. comments (str): Optional comment for logging context. ignore_fields (list[str] | None): Fields to exclude from update. type_matching_dict (dict[str, type] | None): Optional override for type enforcement. Notes: - Uses make_dict_serializeable and get_changed_fields to compare values. - Delegates to apply_json_patch_and_log to persist and log changes. \"\"\" ignore_fields = set ( ignore_fields or []) payload_all = { field_name : getattr ( wtform , field_name ) . data for field_name in get_wtforms_fields ( wtform ) if field_name not in ignore_fields } # Use manual overrides only \u2014 no type_map from form payload_all = make_dict_serializeable ( payload_all , type_map = type_matching_dict , convert_time_to_ca = True ) existing_json = load_model_json_column ( model , json_column ) existing_serialized = make_dict_serializeable ( existing_json , type_map = type_matching_dict , convert_time_to_ca = True ) payload_changes = get_changed_fields ( payload_all , existing_serialized ) if payload_changes : logger . info ( f \"wtform_to_model payload_changes: { payload_changes } \" ) apply_json_patch_and_log ( model , payload_changes , json_column , user = user , comments = comments ) logger . info ( f \"wtform_to_model payload_all: { payload_all } \" )","title":"wtform_to_model"},{"location":"reference/arb/utils/excel/xl_create/","text":"arb.utils.excel.xl_create Module to prepare Excel templates and generate new Excel files using Jinja-rendered payloads. This module performs schema-based templating of Excel spreadsheets for feedback forms, injects metadata, applies default values, and renders Excel files based on structured JSON payloads. Typical Workflow Use a spreadsheet with named ranges and Jinja placeholders. Generate an initial JSON schema using a VBA macro. Refine the schema by injecting typing information. Generate default and test payloads. Populate Excel files using these payloads. Run this file directly to create all schema and payload artifacts for landfill, oil and gas, and energy templates. create_default_types_schema ( diagnostics = False ) Create a JSON file that maps variable names to their default Python value types. Parameters: diagnostics ( bool , default: False ) \u2013 If True, logs each variable name and its type to the debug logger. Returns: dict ( dict ) \u2013 Dictionary mapping variable names to Python types (e.g., str, int, datetime). Example types = create_default_types_schema(diagnostics=True) Notes Output is saved to 'xl_schemas/default_value_types_v01_00.json'. A backup is compared against the newly generated file if present. Field names and types are sourced from xl_hardcoded.default_value_types_v01_00 . Source code in arb\\utils\\excel\\xl_create.py 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 def create_default_types_schema ( diagnostics : bool = False ) -> dict : \"\"\" Create a JSON file that maps variable names to their default Python value types. Args: diagnostics (bool): If True, logs each variable name and its type to the debug logger. Returns: dict: Dictionary mapping variable names to Python types (e.g., str, int, datetime). Example: >>> types = create_default_types_schema(diagnostics=True) Notes: - Output is saved to 'xl_schemas/default_value_types_v01_00.json'. - A backup is compared against the newly generated file if present. - Field names and types are sourced from `xl_hardcoded.default_value_types_v01_00`. \"\"\" from arb.utils.excel.xl_hardcoded import default_value_types_v01_00 logger . debug ( \"create_default_types_schema() called\" ) file_name = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00.json\" file_backup = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00_backup.json\" field_types = dict ( sorted ( default_value_types_v01_00 . items ())) if diagnostics : for name , typ in field_types . items (): logger . debug ( f \"' { name } ': { typ . __name__ } ,\" ) metadata = { \"schema_version\" : \"default_value_types_v01_00\" } json_save_with_meta ( file_name , field_types , metadata = metadata ) if file_name . is_file () and file_backup . is_file (): compare_json_files ( file_name , file_backup ) return field_types create_payload ( payload , file_name , schema_version , metadata = None ) Create a JSON payload file with embedded metadata describing the schema version. Parameters: payload ( dict ) \u2013 Dictionary of values to serialize to JSON. file_name ( Path ) \u2013 Path to output the payload JSON file. schema_version ( str ) \u2013 Identifier for the schema the payload conforms to. metadata ( dict , default: None ) \u2013 Additional metadata to embed. If None, a new dict is created. Returns: None \u2013 None Example create_payload({\"id_case\": \"A42\"}, Path(\"payload.json\"), \"v01_00\") Notes Adds 'schema_version' and a default payload description to metadata. Uses json_save_with_meta() to embed metadata into the JSON file. Logs all key actions and file paths for diagnostics. Source code in arb\\utils\\excel\\xl_create.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 def create_payload ( payload : dict , file_name : Path , schema_version : str , metadata : dict = None ) -> None : \"\"\" Create a JSON payload file with embedded metadata describing the schema version. Args: payload (dict): Dictionary of values to serialize to JSON. file_name (Path): Path to output the payload JSON file. schema_version (str): Identifier for the schema the payload conforms to. metadata (dict, optional): Additional metadata to embed. If None, a new dict is created. Returns: None Example: >>> create_payload({\"id_case\": \"A42\"}, Path(\"payload.json\"), \"v01_00\") Notes: - Adds 'schema_version' and a default payload description to metadata. - Uses `json_save_with_meta()` to embed metadata into the JSON file. - Logs all key actions and file paths for diagnostics. \"\"\" logger . debug ( \"create_payload() called\" ) if metadata is None : metadata = {} metadata [ \"schema_version\" ] = schema_version metadata [ \"payload description\" ] = \"Test of Excel jinja templating system\" logger . debug ( f \"Writing payload to { file_name } with metadata: { metadata } \" ) json_save_with_meta ( file_name , data = payload , metadata = metadata ) create_payloads () Generate and save example payload files for each supported sector (landfill, oil & gas, energy). Returns: None \u2013 None Example create_payloads() Notes Each payload is saved to xl_payloads/{schema_version}_payload_01.json . If a backup file exists, the new payload is compared against it for consistency. The energy payload reuses the oil & gas example data for demonstration purposes. Uses create_payload() to handle serialization and metadata embedding. Source code in arb\\utils\\excel\\xl_create.py 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 def create_payloads () -> None : \"\"\" Generate and save example payload files for each supported sector (landfill, oil & gas, energy). Returns: None Example: >>> create_payloads() Notes: - Each payload is saved to `xl_payloads/{schema_version}_payload_01.json`. - If a backup file exists, the new payload is compared against it for consistency. - The energy payload reuses the oil & gas example data for demonstration purposes. - Uses `create_payload()` to handle serialization and metadata embedding. \"\"\" logger . debug ( \"create_payloads() called\" ) from arb.utils.excel.xl_hardcoded import landfill_payload_01 , oil_and_gas_payload_01 test_sets = [ ( \"landfill_v01_00\" , landfill_payload_01 ), ( \"oil_and_gas_v01_00\" , oil_and_gas_payload_01 ), ( \"energy_v00_01\" , oil_and_gas_payload_01 ), # Reuse oil & gas payload ] for schema_version , payload in test_sets : file_name = PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _payload_01.json\" file_backup = PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _payload_01_backup.json\" create_payload ( payload , file_name , schema_version ) if file_name . is_file () and file_backup . is_file (): compare_json_files ( file_name , file_backup ) create_schemas_and_payloads () Generate all schema, payload, and Excel artifacts for the feedback system. This orchestration function performs the full pipeline Creates the default value types schema. Processes all sector-specific schema files (landfill, oil & gas, energy). Writes default payloads for each schema. Generates test payloads and renders Excel templates. Returns: None \u2013 None Example create_schemas_and_payloads() Notes Creates all required directories under processed_versions . Intended for one-time use during development or deployment setup. Logs each operation and file path for debugging. Source code in arb\\utils\\excel\\xl_create.py 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 def create_schemas_and_payloads () -> None : \"\"\" Generate all schema, payload, and Excel artifacts for the feedback system. This orchestration function performs the full pipeline: - Creates the default value types schema. - Processes all sector-specific schema files (landfill, oil & gas, energy). - Writes default payloads for each schema. - Generates test payloads and renders Excel templates. Returns: None Example: >>> create_schemas_and_payloads() Notes: - Creates all required directories under `processed_versions`. - Intended for one-time use during development or deployment setup. - Logs each operation and file path for debugging. \"\"\" logger . debug ( \"create_schemas_and_payloads() called\" ) ensure_dir_exists ( PROCESSED_VERSIONS / \"xl_schemas\" ) ensure_dir_exists ( PROCESSED_VERSIONS / \"xl_workbooks\" ) ensure_dir_exists ( PROCESSED_VERSIONS / \"xl_payloads\" ) create_default_types_schema ( diagnostics = True ) prep_xl_templates () create_payloads () test_update_xlsx_payloads_01 () prep_xl_templates () Prepare processed Excel templates and payloads for landfill, oil & gas, and energy sectors. This function Copies original schema and Excel files to the processed directory. Converts VBA-generated schema files by injecting type info. Writes upgraded schema and default payload JSON files. Produces Jinja-compatible Excel workbook versions for templating. Returns: None \u2013 None Example prep_xl_templates() Notes File paths are derived from structured configs for each sector. Overwrites files in the output directory if they already exist. Output directories are created if they don't exist. Source code in arb\\utils\\excel\\xl_create.py 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 def prep_xl_templates () -> None : \"\"\" Prepare processed Excel templates and payloads for landfill, oil & gas, and energy sectors. This function: - Copies original schema and Excel files to the processed directory. - Converts VBA-generated schema files by injecting type info. - Writes upgraded schema and default payload JSON files. - Produces Jinja-compatible Excel workbook versions for templating. Returns: None Example: >>> prep_xl_templates() Notes: - File paths are derived from structured configs for each sector. - Overwrites files in the output directory if they already exist. - Output directories are created if they don't exist. \"\"\" logger . debug ( \"prep_xl_templates() called for landfill, oil & gas, and energy schemas\" ) file_specs = [] input_dir = PROJECT_ROOT / \"feedback_forms/current_versions\" output_dir = PROJECT_ROOT / \"feedback_forms/processed_versions\" ensure_dir_exists ( output_dir / \"xl_schemas\" ) ensure_dir_exists ( output_dir / \"xl_workbooks\" ) ensure_dir_exists ( output_dir / \"xl_payloads\" ) template_configs = [ ( \"landfill_v01_00\" , \"landfill_operator_feedback\" , LANDFILL_VERSION ), ( \"oil_and_gas_v01_00\" , \"oil_and_gas_operator_feedback\" , OIL_AND_GAS_VERSION ), ( \"energy_v00_01\" , \"energy_operator_feedback\" , ENERGY_VERSION ), ] for schema_version , prefix , version in template_configs : spec = { \"schema_version\" : schema_version , \"input_schema_vba_path\" : input_dir / f \" { schema_version } _vba.json\" , \"input_xl_path\" : input_dir / f \" { prefix } _ { version } .xlsx\" , \"input_xl_jinja_path\" : input_dir / f \" { prefix } _ { version } _jinja_.xlsx\" , \"output_schema_vba_path\" : output_dir / \"xl_schemas\" / f \" { schema_version } _vba.json\" , \"output_schema_path\" : output_dir / \"xl_schemas\" / f \" { schema_version } .json\" , \"output_xl_path\" : output_dir / \"xl_workbooks\" / f \" { prefix } _ { version } .xlsx\" , \"output_xl_jinja_path\" : output_dir / \"xl_workbooks\" / f \" { prefix } _ { version } _jinja_.xlsx\" , \"output_payload_path\" : output_dir / \"xl_payloads\" / f \" { schema_version } _defaults.json\" , } file_specs . append ( spec ) for spec in file_specs : logger . debug ( f \"Processing schema_version { spec [ 'schema_version' ] } \" ) file_map = [( spec [ \"input_schema_vba_path\" ], spec [ \"output_schema_vba_path\" ]), ( spec [ \"input_xl_path\" ], spec [ \"output_xl_path\" ]), ( spec [ \"input_xl_jinja_path\" ], spec [ \"output_xl_jinja_path\" ]), ] for file_old , file_new in file_map : logger . debug ( f \"Copying file from: { file_old } to: { file_new } \" ) shutil . copy ( file_old , file_new ) update_vba_schema ( spec [ \"schema_version\" ], file_name_in = spec [ \"output_schema_vba_path\" ], file_name_out = spec [ \"output_schema_path\" ]) schema_to_default_json ( file_name_in = spec [ \"output_schema_path\" ], file_name_out = spec [ \"output_payload_path\" ]) run_diagnostics () Execute a full suite of diagnostic routines to verify Excel templating functionality. This includes Creating default value types schema. Generating upgraded schema files and default payloads. Running test payload injection for Jinja-enabled Excel files. Returns: None \u2013 None Example run_diagnostics() Notes Logs each step of the process to the application logger. Catches and logs any exceptions that occur during testing. Intended for developers to verify schema and workbook generation end-to-end. Source code in arb\\utils\\excel\\xl_create.py 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 def run_diagnostics () -> None : \"\"\" Execute a full suite of diagnostic routines to verify Excel templating functionality. This includes: - Creating default value types schema. - Generating upgraded schema files and default payloads. - Running test payload injection for Jinja-enabled Excel files. Returns: None Example: >>> run_diagnostics() Notes: - Logs each step of the process to the application logger. - Catches and logs any exceptions that occur during testing. - Intended for developers to verify schema and workbook generation end-to-end. \"\"\" logger . info ( \"Running diagnostics...\" ) try : logger . info ( \"Step 1: Creating default type schema\" ) create_default_types_schema ( diagnostics = True ) logger . info ( \"Step 2: Creating and verifying schema files and payloads\" ) prep_xl_templates () create_payloads () logger . info ( \"Step 3: Performing test Excel generation\" ) test_update_xlsx_payloads_01 () logger . info ( \"Diagnostics complete. Check output directory and logs for details.\" ) except Exception as e : logger . exception ( f \"Diagnostics failed: { e } \" ) schema_to_default_dict ( schema_file_name ) Generate default values and metadata from an Excel schema JSON file. Parameters: schema_file_name ( Path ) \u2013 Path to the schema JSON file. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: - defaults: Dictionary mapping variable names to default values. * Drop-downs get \"Please Select\". * All other fields get an empty string. - metadata: Metadata dictionary from the schema file. Example defaults, meta = schema_to_default_dict(Path(\"xl_schemas/landfill_v01_00.json\")) Notes The schema must include an \"is_drop_down\" flag for correct default generation. Useful for pre-populating forms with valid placeholder values. Source code in arb\\utils\\excel\\xl_create.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def schema_to_default_dict ( schema_file_name : Path ) -> tuple [ dict , dict ]: \"\"\" Generate default values and metadata from an Excel schema JSON file. Args: schema_file_name (Path): Path to the schema JSON file. Returns: tuple[dict, dict]: - defaults: Dictionary mapping variable names to default values. * Drop-downs get \"Please Select\". * All other fields get an empty string. - metadata: Metadata dictionary from the schema file. Example: >>> defaults, meta = schema_to_default_dict(Path(\"xl_schemas/landfill_v01_00.json\")) Notes: - The schema must include an \"is_drop_down\" flag for correct default generation. - Useful for pre-populating forms with valid placeholder values. \"\"\" logger . debug ( f \"schema_to_default_dict() called for { schema_file_name =} \" ) data , metadata = json_load_with_meta ( schema_file_name ) logger . debug ( f \" { metadata =} \" ) defaults = { variable : PLEASE_SELECT if sub_schema . get ( \"is_drop_down\" ) else \"\" for variable , sub_schema in data . items () } return defaults , metadata schema_to_default_json ( file_name_in , file_name_out = None ) Save default values extracted from a schema into a JSON file with metadata. Parameters: file_name_in ( Path ) \u2013 Input path to the schema JSON file. file_name_out ( Path , default: None ) \u2013 Output path for the defaults JSON. If None, defaults to 'xl_payloads/{schema_version}_defaults.json'. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: - defaults: Dictionary of default values derived from the schema. - metadata: Metadata dictionary included in the output JSON. Example schema_to_default_json(Path(\"xl_schemas/landfill_v01_00.json\")) Notes Drop-down fields default to \"Please Select\". Other fields default to an empty string. Adds a note to metadata explaining default value behavior. Ensures output directory exists before writing. Source code in arb\\utils\\excel\\xl_create.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def schema_to_default_json ( file_name_in : Path , file_name_out : Path = None ) -> tuple [ dict , dict ]: \"\"\" Save default values extracted from a schema into a JSON file with metadata. Args: file_name_in (Path): Input path to the schema JSON file. file_name_out (Path, optional): Output path for the defaults JSON. If None, defaults to 'xl_payloads/{schema_version}_defaults.json'. Returns: tuple[dict, dict]: - defaults: Dictionary of default values derived from the schema. - metadata: Metadata dictionary included in the output JSON. Example: >>> schema_to_default_json(Path(\"xl_schemas/landfill_v01_00.json\")) Notes: - Drop-down fields default to \"Please Select\". - Other fields default to an empty string. - Adds a note to metadata explaining default value behavior. - Ensures output directory exists before writing. \"\"\" logger . debug ( f \"schema_to_default_json() called for { file_name_in =} \" ) defaults , metadata = schema_to_default_dict ( file_name_in ) metadata [ 'notes' ] = ( \"Default values are empty strings unless the field is a drop-down cell. \" \"For drop-down cells, the default is 'Please Select'.\" ) if file_name_out is None : file_name_out = f \"xl_payloads/ { metadata [ 'schema_version' ] } _defaults.json\" ensure_parent_dirs ( file_name_out ) json_save_with_meta ( file_name_out , data = defaults , metadata = metadata , json_options = None ) return defaults , metadata schema_to_json_file ( data , schema_version , file_name = None ) Save an Excel schema to a JSON file with metadata and validate the round-trip. Parameters: data ( dict ) \u2013 The Excel schema to be serialized and written to disk. schema_version ( str ) \u2013 Schema version identifier to include in metadata. file_name ( str , default: None ) \u2013 Output file path. Defaults to \"xl_schemas/{schema_version}.json\". Returns: None \u2013 None Example schema_to_json_file(my_schema, schema_version=\"v01_00\") Notes If file_name is not provided, the output will be saved to \"xl_schemas/{schema_version}.json\". Metadata will include the schema version. Performs a round-trip serialization test to verify that the saved data and metadata match the originals. Source code in arb\\utils\\excel\\xl_create.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def schema_to_json_file ( data : dict , schema_version : str , file_name : str = None ) -> None : \"\"\" Save an Excel schema to a JSON file with metadata and validate the round-trip. Args: data (dict): The Excel schema to be serialized and written to disk. schema_version (str): Schema version identifier to include in metadata. file_name (str, optional): Output file path. Defaults to \"xl_schemas/{schema_version}.json\". Returns: None Example: >>> schema_to_json_file(my_schema, schema_version=\"v01_00\") Notes: - If `file_name` is not provided, the output will be saved to \"xl_schemas/{schema_version}.json\". - Metadata will include the schema version. - Performs a round-trip serialization test to verify that the saved data and metadata match the originals. \"\"\" logger . debug ( f \"schema_to_json_file() called with { schema_version =} , { file_name =} \" ) if file_name is None : file_name = f \"xl_schemas/ { schema_version } .json\" ensure_parent_dirs ( file_name ) metadata = { 'schema_version' : schema_version } logger . debug ( f \"Saving schema to: { file_name } with metadata: { metadata } \" ) json_save_with_meta ( file_name , data = data , metadata = metadata , json_options = None ) # Verify round-trip serialization read_data , read_metadata = json_load_with_meta ( file_name ) if read_data == data and read_metadata == metadata : logger . debug ( \"SUCCESS: JSON serialization round-trip matches original.\" ) else : logger . warning ( \"FAILURE: Mismatch in JSON serialization round-trip.\" ) sort_xl_schema ( xl_schema , sort_by = 'variable_name' ) Sort an Excel schema and its sub-schema dictionaries for easier comparison. This function modifies sub-schemas in place and returns a new dictionary with the top-level keys sorted according to the selected strategy. Sub-schemas are reordered so that keys appear in the order: \"label\", \"label_address\", \"value_address\", \"value_type\", then others. Parameters: xl_schema ( dict ) \u2013 Dictionary where keys are variable names and values are sub-schema dicts. sort_by ( str , default: 'variable_name' ) \u2013 Sorting strategy: - \"variable_name\": Sort top-level keys alphabetically (default). - \"label_address\": Sort based on Excel row order of each sub-schema's 'label_address'. Returns: dict ( dict ) \u2013 A new dictionary with reordered sub-schemas and sorted top-level keys. Raises: ValueError \u2013 If an unrecognized sorting strategy is provided. Example sorted_schema = sort_xl_schema(schema, sort_by=\"label_address\") Source code in arb\\utils\\excel\\xl_create.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def sort_xl_schema ( xl_schema : dict , sort_by : str = \"variable_name\" ) -> dict : \"\"\" Sort an Excel schema and its sub-schema dictionaries for easier comparison. This function modifies sub-schemas in place and returns a new dictionary with the top-level keys sorted according to the selected strategy. Sub-schemas are reordered so that keys appear in the order: \"label\", \"label_address\", \"value_address\", \"value_type\", then others. Args: xl_schema (dict): Dictionary where keys are variable names and values are sub-schema dicts. sort_by (str): Sorting strategy: - \"variable_name\": Sort top-level keys alphabetically (default). - \"label_address\": Sort based on Excel row order of each sub-schema's 'label_address'. Returns: dict: A new dictionary with reordered sub-schemas and sorted top-level keys. Raises: ValueError: If an unrecognized sorting strategy is provided. Example: >>> sorted_schema = sort_xl_schema(schema, sort_by=\"label_address\") \"\"\" logger . debug ( \"sort_xl_schema() called\" ) # Reorder each sub-schema dict for variable_name , sub_schema in xl_schema . items (): reordered = {} for key in ( \"label\" , \"label_address\" , \"value_address\" , \"value_type\" ): if key in sub_schema : reordered [ key ] = sub_schema . pop ( key ) reordered . update ( sub_schema ) xl_schema [ variable_name ] = reordered # Sort top-level dictionary if sort_by == \"variable_name\" : logger . debug ( \"Sorting schema by variable_name\" ) sorted_items = dict ( sorted ( xl_schema . items (), key = lambda item : item [ 0 ])) elif sort_by == \"label_address\" : logger . debug ( \"Sorting schema by label_address\" ) get_xl_row = partial ( xl_address_sort , address_location = \"value\" , sort_by = \"row\" , sub_keys = \"label_address\" ) sorted_items = dict ( sorted ( xl_schema . items (), key = get_xl_row )) else : raise ValueError ( \"sort_by must be 'variable_name' or 'label_address'\" ) return sorted_items test_update_xlsx_payloads_01 () Run test cases that populate Jinja-templated Excel files with known payloads. This test routine helps validate that Excel generation is functioning correctly for all supported sectors (landfill, oil & gas, energy). Returns: None \u2013 None Example test_update_xlsx_payloads_01() Notes Writes populated Excel files to the xl_workbooks directory. Uses both file-based and inline payloads. Intended for development and diagnostic use, not production. Source code in arb\\utils\\excel\\xl_create.py 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def test_update_xlsx_payloads_01 () -> None : \"\"\" Run test cases that populate Jinja-templated Excel files with known payloads. This test routine helps validate that Excel generation is functioning correctly for all supported sectors (landfill, oil & gas, energy). Returns: None Example: >>> test_update_xlsx_payloads_01() Notes: - Writes populated Excel files to the `xl_workbooks` directory. - Uses both file-based and inline payloads. - Intended for development and diagnostic use, not production. \"\"\" logger . debug ( \"test_update_xlsx_payloads_01() called\" ) # Landfill test with two payloads from file update_xlsx_payloads ( PROCESSED_VERSIONS / f \"xl_workbooks/landfill_operator_feedback_ { LANDFILL_VERSION } _jinja_.xlsx\" , PROCESSED_VERSIONS / f \"xl_workbooks/landfill_operator_feedback_ { LANDFILL_VERSION } _populated_01.xlsx\" , [ PROCESSED_VERSIONS / \"xl_payloads/landfill_v01_00_defaults.json\" , PROCESSED_VERSIONS / \"xl_payloads/landfill_v01_00_payload_01.json\" , ] ) # Landfill test with one file payload and one inline dict update_xlsx_payloads ( PROCESSED_VERSIONS / f \"xl_workbooks/landfill_operator_feedback_ { LANDFILL_VERSION } _jinja_.xlsx\" , PROCESSED_VERSIONS / f \"xl_workbooks/landfill_operator_feedback_ { LANDFILL_VERSION } _populated_02.xlsx\" , [ PROCESSED_VERSIONS / \"xl_payloads/landfill_v01_00_payload_01.json\" , { \"id_incidence\" : \"123456\" }, ] ) # Oil and gas test update_xlsx_payloads ( PROCESSED_VERSIONS / f \"xl_workbooks/oil_and_gas_operator_feedback_ { OIL_AND_GAS_VERSION } _jinja_.xlsx\" , PROCESSED_VERSIONS / f \"xl_workbooks/oil_and_gas_operator_feedback_ { OIL_AND_GAS_VERSION } _populated_01.xlsx\" , [ PROCESSED_VERSIONS / \"xl_payloads/oil_and_gas_v01_00_defaults.json\" , PROCESSED_VERSIONS / \"xl_payloads/oil_and_gas_v01_00_payload_01.json\" , ] ) # Energy test with inline payload update_xlsx_payloads ( PROCESSED_VERSIONS / f \"xl_workbooks/energy_operator_feedback_ { ENERGY_VERSION } _jinja_.xlsx\" , PROCESSED_VERSIONS / f \"xl_workbooks/energy_operator_feedback_ { ENERGY_VERSION } _populated_01.xlsx\" , [ PROCESSED_VERSIONS / \"xl_payloads/energy_v00_01_defaults.json\" , { \"id_incidence\" : \"654321\" }, ] ) update_vba_schema ( schema_version , file_name_in = None , file_name_out = None , file_name_default_value_types = None ) Update a VBA-generated Excel schema with value_type info and re-sort it. Parameters: schema_version ( str ) \u2013 Identifier for the schema version. file_name_in ( Path , default: None ) \u2013 Path to the raw VBA schema JSON file. Defaults to \"processed_versions/xl_schemas/{schema_version}_vba.json\". file_name_out ( Path , default: None ) \u2013 Path to output the upgraded schema JSON. Defaults to \"processed_versions/xl_schemas/{schema_version}.json\". file_name_default_value_types ( Path , default: None ) \u2013 Path to JSON file defining default value types. Defaults to \"processed_versions/xl_schemas/default_value_types_v01_00.json\". Returns: dict ( dict ) \u2013 The updated and sorted schema dictionary. Example updated = update_vba_schema(\"landfill_v01_00\") Notes This function ensures that all schema entries include a 'value_type'. Applies sort_xl_schema() with sort_by=\"label_address\". Uses schema_to_json_file() to write the result to disk. Source code in arb\\utils\\excel\\xl_create.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def update_vba_schema ( schema_version : str , file_name_in : Path = None , file_name_out : Path = None , file_name_default_value_types : Path = None ) -> dict : \"\"\" Update a VBA-generated Excel schema with value_type info and re-sort it. Args: schema_version (str): Identifier for the schema version. file_name_in (Path, optional): Path to the raw VBA schema JSON file. Defaults to \"processed_versions/xl_schemas/{schema_version}_vba.json\". file_name_out (Path, optional): Path to output the upgraded schema JSON. Defaults to \"processed_versions/xl_schemas/{schema_version}.json\". file_name_default_value_types (Path, optional): Path to JSON file defining default value types. Defaults to \"processed_versions/xl_schemas/default_value_types_v01_00.json\". Returns: dict: The updated and sorted schema dictionary. Example: >>> updated = update_vba_schema(\"landfill_v01_00\") Notes: - This function ensures that all schema entries include a 'value_type'. - Applies `sort_xl_schema()` with sort_by=\"label_address\". - Uses `schema_to_json_file()` to write the result to disk. \"\"\" logger . debug ( f \"update_vba_schema() called with { schema_version =} , { file_name_in =} , \" f \" { file_name_out =} , { file_name_default_value_types =} \" ) if file_name_in is None : file_name_in = PROCESSED_VERSIONS / \"xl_schemas\" / f \" { schema_version } _vba.json\" if file_name_out is None : file_name_out = PROCESSED_VERSIONS / \"xl_schemas\" / f \" { schema_version } .json\" if file_name_default_value_types is None : file_name_default_value_types = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00.json\" ensure_parent_dirs ( file_name_in ) ensure_parent_dirs ( file_name_out ) ensure_parent_dirs ( file_name_default_value_types ) default_value_types , _ = json_load_with_meta ( file_name_default_value_types ) schema = json_load ( file_name_in , json_options = None ) ensure_key_value_pair ( schema , default_value_types , \"value_type\" ) schema = sort_xl_schema ( schema , sort_by = \"label_address\" ) schema_to_json_file ( schema , schema_version , file_name = file_name_out ) return schema update_vba_schemas () Batch update of known VBA-generated schemas using update_vba_schema() . This function applies schema upgrades to a predefined list of versions, typically for supported sectors like landfill and oil & gas. Returns: None \u2013 None Notes Automatically processes \"landfill_v01_00\" and \"oil_and_gas_v01_00\". Calls update_vba_schema() for each version. Output schemas are written to the processed_versions/xl_schemas directory. Source code in arb\\utils\\excel\\xl_create.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def update_vba_schemas () -> None : \"\"\" Batch update of known VBA-generated schemas using `update_vba_schema()`. This function applies schema upgrades to a predefined list of versions, typically for supported sectors like landfill and oil & gas. Returns: None Notes: - Automatically processes \"landfill_v01_00\" and \"oil_and_gas_v01_00\". - Calls `update_vba_schema()` for each version. - Output schemas are written to the processed_versions/xl_schemas directory. \"\"\" logger . debug ( \"update_vba_schemas() called\" ) for schema_version in [ \"landfill_v01_00\" , \"oil_and_gas_v01_00\" ]: update_vba_schema ( schema_version ) update_xlsx ( file_in , file_out , jinja_dict ) Render a Jinja-templated Excel (.xlsx) file by replacing placeholders with dictionary values. Parameters: file_in ( Path ) \u2013 Path to the input Excel file containing Jinja placeholders. file_out ( Path ) \u2013 Path where the rendered Excel file will be saved. jinja_dict ( dict ) \u2013 Dictionary mapping Jinja template variables to replacement values. Returns: None \u2013 None Example update_xlsx(Path(\"template.xlsx\"), Path(\"output.xlsx\"), {\"site_name\": \"Landfill A\"}) Notes Only modifies 'xl/sharedStrings.xml' within the XLSX zip archive. All other file contents are passed through unchanged. Useful for populating pre-tagged Excel templates with form data. Source code in arb\\utils\\excel\\xl_create.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 def update_xlsx ( file_in : Path , file_out : Path , jinja_dict : dict ) -> None : \"\"\" Render a Jinja-templated Excel (.xlsx) file by replacing placeholders with dictionary values. Args: file_in (Path): Path to the input Excel file containing Jinja placeholders. file_out (Path): Path where the rendered Excel file will be saved. jinja_dict (dict): Dictionary mapping Jinja template variables to replacement values. Returns: None Example: >>> update_xlsx(Path(\"template.xlsx\"), Path(\"output.xlsx\"), {\"site_name\": \"Landfill A\"}) Notes: - Only modifies 'xl/sharedStrings.xml' within the XLSX zip archive. - All other file contents are passed through unchanged. - Useful for populating pre-tagged Excel templates with form data. \"\"\" logger . debug ( f \"Rendering Excel from { file_in } to { file_out } using { jinja_dict } \" ) with zipfile . ZipFile ( file_in , 'r' ) as xlsx , zipfile . ZipFile ( file_out , 'w' ) as new_xlsx : for filename in xlsx . namelist (): with xlsx . open ( filename ) as file : contents = file . read () if filename == 'xl/sharedStrings.xml' : contents = jinja2 . Template ( contents . decode ( 'utf-8' )) . render ( jinja_dict ) . encode ( 'utf-8' ) new_xlsx . writestr ( filename , contents ) update_xlsx_payloads ( file_in , file_out , payloads ) Apply multiple payloads to a Jinja-templated Excel file and render the result. Parameters: file_in ( Path ) \u2013 Path to the input Excel file containing Jinja placeholders. file_out ( Path ) \u2013 Path where the populated Excel file will be written. payloads ( list | tuple ) \u2013 Sequence of dictionaries or JSON file paths. - Payloads are merged in order, with later values overriding earlier ones. Returns: None \u2013 None Example update_xlsx_payloads( ... Path(\"template.xlsx\"), ... Path(\"output.xlsx\"), ... [Path(\"defaults.json\"), {\"id_case\": \"X42\"}] ... ) Notes Each payload may be a dictionary or a Path to a JSON file with metadata. Designed to support tiered rendering: default payload + override. Uses json_load_with_meta() for file payloads. Passes final merged dictionary to update_xlsx() . Source code in arb\\utils\\excel\\xl_create.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def update_xlsx_payloads ( file_in : Path , file_out : Path , payloads : list | tuple ) -> None : \"\"\" Apply multiple payloads to a Jinja-templated Excel file and render the result. Args: file_in (Path): Path to the input Excel file containing Jinja placeholders. file_out (Path): Path where the populated Excel file will be written. payloads (list | tuple): Sequence of dictionaries or JSON file paths. - Payloads are merged in order, with later values overriding earlier ones. Returns: None Example: >>> update_xlsx_payloads( ... Path(\"template.xlsx\"), ... Path(\"output.xlsx\"), ... [Path(\"defaults.json\"), {\"id_case\": \"X42\"}] ... ) Notes: - Each payload may be a dictionary or a Path to a JSON file with metadata. - Designed to support tiered rendering: default payload + override. - Uses `json_load_with_meta()` for file payloads. - Passes final merged dictionary to `update_xlsx()`. \"\"\" logger . debug ( f \"update_xlsx_payloads() called with: { file_in =} , { file_out =} , { payloads =} \" ) new_dict = {} for payload in payloads : if isinstance ( payload , dict ): data = payload else : data , _ = json_load_with_meta ( payload ) new_dict . update ( data ) update_xlsx ( file_in , file_out , new_dict )","title":"arb.utils.excel.xl_create"},{"location":"reference/arb/utils/excel/xl_create/#arbutilsexcelxl_create","text":"Module to prepare Excel templates and generate new Excel files using Jinja-rendered payloads. This module performs schema-based templating of Excel spreadsheets for feedback forms, injects metadata, applies default values, and renders Excel files based on structured JSON payloads. Typical Workflow Use a spreadsheet with named ranges and Jinja placeholders. Generate an initial JSON schema using a VBA macro. Refine the schema by injecting typing information. Generate default and test payloads. Populate Excel files using these payloads. Run this file directly to create all schema and payload artifacts for landfill, oil and gas, and energy templates.","title":"arb.utils.excel.xl_create"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.create_default_types_schema","text":"Create a JSON file that maps variable names to their default Python value types. Parameters: diagnostics ( bool , default: False ) \u2013 If True, logs each variable name and its type to the debug logger. Returns: dict ( dict ) \u2013 Dictionary mapping variable names to Python types (e.g., str, int, datetime). Example types = create_default_types_schema(diagnostics=True) Notes Output is saved to 'xl_schemas/default_value_types_v01_00.json'. A backup is compared against the newly generated file if present. Field names and types are sourced from xl_hardcoded.default_value_types_v01_00 . Source code in arb\\utils\\excel\\xl_create.py 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 def create_default_types_schema ( diagnostics : bool = False ) -> dict : \"\"\" Create a JSON file that maps variable names to their default Python value types. Args: diagnostics (bool): If True, logs each variable name and its type to the debug logger. Returns: dict: Dictionary mapping variable names to Python types (e.g., str, int, datetime). Example: >>> types = create_default_types_schema(diagnostics=True) Notes: - Output is saved to 'xl_schemas/default_value_types_v01_00.json'. - A backup is compared against the newly generated file if present. - Field names and types are sourced from `xl_hardcoded.default_value_types_v01_00`. \"\"\" from arb.utils.excel.xl_hardcoded import default_value_types_v01_00 logger . debug ( \"create_default_types_schema() called\" ) file_name = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00.json\" file_backup = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00_backup.json\" field_types = dict ( sorted ( default_value_types_v01_00 . items ())) if diagnostics : for name , typ in field_types . items (): logger . debug ( f \"' { name } ': { typ . __name__ } ,\" ) metadata = { \"schema_version\" : \"default_value_types_v01_00\" } json_save_with_meta ( file_name , field_types , metadata = metadata ) if file_name . is_file () and file_backup . is_file (): compare_json_files ( file_name , file_backup ) return field_types","title":"create_default_types_schema"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.create_payload","text":"Create a JSON payload file with embedded metadata describing the schema version. Parameters: payload ( dict ) \u2013 Dictionary of values to serialize to JSON. file_name ( Path ) \u2013 Path to output the payload JSON file. schema_version ( str ) \u2013 Identifier for the schema the payload conforms to. metadata ( dict , default: None ) \u2013 Additional metadata to embed. If None, a new dict is created. Returns: None \u2013 None Example create_payload({\"id_case\": \"A42\"}, Path(\"payload.json\"), \"v01_00\") Notes Adds 'schema_version' and a default payload description to metadata. Uses json_save_with_meta() to embed metadata into the JSON file. Logs all key actions and file paths for diagnostics. Source code in arb\\utils\\excel\\xl_create.py 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 def create_payload ( payload : dict , file_name : Path , schema_version : str , metadata : dict = None ) -> None : \"\"\" Create a JSON payload file with embedded metadata describing the schema version. Args: payload (dict): Dictionary of values to serialize to JSON. file_name (Path): Path to output the payload JSON file. schema_version (str): Identifier for the schema the payload conforms to. metadata (dict, optional): Additional metadata to embed. If None, a new dict is created. Returns: None Example: >>> create_payload({\"id_case\": \"A42\"}, Path(\"payload.json\"), \"v01_00\") Notes: - Adds 'schema_version' and a default payload description to metadata. - Uses `json_save_with_meta()` to embed metadata into the JSON file. - Logs all key actions and file paths for diagnostics. \"\"\" logger . debug ( \"create_payload() called\" ) if metadata is None : metadata = {} metadata [ \"schema_version\" ] = schema_version metadata [ \"payload description\" ] = \"Test of Excel jinja templating system\" logger . debug ( f \"Writing payload to { file_name } with metadata: { metadata } \" ) json_save_with_meta ( file_name , data = payload , metadata = metadata )","title":"create_payload"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.create_payloads","text":"Generate and save example payload files for each supported sector (landfill, oil & gas, energy). Returns: None \u2013 None Example create_payloads() Notes Each payload is saved to xl_payloads/{schema_version}_payload_01.json . If a backup file exists, the new payload is compared against it for consistency. The energy payload reuses the oil & gas example data for demonstration purposes. Uses create_payload() to handle serialization and metadata embedding. Source code in arb\\utils\\excel\\xl_create.py 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 def create_payloads () -> None : \"\"\" Generate and save example payload files for each supported sector (landfill, oil & gas, energy). Returns: None Example: >>> create_payloads() Notes: - Each payload is saved to `xl_payloads/{schema_version}_payload_01.json`. - If a backup file exists, the new payload is compared against it for consistency. - The energy payload reuses the oil & gas example data for demonstration purposes. - Uses `create_payload()` to handle serialization and metadata embedding. \"\"\" logger . debug ( \"create_payloads() called\" ) from arb.utils.excel.xl_hardcoded import landfill_payload_01 , oil_and_gas_payload_01 test_sets = [ ( \"landfill_v01_00\" , landfill_payload_01 ), ( \"oil_and_gas_v01_00\" , oil_and_gas_payload_01 ), ( \"energy_v00_01\" , oil_and_gas_payload_01 ), # Reuse oil & gas payload ] for schema_version , payload in test_sets : file_name = PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _payload_01.json\" file_backup = PROCESSED_VERSIONS / f \"xl_payloads/ { schema_version } _payload_01_backup.json\" create_payload ( payload , file_name , schema_version ) if file_name . is_file () and file_backup . is_file (): compare_json_files ( file_name , file_backup )","title":"create_payloads"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.create_schemas_and_payloads","text":"Generate all schema, payload, and Excel artifacts for the feedback system. This orchestration function performs the full pipeline Creates the default value types schema. Processes all sector-specific schema files (landfill, oil & gas, energy). Writes default payloads for each schema. Generates test payloads and renders Excel templates. Returns: None \u2013 None Example create_schemas_and_payloads() Notes Creates all required directories under processed_versions . Intended for one-time use during development or deployment setup. Logs each operation and file path for debugging. Source code in arb\\utils\\excel\\xl_create.py 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 def create_schemas_and_payloads () -> None : \"\"\" Generate all schema, payload, and Excel artifacts for the feedback system. This orchestration function performs the full pipeline: - Creates the default value types schema. - Processes all sector-specific schema files (landfill, oil & gas, energy). - Writes default payloads for each schema. - Generates test payloads and renders Excel templates. Returns: None Example: >>> create_schemas_and_payloads() Notes: - Creates all required directories under `processed_versions`. - Intended for one-time use during development or deployment setup. - Logs each operation and file path for debugging. \"\"\" logger . debug ( \"create_schemas_and_payloads() called\" ) ensure_dir_exists ( PROCESSED_VERSIONS / \"xl_schemas\" ) ensure_dir_exists ( PROCESSED_VERSIONS / \"xl_workbooks\" ) ensure_dir_exists ( PROCESSED_VERSIONS / \"xl_payloads\" ) create_default_types_schema ( diagnostics = True ) prep_xl_templates () create_payloads () test_update_xlsx_payloads_01 ()","title":"create_schemas_and_payloads"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.prep_xl_templates","text":"Prepare processed Excel templates and payloads for landfill, oil & gas, and energy sectors. This function Copies original schema and Excel files to the processed directory. Converts VBA-generated schema files by injecting type info. Writes upgraded schema and default payload JSON files. Produces Jinja-compatible Excel workbook versions for templating. Returns: None \u2013 None Example prep_xl_templates() Notes File paths are derived from structured configs for each sector. Overwrites files in the output directory if they already exist. Output directories are created if they don't exist. Source code in arb\\utils\\excel\\xl_create.py 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 def prep_xl_templates () -> None : \"\"\" Prepare processed Excel templates and payloads for landfill, oil & gas, and energy sectors. This function: - Copies original schema and Excel files to the processed directory. - Converts VBA-generated schema files by injecting type info. - Writes upgraded schema and default payload JSON files. - Produces Jinja-compatible Excel workbook versions for templating. Returns: None Example: >>> prep_xl_templates() Notes: - File paths are derived from structured configs for each sector. - Overwrites files in the output directory if they already exist. - Output directories are created if they don't exist. \"\"\" logger . debug ( \"prep_xl_templates() called for landfill, oil & gas, and energy schemas\" ) file_specs = [] input_dir = PROJECT_ROOT / \"feedback_forms/current_versions\" output_dir = PROJECT_ROOT / \"feedback_forms/processed_versions\" ensure_dir_exists ( output_dir / \"xl_schemas\" ) ensure_dir_exists ( output_dir / \"xl_workbooks\" ) ensure_dir_exists ( output_dir / \"xl_payloads\" ) template_configs = [ ( \"landfill_v01_00\" , \"landfill_operator_feedback\" , LANDFILL_VERSION ), ( \"oil_and_gas_v01_00\" , \"oil_and_gas_operator_feedback\" , OIL_AND_GAS_VERSION ), ( \"energy_v00_01\" , \"energy_operator_feedback\" , ENERGY_VERSION ), ] for schema_version , prefix , version in template_configs : spec = { \"schema_version\" : schema_version , \"input_schema_vba_path\" : input_dir / f \" { schema_version } _vba.json\" , \"input_xl_path\" : input_dir / f \" { prefix } _ { version } .xlsx\" , \"input_xl_jinja_path\" : input_dir / f \" { prefix } _ { version } _jinja_.xlsx\" , \"output_schema_vba_path\" : output_dir / \"xl_schemas\" / f \" { schema_version } _vba.json\" , \"output_schema_path\" : output_dir / \"xl_schemas\" / f \" { schema_version } .json\" , \"output_xl_path\" : output_dir / \"xl_workbooks\" / f \" { prefix } _ { version } .xlsx\" , \"output_xl_jinja_path\" : output_dir / \"xl_workbooks\" / f \" { prefix } _ { version } _jinja_.xlsx\" , \"output_payload_path\" : output_dir / \"xl_payloads\" / f \" { schema_version } _defaults.json\" , } file_specs . append ( spec ) for spec in file_specs : logger . debug ( f \"Processing schema_version { spec [ 'schema_version' ] } \" ) file_map = [( spec [ \"input_schema_vba_path\" ], spec [ \"output_schema_vba_path\" ]), ( spec [ \"input_xl_path\" ], spec [ \"output_xl_path\" ]), ( spec [ \"input_xl_jinja_path\" ], spec [ \"output_xl_jinja_path\" ]), ] for file_old , file_new in file_map : logger . debug ( f \"Copying file from: { file_old } to: { file_new } \" ) shutil . copy ( file_old , file_new ) update_vba_schema ( spec [ \"schema_version\" ], file_name_in = spec [ \"output_schema_vba_path\" ], file_name_out = spec [ \"output_schema_path\" ]) schema_to_default_json ( file_name_in = spec [ \"output_schema_path\" ], file_name_out = spec [ \"output_payload_path\" ])","title":"prep_xl_templates"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.run_diagnostics","text":"Execute a full suite of diagnostic routines to verify Excel templating functionality. This includes Creating default value types schema. Generating upgraded schema files and default payloads. Running test payload injection for Jinja-enabled Excel files. Returns: None \u2013 None Example run_diagnostics() Notes Logs each step of the process to the application logger. Catches and logs any exceptions that occur during testing. Intended for developers to verify schema and workbook generation end-to-end. Source code in arb\\utils\\excel\\xl_create.py 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 def run_diagnostics () -> None : \"\"\" Execute a full suite of diagnostic routines to verify Excel templating functionality. This includes: - Creating default value types schema. - Generating upgraded schema files and default payloads. - Running test payload injection for Jinja-enabled Excel files. Returns: None Example: >>> run_diagnostics() Notes: - Logs each step of the process to the application logger. - Catches and logs any exceptions that occur during testing. - Intended for developers to verify schema and workbook generation end-to-end. \"\"\" logger . info ( \"Running diagnostics...\" ) try : logger . info ( \"Step 1: Creating default type schema\" ) create_default_types_schema ( diagnostics = True ) logger . info ( \"Step 2: Creating and verifying schema files and payloads\" ) prep_xl_templates () create_payloads () logger . info ( \"Step 3: Performing test Excel generation\" ) test_update_xlsx_payloads_01 () logger . info ( \"Diagnostics complete. Check output directory and logs for details.\" ) except Exception as e : logger . exception ( f \"Diagnostics failed: { e } \" )","title":"run_diagnostics"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.schema_to_default_dict","text":"Generate default values and metadata from an Excel schema JSON file. Parameters: schema_file_name ( Path ) \u2013 Path to the schema JSON file. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: - defaults: Dictionary mapping variable names to default values. * Drop-downs get \"Please Select\". * All other fields get an empty string. - metadata: Metadata dictionary from the schema file. Example defaults, meta = schema_to_default_dict(Path(\"xl_schemas/landfill_v01_00.json\")) Notes The schema must include an \"is_drop_down\" flag for correct default generation. Useful for pre-populating forms with valid placeholder values. Source code in arb\\utils\\excel\\xl_create.py 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def schema_to_default_dict ( schema_file_name : Path ) -> tuple [ dict , dict ]: \"\"\" Generate default values and metadata from an Excel schema JSON file. Args: schema_file_name (Path): Path to the schema JSON file. Returns: tuple[dict, dict]: - defaults: Dictionary mapping variable names to default values. * Drop-downs get \"Please Select\". * All other fields get an empty string. - metadata: Metadata dictionary from the schema file. Example: >>> defaults, meta = schema_to_default_dict(Path(\"xl_schemas/landfill_v01_00.json\")) Notes: - The schema must include an \"is_drop_down\" flag for correct default generation. - Useful for pre-populating forms with valid placeholder values. \"\"\" logger . debug ( f \"schema_to_default_dict() called for { schema_file_name =} \" ) data , metadata = json_load_with_meta ( schema_file_name ) logger . debug ( f \" { metadata =} \" ) defaults = { variable : PLEASE_SELECT if sub_schema . get ( \"is_drop_down\" ) else \"\" for variable , sub_schema in data . items () } return defaults , metadata","title":"schema_to_default_dict"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.schema_to_default_json","text":"Save default values extracted from a schema into a JSON file with metadata. Parameters: file_name_in ( Path ) \u2013 Input path to the schema JSON file. file_name_out ( Path , default: None ) \u2013 Output path for the defaults JSON. If None, defaults to 'xl_payloads/{schema_version}_defaults.json'. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: - defaults: Dictionary of default values derived from the schema. - metadata: Metadata dictionary included in the output JSON. Example schema_to_default_json(Path(\"xl_schemas/landfill_v01_00.json\")) Notes Drop-down fields default to \"Please Select\". Other fields default to an empty string. Adds a note to metadata explaining default value behavior. Ensures output directory exists before writing. Source code in arb\\utils\\excel\\xl_create.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def schema_to_default_json ( file_name_in : Path , file_name_out : Path = None ) -> tuple [ dict , dict ]: \"\"\" Save default values extracted from a schema into a JSON file with metadata. Args: file_name_in (Path): Input path to the schema JSON file. file_name_out (Path, optional): Output path for the defaults JSON. If None, defaults to 'xl_payloads/{schema_version}_defaults.json'. Returns: tuple[dict, dict]: - defaults: Dictionary of default values derived from the schema. - metadata: Metadata dictionary included in the output JSON. Example: >>> schema_to_default_json(Path(\"xl_schemas/landfill_v01_00.json\")) Notes: - Drop-down fields default to \"Please Select\". - Other fields default to an empty string. - Adds a note to metadata explaining default value behavior. - Ensures output directory exists before writing. \"\"\" logger . debug ( f \"schema_to_default_json() called for { file_name_in =} \" ) defaults , metadata = schema_to_default_dict ( file_name_in ) metadata [ 'notes' ] = ( \"Default values are empty strings unless the field is a drop-down cell. \" \"For drop-down cells, the default is 'Please Select'.\" ) if file_name_out is None : file_name_out = f \"xl_payloads/ { metadata [ 'schema_version' ] } _defaults.json\" ensure_parent_dirs ( file_name_out ) json_save_with_meta ( file_name_out , data = defaults , metadata = metadata , json_options = None ) return defaults , metadata","title":"schema_to_default_json"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.schema_to_json_file","text":"Save an Excel schema to a JSON file with metadata and validate the round-trip. Parameters: data ( dict ) \u2013 The Excel schema to be serialized and written to disk. schema_version ( str ) \u2013 Schema version identifier to include in metadata. file_name ( str , default: None ) \u2013 Output file path. Defaults to \"xl_schemas/{schema_version}.json\". Returns: None \u2013 None Example schema_to_json_file(my_schema, schema_version=\"v01_00\") Notes If file_name is not provided, the output will be saved to \"xl_schemas/{schema_version}.json\". Metadata will include the schema version. Performs a round-trip serialization test to verify that the saved data and metadata match the originals. Source code in arb\\utils\\excel\\xl_create.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def schema_to_json_file ( data : dict , schema_version : str , file_name : str = None ) -> None : \"\"\" Save an Excel schema to a JSON file with metadata and validate the round-trip. Args: data (dict): The Excel schema to be serialized and written to disk. schema_version (str): Schema version identifier to include in metadata. file_name (str, optional): Output file path. Defaults to \"xl_schemas/{schema_version}.json\". Returns: None Example: >>> schema_to_json_file(my_schema, schema_version=\"v01_00\") Notes: - If `file_name` is not provided, the output will be saved to \"xl_schemas/{schema_version}.json\". - Metadata will include the schema version. - Performs a round-trip serialization test to verify that the saved data and metadata match the originals. \"\"\" logger . debug ( f \"schema_to_json_file() called with { schema_version =} , { file_name =} \" ) if file_name is None : file_name = f \"xl_schemas/ { schema_version } .json\" ensure_parent_dirs ( file_name ) metadata = { 'schema_version' : schema_version } logger . debug ( f \"Saving schema to: { file_name } with metadata: { metadata } \" ) json_save_with_meta ( file_name , data = data , metadata = metadata , json_options = None ) # Verify round-trip serialization read_data , read_metadata = json_load_with_meta ( file_name ) if read_data == data and read_metadata == metadata : logger . debug ( \"SUCCESS: JSON serialization round-trip matches original.\" ) else : logger . warning ( \"FAILURE: Mismatch in JSON serialization round-trip.\" )","title":"schema_to_json_file"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.sort_xl_schema","text":"Sort an Excel schema and its sub-schema dictionaries for easier comparison. This function modifies sub-schemas in place and returns a new dictionary with the top-level keys sorted according to the selected strategy. Sub-schemas are reordered so that keys appear in the order: \"label\", \"label_address\", \"value_address\", \"value_type\", then others. Parameters: xl_schema ( dict ) \u2013 Dictionary where keys are variable names and values are sub-schema dicts. sort_by ( str , default: 'variable_name' ) \u2013 Sorting strategy: - \"variable_name\": Sort top-level keys alphabetically (default). - \"label_address\": Sort based on Excel row order of each sub-schema's 'label_address'. Returns: dict ( dict ) \u2013 A new dictionary with reordered sub-schemas and sorted top-level keys. Raises: ValueError \u2013 If an unrecognized sorting strategy is provided. Example sorted_schema = sort_xl_schema(schema, sort_by=\"label_address\") Source code in arb\\utils\\excel\\xl_create.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def sort_xl_schema ( xl_schema : dict , sort_by : str = \"variable_name\" ) -> dict : \"\"\" Sort an Excel schema and its sub-schema dictionaries for easier comparison. This function modifies sub-schemas in place and returns a new dictionary with the top-level keys sorted according to the selected strategy. Sub-schemas are reordered so that keys appear in the order: \"label\", \"label_address\", \"value_address\", \"value_type\", then others. Args: xl_schema (dict): Dictionary where keys are variable names and values are sub-schema dicts. sort_by (str): Sorting strategy: - \"variable_name\": Sort top-level keys alphabetically (default). - \"label_address\": Sort based on Excel row order of each sub-schema's 'label_address'. Returns: dict: A new dictionary with reordered sub-schemas and sorted top-level keys. Raises: ValueError: If an unrecognized sorting strategy is provided. Example: >>> sorted_schema = sort_xl_schema(schema, sort_by=\"label_address\") \"\"\" logger . debug ( \"sort_xl_schema() called\" ) # Reorder each sub-schema dict for variable_name , sub_schema in xl_schema . items (): reordered = {} for key in ( \"label\" , \"label_address\" , \"value_address\" , \"value_type\" ): if key in sub_schema : reordered [ key ] = sub_schema . pop ( key ) reordered . update ( sub_schema ) xl_schema [ variable_name ] = reordered # Sort top-level dictionary if sort_by == \"variable_name\" : logger . debug ( \"Sorting schema by variable_name\" ) sorted_items = dict ( sorted ( xl_schema . items (), key = lambda item : item [ 0 ])) elif sort_by == \"label_address\" : logger . debug ( \"Sorting schema by label_address\" ) get_xl_row = partial ( xl_address_sort , address_location = \"value\" , sort_by = \"row\" , sub_keys = \"label_address\" ) sorted_items = dict ( sorted ( xl_schema . items (), key = get_xl_row )) else : raise ValueError ( \"sort_by must be 'variable_name' or 'label_address'\" ) return sorted_items","title":"sort_xl_schema"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.test_update_xlsx_payloads_01","text":"Run test cases that populate Jinja-templated Excel files with known payloads. This test routine helps validate that Excel generation is functioning correctly for all supported sectors (landfill, oil & gas, energy). Returns: None \u2013 None Example test_update_xlsx_payloads_01() Notes Writes populated Excel files to the xl_workbooks directory. Uses both file-based and inline payloads. Intended for development and diagnostic use, not production. Source code in arb\\utils\\excel\\xl_create.py 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 def test_update_xlsx_payloads_01 () -> None : \"\"\" Run test cases that populate Jinja-templated Excel files with known payloads. This test routine helps validate that Excel generation is functioning correctly for all supported sectors (landfill, oil & gas, energy). Returns: None Example: >>> test_update_xlsx_payloads_01() Notes: - Writes populated Excel files to the `xl_workbooks` directory. - Uses both file-based and inline payloads. - Intended for development and diagnostic use, not production. \"\"\" logger . debug ( \"test_update_xlsx_payloads_01() called\" ) # Landfill test with two payloads from file update_xlsx_payloads ( PROCESSED_VERSIONS / f \"xl_workbooks/landfill_operator_feedback_ { LANDFILL_VERSION } _jinja_.xlsx\" , PROCESSED_VERSIONS / f \"xl_workbooks/landfill_operator_feedback_ { LANDFILL_VERSION } _populated_01.xlsx\" , [ PROCESSED_VERSIONS / \"xl_payloads/landfill_v01_00_defaults.json\" , PROCESSED_VERSIONS / \"xl_payloads/landfill_v01_00_payload_01.json\" , ] ) # Landfill test with one file payload and one inline dict update_xlsx_payloads ( PROCESSED_VERSIONS / f \"xl_workbooks/landfill_operator_feedback_ { LANDFILL_VERSION } _jinja_.xlsx\" , PROCESSED_VERSIONS / f \"xl_workbooks/landfill_operator_feedback_ { LANDFILL_VERSION } _populated_02.xlsx\" , [ PROCESSED_VERSIONS / \"xl_payloads/landfill_v01_00_payload_01.json\" , { \"id_incidence\" : \"123456\" }, ] ) # Oil and gas test update_xlsx_payloads ( PROCESSED_VERSIONS / f \"xl_workbooks/oil_and_gas_operator_feedback_ { OIL_AND_GAS_VERSION } _jinja_.xlsx\" , PROCESSED_VERSIONS / f \"xl_workbooks/oil_and_gas_operator_feedback_ { OIL_AND_GAS_VERSION } _populated_01.xlsx\" , [ PROCESSED_VERSIONS / \"xl_payloads/oil_and_gas_v01_00_defaults.json\" , PROCESSED_VERSIONS / \"xl_payloads/oil_and_gas_v01_00_payload_01.json\" , ] ) # Energy test with inline payload update_xlsx_payloads ( PROCESSED_VERSIONS / f \"xl_workbooks/energy_operator_feedback_ { ENERGY_VERSION } _jinja_.xlsx\" , PROCESSED_VERSIONS / f \"xl_workbooks/energy_operator_feedback_ { ENERGY_VERSION } _populated_01.xlsx\" , [ PROCESSED_VERSIONS / \"xl_payloads/energy_v00_01_defaults.json\" , { \"id_incidence\" : \"654321\" }, ] )","title":"test_update_xlsx_payloads_01"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.update_vba_schema","text":"Update a VBA-generated Excel schema with value_type info and re-sort it. Parameters: schema_version ( str ) \u2013 Identifier for the schema version. file_name_in ( Path , default: None ) \u2013 Path to the raw VBA schema JSON file. Defaults to \"processed_versions/xl_schemas/{schema_version}_vba.json\". file_name_out ( Path , default: None ) \u2013 Path to output the upgraded schema JSON. Defaults to \"processed_versions/xl_schemas/{schema_version}.json\". file_name_default_value_types ( Path , default: None ) \u2013 Path to JSON file defining default value types. Defaults to \"processed_versions/xl_schemas/default_value_types_v01_00.json\". Returns: dict ( dict ) \u2013 The updated and sorted schema dictionary. Example updated = update_vba_schema(\"landfill_v01_00\") Notes This function ensures that all schema entries include a 'value_type'. Applies sort_xl_schema() with sort_by=\"label_address\". Uses schema_to_json_file() to write the result to disk. Source code in arb\\utils\\excel\\xl_create.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def update_vba_schema ( schema_version : str , file_name_in : Path = None , file_name_out : Path = None , file_name_default_value_types : Path = None ) -> dict : \"\"\" Update a VBA-generated Excel schema with value_type info and re-sort it. Args: schema_version (str): Identifier for the schema version. file_name_in (Path, optional): Path to the raw VBA schema JSON file. Defaults to \"processed_versions/xl_schemas/{schema_version}_vba.json\". file_name_out (Path, optional): Path to output the upgraded schema JSON. Defaults to \"processed_versions/xl_schemas/{schema_version}.json\". file_name_default_value_types (Path, optional): Path to JSON file defining default value types. Defaults to \"processed_versions/xl_schemas/default_value_types_v01_00.json\". Returns: dict: The updated and sorted schema dictionary. Example: >>> updated = update_vba_schema(\"landfill_v01_00\") Notes: - This function ensures that all schema entries include a 'value_type'. - Applies `sort_xl_schema()` with sort_by=\"label_address\". - Uses `schema_to_json_file()` to write the result to disk. \"\"\" logger . debug ( f \"update_vba_schema() called with { schema_version =} , { file_name_in =} , \" f \" { file_name_out =} , { file_name_default_value_types =} \" ) if file_name_in is None : file_name_in = PROCESSED_VERSIONS / \"xl_schemas\" / f \" { schema_version } _vba.json\" if file_name_out is None : file_name_out = PROCESSED_VERSIONS / \"xl_schemas\" / f \" { schema_version } .json\" if file_name_default_value_types is None : file_name_default_value_types = PROCESSED_VERSIONS / \"xl_schemas/default_value_types_v01_00.json\" ensure_parent_dirs ( file_name_in ) ensure_parent_dirs ( file_name_out ) ensure_parent_dirs ( file_name_default_value_types ) default_value_types , _ = json_load_with_meta ( file_name_default_value_types ) schema = json_load ( file_name_in , json_options = None ) ensure_key_value_pair ( schema , default_value_types , \"value_type\" ) schema = sort_xl_schema ( schema , sort_by = \"label_address\" ) schema_to_json_file ( schema , schema_version , file_name = file_name_out ) return schema","title":"update_vba_schema"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.update_vba_schemas","text":"Batch update of known VBA-generated schemas using update_vba_schema() . This function applies schema upgrades to a predefined list of versions, typically for supported sectors like landfill and oil & gas. Returns: None \u2013 None Notes Automatically processes \"landfill_v01_00\" and \"oil_and_gas_v01_00\". Calls update_vba_schema() for each version. Output schemas are written to the processed_versions/xl_schemas directory. Source code in arb\\utils\\excel\\xl_create.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def update_vba_schemas () -> None : \"\"\" Batch update of known VBA-generated schemas using `update_vba_schema()`. This function applies schema upgrades to a predefined list of versions, typically for supported sectors like landfill and oil & gas. Returns: None Notes: - Automatically processes \"landfill_v01_00\" and \"oil_and_gas_v01_00\". - Calls `update_vba_schema()` for each version. - Output schemas are written to the processed_versions/xl_schemas directory. \"\"\" logger . debug ( \"update_vba_schemas() called\" ) for schema_version in [ \"landfill_v01_00\" , \"oil_and_gas_v01_00\" ]: update_vba_schema ( schema_version )","title":"update_vba_schemas"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.update_xlsx","text":"Render a Jinja-templated Excel (.xlsx) file by replacing placeholders with dictionary values. Parameters: file_in ( Path ) \u2013 Path to the input Excel file containing Jinja placeholders. file_out ( Path ) \u2013 Path where the rendered Excel file will be saved. jinja_dict ( dict ) \u2013 Dictionary mapping Jinja template variables to replacement values. Returns: None \u2013 None Example update_xlsx(Path(\"template.xlsx\"), Path(\"output.xlsx\"), {\"site_name\": \"Landfill A\"}) Notes Only modifies 'xl/sharedStrings.xml' within the XLSX zip archive. All other file contents are passed through unchanged. Useful for populating pre-tagged Excel templates with form data. Source code in arb\\utils\\excel\\xl_create.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 def update_xlsx ( file_in : Path , file_out : Path , jinja_dict : dict ) -> None : \"\"\" Render a Jinja-templated Excel (.xlsx) file by replacing placeholders with dictionary values. Args: file_in (Path): Path to the input Excel file containing Jinja placeholders. file_out (Path): Path where the rendered Excel file will be saved. jinja_dict (dict): Dictionary mapping Jinja template variables to replacement values. Returns: None Example: >>> update_xlsx(Path(\"template.xlsx\"), Path(\"output.xlsx\"), {\"site_name\": \"Landfill A\"}) Notes: - Only modifies 'xl/sharedStrings.xml' within the XLSX zip archive. - All other file contents are passed through unchanged. - Useful for populating pre-tagged Excel templates with form data. \"\"\" logger . debug ( f \"Rendering Excel from { file_in } to { file_out } using { jinja_dict } \" ) with zipfile . ZipFile ( file_in , 'r' ) as xlsx , zipfile . ZipFile ( file_out , 'w' ) as new_xlsx : for filename in xlsx . namelist (): with xlsx . open ( filename ) as file : contents = file . read () if filename == 'xl/sharedStrings.xml' : contents = jinja2 . Template ( contents . decode ( 'utf-8' )) . render ( jinja_dict ) . encode ( 'utf-8' ) new_xlsx . writestr ( filename , contents )","title":"update_xlsx"},{"location":"reference/arb/utils/excel/xl_create/#arb.utils.excel.xl_create.update_xlsx_payloads","text":"Apply multiple payloads to a Jinja-templated Excel file and render the result. Parameters: file_in ( Path ) \u2013 Path to the input Excel file containing Jinja placeholders. file_out ( Path ) \u2013 Path where the populated Excel file will be written. payloads ( list | tuple ) \u2013 Sequence of dictionaries or JSON file paths. - Payloads are merged in order, with later values overriding earlier ones. Returns: None \u2013 None Example update_xlsx_payloads( ... Path(\"template.xlsx\"), ... Path(\"output.xlsx\"), ... [Path(\"defaults.json\"), {\"id_case\": \"X42\"}] ... ) Notes Each payload may be a dictionary or a Path to a JSON file with metadata. Designed to support tiered rendering: default payload + override. Uses json_load_with_meta() for file payloads. Passes final merged dictionary to update_xlsx() . Source code in arb\\utils\\excel\\xl_create.py 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 def update_xlsx_payloads ( file_in : Path , file_out : Path , payloads : list | tuple ) -> None : \"\"\" Apply multiple payloads to a Jinja-templated Excel file and render the result. Args: file_in (Path): Path to the input Excel file containing Jinja placeholders. file_out (Path): Path where the populated Excel file will be written. payloads (list | tuple): Sequence of dictionaries or JSON file paths. - Payloads are merged in order, with later values overriding earlier ones. Returns: None Example: >>> update_xlsx_payloads( ... Path(\"template.xlsx\"), ... Path(\"output.xlsx\"), ... [Path(\"defaults.json\"), {\"id_case\": \"X42\"}] ... ) Notes: - Each payload may be a dictionary or a Path to a JSON file with metadata. - Designed to support tiered rendering: default payload + override. - Uses `json_load_with_meta()` for file payloads. - Passes final merged dictionary to `update_xlsx()`. \"\"\" logger . debug ( f \"update_xlsx_payloads() called with: { file_in =} , { file_out =} , { payloads =} \" ) new_dict = {} for payload in payloads : if isinstance ( payload , dict ): data = payload else : data , _ = json_load_with_meta ( payload ) new_dict . update ( data ) update_xlsx ( file_in , file_out , new_dict )","title":"update_xlsx_payloads"},{"location":"reference/arb/utils/excel/xl_file_structure/","text":"arb.utils.excel.xl_file_structure Module to determine the path to the root of the feedback_portal project in a platform-independent way. This module can be invoked from multiple runtime contexts, including: - The utils.excel directory (e.g., for standalone Excel/VBA payload generation). - The portal Flask app directory. Directory structure reference: /feedback_portal/ <-- Base of project directory tree \u251c\u2500\u2500 feedback_forms/ \u251c\u2500\u2500 current_versions/ <-- Current versions of feedback forms (created in Excel/VBA) \u2514\u2500\u2500 processed_versions/ <-- Updated versions created in Python \u251c\u2500\u2500 xl_payloads/ \u251c\u2500\u2500 xl_schemas/ \u2514\u2500\u2500 xl_workbooks/ \u251c\u2500\u2500 source/ \u2514\u2500\u2500 production/ \u2514\u2500\u2500 arb/ \u251c\u2500\u2500 portal/ <-- Flask app \u2514\u2500\u2500 utils/ \u2514\u2500\u2500 excel/ <-- Excel generation scripts Attributes: PROJECT_ROOT ( Path ) \u2013 Resolved root directory of the project. FEEDBACK_FORMS ( Path ) \u2013 Path to the 'feedback_forms' directory. CURRENT_VERSIONS ( Path ) \u2013 Path to Excel files from current official versions. PROCESSED_VERSIONS ( Path ) \u2013 Path to output files generated via Python processing.","title":"arb.utils.excel.xl_file_structure"},{"location":"reference/arb/utils/excel/xl_file_structure/#arbutilsexcelxl_file_structure","text":"Module to determine the path to the root of the feedback_portal project in a platform-independent way. This module can be invoked from multiple runtime contexts, including: - The utils.excel directory (e.g., for standalone Excel/VBA payload generation). - The portal Flask app directory. Directory structure reference: /feedback_portal/ <-- Base of project directory tree \u251c\u2500\u2500 feedback_forms/ \u251c\u2500\u2500 current_versions/ <-- Current versions of feedback forms (created in Excel/VBA) \u2514\u2500\u2500 processed_versions/ <-- Updated versions created in Python \u251c\u2500\u2500 xl_payloads/ \u251c\u2500\u2500 xl_schemas/ \u2514\u2500\u2500 xl_workbooks/ \u251c\u2500\u2500 source/ \u2514\u2500\u2500 production/ \u2514\u2500\u2500 arb/ \u251c\u2500\u2500 portal/ <-- Flask app \u2514\u2500\u2500 utils/ \u2514\u2500\u2500 excel/ <-- Excel generation scripts Attributes: PROJECT_ROOT ( Path ) \u2013 Resolved root directory of the project. FEEDBACK_FORMS ( Path ) \u2013 Path to the 'feedback_forms' directory. CURRENT_VERSIONS ( Path ) \u2013 Path to Excel files from current official versions. PROCESSED_VERSIONS ( Path ) \u2013 Path to output files generated via Python processing.","title":"arb.utils.excel.xl_file_structure"},{"location":"reference/arb/utils/excel/xl_hardcoded/","text":"arb.utils.excel.xl_hardcoded Hardcoded schema definitions and sample payloads for Excel template processing. The new versioning systems uses the naming scheme vxx_yy, where xx represents a major version and yy represents a minor version (without the 'old' in the prefix). These were manually created by inspecting old_v01 and old_v02 versions of now outdated Excel spreadsheets. Contents default_value_types_v01_00 : field types for v01_00 based on old_v01 and old_v02 schemas Sample payloads for oil & gas and landfill forms jinja_names_set : manually compiled field names used in Jinja templates Diagnostic comparison for field coverage (see __main__ )","title":"arb.utils.excel.xl_hardcoded"},{"location":"reference/arb/utils/excel/xl_hardcoded/#arbutilsexcelxl_hardcoded","text":"Hardcoded schema definitions and sample payloads for Excel template processing. The new versioning systems uses the naming scheme vxx_yy, where xx represents a major version and yy represents a minor version (without the 'old' in the prefix). These were manually created by inspecting old_v01 and old_v02 versions of now outdated Excel spreadsheets. Contents default_value_types_v01_00 : field types for v01_00 based on old_v01 and old_v02 schemas Sample payloads for oil & gas and landfill forms jinja_names_set : manually compiled field names used in Jinja templates Diagnostic comparison for field coverage (see __main__ )","title":"arb.utils.excel.xl_hardcoded"},{"location":"reference/arb/utils/excel/xl_misc/","text":"arb.utils.excel.xl_misc Excel address parsing and sorting utilities. This module provides helper functions for interpreting Excel-style address strings (such as \"$A$1\") and using them for sorting data structures. These utilities are used during schema generation, payload creation, and Excel form manipulation. Functions: Name Description - get_excel_row_column Parses an Excel address into column and row components. - xl_address_sort Extracts a sortable row or column value from an Excel address. - run_diagnostics Test harness for verifying address parsing and sorting behavior. Typical Use Case These functions are primarily invoked when organizing Excel schema dictionaries by their physical layout in the worksheet, either by row or column position. Notes Assumes absolute Excel address formatting (e.g., \"$A$1\"). Designed to be used by other modules like xl_create and xl_file_structure. get_excel_row_column ( xl_address ) Extract the Excel column letters and row number from an address string. Excel absolute references take the form \"$A$1\" or \"$BB$12\", where both the column and row are prefixed with dollar signs. Parameters: xl_address ( str ) \u2013 The Excel address to parse (must be in absolute format like \"$A$1\"). Returns: tuple [ str , int ] \u2013 tuple[str, int]: A tuple of (column letters, row number). Raises: ValueError \u2013 If the format is invalid (e.g., not exactly two dollar signs, or row not an integer). Examples: >>> get_excel_row_column ( \"$Z$9\" ) ('Z', 9) >>> get_excel_row_column ( \"$AA$105\" ) ('AA', 105) Source code in arb\\utils\\excel\\xl_misc.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def get_excel_row_column ( xl_address : str ) -> tuple [ str , int ]: \"\"\" Extract the Excel column letters and row number from an address string. Excel absolute references take the form \"$A$1\" or \"$BB$12\", where both the column and row are prefixed with dollar signs. Args: xl_address (str): The Excel address to parse (must be in absolute format like \"$A$1\"). Returns: tuple[str, int]: A tuple of (column letters, row number). Raises: ValueError: If the format is invalid (e.g., not exactly two dollar signs, or row not an integer). Examples: >>> get_excel_row_column(\"$Z$9\") ('Z', 9) >>> get_excel_row_column(\"$AA$105\") ('AA', 105) \"\"\" if xl_address . count ( '$' ) != 2 : raise ValueError ( f \"Excel address must contain exactly two '$' characters: { xl_address } \" ) first_dollar = xl_address . find ( '$' ) last_dollar = xl_address . rfind ( '$' ) column = xl_address [ first_dollar + 1 : last_dollar ] try : row = int ( xl_address [ last_dollar + 1 :]) except ValueError as e : raise ValueError ( f \"Could not parse row number from Excel address: { xl_address } \" ) from e return column , row run_diagnostics () Run demonstration tests for get_excel_row_column() and xl_address_sort(). This function is only called if this module is run directly. Examples: >>> run_diagnostics () Source code in arb\\utils\\excel\\xl_misc.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def run_diagnostics () -> None : \"\"\" Run demonstration tests for get_excel_row_column() and xl_address_sort(). This function is only called if this module is run directly. Examples: >>> run_diagnostics() \"\"\" pp = pprint . PrettyPrinter ( indent = 4 , sort_dicts = False ) print ( \"=== Testing get_excel_row_column ===\" ) valid_addresses = [ \"$C$42\" , \"$AA$99\" ] for addr in valid_addresses : try : result = get_excel_row_column ( addr ) print ( f \" Address: { addr } => { result } \" ) except Exception as e : print ( f \" ERROR for { addr } : { e } \" ) print ( \" \\n === Testing get_excel_row_column (invalid formats) ===\" ) invalid_addresses = [ \"A$1\" , \"$A1\" , \"$A$1$\" , \"$AB$\" , \"$AB$XYZ\" ] for addr in invalid_addresses : try : result = get_excel_row_column ( addr ) print ( f \" UNEXPECTED SUCCESS: { addr } => { result } \" ) except Exception as e : print ( f \" Expected failure for { addr } : { e } \" ) print ( \" \\n === Testing xl_address_sort ===\" ) test_tuple = ( \"$B$10\" , \"Example\" ) print ( f \" Tuple: { test_tuple } => Row: { xl_address_sort ( test_tuple , 'key' , 'row' ) } \" ) nested = ( \"key\" , { \"nested\" : { \"cell\" : \"$D$20\" }}) print ( f \" Tuple: { nested } => Row: { xl_address_sort ( nested , 'value' , 'row' , sub_keys = [ 'nested' , 'cell' ]) } \" ) xl_address_sort ( xl_tuple , address_location = 'key' , sort_by = 'row' , sub_keys = None ) Extract the Excel row or column value from a tuple of key-value pairs for sorting. This is used when sorting collections of data where either the key or the value contains an Excel-style address string. Supports sorting by either row or column. Parameters: xl_tuple ( tuple ) \u2013 A (key, value) tuple where one element contains a string like \"$A$1\". address_location ( str , default: 'key' ) \u2013 Which element contains the Excel address (\"key\" or \"value\"). sort_by ( str , default: 'row' ) \u2013 Whether to sort by \"row\" (int) or \"column\" (str). sub_keys ( str | list [ str ] | None , default: None ) \u2013 Key(s) to retrieve nested address if inside a dict. Returns: int | str \u2013 int | str: The row (int) or column (str) extracted from the address. Raises: ValueError \u2013 If address_location or sort_by has an invalid value. Examples: >>> xl_address_sort (( \"$B$3\" , \"data\" ), address_location = \"key\" , sort_by = \"row\" ) 3 >>> xl_address_sort (( \"item\" , { \"pos\" : \"$C$7\" }), address_location = \"value\" , sort_by = \"column\" , sub_keys = \"pos\" ) 'C' Source code in arb\\utils\\excel\\xl_misc.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def xl_address_sort ( xl_tuple : tuple , address_location : str = \"key\" , sort_by : str = \"row\" , sub_keys : str | list [ str ] | None = None ) -> int | str : \"\"\" Extract the Excel row or column value from a tuple of key-value pairs for sorting. This is used when sorting collections of data where either the key or the value contains an Excel-style address string. Supports sorting by either row or column. Args: xl_tuple (tuple): A (key, value) tuple where one element contains a string like \"$A$1\". address_location (str): Which element contains the Excel address (\"key\" or \"value\"). sort_by (str): Whether to sort by \"row\" (int) or \"column\" (str). sub_keys (str | list[str] | None): Key(s) to retrieve nested address if inside a dict. Returns: int | str: The row (int) or column (str) extracted from the address. Raises: ValueError: If `address_location` or `sort_by` has an invalid value. Examples: >>> xl_address_sort((\"$B$3\", \"data\"), address_location=\"key\", sort_by=\"row\") 3 >>> xl_address_sort((\"item\", {\"pos\": \"$C$7\"}), address_location=\"value\", sort_by=\"column\", sub_keys=\"pos\") 'C' \"\"\" if address_location == \"key\" : address = xl_tuple [ 0 ] elif address_location == \"value\" : if sub_keys is None : address = xl_tuple [ 1 ] else : address = get_nested_value ( xl_tuple [ 1 ], sub_keys ) else : raise ValueError ( \"address_location must be 'key' or 'value'\" ) column , row = get_excel_row_column ( address ) if sort_by == \"row\" : return_value = row elif sort_by == \"column\" : return_value = column else : raise ValueError ( \"sort_by must be 'row' or 'column'\" ) return return_value","title":"arb.utils.excel.xl_misc"},{"location":"reference/arb/utils/excel/xl_misc/#arbutilsexcelxl_misc","text":"Excel address parsing and sorting utilities. This module provides helper functions for interpreting Excel-style address strings (such as \"$A$1\") and using them for sorting data structures. These utilities are used during schema generation, payload creation, and Excel form manipulation. Functions: Name Description - get_excel_row_column Parses an Excel address into column and row components. - xl_address_sort Extracts a sortable row or column value from an Excel address. - run_diagnostics Test harness for verifying address parsing and sorting behavior. Typical Use Case These functions are primarily invoked when organizing Excel schema dictionaries by their physical layout in the worksheet, either by row or column position. Notes Assumes absolute Excel address formatting (e.g., \"$A$1\"). Designed to be used by other modules like xl_create and xl_file_structure.","title":"arb.utils.excel.xl_misc"},{"location":"reference/arb/utils/excel/xl_misc/#arb.utils.excel.xl_misc.get_excel_row_column","text":"Extract the Excel column letters and row number from an address string. Excel absolute references take the form \"$A$1\" or \"$BB$12\", where both the column and row are prefixed with dollar signs. Parameters: xl_address ( str ) \u2013 The Excel address to parse (must be in absolute format like \"$A$1\"). Returns: tuple [ str , int ] \u2013 tuple[str, int]: A tuple of (column letters, row number). Raises: ValueError \u2013 If the format is invalid (e.g., not exactly two dollar signs, or row not an integer). Examples: >>> get_excel_row_column ( \"$Z$9\" ) ('Z', 9) >>> get_excel_row_column ( \"$AA$105\" ) ('AA', 105) Source code in arb\\utils\\excel\\xl_misc.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def get_excel_row_column ( xl_address : str ) -> tuple [ str , int ]: \"\"\" Extract the Excel column letters and row number from an address string. Excel absolute references take the form \"$A$1\" or \"$BB$12\", where both the column and row are prefixed with dollar signs. Args: xl_address (str): The Excel address to parse (must be in absolute format like \"$A$1\"). Returns: tuple[str, int]: A tuple of (column letters, row number). Raises: ValueError: If the format is invalid (e.g., not exactly two dollar signs, or row not an integer). Examples: >>> get_excel_row_column(\"$Z$9\") ('Z', 9) >>> get_excel_row_column(\"$AA$105\") ('AA', 105) \"\"\" if xl_address . count ( '$' ) != 2 : raise ValueError ( f \"Excel address must contain exactly two '$' characters: { xl_address } \" ) first_dollar = xl_address . find ( '$' ) last_dollar = xl_address . rfind ( '$' ) column = xl_address [ first_dollar + 1 : last_dollar ] try : row = int ( xl_address [ last_dollar + 1 :]) except ValueError as e : raise ValueError ( f \"Could not parse row number from Excel address: { xl_address } \" ) from e return column , row","title":"get_excel_row_column"},{"location":"reference/arb/utils/excel/xl_misc/#arb.utils.excel.xl_misc.run_diagnostics","text":"Run demonstration tests for get_excel_row_column() and xl_address_sort(). This function is only called if this module is run directly. Examples: >>> run_diagnostics () Source code in arb\\utils\\excel\\xl_misc.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def run_diagnostics () -> None : \"\"\" Run demonstration tests for get_excel_row_column() and xl_address_sort(). This function is only called if this module is run directly. Examples: >>> run_diagnostics() \"\"\" pp = pprint . PrettyPrinter ( indent = 4 , sort_dicts = False ) print ( \"=== Testing get_excel_row_column ===\" ) valid_addresses = [ \"$C$42\" , \"$AA$99\" ] for addr in valid_addresses : try : result = get_excel_row_column ( addr ) print ( f \" Address: { addr } => { result } \" ) except Exception as e : print ( f \" ERROR for { addr } : { e } \" ) print ( \" \\n === Testing get_excel_row_column (invalid formats) ===\" ) invalid_addresses = [ \"A$1\" , \"$A1\" , \"$A$1$\" , \"$AB$\" , \"$AB$XYZ\" ] for addr in invalid_addresses : try : result = get_excel_row_column ( addr ) print ( f \" UNEXPECTED SUCCESS: { addr } => { result } \" ) except Exception as e : print ( f \" Expected failure for { addr } : { e } \" ) print ( \" \\n === Testing xl_address_sort ===\" ) test_tuple = ( \"$B$10\" , \"Example\" ) print ( f \" Tuple: { test_tuple } => Row: { xl_address_sort ( test_tuple , 'key' , 'row' ) } \" ) nested = ( \"key\" , { \"nested\" : { \"cell\" : \"$D$20\" }}) print ( f \" Tuple: { nested } => Row: { xl_address_sort ( nested , 'value' , 'row' , sub_keys = [ 'nested' , 'cell' ]) } \" )","title":"run_diagnostics"},{"location":"reference/arb/utils/excel/xl_misc/#arb.utils.excel.xl_misc.xl_address_sort","text":"Extract the Excel row or column value from a tuple of key-value pairs for sorting. This is used when sorting collections of data where either the key or the value contains an Excel-style address string. Supports sorting by either row or column. Parameters: xl_tuple ( tuple ) \u2013 A (key, value) tuple where one element contains a string like \"$A$1\". address_location ( str , default: 'key' ) \u2013 Which element contains the Excel address (\"key\" or \"value\"). sort_by ( str , default: 'row' ) \u2013 Whether to sort by \"row\" (int) or \"column\" (str). sub_keys ( str | list [ str ] | None , default: None ) \u2013 Key(s) to retrieve nested address if inside a dict. Returns: int | str \u2013 int | str: The row (int) or column (str) extracted from the address. Raises: ValueError \u2013 If address_location or sort_by has an invalid value. Examples: >>> xl_address_sort (( \"$B$3\" , \"data\" ), address_location = \"key\" , sort_by = \"row\" ) 3 >>> xl_address_sort (( \"item\" , { \"pos\" : \"$C$7\" }), address_location = \"value\" , sort_by = \"column\" , sub_keys = \"pos\" ) 'C' Source code in arb\\utils\\excel\\xl_misc.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def xl_address_sort ( xl_tuple : tuple , address_location : str = \"key\" , sort_by : str = \"row\" , sub_keys : str | list [ str ] | None = None ) -> int | str : \"\"\" Extract the Excel row or column value from a tuple of key-value pairs for sorting. This is used when sorting collections of data where either the key or the value contains an Excel-style address string. Supports sorting by either row or column. Args: xl_tuple (tuple): A (key, value) tuple where one element contains a string like \"$A$1\". address_location (str): Which element contains the Excel address (\"key\" or \"value\"). sort_by (str): Whether to sort by \"row\" (int) or \"column\" (str). sub_keys (str | list[str] | None): Key(s) to retrieve nested address if inside a dict. Returns: int | str: The row (int) or column (str) extracted from the address. Raises: ValueError: If `address_location` or `sort_by` has an invalid value. Examples: >>> xl_address_sort((\"$B$3\", \"data\"), address_location=\"key\", sort_by=\"row\") 3 >>> xl_address_sort((\"item\", {\"pos\": \"$C$7\"}), address_location=\"value\", sort_by=\"column\", sub_keys=\"pos\") 'C' \"\"\" if address_location == \"key\" : address = xl_tuple [ 0 ] elif address_location == \"value\" : if sub_keys is None : address = xl_tuple [ 1 ] else : address = get_nested_value ( xl_tuple [ 1 ], sub_keys ) else : raise ValueError ( \"address_location must be 'key' or 'value'\" ) column , row = get_excel_row_column ( address ) if sort_by == \"row\" : return_value = row elif sort_by == \"column\" : return_value = column else : raise ValueError ( \"sort_by must be 'row' or 'column'\" ) return return_value","title":"xl_address_sort"},{"location":"reference/arb/utils/excel/xl_parse/","text":"arb.utils.excel.xl_parse Module to parse and ingest Excel spreadsheet contents. This module provides logic to convert Excel forms into structured dictionary representations, including extraction of tab contents, metadata, and schema references. It is primarily used to support automated feedback template parsing. Notes schema_file_map is a dict where keys are schema names and values are paths to JSON files. schema_map is a dict where keys are schema names and values are: {\"schema\": schema_dict, \"metadata\": metadata_dict} Example xl_schema_map[\"oil_and_gas_v03\"][\"schema\"] create_schema_file_map ( schema_path = None , schema_names = None ) Create a dictionary mapping schema names to their JSON file paths. Parameters: schema_path ( str | Path | None , default: None ) \u2013 Folder containing schema files. Defaults to processed versions dir. schema_names ( list [ str ] | None , default: None ) \u2013 Names of schemas to include. Defaults to known schemas. Returns: dict [ str , Path ] \u2013 dict[str, Path]: Map from schema name to schema file path. Source code in arb\\utils\\excel\\xl_parse.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def create_schema_file_map ( schema_path : str | Path | None = None , schema_names : list [ str ] | None = None ) -> dict [ str , Path ]: \"\"\" Create a dictionary mapping schema names to their JSON file paths. Args: schema_path (str | Path | None): Folder containing schema files. Defaults to processed versions dir. schema_names (list[str] | None): Names of schemas to include. Defaults to known schemas. Returns: dict[str, Path]: Map from schema name to schema file path. \"\"\" logger . debug ( f \"create_schema_file_map() called with { schema_path =} , { schema_names =} \" ) if schema_path is None : schema_path = PROCESSED_VERSIONS / \"xl_schemas\" if schema_names is None : schema_names = [ \"landfill_v01_00\" , \"oil_and_gas_v01_00\" , \"energy_v00_01\" , ] schema_file_map = {} for schema_name in schema_names : schema_file_name = schema_name + \".json\" schema_file_path = schema_path / schema_file_name schema_file_map [ schema_name ] = schema_file_path return schema_file_map extract_tabs ( wb , schema_map , xl_as_dict ) Extract data from the data tabs that are enumerated in the schema tab. Parameters: wb ( Workbook ) \u2013 OpenPyXL workbook object. schema_map ( dict [ str , dict ] ) \u2013 Schema map with schema definitions. xl_as_dict ( dict ) \u2013 Parsed Excel content, including 'schemas' and 'metadata'. Dictionary with schema tab where keys are the data tab names and values are the formatting_schema to parse the tab. Returns: dict ( dict ) \u2013 Updated xl_as_dict including parsed 'tab_contents'. Source code in arb\\utils\\excel\\xl_parse.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def extract_tabs ( wb : openpyxl . workbook . workbook . Workbook , schema_map : dict [ str , dict ], xl_as_dict : dict ) -> dict : \"\"\" Extract data from the data tabs that are enumerated in the schema tab. Args: wb (Workbook): OpenPyXL workbook object. schema_map (dict[str, dict]): Schema map with schema definitions. xl_as_dict (dict): Parsed Excel content, including 'schemas' and 'metadata'. Dictionary with schema tab where keys are the data tab names and values are the formatting_schema to parse the tab. Returns: dict: Updated xl_as_dict including parsed 'tab_contents'. \"\"\" skip_please_selects = False result = copy . deepcopy ( xl_as_dict ) for tab_name , formatting_schema in result [ 'schemas' ] . items (): logger . debug ( f \"Extracting data from ' { tab_name } ', using the formatting schema ' { formatting_schema } '\" ) result [ 'tab_contents' ][ tab_name ] = {} ws = wb [ tab_name ] format_dict = schema_map [ formatting_schema ][ 'schema' ] for html_field_name , lookup in format_dict . items (): value_address = lookup [ 'value_address' ] value_type = lookup [ 'value_type' ] is_drop_down = lookup [ 'is_drop_down' ] value = ws [ value_address ] . value if skip_please_selects is True : if is_drop_down and value == PLEASE_SELECT : logger . debug ( f \"Skipping { html_field_name } because it is a drop down and is set to { PLEASE_SELECT } \" ) continue # Try to cast the spreadsheet data to the desired type if possible if value is not None : if not isinstance ( value , value_type ): # if it is not supposed to be of type string, but it is a zero length string, turn it to None if value == \"\" : value = None else : logger . warning ( f \"Warning: < { html_field_name } > value at < { lookup [ 'value_address' ] } > is < { value } > \" f \"and is of type < { type ( value ) } > whereas it should be of type < { value_type } >. \" f \"Attempting to convert the value to the correct type\" ) try : # convert to datetime using a parser if possible if value_type == datetime . datetime : local_datetime = parse_unknown_datetime ( value ) if local_datetime and not is_datetime_naive ( local_datetime ): logger . warning ( f \"Date time { value } is not a naive datetime, skipping to avoid data corruption\" ) continue value = local_datetime else : # Use default repr-like conversion if not a datetime value = value_type ( value ) logger . info ( f \"Type conversion successful. value is now < { value } > with type: < { type ( value ) } >\" ) except ( ValueError , TypeError ) as e : logger . warning ( f \"Type conversion failed, resetting value to None\" ) value = None result [ 'tab_contents' ][ tab_name ][ html_field_name ] = value if 'label_address' in lookup and 'label' in lookup : label_address = lookup [ 'label_address' ] label_xl = ws [ label_address ] . value label_schema = lookup [ 'label' ] if label_xl != label_schema : logger . warning ( f \"Schema data label and spreadsheet data label differ.\" f \" \\n\\t schema label = { label_schema } \\n\\t spreadsheet label ( { label_address } ) = { label_xl } \" ) logger . debug ( f \"Initial spreadsheet extraction of ' { tab_name } ' yields { result [ 'tab_contents' ][ tab_name ] } \" ) # Some cells should be spit into multiple dictionary entries (such as full name, lat/log) split_compound_keys ( result [ 'tab_contents' ][ tab_name ]) # Excel drop-downs save the value not the key, so you have to reverse lookup their values logger . debug ( f \"Final corrected spreadsheet extraction of ' { tab_name } ' yields { result [ 'tab_contents' ][ tab_name ] } \" ) return result get_json_file_name ( file_name ) Convert a file name (Excel or JSON) into a JSON file name, parsing if needed. Parameters: file_name ( Path ) \u2013 The uploaded file. Returns: Path | None \u2013 Path | None: JSON file path if parsed or detected, otherwise None. Notes: - If the file_name is a json file (has .json extension), the file_name is the json_file_name. - If the file is an Excel file (.xlsx extension), try to parse it into a json file and return the json file name of the parsed contents. - If the file is neither a json file nr a spreadsheet that can be parsed into a json file, return None. - If the file was already a json file, return the file name unaltered. - If the file was an Excel file, return the json file that its data was extracted to. - For all other file types return None. Source code in arb\\utils\\excel\\xl_parse.py 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 def get_json_file_name ( file_name : Path ) -> Path | None : \"\"\" Convert a file name (Excel or JSON) into a JSON file name, parsing if needed. Args: file_name (Path): The uploaded file. Returns: Path | None: JSON file path if parsed or detected, otherwise None. Notes: - If the file_name is a json file (has .json extension), the file_name is the json_file_name. - If the file is an Excel file (.xlsx extension), try to parse it into a json file and return the json file name of the parsed contents. - If the file is neither a json file nr a spreadsheet that can be parsed into a json file, return None. - If the file was already a json file, return the file name unaltered. - If the file was an Excel file, return the json file that its data was extracted to. - For all other file types return None. \"\"\" json_file_name = None extension = file_name . suffix # logger.debug(f\"{extension=}\") if extension == \".xlsx\" : logger . debug ( f \"Excel file upload detected. File name: { file_name } \" ) # xl_as_dict = xl_path_to_dict(file_name) xl_as_dict = parse_xl_file ( file_name ) logger . debug ( f \" { xl_as_dict =} \" ) json_file_name = file_name . with_suffix ( '.json' ) logger . debug ( f \"Saving extracted data from Excel as: { json_file_name } \" ) json_save_with_meta ( json_file_name , xl_as_dict ) elif extension == \".json\" : logger . debug ( f \"Json file upload detected. File name: { file_name } \" ) json_file_name = Path ( file_name ) else : logger . debug ( f \"Unknown file type upload detected. File name: { file_name } \" ) return json_file_name get_spreadsheet_key_value_pairs ( wb , tab_name , top_left_cell ) Read key-value pairs from a worksheet starting at a given cell. Parameters: wb ( Workbook ) \u2013 OpenPyXL workbook object. tab_name ( str ) \u2013 Name of the worksheet tab. top_left_cell ( str ) \u2013 Top-left cell of the key/value pair region. Returns: dict [ str , str | None] \u2013 dict[str, str | None]: Parsed key-value pairs. Source code in arb\\utils\\excel\\xl_parse.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def get_spreadsheet_key_value_pairs ( wb : openpyxl . workbook . workbook . Workbook , tab_name : str , top_left_cell : str ) -> dict [ str , str | None ]: \"\"\" Read key-value pairs from a worksheet starting at a given cell. Args: wb (Workbook): OpenPyXL workbook object. tab_name (str): Name of the worksheet tab. top_left_cell (str): Top-left cell of the key/value pair region. Returns: dict[str, str | None]: Parsed key-value pairs. \"\"\" # logger.debug(f\"{type(wb)=}, \") ws = wb [ tab_name ] # logger.debug(f\"{type(ws)=}, \") return_dict = {} row_offset = 0 while True : key = ws [ top_left_cell ] . offset ( row = row_offset ) . value value = ws [ top_left_cell ] . offset ( row = row_offset , column = 1 ) . value if key not in [ \"\" , None ]: return_dict [ key ] = value row_offset += 1 else : break return return_dict initialize_module () Initialize the module by calling set_globals(). This function loads default schema mappings into global variables. Source code in arb\\utils\\excel\\xl_parse.py 42 43 44 45 46 47 48 49 def initialize_module () -> None : \"\"\" Initialize the module by calling set_globals(). This function loads default schema mappings into global variables. \"\"\" logger . debug ( f \"initialize_module() called\" ) set_globals () load_schema_file_map ( schema_file_map ) Load JSON schema and metadata from a mapping of schema name to file path. Parameters: schema_file_map ( dict [ str , Path ] ) \u2013 Keys are schema names, values are JSON schema file paths. Returns: dict [ str , dict ] \u2013 dict[str, dict]: Dictionary where keys are schema names and values are dicts with: - \"schema\": The schema dictionary. - \"metadata\": Metadata extracted from the JSON. Source code in arb\\utils\\excel\\xl_parse.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def load_schema_file_map ( schema_file_map : dict [ str , Path ]) -> dict [ str , dict ]: \"\"\" Load JSON schema and metadata from a mapping of schema name to file path. Args: schema_file_map (dict[str, Path]): Keys are schema names, values are JSON schema file paths. Returns: dict[str, dict]: Dictionary where keys are schema names and values are dicts with: - \"schema\": The schema dictionary. - \"metadata\": Metadata extracted from the JSON. \"\"\" logger . debug ( f \"load_schema_file_map() called with { schema_file_map =} \" ) schema_map = {} for schema_name , json_path in schema_file_map . items (): schema , metadata = json_load_with_meta ( json_path ) schema_map [ schema_name ] = { \"schema\" : schema , \"metadata\" : metadata } return schema_map load_xl_schema ( file_name ) Load schema and metadata from a JSON file. Parameters: file_name ( str | Path ) \u2013 Path to a JSON schema file. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: Tuple of (schema dict, metadata dict). Source code in arb\\utils\\excel\\xl_parse.py 139 140 141 142 143 144 145 146 147 148 149 150 151 def load_xl_schema ( file_name : str | Path ) -> tuple [ dict , dict ]: \"\"\" Load schema and metadata from a JSON file. Args: file_name (str | Path): Path to a JSON schema file. Returns: tuple[dict, dict]: Tuple of (schema dict, metadata dict). \"\"\" logger . debug ( f \"load_xl_schema() called with { file_name =} \" ) schema , metadata = json_load_with_meta ( file_name ) return schema , metadata main () Run all schema and Excel file parsing test functions for diagnostic purposes. Source code in arb\\utils\\excel\\xl_parse.py 428 429 430 431 432 433 434 def main () -> None : \"\"\" Run all schema and Excel file parsing test functions for diagnostic purposes. \"\"\" test_load_xl_schemas () test_load_schema_file_map () test_parse_xl_file () parse_xl_file ( xl_path , schema_map = None ) Parse a spreadsheet and return a dictionary representation using the given schema. Parameters: xl_path ( str | Path ) \u2013 Path to the Excel spreadsheet. schema_map ( dict [ str , dict ] | None , default: None ) \u2013 Map of schema names to their definitions. Returns: dict ( dict ) \u2013 Dictionary with extracted metadata, schemas, and tab contents. Notes: - tutorial on openpyxl: https://openpyxl.readthedocs.io/en/stable/tutorial.html Source code in arb\\utils\\excel\\xl_parse.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def parse_xl_file ( xl_path , schema_map = None ) -> dict : \"\"\" Parse a spreadsheet and return a dictionary representation using the given schema. Args: xl_path (str | Path): Path to the Excel spreadsheet. schema_map (dict[str, dict] | None): Map of schema names to their definitions. Returns: dict: Dictionary with extracted metadata, schemas, and tab contents. Notes: - tutorial on openpyxl: https://openpyxl.readthedocs.io/en/stable/tutorial.html \"\"\" logger . debug ( f \"parse_xl_with_schema_dict() called with { xl_path =} , { schema_map =} \" ) if schema_map is None : schema_map = xl_schema_map # Dictionary data structure to store Excel contents result = {} result [ 'metadata' ] = {} result [ 'schemas' ] = {} result [ 'tab_contents' ] = {} # Notes on data_only argument. By default, .value returns the 'formula' in the cell. # If data_only=True, then .value returns the last 'value' that was evaluated at the cell. wb = openpyxl . load_workbook ( xl_path , keep_vba = False , data_only = True ) # Extract metadata and schema information from hidden tabs if EXCEL_METADATA_TAB_NAME in wb . sheetnames : logger . debug ( f \"metadata tab detected in Excel file\" ) result [ 'metadata' ] = get_spreadsheet_key_value_pairs ( wb , EXCEL_METADATA_TAB_NAME , EXCEL_TOP_LEFT_KEY_VALUE_CELL ) if EXCEL_SCHEMA_TAB_NAME in wb . sheetnames : logger . debug ( f \"Schema tab detected in Excel file\" ) result [ 'schemas' ] = get_spreadsheet_key_value_pairs ( wb , EXCEL_SCHEMA_TAB_NAME , EXCEL_TOP_LEFT_KEY_VALUE_CELL ) else : ValueError ( f 'Spreadsheet must have a { EXCEL_SCHEMA_TAB_NAME } tab' ) # extract data tabs content using specified schemas new_result = extract_tabs ( wb , schema_map , result ) return new_result set_globals ( xl_schema_file_map_ = None ) Set module-level global variables for schema file map and loaded schema map. Parameters: xl_schema_file_map_ ( dict [ str , Path ] | None , default: None ) \u2013 Optional override for schema file map. If not provided, uses a default list of pre-defined schema files. Notes Calls load_schema_file_map() to populate xl_schema_map from JSON files. Source code in arb\\utils\\excel\\xl_parse.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def set_globals ( xl_schema_file_map_ : dict [ str , Path ] | None = None ) -> None : \"\"\" Set module-level global variables for schema file map and loaded schema map. Args: xl_schema_file_map_ (dict[str, Path] | None): Optional override for schema file map. If not provided, uses a default list of pre-defined schema files. Notes: - Calls `load_schema_file_map()` to populate xl_schema_map from JSON files. \"\"\" global xl_schema_file_map , xl_schema_map # todo - update default roots with module paths, may make sense to remove globals and have # a different logic since this is outdated given the project root approach logger . debug ( f \"set_globals() called with { xl_schema_file_map_ =} \" ) # todo - not sure if these should be hard coded here ... if xl_schema_file_map_ is None : xl_schema_file_map = { \"landfill_v01_00\" : PROCESSED_VERSIONS / \"xl_schemas\" / \"landfill_v01_00.json\" , \"oil_and_gas_v01_00\" : PROCESSED_VERSIONS / \"xl_schemas\" / \"oil_and_gas_v01_00.json\" , \"energy_v00_01\" : PROCESSED_VERSIONS / \"xl_schemas\" / \"energy_v00_01.json\" , } else : xl_schema_file_map = xl_schema_file_map_ # load all the schemas if xl_schema_file_map : xl_schema_map = load_schema_file_map ( xl_schema_file_map ) logger . debug ( f \"globals are now: { xl_schema_file_map =} , { xl_schema_map =} \" ) split_compound_keys ( dict_ ) Decompose compound keys into atomic fields. Remove key/value pairs of entries that potentially contain compound keys and replace them with key value pairs that are more atomic. Parameters: dict_ ( dict ) \u2013 Dictionary with potentially compound fields (e.g., lat_and_long). Raises: ValueError \u2013 If 'lat_and_long' is improperly formatted. Source code in arb\\utils\\excel\\xl_parse.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def split_compound_keys ( dict_ : dict ) -> None : \"\"\" Decompose compound keys into atomic fields. Remove key/value pairs of entries that potentially contain compound keys and replace them with key value pairs that are more atomic. Args: dict_ (dict): Dictionary with potentially compound fields (e.g., lat_and_long). Raises: ValueError: If 'lat_and_long' is improperly formatted. \"\"\" for html_field_name in list ( dict_ . keys ()): value = dict_ [ html_field_name ] if html_field_name == 'lat_and_long' : if value : lat_longs = value . split ( ',' ) if len ( lat_longs ) == 2 : dict_ [ 'lat_arb' ] = lat_longs [ 0 ] dict_ [ 'long_arb' ] = lat_longs [ 1 ] else : raise ValueError ( f \"Lat long must be a blank or a comma separated list of lat/long pairs\" ) del dict_ [ html_field_name ] test_load_schema_file_map () Debug test for loading schema file map and displaying contents. Source code in arb\\utils\\excel\\xl_parse.py 395 396 397 398 399 400 401 402 def test_load_schema_file_map () -> None : \"\"\" Debug test for loading schema file map and displaying contents. \"\"\" logger . debug ( f \"test_load_schema_file_map() called\" ) schema_map = create_schema_file_map () schemas = load_schema_file_map ( schema_map ) logger . debug ( f \" { schemas =} \" ) test_load_xl_schemas () Debug test for loading default schemas from xl_schema_file_map. Source code in arb\\utils\\excel\\xl_parse.py 405 406 407 408 409 410 411 412 def test_load_xl_schemas () -> None : \"\"\" Debug test for loading default schemas from xl_schema_file_map. \"\"\" logger . debug ( f \"Testing load_xl_schemas() with test_load_xl_schemas\" ) schemas = load_schema_file_map ( xl_schema_file_map ) logger . debug ( f \"Testing load_xl_schemas() with test_load_xl_schemas\" ) logging . debug ( f \"schemas = { pp_log ( schemas ) } \" ) test_parse_xl_file () Debug test to parse a known Excel file into dictionary form using schemas. Source code in arb\\utils\\excel\\xl_parse.py 416 417 418 419 420 421 422 423 424 def test_parse_xl_file () -> None : \"\"\" Debug test to parse a known Excel file into dictionary form using schemas. \"\"\" logger . debug ( f \"test_parse_xl_file() called\" ) xl_path = PROCESSED_VERSIONS / \"xl_workbooks\" / \"landfill_operator_feedback_v070_populated_01.xlsx\" print ( f \" { xl_path =} \" ) result = parse_xl_file ( xl_path , xl_schema_map ) logger . debug ( f \" { result =} \" )","title":"arb.utils.excel.xl_parse"},{"location":"reference/arb/utils/excel/xl_parse/#arbutilsexcelxl_parse","text":"Module to parse and ingest Excel spreadsheet contents. This module provides logic to convert Excel forms into structured dictionary representations, including extraction of tab contents, metadata, and schema references. It is primarily used to support automated feedback template parsing. Notes schema_file_map is a dict where keys are schema names and values are paths to JSON files. schema_map is a dict where keys are schema names and values are: {\"schema\": schema_dict, \"metadata\": metadata_dict} Example xl_schema_map[\"oil_and_gas_v03\"][\"schema\"]","title":"arb.utils.excel.xl_parse"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.create_schema_file_map","text":"Create a dictionary mapping schema names to their JSON file paths. Parameters: schema_path ( str | Path | None , default: None ) \u2013 Folder containing schema files. Defaults to processed versions dir. schema_names ( list [ str ] | None , default: None ) \u2013 Names of schemas to include. Defaults to known schemas. Returns: dict [ str , Path ] \u2013 dict[str, Path]: Map from schema name to schema file path. Source code in arb\\utils\\excel\\xl_parse.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def create_schema_file_map ( schema_path : str | Path | None = None , schema_names : list [ str ] | None = None ) -> dict [ str , Path ]: \"\"\" Create a dictionary mapping schema names to their JSON file paths. Args: schema_path (str | Path | None): Folder containing schema files. Defaults to processed versions dir. schema_names (list[str] | None): Names of schemas to include. Defaults to known schemas. Returns: dict[str, Path]: Map from schema name to schema file path. \"\"\" logger . debug ( f \"create_schema_file_map() called with { schema_path =} , { schema_names =} \" ) if schema_path is None : schema_path = PROCESSED_VERSIONS / \"xl_schemas\" if schema_names is None : schema_names = [ \"landfill_v01_00\" , \"oil_and_gas_v01_00\" , \"energy_v00_01\" , ] schema_file_map = {} for schema_name in schema_names : schema_file_name = schema_name + \".json\" schema_file_path = schema_path / schema_file_name schema_file_map [ schema_name ] = schema_file_path return schema_file_map","title":"create_schema_file_map"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.extract_tabs","text":"Extract data from the data tabs that are enumerated in the schema tab. Parameters: wb ( Workbook ) \u2013 OpenPyXL workbook object. schema_map ( dict [ str , dict ] ) \u2013 Schema map with schema definitions. xl_as_dict ( dict ) \u2013 Parsed Excel content, including 'schemas' and 'metadata'. Dictionary with schema tab where keys are the data tab names and values are the formatting_schema to parse the tab. Returns: dict ( dict ) \u2013 Updated xl_as_dict including parsed 'tab_contents'. Source code in arb\\utils\\excel\\xl_parse.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def extract_tabs ( wb : openpyxl . workbook . workbook . Workbook , schema_map : dict [ str , dict ], xl_as_dict : dict ) -> dict : \"\"\" Extract data from the data tabs that are enumerated in the schema tab. Args: wb (Workbook): OpenPyXL workbook object. schema_map (dict[str, dict]): Schema map with schema definitions. xl_as_dict (dict): Parsed Excel content, including 'schemas' and 'metadata'. Dictionary with schema tab where keys are the data tab names and values are the formatting_schema to parse the tab. Returns: dict: Updated xl_as_dict including parsed 'tab_contents'. \"\"\" skip_please_selects = False result = copy . deepcopy ( xl_as_dict ) for tab_name , formatting_schema in result [ 'schemas' ] . items (): logger . debug ( f \"Extracting data from ' { tab_name } ', using the formatting schema ' { formatting_schema } '\" ) result [ 'tab_contents' ][ tab_name ] = {} ws = wb [ tab_name ] format_dict = schema_map [ formatting_schema ][ 'schema' ] for html_field_name , lookup in format_dict . items (): value_address = lookup [ 'value_address' ] value_type = lookup [ 'value_type' ] is_drop_down = lookup [ 'is_drop_down' ] value = ws [ value_address ] . value if skip_please_selects is True : if is_drop_down and value == PLEASE_SELECT : logger . debug ( f \"Skipping { html_field_name } because it is a drop down and is set to { PLEASE_SELECT } \" ) continue # Try to cast the spreadsheet data to the desired type if possible if value is not None : if not isinstance ( value , value_type ): # if it is not supposed to be of type string, but it is a zero length string, turn it to None if value == \"\" : value = None else : logger . warning ( f \"Warning: < { html_field_name } > value at < { lookup [ 'value_address' ] } > is < { value } > \" f \"and is of type < { type ( value ) } > whereas it should be of type < { value_type } >. \" f \"Attempting to convert the value to the correct type\" ) try : # convert to datetime using a parser if possible if value_type == datetime . datetime : local_datetime = parse_unknown_datetime ( value ) if local_datetime and not is_datetime_naive ( local_datetime ): logger . warning ( f \"Date time { value } is not a naive datetime, skipping to avoid data corruption\" ) continue value = local_datetime else : # Use default repr-like conversion if not a datetime value = value_type ( value ) logger . info ( f \"Type conversion successful. value is now < { value } > with type: < { type ( value ) } >\" ) except ( ValueError , TypeError ) as e : logger . warning ( f \"Type conversion failed, resetting value to None\" ) value = None result [ 'tab_contents' ][ tab_name ][ html_field_name ] = value if 'label_address' in lookup and 'label' in lookup : label_address = lookup [ 'label_address' ] label_xl = ws [ label_address ] . value label_schema = lookup [ 'label' ] if label_xl != label_schema : logger . warning ( f \"Schema data label and spreadsheet data label differ.\" f \" \\n\\t schema label = { label_schema } \\n\\t spreadsheet label ( { label_address } ) = { label_xl } \" ) logger . debug ( f \"Initial spreadsheet extraction of ' { tab_name } ' yields { result [ 'tab_contents' ][ tab_name ] } \" ) # Some cells should be spit into multiple dictionary entries (such as full name, lat/log) split_compound_keys ( result [ 'tab_contents' ][ tab_name ]) # Excel drop-downs save the value not the key, so you have to reverse lookup their values logger . debug ( f \"Final corrected spreadsheet extraction of ' { tab_name } ' yields { result [ 'tab_contents' ][ tab_name ] } \" ) return result","title":"extract_tabs"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.get_json_file_name","text":"Convert a file name (Excel or JSON) into a JSON file name, parsing if needed. Parameters: file_name ( Path ) \u2013 The uploaded file. Returns: Path | None \u2013 Path | None: JSON file path if parsed or detected, otherwise None. Notes: - If the file_name is a json file (has .json extension), the file_name is the json_file_name. - If the file is an Excel file (.xlsx extension), try to parse it into a json file and return the json file name of the parsed contents. - If the file is neither a json file nr a spreadsheet that can be parsed into a json file, return None. - If the file was already a json file, return the file name unaltered. - If the file was an Excel file, return the json file that its data was extracted to. - For all other file types return None. Source code in arb\\utils\\excel\\xl_parse.py 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 def get_json_file_name ( file_name : Path ) -> Path | None : \"\"\" Convert a file name (Excel or JSON) into a JSON file name, parsing if needed. Args: file_name (Path): The uploaded file. Returns: Path | None: JSON file path if parsed or detected, otherwise None. Notes: - If the file_name is a json file (has .json extension), the file_name is the json_file_name. - If the file is an Excel file (.xlsx extension), try to parse it into a json file and return the json file name of the parsed contents. - If the file is neither a json file nr a spreadsheet that can be parsed into a json file, return None. - If the file was already a json file, return the file name unaltered. - If the file was an Excel file, return the json file that its data was extracted to. - For all other file types return None. \"\"\" json_file_name = None extension = file_name . suffix # logger.debug(f\"{extension=}\") if extension == \".xlsx\" : logger . debug ( f \"Excel file upload detected. File name: { file_name } \" ) # xl_as_dict = xl_path_to_dict(file_name) xl_as_dict = parse_xl_file ( file_name ) logger . debug ( f \" { xl_as_dict =} \" ) json_file_name = file_name . with_suffix ( '.json' ) logger . debug ( f \"Saving extracted data from Excel as: { json_file_name } \" ) json_save_with_meta ( json_file_name , xl_as_dict ) elif extension == \".json\" : logger . debug ( f \"Json file upload detected. File name: { file_name } \" ) json_file_name = Path ( file_name ) else : logger . debug ( f \"Unknown file type upload detected. File name: { file_name } \" ) return json_file_name","title":"get_json_file_name"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.get_spreadsheet_key_value_pairs","text":"Read key-value pairs from a worksheet starting at a given cell. Parameters: wb ( Workbook ) \u2013 OpenPyXL workbook object. tab_name ( str ) \u2013 Name of the worksheet tab. top_left_cell ( str ) \u2013 Top-left cell of the key/value pair region. Returns: dict [ str , str | None] \u2013 dict[str, str | None]: Parsed key-value pairs. Source code in arb\\utils\\excel\\xl_parse.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 def get_spreadsheet_key_value_pairs ( wb : openpyxl . workbook . workbook . Workbook , tab_name : str , top_left_cell : str ) -> dict [ str , str | None ]: \"\"\" Read key-value pairs from a worksheet starting at a given cell. Args: wb (Workbook): OpenPyXL workbook object. tab_name (str): Name of the worksheet tab. top_left_cell (str): Top-left cell of the key/value pair region. Returns: dict[str, str | None]: Parsed key-value pairs. \"\"\" # logger.debug(f\"{type(wb)=}, \") ws = wb [ tab_name ] # logger.debug(f\"{type(ws)=}, \") return_dict = {} row_offset = 0 while True : key = ws [ top_left_cell ] . offset ( row = row_offset ) . value value = ws [ top_left_cell ] . offset ( row = row_offset , column = 1 ) . value if key not in [ \"\" , None ]: return_dict [ key ] = value row_offset += 1 else : break return return_dict","title":"get_spreadsheet_key_value_pairs"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.initialize_module","text":"Initialize the module by calling set_globals(). This function loads default schema mappings into global variables. Source code in arb\\utils\\excel\\xl_parse.py 42 43 44 45 46 47 48 49 def initialize_module () -> None : \"\"\" Initialize the module by calling set_globals(). This function loads default schema mappings into global variables. \"\"\" logger . debug ( f \"initialize_module() called\" ) set_globals ()","title":"initialize_module"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.load_schema_file_map","text":"Load JSON schema and metadata from a mapping of schema name to file path. Parameters: schema_file_map ( dict [ str , Path ] ) \u2013 Keys are schema names, values are JSON schema file paths. Returns: dict [ str , dict ] \u2013 dict[str, dict]: Dictionary where keys are schema names and values are dicts with: - \"schema\": The schema dictionary. - \"metadata\": Metadata extracted from the JSON. Source code in arb\\utils\\excel\\xl_parse.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def load_schema_file_map ( schema_file_map : dict [ str , Path ]) -> dict [ str , dict ]: \"\"\" Load JSON schema and metadata from a mapping of schema name to file path. Args: schema_file_map (dict[str, Path]): Keys are schema names, values are JSON schema file paths. Returns: dict[str, dict]: Dictionary where keys are schema names and values are dicts with: - \"schema\": The schema dictionary. - \"metadata\": Metadata extracted from the JSON. \"\"\" logger . debug ( f \"load_schema_file_map() called with { schema_file_map =} \" ) schema_map = {} for schema_name , json_path in schema_file_map . items (): schema , metadata = json_load_with_meta ( json_path ) schema_map [ schema_name ] = { \"schema\" : schema , \"metadata\" : metadata } return schema_map","title":"load_schema_file_map"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.load_xl_schema","text":"Load schema and metadata from a JSON file. Parameters: file_name ( str | Path ) \u2013 Path to a JSON schema file. Returns: tuple [ dict , dict ] \u2013 tuple[dict, dict]: Tuple of (schema dict, metadata dict). Source code in arb\\utils\\excel\\xl_parse.py 139 140 141 142 143 144 145 146 147 148 149 150 151 def load_xl_schema ( file_name : str | Path ) -> tuple [ dict , dict ]: \"\"\" Load schema and metadata from a JSON file. Args: file_name (str | Path): Path to a JSON schema file. Returns: tuple[dict, dict]: Tuple of (schema dict, metadata dict). \"\"\" logger . debug ( f \"load_xl_schema() called with { file_name =} \" ) schema , metadata = json_load_with_meta ( file_name ) return schema , metadata","title":"load_xl_schema"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.main","text":"Run all schema and Excel file parsing test functions for diagnostic purposes. Source code in arb\\utils\\excel\\xl_parse.py 428 429 430 431 432 433 434 def main () -> None : \"\"\" Run all schema and Excel file parsing test functions for diagnostic purposes. \"\"\" test_load_xl_schemas () test_load_schema_file_map () test_parse_xl_file ()","title":"main"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.parse_xl_file","text":"Parse a spreadsheet and return a dictionary representation using the given schema. Parameters: xl_path ( str | Path ) \u2013 Path to the Excel spreadsheet. schema_map ( dict [ str , dict ] | None , default: None ) \u2013 Map of schema names to their definitions. Returns: dict ( dict ) \u2013 Dictionary with extracted metadata, schemas, and tab contents. Notes: - tutorial on openpyxl: https://openpyxl.readthedocs.io/en/stable/tutorial.html Source code in arb\\utils\\excel\\xl_parse.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def parse_xl_file ( xl_path , schema_map = None ) -> dict : \"\"\" Parse a spreadsheet and return a dictionary representation using the given schema. Args: xl_path (str | Path): Path to the Excel spreadsheet. schema_map (dict[str, dict] | None): Map of schema names to their definitions. Returns: dict: Dictionary with extracted metadata, schemas, and tab contents. Notes: - tutorial on openpyxl: https://openpyxl.readthedocs.io/en/stable/tutorial.html \"\"\" logger . debug ( f \"parse_xl_with_schema_dict() called with { xl_path =} , { schema_map =} \" ) if schema_map is None : schema_map = xl_schema_map # Dictionary data structure to store Excel contents result = {} result [ 'metadata' ] = {} result [ 'schemas' ] = {} result [ 'tab_contents' ] = {} # Notes on data_only argument. By default, .value returns the 'formula' in the cell. # If data_only=True, then .value returns the last 'value' that was evaluated at the cell. wb = openpyxl . load_workbook ( xl_path , keep_vba = False , data_only = True ) # Extract metadata and schema information from hidden tabs if EXCEL_METADATA_TAB_NAME in wb . sheetnames : logger . debug ( f \"metadata tab detected in Excel file\" ) result [ 'metadata' ] = get_spreadsheet_key_value_pairs ( wb , EXCEL_METADATA_TAB_NAME , EXCEL_TOP_LEFT_KEY_VALUE_CELL ) if EXCEL_SCHEMA_TAB_NAME in wb . sheetnames : logger . debug ( f \"Schema tab detected in Excel file\" ) result [ 'schemas' ] = get_spreadsheet_key_value_pairs ( wb , EXCEL_SCHEMA_TAB_NAME , EXCEL_TOP_LEFT_KEY_VALUE_CELL ) else : ValueError ( f 'Spreadsheet must have a { EXCEL_SCHEMA_TAB_NAME } tab' ) # extract data tabs content using specified schemas new_result = extract_tabs ( wb , schema_map , result ) return new_result","title":"parse_xl_file"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.set_globals","text":"Set module-level global variables for schema file map and loaded schema map. Parameters: xl_schema_file_map_ ( dict [ str , Path ] | None , default: None ) \u2013 Optional override for schema file map. If not provided, uses a default list of pre-defined schema files. Notes Calls load_schema_file_map() to populate xl_schema_map from JSON files. Source code in arb\\utils\\excel\\xl_parse.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def set_globals ( xl_schema_file_map_ : dict [ str , Path ] | None = None ) -> None : \"\"\" Set module-level global variables for schema file map and loaded schema map. Args: xl_schema_file_map_ (dict[str, Path] | None): Optional override for schema file map. If not provided, uses a default list of pre-defined schema files. Notes: - Calls `load_schema_file_map()` to populate xl_schema_map from JSON files. \"\"\" global xl_schema_file_map , xl_schema_map # todo - update default roots with module paths, may make sense to remove globals and have # a different logic since this is outdated given the project root approach logger . debug ( f \"set_globals() called with { xl_schema_file_map_ =} \" ) # todo - not sure if these should be hard coded here ... if xl_schema_file_map_ is None : xl_schema_file_map = { \"landfill_v01_00\" : PROCESSED_VERSIONS / \"xl_schemas\" / \"landfill_v01_00.json\" , \"oil_and_gas_v01_00\" : PROCESSED_VERSIONS / \"xl_schemas\" / \"oil_and_gas_v01_00.json\" , \"energy_v00_01\" : PROCESSED_VERSIONS / \"xl_schemas\" / \"energy_v00_01.json\" , } else : xl_schema_file_map = xl_schema_file_map_ # load all the schemas if xl_schema_file_map : xl_schema_map = load_schema_file_map ( xl_schema_file_map ) logger . debug ( f \"globals are now: { xl_schema_file_map =} , { xl_schema_map =} \" )","title":"set_globals"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.split_compound_keys","text":"Decompose compound keys into atomic fields. Remove key/value pairs of entries that potentially contain compound keys and replace them with key value pairs that are more atomic. Parameters: dict_ ( dict ) \u2013 Dictionary with potentially compound fields (e.g., lat_and_long). Raises: ValueError \u2013 If 'lat_and_long' is improperly formatted. Source code in arb\\utils\\excel\\xl_parse.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 def split_compound_keys ( dict_ : dict ) -> None : \"\"\" Decompose compound keys into atomic fields. Remove key/value pairs of entries that potentially contain compound keys and replace them with key value pairs that are more atomic. Args: dict_ (dict): Dictionary with potentially compound fields (e.g., lat_and_long). Raises: ValueError: If 'lat_and_long' is improperly formatted. \"\"\" for html_field_name in list ( dict_ . keys ()): value = dict_ [ html_field_name ] if html_field_name == 'lat_and_long' : if value : lat_longs = value . split ( ',' ) if len ( lat_longs ) == 2 : dict_ [ 'lat_arb' ] = lat_longs [ 0 ] dict_ [ 'long_arb' ] = lat_longs [ 1 ] else : raise ValueError ( f \"Lat long must be a blank or a comma separated list of lat/long pairs\" ) del dict_ [ html_field_name ]","title":"split_compound_keys"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.test_load_schema_file_map","text":"Debug test for loading schema file map and displaying contents. Source code in arb\\utils\\excel\\xl_parse.py 395 396 397 398 399 400 401 402 def test_load_schema_file_map () -> None : \"\"\" Debug test for loading schema file map and displaying contents. \"\"\" logger . debug ( f \"test_load_schema_file_map() called\" ) schema_map = create_schema_file_map () schemas = load_schema_file_map ( schema_map ) logger . debug ( f \" { schemas =} \" )","title":"test_load_schema_file_map"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.test_load_xl_schemas","text":"Debug test for loading default schemas from xl_schema_file_map. Source code in arb\\utils\\excel\\xl_parse.py 405 406 407 408 409 410 411 412 def test_load_xl_schemas () -> None : \"\"\" Debug test for loading default schemas from xl_schema_file_map. \"\"\" logger . debug ( f \"Testing load_xl_schemas() with test_load_xl_schemas\" ) schemas = load_schema_file_map ( xl_schema_file_map ) logger . debug ( f \"Testing load_xl_schemas() with test_load_xl_schemas\" ) logging . debug ( f \"schemas = { pp_log ( schemas ) } \" )","title":"test_load_xl_schemas"},{"location":"reference/arb/utils/excel/xl_parse/#arb.utils.excel.xl_parse.test_parse_xl_file","text":"Debug test to parse a known Excel file into dictionary form using schemas. Source code in arb\\utils\\excel\\xl_parse.py 416 417 418 419 420 421 422 423 424 def test_parse_xl_file () -> None : \"\"\" Debug test to parse a known Excel file into dictionary form using schemas. \"\"\" logger . debug ( f \"test_parse_xl_file() called\" ) xl_path = PROCESSED_VERSIONS / \"xl_workbooks\" / \"landfill_operator_feedback_v070_populated_01.xlsx\" print ( f \" { xl_path =} \" ) result = parse_xl_file ( xl_path , xl_schema_map ) logger . debug ( f \" { result =} \" )","title":"test_parse_xl_file"}]}